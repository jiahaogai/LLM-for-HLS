{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. **Treat the following function as a hardware kernel for acceleration**\n   - Description: Specify the function as a hardware kernel for acceleration\n   - HLS Code: `#pragma ACCEL kernel`\n\n2. **Don't automatically pipeline this loop**\n   - Description: Disable automatic loop pipelining\n   - HLS Code: `#pragma ACCEL PIPELINE \"off\"`\n\n3. **Process this loop in its original, full form without breaking it into smaller chunks**\n   - Description: Set tile factor to 1 to process loop in its original form\n   - HLS Code: `#pragma ACCEL TILE FACTOR=1`\n\n4. **Run the iterations of this loop one after the other, not in parallel**\n   - Description: Set parallel factor to 1 to run loop iterations sequentially\n   - HLS Code: `#pragma ACCEL PARALLEL FACTOR=1`\n\nThese natural language instructions can be used to generate the HLS codes for the given C function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes:\n\n- Apply automatic pipeline optimization to the loop.\n- Set the tile factor to 'auto' so that the loop is processed in its original, full form without breaking it into smaller chunks.\n- Set the parallel factor to 'auto{2}' for the outer loop and 'auto{1}' for the first inner loop, and 'auto{8}' for the second inner loop.\n- Treat the function 'kernel_jacobi_1d' as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, apply the following natural language instructions:\n\n1. Use automatic pipelining for the loop.\n2. Apply tiling with an automatic factor of 1 to the loop.\n3. Parallelize the loop with an automatic factor of 2.\n4. Apply parallelization with an automatic factor of 2 to the inner loop.\n5. Further parallelize the loop with an automatic factor of 10.\n\nThese instructions correspond to the given HLS pragmas in the C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, utilize the following concise natural language instructions for specific #pragma values:\n\n- To turn off automatic loop pipelining: \"Disable automatic loop pipelining\"\n- To set the tile factor to 1 for original loop processing: \"Use a tile factor of 1 for loop processing\"\n- To run loop iterations in parallel with a factor of 2: \"Execute loop iterations in parallel with a factor of 2\"\n- To designate the function as a hardware acceleration kernel: \"Designate the following function as a hardware kernel for acceleration\"\n\nOnce implemented with these instructions in mind, the HLS codes when written in C, will execute the desired functionalities.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code with the following natural language instruction:\n\"Mark this function as a hardware kernel for acceleration, pipeline the outer loop automatically, tile the loop with a factor of 1 automatically, and parallelize the loops with a factor of 2 and 6 respectively.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n   Use: 'Treat the following function as a hardware kernel for acceleration'\n\n2. Don't automatically pipeline the outer loop.\n   Use: '#pragma ACCEL PIPELINE \"off\"'\n\n3. Ensure the loop is processed in its original, full form without breaking it into smaller chunks.\n   Use: '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of the outer loop one after the other, not in parallel.\n   Use: '#pragma ACCEL PARALLEL FACTOR=1'\n\n5. Optionally, specify the auto factor value for the pipeline, tile, and parallel pragma directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the kernel loop in its original, full form without breaking it into smaller chunks. Then, don't automatically pipeline this loop. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop and flatten it.\n3. Process this loop in its original, full form without breaking it into smaller chunks automatically.\n4. Run the iterations of this loop one after the other, not in parallel with a factor of 2.\n5. Run the iterations of this loop in parallel with a factor of 16.\n6. Run the iterations of this loop one after the other, not in parallel with a factor of 1.\n\nThe corresponding #pragma values are:\n1. #pragma ACCEL kernel\n2. #pragma ACCEL PIPELINE auto{flatten}\n3. #pragma ACCEL TILE FACTOR=auto{1}\n4. #pragma ACCEL PARALLEL FACTOR=auto{2}\n5. #pragma ACCEL PARALLEL FACTOR=auto{16}\n6. #pragma ACCEL PARALLEL FACTOR=auto{1}", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions for the specific #pragma values:\n\n1. For the function `kernel_jacobi_1d`:\n    - Treat the following function as a hardware kernel for acceleration.\n\n2. For the first loop:\n    - Don't automatically pipeline this loop.\n\n3. For tiling:\n    - Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. For parallelization with a factor of 2:\n    - Run the iterations of this loop in parallel with a parallel factor of 2.\n\n5. For the inner loop (first usage):\n    - Run the iterations of this loop one after the other, not in parallel.\n\n6. For the inner loop (second usage):\n    - Run the iterations of this loop in parallel with a parallel factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following concise natural language instruction for the process and pragma values:\n\n\"Designate the following function as a hardware kernel for acceleration\" - this describes the pragma value '#pragma ACCEL kernel'\n\n\"Automatically pipeline this loop without flattening\" - this maps to the pragma value '#pragma ACCEL PIPELINE auto{flatten}'\n\n\"Set the tile factor for loop tiling to be automatically determined as 1\" - this corresponds to the pragma value '#pragma ACCEL TILE FACTOR=auto{1}'\n\n\"Run the iterations of the following loops in parallel with a factor to be automatically determined as 2\" - for both loops, the pragma value is '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n\nFor the provided code snippet, using these natural language instructions will generate the corresponding HLS codes in C with the specified pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply pipeline optimization to the outer loop, disable automatic pipelining for the middle loop, and flatten the inner loop for parallelization using suitable HLS pragma directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. Treat the loop iterations as if they are a pipeline, automatically determining the optimal settings. Run two iterations of this loop in parallel. Within this loop, process 24 iterations in parallel, and then process the remaining iterations two at a time in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{24}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel for acceleration. Use parallel and pipeline optimizations for the nested loops. Ensure the outer loop runs in full form without breaking into smaller chunks, and the inner loops process iterations one after another, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction for the specific #pragma values:\n\n1. To automatically pipeline the loop:\n   Use: \"Automatically pipeline this loop\"\n   Code: `#pragma ACCEL PIPELINE auto{}`\n\n2. To tile the loop with a factor determined automatically:\n   Use: \"Automatically determine the tiling factor for this loop\"\n   Code: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. To run iterations of the loop in parallel with an automatically determined factor:\n   Use: \"Automatically determine the parallelization factor for this loop\"\n   Code: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n4. To set a specific parallelization factor for the inner loop:\n   Use: \"Set the parallelization factor for the inner loop to 15\"\n   Code: `#pragma ACCEL PARALLEL FACTOR=auto{15}`\n\n5. To set a specific parallelization factor for another inner loop:\n   Use: \"Set the parallelization factor for the inner loop to 3\"\n   Code: `#pragma ACCEL PARALLEL FACTOR=auto{3}`\n\nBy following these natural language instructions, the given HLS codes can be generated in C with the appropriate #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the iterations of the following loop in parallel, utilizing automatic factor calculation.\nDon't automatically pipeline this loop.\nProcess this loop in its original, full form without breaking it into smaller chunks.\nRun the iterations of this loop one after the other, not in parallel. Also, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C:\n```c\n#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps, int n, double A[120], double B[120])\n{\n  int t;\n  int i;\n#pragma ACCEL PIPELINE auto{flatten}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n#pragma ACCEL PARALLEL FACTOR=auto{15}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n}\n```\n\nYou can use the following concise natural language instructions for each #pragma directive:\n\n1. \"Treat the following function as a hardware kernel for acceleration\"\n   - `#pragma ACCEL kernel`\n\n2. \"Don't automatically pipeline this loop, maintain original structure\"\n   - `#pragma ACCEL PIPELINE auto{flatten}` \n\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   - `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. \"Run the iterations of this loop in parallel with the specified factor\"\n   - `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the outer loop\n   - `#pragma ACCEL PARALLEL FACTOR=auto{15}` for the first inner loop\n   - `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the second inner loop", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nEnsure the loop iterations run in parallel, each with an auto-determined factor.\nDisable automatic loop pipelining.\nSet the loop tiling factor to be determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration. Sequentially process the iterations of the outer loop, and parallelize the inner loop with a factor of 15. Automatically pipeline the loops and use a tiling factor of 1. Additionally, parallelize the second inner loop with a factor of 1.\"\n\nThis instruction encompasses the use of the following #pragma values:\n- '#pragma ACCEL kernel' to designate the function as a hardware kernel\n- '#pragma ACCEL PIPELINE auto{flatten}' to automatically pipeline the loops\n- '#pragma ACCEL TILE FACTOR=auto{1}' to apply a tiling factor of 1\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}' to parallelize the outer loop\n- '#pragma ACCEL PARALLEL FACTOR=auto{15}' to parallelize the first inner loop with a factor of 15\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' to parallelize the second inner loop with a factor of 1", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with HLS pragmas, you should follow the natural language instructions below:\n\n- Apply automatic pipelining to the loop: '#pragma ACCEL PIPELINE auto{}'\n- Tile the loop with a factor determined automatically: '#pragma ACCEL TILE FACTOR=auto{1}'\n- Parallelize the loop with a factor determined automatically: '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n- Parallelize this nested loop with a factor determined automatically: '#pragma ACCEL PARALLEL FACTOR=auto{3}'\n- Parallelize this nested loop with a factor determined automatically: '#pragma ACCEL PARALLEL FACTOR=auto{10}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the iterations of the first loop in parallel with a factor determined automatically, then disable automatic pipelining for this loop, and finally, process the second nested loop in parallel with a factor determined automatically and with a reduction performed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\n#pragma ACCEL PIPELINE auto{off}\n\nDivide the loop into smaller chunks with a factor of 1 along the specified dimension\n#pragma ACCEL TILE FACTOR=auto{1}\n\nRun the iterations of this loop in parallel with a factor of 2\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n\nTreat the following function as a hardware kernel for acceleration\n#pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following nested loops with tiling, automatically determine the tile factor for the outermost loop and use a tile factor of 8 for the middle loop. Pipeline the loops automatically, except for the innermost loop, which should be pipelined with a flatten option.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the natural language instructions provided below:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   Use: 'Treat the following function as a hardware kernel for acceleration'.\n\n2. Don't automatically pipeline the loop.\n   Use: '#pragma ACCEL PIPELINE \"off\"'\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   Use: '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   Use: '#pragma ACCEL PARALLEL FACTOR=1'\n\n5. Auto-pipeline the loop with default settings.\n   Use: '#pragma ACCEL PIPELINE auto{}'\n\n6. Set the tiling factor of the loop to be determined automatically.\n   Use: '#pragma ACCEL TILE FACTOR=auto{1}'\n\n7. Set the parallelization factor of the loop to be determined automatically, with a default factor of 2.\n   Use: '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n\n8. Set the parallelization factor of the loop to be determined automatically, with a specific factor of 16.\n   Use: '#pragma ACCEL PARALLEL FACTOR=auto{16}'\n\n9. Set the parallelization factor of the loop to be determined automatically, with a specific factor of 3.\n   Use: '#pragma ACCEL PARALLEL FACTOR=auto{3}'\n\nBy following these natural language instructions, you can generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Annotate the function as a hardware kernel for acceleration, and for the loop iterations:\n- Enable automatic loop pipelining\n- Use an automatic tile factor of 1\n- Parallelize the outer loop with a factor of 2, and the inner loops with factors of 3 and 1 respectively.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for hardware acceleration, you should use the following HLS pragmas:\n\n1. Use `#pragma ACCEL PIPELINE off` to prevent automatic pipelining of the loop.\n\n2. Use `#pragma ACCEL TILE FACTOR=1` to process the loop in its original, full form without breaking it into smaller chunks.\n\n3. Use `#pragma ACCEL PARALLEL FACTOR=2` to execute iterations of the loop in parallel with a factor of 2.\n\n4. Finally, declare the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{24}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Activate kernel acceleration for the function 'kernel_jacobi_1d'.\n2. Deactivate automatic loop pipelining for all loops in the function.\n3. Set the loop tiling factor to 1 for maximum performance.\n4. Set the loop parallelization factor for the outer loop to 2, and for the inner loops to 3 and 12 respectively.\n\nThis set of natural language instructions describes how to use specific #pragma values for HLS code generation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for the given C code: \n\nUse the following pragma directives:\n1. Mark the function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n2. Turn off automatic pipelining for the outer two loops: `#pragma ACCEL PIPELINE auto{off}`\n3. Process the outer loops in their original, full form without breaking them into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Apply automatic pipelining for the innermost loop: `#pragma ACCEL PIPELINE auto{}`\n5. Divide the innermost loop into chunks of 4 for tiling: `#pragma ACCEL TILE FACTOR=auto{4}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Process the loops in their original, full form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions and corresponding #pragma values:\n\n1. **Process this loop without automatic pipelining**: \n   - Describe: Ensure manual control over loop pipelining\n   - Pragma: `#pragma ACCEL PIPELINE \"off\"`\n\n2. **Keep the loop in its original, full form without breaking it into smaller chunks**:\n   - Describe: Set loop tiling factor to 1\n   - Pragma: `#pragma ACCEL TILE FACTOR=1`\n\n3. **Run the iterations of this loop one after the other, not in parallel**:\n   - Describe: Set loop parallelization factor to 1\n   - Pragma: `#pragma ACCEL PARALLEL FACTOR=1`\n\n4. **Treat the following function as a hardware kernel for acceleration**:\n   - Describe: Mark the function as a hardware accelerator\n   - Pragma: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the outer loop, automatically pipeline the loop without flattening it: \n   ```\n   #pragma ACCEL PIPELINE auto{flatten}\n   ```\n\n2. Tile the loop with a factor automatically determined to be 1: \n   ```\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n3. Parallelize the outer loop with a factor automatically determined to be 2: \n   ```\n   #pragma ACCEL PARALLEL FACTOR=auto{2}\n   ```\n\n4. Parallelize the inner loop with a factor automatically determined to be 20: \n   ```\n   #pragma ACCEL PARALLEL FACTOR=auto{20}\n   ```\n\n5. Parallelize another inner loop with a factor automatically determined to be 1: \n   ```\n   #pragma ACCEL PARALLEL FACTOR=auto{1}\n   ```\n\nAdditionally, treat the `kernel_jacobi_1d` function as a hardware kernel for acceleration:\n```\n#pragma ACCEL kernel\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following high-level synthesis (HLS) pragmas:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Natural language instruction: \"Designate this function as a hardware kernel\"\n   - HLS pragma: #pragma ACCEL kernel\n\n2. Don't automatically pipeline this loop\n   - Natural language instruction: \"Do not automatically pipeline this loop\"\n   - HLS pragma: #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Natural language instruction: \"Process this loop with a tile factor of 1\"\n   - HLS pragma: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Natural language instruction: \"Execute this loop sequentially, without parallelization\"\n   - HLS pragma: #pragma ACCEL PARALLEL FACTOR=1\n\nFor the placeholders \"auto\", \"__PARA__L0\", \"__PARA__L4\", \"__PARA__L1\", \"__PARA__L5\", \"__PARA__L2\", \"__PIPE__L3\", \"__TILE__L3\", \"__PIPE__L7\", and \"__PARA__L7_0\", the specific values for these placeholders should be determined based on the specific design and optimization goals. These values need to be chosen based on the particular characteristics and requirements of the target hardware architecture.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations in parallel with an automatically determined factor, treat the following function as a hardware kernel for acceleration. For the loop that initializes array 's' with 0 values, do not automatically pipeline the loop. For the nested loop that performs matrix multiplication and reductions, process it in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, and automatically pipeline the loop. Then, process the loop in its original, full form without breaking it into smaller chunks, and run the iterations of this loop in parallel with a factor of 2. Within the loop, also run certain iterations in further parallel with a factor of 3 and 10.\"\n\nThis instruction covers the use of the following #pragma values:\n1. '#pragma ACCEL kernel' to designate the function as a hardware kernel for acceleration\n2. '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loop\n3. '#pragma ACCEL TILE FACTOR=auto{1}' to process the loop in its original, full form without breaking it into smaller chunks\n4. '#pragma ACCEL PARALLEL FACTOR=auto{2}' to run the iterations of the loop in parallel with a factor of 2\n5. '#pragma ACCEL PARALLEL FACTOR=auto{3}' and '#pragma ACCEL PARALLEL FACTOR=auto{10}' to run specific iterations within the loop in further parallel with factors of 3 and 10, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire loop in its original, full form without breaking it into smaller chunks, run the iterations of this loop in parallel with automatic factor determination, and avoid automatically pipelining this loop. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you would use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration and then specify specific optimizations for loops, such as: run the iterations of the loop one after the other, not in parallel; don't automatically pipeline this loop; and process this loop in its original, full form without breaking it into smaller chunks. Also, parallelize the following loop with a factor of 'auto'.\"\n\nThis comprehensive instruction covers the pragma values used in the provided HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instruction:\n\n- \"Designate the following function as a hardware kernel for acceleration\"\n- \"For the outer loop, automatically pipeline the processing\"\n- \"Break the outer loop into smaller chunks in a way that best suits the target platform\"\n- \"For the middle loop, turn off automatic pipelining\"\n- \"Process the middle loop in its original, full form without breaking it into smaller chunks\"\n- \"For the innermost loop, flatten the loop for parallel processing\"\n- \"Process the innermost loop in its original, full form without breaking it into smaller chunks\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the calculations in the kernel function in parallel and treat the function as a hardware kernel for acceleration. The first loop should be parallelized with a parallel factor calculated automatically and the loop body processed without automatic pipelining. Use a tiling factor calculated automatically for the second loop and parallelize it with a factor calculated automatically, then add a reduction clause to the inner loop with a parallel factor calculated automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks and don't automatically pipeline this loop, run the iterations of this loop one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following comprehensive but very concise natural language instructions for the specific #pragma values:\n\n- Treat the following function as a hardware kernel for acceleration\n  - describe: `#pragma ACCEL kernel`\n\n- Don't automatically pipeline this loop\n  - describe: `#pragma ACCEL PIPELINE \"off\"`\n\n- Process this loop in its original, full form without breaking it into smaller chunks\n  - describe: `#pragma ACCEL TILE FACTOR=1`\n\n- Run the iterations of this loop one after the other, not in parallel\n  - describe: `#pragma ACCEL PARALLEL FACTOR=1`\n\nThese instructions correspond to the HLS codes and their respective #pragma values in the given C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, you should:\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops.\n3. Process the loops with their original, full form without breaking them into smaller chunks.\n4. Run the iterations of the loops one after the other, not in parallel.\n\nThe natural language instructions for the pragma values are as follows:\n- '#pragma ACCEL kernel': Treat the following function as a hardware kernel for acceleration.\n- '#pragma ACCEL PIPELINE auto{off}': Don't automatically pipeline the loops.\n- '#pragma ACCEL TILE FACTOR=auto{1}': Process the loops with their original, full form without breaking them into smaller chunks.\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}': Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel with a factor of 1, then pipeline the loop, and finally, process the second loop in parallel with a factor of 1 and utilize reduction for the inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, apply the following comprehensive natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the outermost loop.\n\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n\n4. Automatically pipeline the middle loop.\n\n5. Process the middle loop in its original, full form without breaking it into smaller chunks.\n\n6. Automatically pipeline the innermost loop.\n\n7. Process the innermost loop in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Run the iterations of the first loop one after the other, not in parallel. Don't automatically pipeline the second loop. Process the second loop in its original, full form without breaking it into smaller chunks. Run the iterations of the third loop and the inner loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the nested loops in their original form, without breaking them into smaller chunks, and run the iterations of the loops one after the other, not in parallel. Additionally, treat the 'kernel_doitgen' function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nApply these instructions to the respective pragma values in the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks, and do not automatically pipeline it. Also, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop with the original code as a hardware kernel for acceleration and automatically parallelize and pipeline the loop while leveraging reduction for parallelization. Use the pragma '#pragma ACCEL kernel' to mark the function as a hardware kernel, '#pragma ACCEL PARALLEL FACTOR=auto{1}' to automatically parallelize the loop, '#pragma ACCEL PIPELINE auto{flatten}' to automatically pipeline the loop, and '#pragma ACCEL PARALLEL reduction FACTOR=auto{1}' to automatically leverage reduction for parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given C code, the comprehensive but very concise natural language instruction would be:\n\n\"Process this function as a hardware kernel for acceleration, turn off automatic pipelining on the loop, set the tile factor to 1, and run the loops in parallel with a factor of 2 and 3 respectively.\"\n\nThis would correspond to the following #pragma directives:\n```c\n#pragma ACCEL kernel\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n#pragma ACCEL PARALLEL FACTOR=auto{3}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loops inside this function, do not automatically pipeline them, and process them in their original, full form without breaking them into smaller chunks. However, run the iterations of each loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but very concise natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE \"off\"`\n- Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n- Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the \"doitgen\" function as a hardware kernel for acceleration. Pipeline the outer loop, disable automatic pipelining for the middle loop, and flatten the inner loop. Use tiling with a factor of 8 for the outer loop, and a factor of 1 for the middle and inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in parallel with automatic factor determination to optimize performance\nUse pipeline optimization by automatic flattening to improve throughput\nApply tiling optimization with automatic factor determination\nUtilize parallel reduction for inner loop computation with automatic factor determination", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loop within, pipeline it automatically. Then, tile the loop with a factor of 25. Parallelize the loop with a factor of \"auto{__PARA__L0}\", and within that parallel loop, pipeline it off. Then, tile the next loop with a factor of 1, parallelize it with a factor of \"auto{__PARA__L1}\", and within that parallel loop, pipeline it automatically. Then, tile the next loop with a factor of 1, parallelize it with a factor of \"auto{__PARA__L2}\", and within that parallel loop, pipeline it with \"__PIPE__L3\". Finally, parallelize the remaining loop with a factor of 1, and within that loop, pipeline it with \"__PIPE__L7\".", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel, automatically determine the parallel factor for optimization. Pipeline the loop automatically using the flatten design. For the second loop, use automatic tiling with a factor of 1. Also, process the second loop in parallel, automatically determining the parallel factor. Apply reduction for the inner parallel loop automatically to optimize. Finally, treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following directives to the loops:\n1. Disable automatic pipelining for the outermost loop.\n2. Use a tile factor of 1 for both the outermost and the middle loops.\n3. Disable automatic pipelining for the innermost loop, and use a tile factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, you should:\n- Use the pragma #pragma ACCEL kernel to tag the function as a hardware kernel for acceleration.\n- Use #pragma ACCEL PIPELINE auto{} to automatically pipeline the loops.\n- Use #pragma ACCEL TILE FACTOR=8 to break the loop into smaller chunks of size 8.\n- Use #pragma ACCEL PARALLEL FACTOR=__PARA__L0 to run the iterations of the loop in parallel with the specified factor.\n- Use #pragma ACCEL PIPELINE auto{off} to turn off automatic pipelining.\n- Use #pragma ACCEL TILE FACTOR=1 to process the loop in its original, full form without breaking it into smaller chunks.\n- Use #pragma ACCEL PARALLEL FACTOR=__PARA__L1 to run the iterations of the loop in parallel with the specified factor.\n- Use #pragma ACCEL PIPELINE auto{off} to explicitly turn off automatic pipelining.\n- Use #pragma ACCEL TILE FACTOR=1 to process the loop in its original, full form without breaking it into smaller chunks.\n- Use #pragma ACCEL PARALLEL FACTOR=__PARA__L2 to run the iterations of the loop in parallel with the specified factor.\n- Use #pragma ACCEL PIPELINE auto{__PIPE__L3} to pipeline the loop with a specific pipeline identifier.\n- Use #pragma ACCEL TILE FACTOR=__TILE__L3 to break the loop into smaller chunks and specify a tile identifier.\n- Use #pragma ACCEL PARALLEL FACTOR=1 to run the iterations of the loop in parallel with a factor of 1.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following concise natural language instructions for each #pragma directive:\n\n1. For the directive '#pragma ACCEL PIPELINE auto{off}', use \"Don't automatically pipeline this loop.\"\n2. For the directive '#pragma ACCEL TILE FACTOR=auto{1}', use \"Process this loop in its original, full form without breaking it into smaller chunks.\"\n3. For the directive '#pragma ACCEL PARALLEL FACTOR=auto{2}', use \"Run the iterations of this loop one after the other, not in parallel.\"\n4. For the directive '#pragma ACCEL PARALLEL FACTOR=auto{3}', use \"Run iterations of this loop in parallel with a factor of 3.\"\n5. For the directive '#pragma ACCEL PARALLEL FACTOR=auto{12}', use \"Run iterations of this loop in parallel with a factor of 12.\"\n\nBy following these natural language instructions, you can organize the HLS codes in C as specified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you can use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Then, for the loops inside the function: run the iterations of the first loop in parallel, pipeline the loop after that, and then break down the third loop into smaller chunks using tiling, and finally, run the innermost loop iterations in parallel with reduction.\"\n\nThis instruction gives a comprehensive but concise overview of the process and the specific #pragma values to use.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following operations on the given C code:\n\n- Treat the function as a hardware kernel for acceleration.\n- Run the iterations of the first loop in parallel.\n- Apply automatic pipeline flattening to the loop.\n- Keep the loop in its original form without breaking it into smaller chunks.\n- Run the iterations of the second loop one after the other, not in parallel, and use reduction for parallel operation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following natural language instruction:\n\"Declare the following function as a hardware kernel for acceleration. Then, don't automatically pipeline the loop, process the loop in its original, full form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel. Set the tile factor and parallel factor as auto for optimization.\"\n\nThese instructions correspond to the following #pragma values:\n- '#pragma ACCEL kernel'\n- '#pragma ACCEL PIPELINE auto{}'\n- '#pragma ACCEL TILE FACTOR=auto{1}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{10}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{6}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the respective #pragma values:\n\n1. For the function 'kernel_jacobi_1d', mark it as a hardware kernel for acceleration.  \n   Instruction: \"Treat the following function as a hardware kernel for acceleration\"\n\n2. For the loop containing the 't' variable, do not automatically pipeline it.  \n   Instruction: \"Don't automatically pipeline this loop\"\n\n3. For the loop containing the 't' variable, process it in its original, full form without breaking it into smaller chunks.  \n   Instruction: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n4. For the loop containing the 't' variable, run the iterations one after the other, not in parallel.  \n   Instruction: \"Run the iterations of this loop one after the other, not in parallel\"\n\n5. For the inner loop containing the 'i' variable, run the iterations in parallel with a factor of 2.  \n   Instruction: \"Run the iterations of this loop in parallel with a factor of 2\"\n\n6. For the inner loop containing the 'i' variable, run the iterations in parallel with a factor of 3.  \n   Instruction: \"Run the iterations of this loop in parallel with a factor of 3\"\n\n7. For the inner loop containing the 'i' variable, run the iterations in parallel with a factor of 10.  \n   Instruction: \"Run the iterations of this loop in parallel with a factor of 10\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, automatically parallelize the outer loop with factor 1, and automatically pipeline the inner loop with the flatten optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but concise natural language instruction:\n\n- Mark the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline the loops.\n- Process the loops with their original, full form without breaking them into smaller chunks.\n- Run the iterations of the loops one after the other, not in parallel.\n- Turn on automatic parallelization with a factor of 30.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire function as a hardware kernel and automatically pipeline the outer loop, tile the loop with a factor of 1 and parallelize the outer loop with a factor of 2 and the inner loops with factors of 30 and 10 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   - Use `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop:\n   - Use `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   - Use `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   - Use `#pragma ACCEL PARALLEL FACTOR=1`\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code with HLS, first, mark the function as a hardware kernel using '#pragma ACCEL kernel'. Then, disable automatic pipelining with '#pragma ACCEL PIPELINE auto{off}'. After that, set the tiling factor to 1 for the outermost loop with '#pragma ACCEL TILE FACTOR=auto{1}'. Inside the nested loops, enable automatic pipelining and set the tiling factor to 4 using '#pragma ACCEL PIPELINE auto{flatten}' and '#pragma ACCEL TILE FACTOR=auto{4}' respectively. Additionally, disable automatic pipelining and set the tiling factor to 1 for the innermost loop using '#pragma ACCEL PIPELINE auto{off}' and '#pragma ACCEL TILE FACTOR=auto{1}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel considering an optimal factor automatically. Disable automatic pipelining for this section of code. Divide the iterations of the next loop into smaller chunks for acceleration, using an optimal factor. Finally, execute the following loop in parallel, automatically determining the optimal factor for parallelization and including reduction operations. Remember that the entire function should be treated as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these comprehensive but concise natural language instructions:\n- Mark the function as a hardware kernel for acceleration\n- Do not automatically pipeline the loop\n- Process the loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel\n- Run the iterations of this loop with a parallel factor of 30\n- Run the iterations of this loop with a parallel factor of 10", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive natural language instruction:\n\n\"Transform the function into a hardware kernel for acceleration. Then, disable automatic pipelining for the loop, set the tiling factor to 1, and ensure parallelism by setting the parallel factor to 2. Within the loop, apply a parallel factor of 30 for the first inner loop and 10 for the second inner loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. Automatically pipeline and flatten the loop. Run the iterations of the loop in parallel with a reduction factor of 1_0 for the temporary array and a reduction factor of 1_1 for the final result array. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, then automatically pipeline the outer loop, apply a tile factor of 1 to break the loop into smaller chunks, and run the iterations of the inner loops in parallel with a factor of 2 for the outer loop and a factor of 30 for the first inner loop and a factor of 10 for the second inner loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, the following pragma values are used:\n\n1. Treat the following function as a hardware kernel for acceleration: '#pragma ACCEL kernel'\n2. Don't automatically pipeline this loop: '#pragma ACCEL PIPELINE \"off\"'\n3. Process this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=1'\n4. Run the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=1'\n\nThe comprehensive but very concise natural language instruction to generate these HLS codes would be:\n\"Mark the function as a hardware kernel, turn off auto-pipelining for the outer loop, set the tile factor to 1 for all loops, and run the iterations sequentially instead of in parallel.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel with automatic factor determination, turn off automatic loop pipelining, use automatic tiling with factor 1, and process the second loop in parallel with automatic factor determination and reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To compile this C code with the correct HLS pragmas, do the following:\n\n1. For the outer loop, use \"#pragma ACCEL PIPELINE auto\" to automatically pipeline the loop.\n\n2. For the outer loop, use \"#pragma ACCEL TILE FACTOR=auto{1}\" to break it into smaller chunks with automatic factor determination.\n\n3. For the first inner loop, use \"#pragma ACCEL PARALLEL FACTOR=auto{12}\" to run iterations in parallel with an auto-determined factor.\n\n4. For the second inner loop, use \"#pragma ACCEL PARALLEL FACTOR=auto{3}\" to run iterations in parallel with an auto-determined factor.\n\nFinally, annotate the 'kernel_jacobi_1d' function with the \"#pragma ACCEL kernel\" pragma to treat it as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for each specific #pragma value:\n\n- \"Automatically flatten and pipeline this loop\" - for the line `#pragma ACCEL PIPELINE auto{flatten}`.\n- \"Set the tile factor automatically to 1\" - for the line `#pragma ACCEL TILE FACTOR=auto{1}`.\n- \"Automatically parallelize this loop with a factor of 2\" - for the line `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n- \"Automatically parallelize this loop with a factor of 12\" - for the line `#pragma ACCEL PARALLEL FACTOR=auto{12}`.\n- \"Automatically parallelize this loop with a factor of 1\" - for the line `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions:\n\n- For the `kernel_jacobi_1d` function, insert the directive to treat the function as a hardware kernel for acceleration.\n- For the outer loop, automatically pipeline the entire loop in the code without flattening the multi-dimensional access pattern.\n- For the outer loop, use auto{1} factor to tile the loop into smaller chunks with size determined automatically.\n- For the outer loop, use auto{2} factor to parallelize the loop with an auto-determined parallel factor.\n- For the inner loop, use auto{30} factor to parallelize the loop with an auto-determined parallel factor.\n- For the second inner loop, use auto{1} factor to parallelize the loop with an auto-determined parallel factor.\n\nBy implementing these natural language instructions, you can generate the given HLS codes in C with the corresponding #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you need to treat the following function as a hardware kernel for acceleration and use the following #pragma values:\n\n1. Use `#pragma ACCEL PIPELINE auto{flatten}` to automatically pipeline the loop.\n2. Use `#pragma ACCEL TILE FACTOR=auto{1}` to process the loop in its original, full form without breaking it into smaller chunks.\n3. Use `#pragma ACCEL PARALLEL FACTOR=auto{2}` to run the iterations of the loop in parallel with a factor of 2.\n4. Use `#pragma ACCEL PARALLEL FACTOR=auto{12}` for the inner loop to run in parallel with a factor of 12.\n5. Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the subsequent loop to run in parallel with a factor of 1.\n\nGiven these instructions and #pragma values, the HLS codes should be generated in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outer loop in parallel, processing iterations in parallel as much as possible. Pipeline the inner loops to improve throughput. Use a tile factor of 1 to retain the original loop structure. Additionally, treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you need to treat the `kernel_doitgen` function as a hardware kernel for acceleration and apply the following specific #pragma values for optimization:\n\n1. Disable automatic pipelining for the outermost loop:\n   `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process the outermost loop in its original, full form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Enable automatic loop pipelining with flattening for the middle loop:\n   `#pragma ACCEL PIPELINE auto{flatten}`\n\n4. Process the middle loop in its original, full form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n5. Disable automatic pipelining for the innermost loop:\n   `#pragma ACCEL PIPELINE auto{off}`\n\n6. Process the innermost loop in its original, full form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\nWith the above #pragma values, the given C code can be optimized for HLS processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following steps to optimize the C code using HLS pragmas:\n- First, mark the function as a hardware kernel for acceleration using '#pragma ACCEL kernel'.\n- Then, parallelize the loop iterating over 's' array by replacing '#pragma ACCEL PARALLEL FACTOR=auto{1}' with '#pragma ACCEL PARALLEL FACTOR=1'.\n- Next, do not automatically pipeline the loop iterating over 's' array by replacing '#pragma ACCEL PIPELINE auto{off}' with '#pragma ACCEL PIPELINE off'.\n- Finally, keep the loop iterating over 'q' array in its original form without breaking it into smaller chunks and process it without parallelization by keeping '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}' unchanged.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and within the loops, turn off automatic pipelining and use a tile factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for HLS acceleration, use the following comprehensive but concise natural language instruction:\n\n\"Set the pipeline directive to manual, do not automatically pipeline the loops, and set the tile factor to 8 for the outermost loop; keep the loops in their original, full form without breaking them into smaller chunks, and process them one after the other without parallelization, treating the function as a hardware kernel for acceleration.\"\n\nThis instruction encompasses the necessary #pragma values and their specific settings for the given HLS code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop for the variable 'i' in parallel, run all the iterations of this loop one after the other, not in parallel, and treat this function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the first loop, use automatic tile factor of 2, and run iterations of the second loop in parallel with reduction operations for variables 'tmp' and 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n1. Mark the function as a hardware kernel for acceleration.\n2. Turn off automatic pipeline for the following loop.\n3. Keep the loop in its original form without breaking it into smaller chunks.\n4. Run iterations of the loop in parallel with a factor of 2.\n5. Run the inner loop iterations in parallel with a factor of 30.\n6. Run the inner loop iterations in parallel with a factor of 12.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop with automatic pipelining, use automatic tiling, and run the iterations of the loop in parallel with a reduction clause. Also, treat the following function as a hardware kernel for acceleration.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[410][390],double s[390],double q[410],double p[390],double r[410])\n{\n  int i;\n  int j;\n  for (i = 0; i < 390; i++) \n    s[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 410; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 390; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, and for the loops within it:\n1. Disable automatic pipelining.\n2. Process the loops in their original, full form without breaking them into smaller chunks.\n3. Perform pipelining with flattening for the innermost loop.\n4. Use a tile factor of 8 for the innermost loop.\"\n\nTranslate the above natural language instruction to the specific #pragma values as follows:\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    \n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop sequentially without pipeline optimization: `#pragma ACCEL PIPELINE auto{off}`\nProcess the loop using its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\nRun the iterations of this loop in parallel by breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{4}`\nTreat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C, use the following natural language instruction:\n\n\"Optimize hardware pipeline manually instead of automatic, do not tile the loop, and parallelize with a factor of 2 for the outer loop, 30 for the first inner loop, and 10 for the second inner loop. Treat the function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the `kernel_doitgen` function as a hardware kernel for acceleration, and for the loops within, use the following pragma values:\n- For the outer loop over `r`, do not automatically pipeline it (`#pragma ACCEL PIPELINE auto{off}`).\n- For the loop over `r`, break it into smaller chunks with a factor of 2 in tiling (`#pragma ACCEL TILE FACTOR=2`).\n- For the loop over `q`, automatically pipeline it in a flattened manner (`#pragma ACCEL PIPELINE auto{flatten}`), and do not break it into smaller chunks (`#pragma ACCEL TILE FACTOR=1`).\n- For the loop over `p`, do not automatically pipeline it (`#pragma ACCEL PIPELINE auto{off}`), and do not break it into smaller chunks (`#pragma ACCEL TILE FACTOR=1`).", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following natural language instructions:\n\n1. Enable automatic pipelining for the loop: `#pragma ACCEL PIPELINE auto`\n\n2. Set the tiling factor automatically to 1 for the loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Set the parallelization factor automatically to 2 for the outer loop: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n4. Set the parallelization factor automatically to 30 for the inner loop (first computation loop): `#pragma ACCEL PARALLEL FACTOR=auto{30}`\n\n5. Set the parallelization factor automatically to 10 for the inner loop (second computation loop): `#pragma ACCEL PARALLEL FACTOR=auto{10}`\n\nRemember, in each pragma, the 'auto' specifies that the value should be determined automatically by the hardware.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   ```\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline the outermost loop\n   ```\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n3. Process the outermost loop with a factor of 4\n   ```\n   #pragma ACCEL TILE FACTOR=auto{4}\n   ```\n\n4. Automatically pipeline the middle loop with flattening\n   ```\n   #pragma ACCEL PIPELINE auto{flatten}\n   ```\n\n5. Process the middle loop in its original, full form without breaking it into smaller chunks\n   ```\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n6. Don't automatically pipeline the innermost loop\n   ```\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n7. Process the innermost loop in its original, full form without breaking it into smaller chunks\n   ```\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, for the function `kernel_jacobi_1d`, use the following natural language instructions for the respective #pragma values:\n\n1. \"Automatically pipeline the loops for parallel execution\" - for `#pragma ACCEL PIPELINE auto{flatten}`\n2. \"Divide the loops into smaller units of iteration to be processed in parallel, with the specified automatic factor\" - for `#pragma ACCEL TILE FACTOR=auto{1}`\n3. \"Execute the iterations of the loops in parallel with the specified automatic factor\" - for `#pragma ACCEL PARALLEL FACTOR=auto{2}` (first loop) and `#pragma ACCEL PARALLEL FACTOR=auto{30}` (second loop)\n4. \"Treat the following function as a hardware kernel for acceleration\" - for `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline the first loop. Use a tile factor of 1 for loop tiling. Run the iterations of the second loop one after the other, not in parallel, with reduction of 'tmp' and 'y' using a factor of 1 for both reductions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instructions for the specific #pragma values:\n\n1. Mark the function as a hardware kernel for acceleration: \n   - \"Treat the following function as a hardware kernel for acceleration\"\n\n2. Pipeline the loop with auto scheduling:\n   - \"Automatically pipeline this loop\"\n\n3. Tile the loop with a factor of 4:\n   - \"Break this loop into smaller chunks with a factor of 4\"\n\n4. Run the iterations one after the other, not in parallel, with automatic scheduling:\n   - \"Automatically process this loop without running iterations in parallel\"\n\n5. Turn off automatic pipelining for the loop:\n   - \"Turn off automatic pipelining for this loop\"\n\n6. Tile the loop with a factor of 1:\n   - \"Break this loop into smaller chunks with a factor of 1\"\n\n7. Run the iterations one after the other, not in parallel, with automatic scheduling:\n   - \"Automatically process this loop without running iterations in parallel\"\n\n8. Pipeline the loop with auto scheduling:\n   - \"Automatically pipeline this loop\"\n\n9. Tile the loop with a factor of 1:\n   - \"Break this loop into smaller chunks with a factor of 1\"\n\n10. Run the iterations one after the other, not in parallel, with automatic scheduling:\n    - \"Automatically process this loop without running iterations in parallel\"\n\n11. Pipeline the loop with a specific identifier (__PIPE__L3) for scheduling:\n    - \"Pipeline this loop with a specific identifier\"\n\n12. Tile the loop with a specific identifier (__TILE__L3) for scheduling:\n    - \"Break this loop into smaller chunks with a specific identifier\"\n\n13. Run the iterations in parallel with a factor of 1:\n    - \"Run the iterations of this loop in parallel with a factor of 1\"\n\nBy implementing these instructions, the HLS codes in C can be generated as described.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but concise natural language instruction:\n\"Mark the following function as a hardware kernel for acceleration, then pipeline the loop automatically, tile the loop with a factor of 8, and run the iterations of the loop in parallel with a parallel factor of 1.0 for reduction variables 'tmp' and 1.1 for reduction variables 'y'.\"\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you need to treat the function as a hardware kernel for acceleration. Then, don't automatically pipeline the loop, process the loop in its original form without breaking it into smaller chunks, and run the iterations of the loop one after the other, not in parallel. Finally, reduce the loop variables 'tmp' and 'y' in separate parallel processes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\nRun the iterations of this loop one after the other, not in parallel\nTreat the following function as a hardware kernel for acceleration", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\"Designate the following function as a hardware kernel for acceleration. Pipeline the outer loop automatically with flattening. Break the loop into tiles with a factor chosen automatically. Run the iterations of the outer loop in parallel with a factor of 2, followed by running the inner loops in parallel with a factor of 16 and then a factor of 1.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction: \"Treat the following function as a hardware kernel for acceleration, process the loops in their original, full form without breaking them into smaller chunks, don't automatically pipeline the loops, and run the iterations of the loops one after the other, not in parallel.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function as a hardware kernel for acceleration.\nDisable automatic pipelining for all loops.\nDo not break the loops into smaller chunks.\nSet the tile factor to 1 for all loops.\nEnable automatic pipelining for the innermost loop with a tile factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following comprehensive but concise natural language instruction:\n\n\"Optimize loop iteration processing for acceleration by automatically pipelining it, breaking it into smaller chunks, and running parallel iterations with specific factors. Treat the function as a hardware kernel for acceleration.\"\n\nThis instruction encompasses the use of the following specific #pragma values:\n- '#pragma ACCEL PIPELINE auto{flatten}' for automatically pipelining the loop\n- '#pragma ACCEL TILE FACTOR=auto{1}' for automatically breaking the loop into smaller chunks\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}' to run parallel iterations with a specific factor\n- '#pragma ACCEL kernel' to treat the function as a hardware kernel for acceleration", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Invoke hardware acceleration for the following function\n   ```c\n   #pragma ACCEL kernel\n   void kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n   {\n     // code\n   }\n   ```\n\n2. Apply pipeline optimization to the loop\n   ```c\n   #pragma ACCEL PIPELINE auto{flatten}\n   ```\n\n3. Perform tiling with a factor of 1 for loop optimization\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n4. Enable loop parallelization with a factor of 2\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{2}\n   ```\n\n5. Parallelize the loop with a factor of 16\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{16}\n   ```\n\n6. Sequentially execute the loop iterations with a factor of 1 for optimization\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{1}\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C codes, follow these natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline the outer loop.\n- Process the loops in their original, full form without breaking them into smaller chunks.\n- Run the iterations of the inner loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration. Also, don't automatically pipeline the loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and for the loop nest within, don't automatically pipeline the outermost loop and process it in its original, full form without breaking it into smaller chunks. For the inner loops, automatically pipeline the loops with a factor of 8 for the second loop, and flatten the third loop automatically with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes, use the following natural language instructions:\n\n- For the outer loop, \"Don't automatically pipeline this loop\" and \"Process this loop in its original, full form without breaking it into smaller chunks\" should be used to turn off autopipelining and set the tile factor to 1.\n- For the second loop, \"Don't automatically pipeline this loop\" and \"Process this loop in its original, full form without breaking it into smaller chunks\" should be used to turn off autopipelining and set the tile factor to 1.\n- For the innermost loop, \"Automatically pipeline this loop\" and \"Set the tile factor to 8\" should be used to enable autopipelining and set the tile factor to 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks.  \nRun the iterations of this loop one after the other, not in parallel.  \nTreat the following function as a hardware kernel for acceleration.  \nAutomatically pipeline this loop, flatten the loop body.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code written in C, use the \"pragma\" directives in the following natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration:\n   - Use: `#pragma ACCEL kernel`\n\n2. Implement a loop that is pipelined automatically by the HLS tool:\n   - Use: `#pragma ACCEL PIPELINE auto{flatten}`\n\n3. Break the loop into smaller chunks for processing, with the chunk factor determined automatically:\n   - Use: `#pragma ACCEL TILE FACTOR=auto{4}`\n\n4. Run the iterations of this loop one after the other, not in parallel, with parallel factor determined automatically:\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n5. For the inner loop, perform parallel reduction on variable \"tmp\" with the factor determined automatically:\n   - Use: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}`\n\n6. For the inner loop, perform parallel reduction on variable \"y\" with the factor determined automatically:\n   - Use: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following pragma instructions:\n- Treat the following function as a hardware kernel for acceleration: '#pragma ACCEL kernel'\n- Don't automatically pipeline this loop: '#pragma ACCEL PIPELINE auto{off}'\n- Process this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{1}'\n- Run the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n- Automatically pipeline this loop and fully unroll the inner loop: '#pragma ACCEL PIPELINE auto{flatten}' and '#pragma ACCEL TILE FACTOR=auto{4}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the following C code:\n\n'Process the entire loop without automatic pipeline'\n\n'Process the loop in its original, full form without smaller tiling'\n\n'Pipeline the loop without automatic parallelization'\n\n'Treat the function as a hardware kernel for acceleration'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these concise natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n2. Turn off automatic loop pipelining for the following loop.\n3. Use the original, full form of the loop without breaking it into smaller chunks.\n4. Run the iterations of the loop with a parallel factor of 2.\n5. Run the iterations of the first inner loop with a parallel factor of 3.\n6. Run the iterations of the second inner loop with a parallel factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is treated as a hardware kernel for acceleration. Also, do not automatically pipeline the outermost loop, and process it in its original, full form without breaking it into smaller chunks. For the innermost loops, pipeline them to a factor of 4 and flatten the loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks.\nRun the iterations of this loop two at a time in parallel.\nAutomatically pipeline this loop and flatten its structure to optimize performance.\nTreat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes for the C program, use the following comprehensive but concise natural language instruction set:\n- Use hardware acceleration for the function: 'Treat the following function as a hardware kernel for acceleration'\n- Pipeline the outer loop: 'Automatically pipeline this loop'\n- Process the loop in its original, full form without breaking it into smaller chunks: 'Set the tile factor to 1'\n- Run the iterations of the outer loop in parallel with a factor of 2: 'Set the parallel factor to 2 for the outer loop'\n- Run the inner loops in parallel with a factor of 32 and 10 respectively: 'Set the parallel factor to 32 for the first inner loop and 10 for the second inner loop'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language compilation instructions:\n\n1. \"Process this function as a hardware kernel for acceleration\": \n```c\n#pragma ACCEL kernel\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n```\n\n2. \"Don't automatically pipeline this loop\":\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\":\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. \"Run the iterations of this loop one after the other, not in parallel\":\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes for the C function 'kernel_doitgen', follow these natural language instructions:\n\n1. Mark the entire function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outermost loop.\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n4. Don't automatically pipeline the middle loop.\n5. Process the middle loop in its original, full form without breaking it into smaller chunks.\n6. Automatically pipeline the innermost loop.\n7. Break the innermost loop into smaller chunks with a tile factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following pragma values:\n\n1. For the outermost loop, do not automatically pipeline:\n   - Use `#pragma ACCEL PIPELINE auto{off}`\n\n2. For the outermost loop, process it in its original, full form without breaking it into smaller chunks:\n   - Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. For the middle loop, do not automatically pipeline:\n   - Use `#pragma ACCEL PIPELINE auto{off}`\n\n4. For the middle loop, process it in its original, full form without breaking it into smaller chunks:\n   - Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n5. For the innermost loop, automatically pipeline:\n   - Use `#pragma ACCEL PIPELINE auto{}`\n\n6. For the innermost loop, process it with a factor of 4:\n   - Use `#pragma ACCEL TILE FACTOR=auto{4}`\n\nRemember to also use `#pragma ACCEL kernel` to treat the given function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C:\n\n\"Declare the following function as a hardware kernel for acceleration, then for the loop within the function:\n  - Implement automatic pipelining\n  - Use the original loop form without breaking it into smaller chunks, and\n  - Run the iterations of the first inner loop in parallel, and then the iterations of the second inner loop in parallel with a factor of 3, followed by a factor of 16.\"\n\nThe corresponding #pragma values to use are:\n- Hardware kernel declaration: #pragma ACCEL kernel\n- Pipelining: #pragma ACCEL PIPELINE auto{}\n- Loop full form: #pragma ACCEL TILE FACTOR=auto{1}\n- Inner loop parallelization: \n  - First inner loop: #pragma ACCEL PARALLEL FACTOR=auto{16}\n  - Second inner loop: #pragma ACCEL PARALLEL FACTOR=auto{3}", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specific HLS codes in C, you need to follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outermost loop.\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n4. Don't automatically pipeline the mid-level loop.\n5. Process the mid-level loop in its original, full form without breaking it into smaller chunks.\n6. Automatically pipeline the innermost loop with flattening optimization.\n7. Break the innermost loop into smaller chunks with a tile factor of 2 for optimization.\n\nBy following these instructions, you can generate the given HLS codes for the C program.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"For the kernel function 'kernel_jacobi_1d' with parameters 'tsteps', 'n', 'A[120]', and 'B[120]', first apply automatic loop pipelining. Then, tile the loops with an automatic factor of 1, and process the loops in parallel with an automatic factor of 2. Inside the outer loop, parallelize the loop iterations with a factor of 16. Followed by that, parallelize the loop iterations with a factor of 3. End by treating the function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the given C code using HLS pragma values, follow the following comprehensive but very concise natural language instruction for each pragma:\n\n1. For the loop pipeline, use `Don't automatically pipeline this loop` to describe `#pragma ACCEL PIPELINE auto{off}`.\n2. For the loop tiling, use `Process this loop with its original, full form without breaking it into smaller chunks` to describe `#pragma ACCEL TILE FACTOR=auto{1}`.\n3. For the loop parallelization, use `Run the iterations of this loop one after the other, not in parallel` to describe `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n4. Treat the following function as a hardware kernel for acceleration by using `Treat the following function as a hardware kernel for acceleration` to describe `#pragma ACCEL kernel`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes:\n1. Designate the following function as a hardware kernel for acceleration using the directive '#pragma ACCEL kernel'.\n2. Don't automatically pipeline the outer loop using '#pragma ACCEL PIPELINE \"off\"'.\n3. Process the outer loop in its full form without breaking it into smaller chunks using '#pragma ACCEL TILE FACTOR=1'.\n4.  Don't automatically pipeline the middle loop using '#pragma ACCEL PIPELINE \"off\"'.\n5. Process the middle loop in chunks of size 2 using '#pragma ACCEL TILE FACTOR=2'.\n6. Flatten the inner loop automatically using '#pragma ACCEL PIPELINE auto{flatten}'.\n7. Process the inner loop in its full form without breaking it into smaller chunks using '#pragma ACCEL TILE FACTOR=1'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, and within the function, apply automatic loop pipelining, tile factor and parallelization for optimization.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C for hardware acceleration, use the following comprehensive but concise natural language instruction:\n\n\"Declare the following function as a hardware kernel for acceleration and apply specific optimizations to the loops: turn off automatic pipelining, set the tile factor to 1 for original full processing, and parallelize the loops with specific factors of 2, 32, and 12.\"\n\nThis instruction covers the key pragma values to use:\n\n- '#pragma ACCEL kernel' for hardware acceleration\n- '#pragma ACCEL PIPELINE auto{off}' for turning off automatic pipelining\n- '#pragma ACCEL TILE FACTOR=auto{1}' for setting the tile factor to 1\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}' for parallelizing the loop with a factor of 2\n- '#pragma ACCEL PARALLEL FACTOR=auto{32}' for parallelizing the loop with a factor of 32\n- '#pragma ACCEL PARALLEL FACTOR=auto{12}' for parallelizing the loop with a factor of 12", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the #pragma values:\n\n1. Apply automatic pipelining to the loop: \n   Use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loop.\n   \n2. Set the tiling factor for the loop to its automatic value:\n   Use '#pragma ACCEL TILE FACTOR=auto{1}' to set the tiling factor automatically.\n   \n3. Set the parallelization factor for the first loop to its automatic value:\n   Use '#pragma ACCEL PARALLEL FACTOR=auto{2}' to parallelize the loop automatically.\n\n4. Set a specific parallelization factor for the inner loop to its automatic value:\n   Use '#pragma ACCEL PARALLEL FACTOR=auto{32}' to parallelize the inner loop with the specified factor.\n\n5. Set a different parallelization factor for the subsequent inner loop to its automatic value:\n   Use '#pragma ACCEL PARALLEL FACTOR=auto{12}' to parallelize the second inner loop with the specified factor.\n\nBy following these instructions, you can generate the given HLS codes in C with the appropriate #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To auto pipeline the loop, use `#pragma ACCEL PIPELINE auto`  \nTo auto tile the loop with factor 1, use `#pragma ACCEL TILE FACTOR=auto{1}`  \nTo auto parallelize the loop with factor 2, use `#pragma ACCEL PARALLEL FACTOR=auto{2}`  \nTo auto parallelize the inner loop with factor 32, use `#pragma ACCEL PARALLEL FACTOR=auto{32}`  \nTo auto parallelize the inner loop with factor 12, use `#pragma ACCEL PARALLEL FACTOR=auto{12}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions:\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop in parallel with a factor of 2.\n5. Run the iterations of this loop in parallel with a factor of 4.\n6. Run the iterations of this loop in parallel with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To automatically pipeline the loop, set the pragma value to '#pragma ACCEL PIPELINE auto{}'. To process the loop in its original, full form without breaking it into smaller chunks, set the pragma value to '#pragma ACCEL TILE FACTOR=auto{1}'. To run the iterations of the loop one after the other, not in parallel, set the pragma value to '#pragma ACCEL PARALLEL FACTOR=auto{2}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Code the loops for r, q, and p to be pipelined without using automatic pipeline. Use a tile factor of 1 for each loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C:\nUse '#pragma ACCEL kernel' to designate the following function as a hardware kernel for acceleration.\n\nUse '#pragma ACCEL PIPELINE auto{}' to enable automatic pipeline scheduling for the loop.\n\nUse '#pragma ACCEL TILE FACTOR=auto{1}' to process the loop in its original, full form without breaking it into smaller chunks.\n\nUse '#pragma ACCEL PARALLEL FACTOR=auto{2}' to run iterations of the loop in parallel, with a parallel factor of 2.\n\nUse '#pragma ACCEL PARALLEL FACTOR=auto{32}' to run the inner loop iterations in parallel, with a parallel factor of 32.\n\nUse '#pragma ACCEL PARALLEL FACTOR=auto{12}' to run iterations of the inner loop in parallel, with a parallel factor of 12.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware kernel for acceleration, and apply parallelization to the outer and inner loops. Also, pipeline the loop for better performance.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you need to apply the following #pragma values:\n\n1. For the outermost loop:\n   - Use `#pragma ACCEL PIPELINE \"off\"` to prevent automatic pipelining.\n\n2. For the nested loops:\n   - Use `#pragma ACCEL TILE FACTOR=1` to process the loops in their original, full form without breaking them into smaller chunks.\n\n3. Inside the innermost loop:\n   - Use `#pragma ACCEL PIPELINE \"off\"` to disable automatic pipelining.\n\nCombine these pragma directives with the function declaration `#pragma ACCEL kernel` to treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loops in their original, full form without breaking them into smaller chunks and automatically pipeline each loop. Treat the 'kernel_doitgen' function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply pipeline optimization to the outermost loop, and enable tiling with a factor of 2 for parallelism. Disable pipeline optimization and set tiling factor to 1 for the inner loops to maintain the original, unbroken form. Additionally, mark the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes:\n\n1. Treat the following function `kernel_jacobi_1d` as a hardware kernel for acceleration - Use `#pragma ACCEL kernel` before the function declaration.\n\n2. Don't automatically pipeline the loop - Use `#pragma ACCEL PIPELINE \"off\"` before the loop to disable automatic pipelining.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks - Use `#pragma ACCEL TILE FACTOR=1` before the loop to set the tile factor to 1.\n\n4. Run the iterations of this loop one after the other, not in parallel - Use `#pragma ACCEL PARALLEL FACTOR=1` before the loop to set the parallel factor to 1.\n\nThese instructions will help you generate the given HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the functions 'kernel_doitgen' as hardware kernels for acceleration. For the loops in the function, ensure they are not automatically pipelined and are processed in their original, full form without breaking them into smaller chunks. Additionally, ensure that the iterations of the loops are not run in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply automatic pipeline directive to the loop\nApply automatic tiling directive to the loop with a factor of 1\nApply automatic parallelization directive to the loop with a factor of 2\nApply automatic parallelization directive to the inner loop with a factor of 4\nApply automatic parallelization directive to the inner loop with a factor of 8", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the 'kernel_jacobi_1d' function, use the following comprehensive but very concise natural language instruction:\n\n\"Automatically pipeline the loop, break it into smaller chunks, and run iterations in parallel with specific factors.\"\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following concise natural language instructions for the specific #pragma values:\n\n1. \"Process this loop in its original, full form without breaking it into smaller chunks and don't automatically pipeline it\" to describe `#pragma ACCEL PIPELINE off`\n2. \"Break this loop into smaller chunks of the same size for optimization\" to describe `#pragma ACCEL TILE FACTOR=1`\n3. \"Run the iterations of this loop in parallel with a factor of 2\" to describe `#pragma ACCEL PARALLEL FACTOR=2`\n4. \"Treat the following function as a hardware kernel for acceleration\" to describe `#pragma ACCEL kernel`\n\nBy using these natural language instructions, you can generate the HLS codes written in C as provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions for specific #pragma values:\n\nEnsure this function is treated as a hardware kernel for acceleration:\n```c\n#pragma ACCEL kernel\n```\n\nDon't automatically pipeline the outermost loop:\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\nProcess the loops in their original, full form without breaking them into smaller chunks:\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\nRun the iterations of the inner loop one after the other, not in parallel:\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "### Instruction: \nOptimize the loop to run efficiently for hardware acceleration, and attempt automatic strategies for pipelining, tiling, and parallelization.\n\n### Pragma Values:\n- `#pragma ACCEL PIPELINE auto{off}`: Don't automatically pipeline this loop\n- `#pragma ACCEL TILE FACTOR=auto{1}`: Process this loop in its original, full form without breaking it into smaller chunks\n- `#pragma ACCEL PARALLEL FACTOR=auto{2}`: Run the iterations of this loop in parallel with factor 2\n- `#pragma ACCEL PARALLEL FACTOR=auto{3}`: Run the inner loop iterations in parallel with factor 3\n- `#pragma ACCEL PARALLEL FACTOR=auto{12}`: Run the outer loop iterations in parallel with factor 12\n- `#pragma ACCEL kernel`: Treat the following function as a hardware kernel for acceleration", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks, and do not automatically pipeline the loops. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process both loops in their original, full form without breaking them into smaller chunks; run the iterations of the loops in parallel with a factor of 2; pipeline both loops automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these concise natural language instructions:\n- Optimize the loop using automatic pipelining.\n- Split the loop into smaller chunks with a factor of 1.\n- Parallelize the loop with a factor of 2.\n- Further parallelize the inner loop with a factor of 4.\n- Further parallelize the loop with a factor of 8.\n\nThis will result in the same HLS codes in C using #pragma directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the natural language instruction:\n\n\"For the function `kernel_doitgen`, mark it as a hardware kernel for acceleration by treating it as such.\"\nUse the following #pragma values:\n1. `#pragma ACCEL kernel` - Treat the following function as a hardware kernel for acceleration.\n\nThis will correctly generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize nested loops for FPGA acceleration by automatically parallelizing and pipelining the outer and inner loops. Use the specified array reduction factor for parallelism. Apply tiling to the loops with a factor of 1 to maintain the original loop structure. Treat the \"kernel_big\" function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with HLS pragma directives::\n1. Mark the function as a hardware kernel for acceleration: Use `#pragma ACCEL kernel` before the function definition.\n2. Turn off automatic pipeline for the outer loop: Use `#pragma ACCEL PIPELINE auto{off}` before the outer loop.\n3. Set the tile factor to 1 for the outer loop: Use `#pragma ACCEL TILE FACTOR=auto{1}` before the outer loop.\n4. Turn off automatic pipeline for the inner loops: Use `#pragma ACCEL PIPELINE auto{off}` before the inner loops.\n5. Set the tile factor to 1 for the inner loops: Use `#pragma ACCEL TILE FACTOR=auto{1}` before the inner loops.\n\nThese directives will configure the hardware design for the given C code and control the pipelining and tiling behavior for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the `doitgen` function as a hardware kernel for acceleration. \nFor the outer loop with loop variable `r`, do not automatically pipeline and process in its original, full form without breaking it into smaller chunks. For the inner loop with loop variable `q`, automatically pipeline and process in smaller chunks with factor of 8. And for the innermost loop with loop variable `p`, automatically pipeline with flattening and process in its original form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks: #pragma ACCEL TILE FACTOR=1\n\nRun the iterations of this loop one after the other, not in parallel: #pragma ACCEL PARALLEL FACTOR=1\n\nDon't automatically pipeline this loop: #pragma ACCEL PIPELINE \"off\"\n\nTreat the following function as a hardware kernel for acceleration: #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop of the function 'kernel_jacobi_1d' in its original, full form without breaking it into smaller chunks. Automatically pipeline this loop. Run the iterations of this loop in parallel with a factor of 2. Then run the inner loop iterations of this loop in parallel with a factor of 8. Finally, run the inner loop iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Don't automatically pipeline the loop: '#pragma ACCEL PIPELINE auto{off}'\n2. Process this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Run the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n4. Treat the following function as a hardware kernel for acceleration: '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following pragma values:\n\n1.  Treat the following function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2.  Don't automatically pipeline this loop.\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3.  Process this loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4.  Run the iterations of this loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following concise natural language instruction:\n\n\"Optimize the loop iterations for pipelining, tiling with a factor of 1, and parallelizing with factors of 2, 5, and 8, respectively. Treat the function as a hardware kernel for acceleration.\"\n\nThis instruction captures the process of the codes and specifies the appropriate #pragma values to use.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Parallelize the outer loop with a factor chosen automatically. Pipeline the loops using flattening. Tile the loops with a factor chosen automatically. Parallelize the outer and inner loops with a factor chosen automatically and perform reduction on inner loop. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following pragma values:\n1. For the outer loop of 'r', turn off automatic pipelining and maintain the loop in its original, full form without breaking it into smaller chunks.\n2. For the inner loop of 'q', also turn off automatic pipelining and maintain the loop in its original, full form without breaking it into smaller chunks.\n3. For the most inner loop of 'p', again turn off automatic pipelining and maintain the loop in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Convert the function to a hardware kernel for acceleration and apply automatic pipeline, tile factor, and parallel factor optimizations based on the compiler's decision.\"\n\nThis instruction covers the specific pragmas like \"#pragma ACCEL kernel\", \"#pragma ACCEL PIPELINE auto{}\", \"#pragma ACCEL TILE FACTOR=auto{1}\", and \"#pragma ACCEL PARALLEL FACTOR=auto{}\" and sets the compiler to make automatic decisions for pipeline, tiling, and parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks; don't automatically pipeline this loop; run the iterations of this loop one after the other, not in parallel; treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, then automatically pipeline the outer loop, tile the loops with a factor of 1, and parallelize the outer loop with a factor of 2 and the inner loops with factors of 6 and 5 respectively.\"\n\nI hope this helps!", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n- Enhance parallelism by automatically pipelining this loop using '#pragma ACCEL PIPELINE auto{}'\n- Process this loop with its original, full form without breaking it into smaller chunks using '#pragma ACCEL TILE FACTOR=auto{1}'\n- Run the iterations of this loop in parallel with a factor automatically determined using '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n- Further maximize parallelism by automatically determining the parallel factor of 5 for the inner loop using '#pragma ACCEL PARALLEL FACTOR=auto{5}'\n- Finally, achieve higher parallelism with an automatic parallel factor of 8 for the second inner loop using '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n\nEnsure these pragmas are placed appropriately in the code to achieve the specified hardware acceleration behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration'\n   Use: `#pragma ACCEL kernel`\n\n2. 'Don't automatically pipeline this loop'\n   Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks'\n   Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. 'Run the iterations of this loop one after the other, not in parallel'\n   Use: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. \"Treat the following function as a hardware kernel for acceleration\" - Use `#pragma ACCEL kernel` to indicate that the function should be treated as a hardware kernel for acceleration.\n\n2. \"Don't automatically pipeline this loop\" - Use `#pragma ACCEL PIPELINE \"off\"` to turn off automatic pipelining for the loop.\n\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\" - Use `#pragma ACCEL TILE FACTOR=1` to set the tiling factor to 1, indicating that the loop should not be broken into smaller chunks.\n\n4. \"Run the iterations of this loop one after the other, not in parallel\" - Use `#pragma ACCEL PARALLEL FACTOR=1` to specify that the loop iterations should not be run in parallel and should instead be executed one after the other.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Pipeline the outer loop with offloading disabled. Use an automatic tiling factor of 2 for the outer loop. Pipeline the middle loop with flattening enabled and use an automatic tiling factor of 1 for the middle and inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the nested loops in their original, full form without breaking them into smaller chunks, and treat the function as a hardware kernel for acceleration. Additionally, don't automatically pipeline the loops and use a tile factor of 1 for each loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and don't automatically pipeline the outermost loop; process the outermost loop in its original, full form without breaking it into smaller chunks; process the inner loops with a tile factor of 1, and turn off automatic pipelining for the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the three nested loops in function 'kernel_doitgen' in the original, full form without breaking them into smaller chunks. Ensure that the loops are not automatically pipelined and that they are not treated as flattened. Set the tile factor as 1 for the outer two loops and 8 for the inner loop to optimize for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following comprehensive but concise natural language instructions for the #pragma values:\n\n1. Don't automatically pipeline this loop.\n2. Process this loop in its original, full form without breaking it into smaller chunks.\n3. Run the iterations of this loop two at a time in parallel.\n4. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipeline and tiling, and use the default tile factor for the tiling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following pragma values: Do not automatically pipeline the loops using pragma ACCEL PIPELINE auto{off}, and process the loops without breaking them into smaller chunks using pragma ACCEL TILE FACTOR=2, and for each loop, turn off automatic pipelining with pragma ACCEL PIPELINE auto{off} and use pragma ACCEL TILE FACTOR=1 to process the iterations in their original, full form without breaking them into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n- For the top-level kernel function: 'Treat the following function as a hardware kernel for acceleration'\n- For the outermost loop over 'r': 'Don't automatically pipeline this loop' and 'Process this loop in its original, full form without breaking it into smaller chunks'\n- For the loop over 'q' inside the 'r' loop: 'Don't automatically pipeline this loop' and 'Process this loop in its original, full form without breaking it into smaller chunks'\n- For the innermost loop over 'p' inside the 'q' loop: 'Don't automatically pipeline this loop' and 'Process this loop in its original, full form without breaking it into smaller chunks'\n\nThese natural language instructions map to the following specific #pragma values in the C code:\n- '#pragma ACCEL kernel' for the top-level kernel function\n- '#pragma ACCEL PIPELINE \"off\"' for the outermost loop over 'r'\n- '#pragma ACCEL TILE FACTOR=1' for the outermost loop over 'r'\n- '#pragma ACCEL PIPELINE \"off\"' for the loop over 'q' inside the 'r' loop\n- '#pragma ACCEL TILE FACTOR=1' for the loop over 'q' inside the 'r' loop\n- '#pragma ACCEL PIPELINE \"off\"' for the loop over 'p' inside the 'q' loop\n- '#pragma ACCEL TILE FACTOR=1' for the loop over 'p' inside the 'q' loop", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions for the #pragma values:\n\n1. Enable automatic pipelining for the loop: '#pragma ACCEL PIPELINE auto{flatten}'\n2. Specify the tiling factor automatically for the loop: '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Set the parallelization factor automatically for the loop: '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n4. Set the parallelization factor automatically for the inner loop: '#pragma ACCEL PARALLEL FACTOR=auto{6}' (inside the outer loop)\n5. Set the parallelization factor automatically for the inner loop: '#pragma ACCEL PARALLEL FACTOR=auto{1}' (inside the outer loop)\n\nBy following these instructions, you can effectively incorporate hardware acceleration into the C codes using the specified HLS #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n- Mark the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Don't automatically pipeline any of the loops: `#pragma ACCEL PIPELINE \"off\"`\n- Process each loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n- Run the iterations of each loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipelining and tiling. Also, use automatic flattening for the innermost loop to maximize parallelism. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Automatically pipeline the loops' \nfor '#pragma ACCEL PIPELINE auto{}'\n\n'Use automatic tiling factor for the loop' \nfor '#pragma ACCEL TILE FACTOR=auto{1}'\n\n'Automatically parallelize the loop with maximum factor of 2' \nfor '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n\n'Automatically parallelize the inner loop with factor 8'\nfor 'pragma ACCEL PARALLEL FACTOR=auto{8}'\n\n'Automatically parallelize the inner loop with factor 6'\nfor 'pragma ACCEL PARALLEL FACTOR=auto{6}'\n\n'Treat the following function as a hardware kernel for acceleration' \nfor '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n\"Designate the following function as a hardware kernel for acceleration\"\n\n\"Automatically pipeline this loop, flattening the loop hierarchy\"\n\n\"Break this loop into smaller chunks with a factor of 8 for better acceleration\"\n\n\"Run the iterations of this loop in parallel, with a factor of 2\"\n\n\"Run the iterations of this inner loop in parallel, with a factor of 8\"\n\n\"Run the iterations of this loop one after the other, not in parallel\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following concise but comprehensive natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, pipeline the loop automatically, set the tile factor to 1 automatically, and parallelize the loops with factors of 2, 8, and 12 automatically.\"\n\nThis instruction encompasses the specific #pragma values and their configurations needed to generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply full pipeline off and tile factor of 1 to the outer and inner loops of the 'kernel_doitgen' function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipelining and tiling with a factor of 1, and flatten the inner loop with a tiling factor of 4. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Perform loop pipelining and do not automatically parallelize the loops; tile the loops with a factor of 1 for r, q, and p loops; and treat the function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Ensure that the loop is automatically pipelined: `#pragma ACCEL PIPELINE auto{flatten}`\n\n2. Break the loop into smaller chunks with automatic tile factor calculation: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Parallelize the loop with automatic factor calculation: `#pragma ACCEL PARALLEL FACTOR=auto{3}`\n\n4. Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\nEnsure that the following function is treated as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with tiling, automatically determining the factor for tiling. Pipeline the outer loop and the innermost loops, with automatic determination of pipeline scheduling. Use flatten pipeline for the innermost loop to optimize for vectorization. This function is a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in this function in parallel and pipeline the innermost loop. Use auto scheduling for both tiling and pipelining.\nUse \"#pragma ACCEL PIPELINE auto{}\" for pipelining,\nand \"#pragma ACCEL TILE FACTOR=auto{8}\" for the outer loop,\n\"#pragma ACCEL TILE FACTOR=auto{1}\" for the middle loop,\nand \"#pragma ACCEL TILE FACTOR=auto{1}\" for the innermost loop.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks, and do not automatically pipeline the loops. Treat the following function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr, int nq, int np, double A[25][20][30], double C4[30][30], double sum[30]) {\n  int r;\n  int q;\n  int p;\n  int s;\n  \n  #pragma ACCEL PIPELINE auto{off}\n  #pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    #pragma ACCEL PIPELINE auto{off}\n    #pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      #pragma ACCEL PIPELINE auto{off}\n      #pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n1. Mark the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n2. Don't automatically pipeline the outer loop: `#pragma ACCEL PIPELINE \"off\"`\n3. Process the outer loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n4. Run the iterations of the outer loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipeline optimization, and keep the original loop structure without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the following C code for hardware acceleration, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Enable automatic pipeline optimization for loops: Use '#pragma ACCEL PIPELINE auto{flatten}' to automatically optimize loop pipelining.\n\n2. Keep the original loop structure intact without breaking it into smaller chunks: Use '#pragma ACCEL TILE FACTOR=auto{1}' to maintain the original loop structure.\n\n3. Run loop iterations in parallel, with a factor of 3: Use '#pragma ACCEL PARALLEL FACTOR=auto{3}' to parallelize the loop iterations with a factor of 3.\n\n4. Treat the following function as a hardware kernel for acceleration: Use '#pragma ACCEL kernel' to specify the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nProcess the loop without automatic pipelining.\n\nBreak the loop into smaller chunks of size 1 for tiling.\n\nRun the iterations of the loop in parallel, dividing into 2 parallel tasks for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. \n\nDon't automatically pipeline this loop.\n\nRun the iterations of this loop one after the other, not in parallel.\n\nTreat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n\n\"For the function `kernel_doitgen`, treat it as a hardware kernel for acceleration. Then, automatically pipeline the loops, and apply tiling factor 4 for the outermost loop and tiling factor 1 for the inner loops. Additionally, use flattening for the innermost loop to pipeline it.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the outer loop. Use an automatic tile factor for the loop, and parallelize the loop with a factor of 4. Within the outer loop, parallelize the inner loops with a factor of 8 and 1 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks and automatically pipeline each loop. Also, treat the following function as a hardware kernel for acceleration.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\nSpecify the following pragma values for the related loops and functions:\n- Use the '#pragma ACCEL kernel' for the function 'kernel_jacobi_1d'\n- Implement automatic pipeline for the outer loops using '#pragma ACCEL PIPELINE auto{}'\n- Set the tile factor to 'auto{1}' for loop blocking using '#pragma ACCEL TILE FACTOR=auto{1}'\n- Apply automatic parallelization with a factor of 'auto{2}' for the outer loop using '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n- Apply automatic parallelization with a factor of 'auto{30}' and 'auto{12}' for the inner loops using '#pragma ACCEL PARALLEL FACTOR=auto{30}' and '#pragma ACCEL PARALLEL FACTOR=auto{12}' respectively", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the HLS code:\n\n\"Apply automatic pipeline directives to the loop\"\n```c\n#pragma ACCEL PIPELINE auto\n```\n\n\"Automatically determine the tiling factor for the loop\"\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n\"Automatically determine the parallelization factor for the loop, allowing up to 3 parallel instances\"\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{3}\n```\n\n\"Automatically determine the parallelization factor for the inner loop, allowing up to 10 parallel instances\"\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{10}\n```\n\n\"Automatically determine the parallelization factor for the inner loop, allowing up to 32 parallel instances\"\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{32}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n1. Use '#pragma ACCEL kernel' to designate the function as a hardware kernel for acceleration.\n2. Use '#pragma ACCEL PIPELINE auto{off}' to disable automatic pipelining for the loop.\n3. Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the loop in its original, full form without breaking it into smaller chunks.\n4. Use '#pragma ACCEL PARALLEL FACTOR=auto{4}' to run the iterations of the loop in parallel with a factor of 4.\n5. Use '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run the iterations of the inner loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use the following natural language instruction to generate the given HLS codes in C:\n\"Convert the following function into a hardware kernel for acceleration and use specific pragma directives to control loop optimization: turn off automatic pipeline, set the tiling factor to 1, and activate parallelization with a factor of 8 for the outer loop and 1 for the inner loops.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outermost loops in their full form, without breaking them into smaller chunks, and then run the iterations of these loops one after the other, not in parallel. For the innermost loops, automatically pipeline them and process them in their original, full form without breaking them into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but concise natural language instruction for each #pragma value:\n\n1. Use '#pragma ACCEL kernel' to treat the following function as a hardware kernel for acceleration.\n2. Use '#pragma ACCEL PIPELINE auto{off}' to disable automatic loop pipelining.\n3. Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the loop in its original, full form without breaking it into smaller chunks.\n4. Use '#pragma ACCEL PARALLEL FACTOR=auto{32}' to run the iterations of the loop in parallel with a factor of 32.\n5. Use '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run the iterations of the loop one after the other, not in parallel, with a factor of 1.\n6. Use '#pragma endscop' to end the loop descriptions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following concise natural language instruction:\n\n    \"Treat the following function as a hardware kernel for acceleration. Automatically pipeline the outer loop, and apply a tile factor of 1 to break the loop into smaller chunks. Further, pipeline the inner loops and flatten the loop body to optimize performance.\"\n\nThis instruction covers the necessary #pragma directives to achieve the desired HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, use the following comprehensive but very concise natural language instruction:\n\"Designate the function as a hardware kernel for acceleration, enable automatic pipeline flattening, set the tile factor to 1, and parallelize the outer loop by a factor of 4, followed by parallelizing the inner loops by a factor of 1.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the specific #pragma values:\n\n1. \"Treat the following function as a hardware kernel for acceleration\"\n   - This describes the pragma: `#pragma ACCEL kernel`\n\n2. \"Don't automatically pipeline this loop\"\n   - This describes the pragma: `#pragma ACCEL PIPELINE \"off\"`\n\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   - This describes the pragma: `#pragma ACCEL TILE FACTOR=1`\n\n4. \"Run the iterations of this loop one after the other, not in parallel\"\n   - This describes the pragma: `#pragma ACCEL PARALLEL FACTOR=1`\n\n5. \"Run the iterations of this loop in parallel with a factor of 3\"\n   - This describes the pragma: `#pragma ACCEL PARALLEL FACTOR=3`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following concise natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration.\n\n   - Natural language instruction: \"Treat the following function as a hardware kernel for acceleration\"\n\n   - Corresponding #pragma values: \"#pragma ACCEL kernel\"\n\n2. Automatically pipeline the loop without further optimization directives.\n\n   - Natural language instruction: \"Automatically pipeline this loop\"\n\n   - Corresponding #pragma values: \"#pragma ACCEL PIPELINE auto{flatten}\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   - Natural language instruction: \"Set the tile factor to 1\"\n\n   - Corresponding #pragma values: \"#pragma ACCEL TILE FACTOR=auto{1}\"\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n   - Natural language instruction: \"Set the parallel factor to 1 for this loop\"\n\n   - Corresponding #pragma values: \"#pragma ACCEL PARALLEL FACTOR=auto{3}\" (for the outer loop) and \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" (for the inner loops)", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, include the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Use: `#pragma ACCEL kernel`\n\n2. Automatically pipeline this loop\n   - Use: `#pragma ACCEL PIPELINE auto{}`\n\n3. Process this loop with its original, full form without breaking into smaller chunks\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Disable automatic pipelining for this loop\n   - Use: `#pragma ACCEL PIPELINE auto{off}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following function is treated as a hardware kernel for acceleration. Use automatic pipeline and tiling optimizations for the loops, with specific factors determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with HLS pragmas, use the following concise natural language instruction for each pragma:\n\n1. 'Parallelize this loop with a factor of 3' - for '#pragma ACCEL PARALLEL FACTOR=3'\n2. 'Pipeline this loop automatically' - for '#pragma ACCEL PIPELINE auto'\n3. 'No tiling needed for this loop' - for '#pragma ACCEL TILE FACTOR=1'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and then apply the following optimizations:\n- Disable automatic loop pipelining\n- Use a tile factor of 1 for the outer loop\n- Disable automatic loop pipelining and use a tile factor of 1 for the middle loop\n- Enable automatic loop pipelining and use a tile factor of 8 for the inner loop\"\n\nThis comprehensive but concise instruction covers the process of the codes and specifies the pragma values to use.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the above HLS codes in C, you should use the following #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration' for the function definition:\n   ```c\n   #pragma ACCEL kernel\n   void kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n   ```\n\n2. 'Don't automatically pipeline this loop' for the outermost loop:\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   for (r = 0; r < 25; r++) {\n   ```\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' for the outermost loop:\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   for (r = 0; r < 25; r++) {\n   ```\n\n4. 'Don't automatically pipeline this loop' for the second loop:\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   for (q = 0; q < 20; q++) {\n   ```\n\n5. 'Process this loop in its original, full form without breaking it into smaller chunks' for the second loop:\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   for (q = 0; q < 20; q++) {\n   ```\n\n6. 'Don't automatically pipeline this loop' for the innermost loop:\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   for (p = 0; p < 30; p++) {\n   ```\n\n7. 'Break this loop into smaller chunks to be processed in parallel' with a factor of 4 for the innermost loop:\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{4}\n   for (p = 0; p < 30; p++) {\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration\nTreat this loop as a non-pipelined section\nSplit this loop into smaller chunks of size 1\nRun the iterations of this loop in parallel with a factor of 8\nRun the iterations of this loop in parallel with a factor of 1\nRun the iterations of this loop in parallel with a factor of 16", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop iteration in parallel, with a factor of 8 to maximize parallelism. Additionally, apply automatic loop pipelining to optimize throughput.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Enable automatic pipelining for the loop\n   - Natural language instruction: 'Automatically pipeline this loop'\n   - Corresponding pragma: `#pragma ACCEL PIPELINE auto{}`\n\n2. Use automatic tiling factor for the loop to process it in its original, full form without breaking it into smaller chunks\n   - Natural language instruction: 'Use an automatic tiling factor for this loop'\n   - Corresponding pragma: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Enable automatic parallelization with a factor of 2 for the loop\n   - Natural language instruction: 'Automatically parallelize this loop with a factor of 2'\n   - Corresponding pragma: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n4. Enable automatic parallelization with a factor of 8 for the inner loop\n   - Natural language instruction: 'Automatically parallelize the inner loop with a factor of 8'\n   - Corresponding pragma: `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n\n5. Enable automatic parallelization with a factor of 5 for the second inner loop\n   - Natural language instruction: 'Automatically parallelize the second inner loop with a factor of 5'\n   - Corresponding pragma: `#pragma ACCEL PARALLEL FACTOR=auto{5}`\n\nBy following these natural language instructions, you can generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the given loop using an automated approach for pipelining. Also, break the loop into smaller chunks for better performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original form and don't automatically pipeline them. Use a tiling factor of 1 for the outer loops and a tiling factor of 4 for the innermost loop. Treat the function 'kernel_doitgen' as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Convert the following function into a hardware kernel for acceleration: \n   `#pragma ACCEL kernel`\n\n2. Pipeline the outermost loop automatically: \n   `#pragma ACCEL PIPELINE auto{}`\n\n3. Tile the outermost loop with a factor of 1: \n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Turn off automatic loop pipelining for the middle loop: \n   `#pragma ACCEL PIPELINE auto{off}`\n\n5. Tile the middle loop with a factor of 1: \n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n6. Pipeline the innermost loop automatically: \n   `#pragma ACCEL PIPELINE auto{}`\n\n7. Tile the innermost loop with a factor of 1: \n   `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'doitgen' function as a hardware kernel for acceleration. Pipeline the outer loop with a pipeline factor of 4, and tile it with a tile factor of 1. Pipeline the middle loop with a pipeline factor of 1, and flatten it. Tile the inner loop with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration. Disable automatic loop pipelining. Set the tiling factor to 1 to preserve the loop's original form. Parallelize the outer loop with a factor of 4 and the inner loops with factors of 1 and 8 respectively.\"\n\nThis instruction covers the use of the following #pragma values:\n- '#pragma ACCEL kernel'\n- '#pragma ACCEL PIPELINE auto{off}'\n- '#pragma ACCEL TILE FACTOR=auto{1}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{4}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{8}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the outer loop and do not flatten the pipeline. Use a tiling factor of 8 for the outer loop and 1 for the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with specific #pragma values, use the following natural language instruction: \"Designate the 'doitgen' function as a hardware kernel for acceleration, apply automatic pipeline optimization to the loops, and tile the loops with factor 2 to improve parallelism. Additionally, pipeline the innermost loop with the flatten directive and tile it with factor 1 for efficient hardware execution.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n- Treat the following function as a hardware kernel for acceleration\n- Process the outer loop in a pipeline with default settings\n- Break the outer loop into smaller chunks with a factor of 4 for optimization\n- Turn off automatic pipelining for the middle loop\n- Process the middle loop in its original, full form without breaking it into smaller chunks\n- Flatten the innermost loop for better performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the hardware acceleration of the `kernel_doitgen` function in C, use the following #pragma values:\n- 'Disable automatic loop pipelining' with `#pragma ACCEL PIPELINE \"off\"`\n- 'Use original loop structure without tiling' with `#pragma ACCEL TILE FACTOR=1`\n- 'Flatten the loop for parallelization' with `#pragma ACCEL PIPELINE \"flatten\"`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C with specific #pragma values, use the following natural language instruction: \n\n\"Transform the following function into a hardware kernel for acceleration, and process the nested loops with pipeline off, and a tile factor of 1 for the outer loops, and a tile factor of 4 for the innermost loop. \n```C\n#pragma ACCEL kernel\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30]){\n  int r;\n  int q;\n  int p;\n  int s;\n  #pragma ACCEL PIPELINE auto{off}\n  #pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    #pragma ACCEL PIPELINE auto{off}\n    #pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      #pragma ACCEL PIPELINE auto{off}\n      #pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\nFor the `kernel_jacobi_1d` function, use the following natural language instructions for the #pragma values:\n- \"Don't automatically pipeline this loop\"\n  (`#pragma ACCEL PIPELINE auto{off}`)\n- \"Process this loop in its original, full form without breaking it into smaller chunks\"\n  (`#pragma ACCEL TILE FACTOR=auto{1}`)\n- \"Run the iterations of this loop one after the other, not in parallel\"\n  (`#pragma ACCEL PARALLEL FACTOR=auto{1}`)\n- \"Treat the following function as a hardware kernel for acceleration\"\n  (`#pragma ACCEL kernel`)", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following comprehensive but concise natural language instruction:  \n\"Auto-pipeline the loops. Use a tile factor of 1 for the loop. Parallelize the loop with a factor of 8. Within the loop, run the iterations in parallel with a factor of 1 and then with a factor of 8. Treat the function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop\n3. Process this loop in its original, full form without breaking it into smaller chunks\n4. Process the iterations of this loop in parallel with the specified factor\n5. Automatically pipeline this loop with flatten directive\n\nFollowing these instructions, you can generate the provided HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. 'Automatically pipeline this loop' -> '#pragma ACCEL PIPELINE auto{}'\n2. 'Break this loop into smaller chunks, with automatic factor' -> '#pragma ACCEL TILE FACTOR=auto{1}'\n3. 'Run the iterations of this loop in parallel, with automatic factor of 8' -> '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n4. 'Run the iterations of this loop one after the other, not in parallel' -> '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n5. 'Treat the following function as a hardware kernel for acceleration' -> '#pragma ACCEL kernel'\n\nBy using these instructions, you can generate the given HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\nFor the kernel \"jacobi_1d\", use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n- Pipeline the loops automatically\n- Tile the loops with a factor of 1\n- Parallelize the outer loop with a factor of 8, and parallelize the inner loops with a factor of 1\n\nThese instructions will result in the respective #pragma values for each desired transformation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for the following C code:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n```\nNatural language instruction:\n- Apply manual pipelining for the loop to prevent automatic pipelining\n- Process the loop without breaking it into smaller chunks\n- Execute the loop in parallel using a factor of 8\n- Run the iterations of the loop in parallel with a factor of 1\n- Run the iterations of the loop in parallel with a factor of 2", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for C:\n\nProcess the specified loop in its original, full form, without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\nTreat the specified function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\nDon't automatically pipeline the specified loop.\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\nRun the iterations of the specified loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nDon't automatically pipeline the loops.\n\nProcess the loops in their original, full form without breaking them into smaller chunks.\n\nRun the iterations of the following loops one after the other, not in parallel.\n\nDon't automatically pipeline the loops.\n\nProcess the loops in their original, full form without breaking them into smaller chunks.\n\nRun the iterations of the following loops with a factor of 4 for tiling.\n\nTreat the iterations in the innermost loop as pipeline loops with factor of 1.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is treated as a hardware kernel for acceleration. Pipeline the loops with a factor of \"off\" for automatic pipelining. Use a tiling factor of \"1\" for the loops to be processed in their original, full form. Parallelize the loops with a factor of \"__PARA__L0\" for the outermost loop, \"__PARA__L1\" for the second loop, and \"__PARA__L2\" for the third loop. Additionally, use reduction operations with factors \"__PARA__L3\", \"__PARA__L4\", and \"__PARA__L6\" for appropriate loops. Lastly, set a tiling factor of \"20\" for the second loop.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for HLS, apply the following #pragma directives:\n- Use '#pragma ACCEL kernel' to specify that the 'doitgen' function should be treated as a hardware kernel for acceleration.\n- Disable auto-pipelining in the outer loop using '#pragma ACCEL PIPELINE auto{off}' to avoid automatic pipelining and explicitly control the pipeline initiation interval.\n- Set the tiling factor of the outer loop to 2 using '#pragma ACCEL TILE FACTOR=auto{2}' to partition the loop into smaller chunks for parallel processing.\n- Disable auto-pipelining in the inner loop using '#pragma ACCEL PIPELINE auto{off}' to manually manage the pipeline initiation interval.\n- Set the tiling factor of the inner loop to 1 using '#pragma ACCEL TILE FACTOR=auto{1}' to keep the loop in its original, full form without breaking it into smaller chunks.\n- Apply loop flattening using '#pragma ACCEL PIPELINE auto{flatten}' to collapse the loop nest to a single pipeline stage.\n\nThese directives will help optimize the given C code for HLS synthesis.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the outer loop of the function.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process the following loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of the outer loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To fully accelerate the function `kernel_doitgen`, use the following instructions:\n- Add automatic pipeline directives to enable the tool to generate pipeline pragmas as needed.\n- Set the tile factor to 8 for the outer loop over variable `r`.\n- Disable automatic pipeline for the loop over variable `r` to avoid automatic pipelining.\n- Set the tile factor to 1 for the loops over variables `q` and `p`.\n- Use the `flatten` directive for automatic pipeline as it helps in flattening the loops for better parallelism and performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n\"For the kernel function `doitgen`, apply the following optimizations:\n- Disable automatic loop pipelining for all loops\n- Set the tiling factor to 1 for the outermost loop, and 2 for the middle loop\n- Flatten the innermost loop and set its tiling factor to 1.\"\n\nThese natural language instructions correspond to the following #pragma values:\n1. Disable automatic loop pipelining: #pragma ACCEL PIPELINE \"off\"\n2. Set tiling factor for outer loop: #pragma ACCEL TILE FACTOR=1\n3. Set tiling factor for middle loop: #pragma ACCEL TILE FACTOR=2\n4. Flatten innermost loop and set tiling factor: #pragma ACCEL PIPELINE \"flatten\" and #pragma ACCEL TILE FACTOR=1", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Turn off automatic pipelining for the outer and inner loops.\n3. Use a tile factor of 1 for both outer and inner loops.\n4. Flatten the innermost loop to enable parallelization with a tile factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language pragma instructions:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   - `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop:\n   - `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   - `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   - `#pragma ACCEL PARALLEL FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_jacobi_1d' function as a hardware kernel for acceleration. Don't automatically pipeline the outer loop. Use a tile factor of 1 for tiling the outer loop. Use a parallel factor of 8 for the outer loop iterations and a parallel factor of 2 for the inner loop iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function 'kernel_doitgen' as a hardware kernel for acceleration.\n\nThen, for the loop over variable 'r', set the pipeline off. For the same loop, use a tile factor of 1.\n\nFor the loop over variable 'q', use auto pipelining and a tile factor of 8.\n\nFor the loop over variable 'p', set automatic flattening pipelining and use a tile factor of 1.\n\nThis set of instructions will help to accelerate the code using HLS pragmas.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given C code, use the following natural language instructions to generate the corresponding HLS pragmas:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nBy following these instructions, you can generate the specific #pragma values for the given code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks, and do not automatically pipeline these loops #pragma ACCEL TILE FACTOR=1, #pragma ACCEL PIPELINE \"off\"\n\nTreat the following function as a hardware kernel for acceleration #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions:\n\n1. For the kernel function `kernel_jacobi_1d`, treat it as a hardware kernel for acceleration by adding the pragma `#pragma ACCEL kernel` above the function definition.\n\n2. To turn off automatic pipelining for the following loops in the code, use the pragma `#pragma ACCEL PIPELINE auto{off}` inside each loop.\n\n3. For the outermost loop, process it in its original, full form without breaking it into smaller chunks by adding the pragma `#pragma ACCEL TILE FACTOR=auto{1}`.\n\n4. Run the iterations of the outer loop in parallel with a factor of 8 by adding the pragma `#pragma ACCEL PARALLEL FACTOR=auto{8}` above the outer loop.\n\n5. Run the iterations of the inner loops one after the other, not in parallel, using `#pragma ACCEL PARALLEL FACTOR=auto{1}` inside each inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, we can use the following concise natural language instruction:\n\n\"Declare the following function as a hardware kernel for acceleration, then apply automatic pipeline flattening, use a tile factor of 1 for tiling, set parallel factor to 2 for the outer loop, and 5 for the first inner loop, and finally 1 for the second inner loop.\"\n\nThis instruction covers the process of the given codes and specifies the appropriate #pragma values to use.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the corresponding #pragma values:\n\n1. 'Turn off automatic loop pipelining for the designated loop' - This describes the process for '#pragma ACCEL PIPELINE auto{off}'.\n\n2. 'Use the original, full version of the loop without breaking it into smaller chunks' - This describes the process for '#pragma ACCEL TILE FACTOR=auto{1}'.\n\n3. 'Run the iterations of this loop in parallel with a factor of 2' - This describes the process for '#pragma ACCEL PARALLEL FACTOR=auto{2}'.\n\n4. 'Run the iterations of this loop in parallel with a factor of 32' - This describes the process for '#pragma ACCEL PARALLEL FACTOR=auto{32}'.\n\n5. 'Run the iterations of this loop in parallel with a factor of 12' - This describes the process for '#pragma ACCEL PARALLEL FACTOR=auto{12}'.\n\nAfter applying these natural language instructions, you can generate the corresponding HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To automatically pipeline the loops, tile with a factor of 25, and run the iterations of the loops in parallel with specific reduction factors, use the following #pragma values:\n- For pipeline: '#pragma ACCEL PIPELINE auto{off}'\n- For tiling: '#pragma ACCEL TILE FACTOR=auto{25}'\n- For parallel execution with reduction: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' (outer loop), '#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}' (mean calculation), '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' (data manipulation), and '#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}' (covariance calculation)\n- Also, use '#pragma ACCEL PIPELINE auto{off}' for the subsequent pipeline and '#pragma ACCEL TILE FACTOR=auto{1}' for tiling in the following loops.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for #pragma values:\n\n1. Disable automatic loop pipelining: \n   Use \"#pragma ACCEL PIPELINE auto{off}\"\n\n2. Process the loop without breaking it into smaller chunks:\n   Employ \"#pragma ACCEL TILE FACTOR=auto{1}\"\n\n3. Run loop iterations in parallel with a factor of 8:\n   Utilize \"#pragma ACCEL PARALLEL FACTOR=auto{8}\"\n\n4. Run the inner loop iterations in parallel with a factor of 12:\n   Implement \"#pragma ACCEL PARALLEL FACTOR=auto{12}\"\n\n5. Run the outer loop iterations in parallel with a factor of 10:\n   Employ \"#pragma ACCEL PARALLEL FACTOR=auto{10}\"\n\nEnsure to use these natural language instructions to generate the appropriate HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions with specific #pragma values:\n\n1. Keep this loop from being automatically pipelined - Use '#pragma ACCEL PIPELINE auto{off}'\n2. Process this loop in its original, full form without breaking it into smaller chunks - Use '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Run the iterations of this loop in parallel with a factor of 8 - Use '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n4. Run the inner loops in parallel with a factor of 12 - Use '#pragma ACCEL PARALLEL FACTOR=auto{12}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks. Automatically pipeline the loops where possible and treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Set the loop to not automatically pipeline:  \n   Use `#pragma ACCEL PIPELINE auto {off}`\n\n2. Process the loop in its original, full form without breaking it into smaller chunks:  \n   Use `#pragma ACCEL TILE FACTOR=auto {1}`\n\n3. Flatten the pipeline for the innermost loop:  \n   Use `#pragma ACCEL PIPELINE auto {flatten}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the nested loops with original dimensions and full iterations, enable automatic pipelining for all loops, and treat the function as a hardware kernel for acceleration. Set the tile factor to 1 for each level of nesting and set parallelization factor to automatic for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the hardware acceleration for loop parallelization by automatically pipelining, tiling, and parallelizing the loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_doitgen' function as a hardware kernel for acceleration.\n\nTreat the loops in the 'kernel_doitgen' function to not be automatically pipelined. \n\nProcess the loops in the 'kernel_doitgen' function in their original, full form without breaking them into smaller chunks, and run the iterations of the loops one after the other, not in parallel.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   \n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline this loop.\n\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n   ```\n\n5. Don't automatically pipeline this loop.\n\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n7. Run the iterations of this loop one after the other, not in parallel.\n\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n   ```\n\n8. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{8}\n   ```\n\n9. Run the iterations of this loop one after the other, not in parallel.\n\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n   ```\n\n10. Treat the iterations of this loop to be processed one after the other.\n\n    ```c\n    #pragma ACCEL PIPELINE auto{__PIPE__L5}\n    ```\n\nEnsure to apply these instructions to effectively generate the required HLS codes in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the kernel function 'doitgen' as a hardware kernel for acceleration. Pipeline the loop for variable 'r' without automatic pipelining. Tile the loop for variable 'r' with a tile factor of 4. Pipeline the loop for variable 'q' without automatic pipelining. Tile the loop for variable 'q' with a tile factor of 1. Flatten the loop for variable 'p' without automatic pipelining. Tile the loop for variable 'p' with a tile factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n2. Turn off automatic pipelining for the loop.\n3. Keep the loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of the loop in parallel with a factor of 8.\n5. Run the iterations of the inner loop in parallel with a factor of 2.\n6. Run the iterations of the second inner loop in parallel with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'doitgen' kernel function as a hardware kernel for acceleration, and then apply the following pragma directives:\n1. Use '#pragma ACCEL PIPELINE auto{off}' to turn off automatic pipelining for the outer loop.\n2. Use '#pragma ACCEL TILE FACTOR=auto{8}' to set the tiling factor to 8 for the middle loop.\n3. Use '#pragma ACCEL PIPELINE auto{flatten}' to flatten the inner loop for parallel processing.\n4. Use '#pragma ACCEL TILE FACTOR=auto{1}' to set the tiling factor to 1 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\n\nAutomatically pipeline this loop\n\nRun the iterations of this loop in parallel with a factor of 8\n\nRun the iterations of this loop in parallel with a factor of 30\n\nRun the iterations of this loop in parallel with a factor of 8", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply pipelining off for the outer and inner loops and use a tile factor of 1 for both loops. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks.  \nRun the iterations of this loop one after the other, not in parallel.  \nTreat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks, treat the following function as a hardware kernel for acceleration, automatically pipeline this loop, parallelize the loop with a factor of 2, and parallelize the inner loops with a factor of 8 and 1 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration. Disable automatic pipelining for the outer and inner loops. Set the tiling factor to 1 for both loops. Flatten the innermost loop. Automatically tile the innermost loop with a factor of 2.\"\n\nThis natural language instruction corresponds to the following pragma values in the C code:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PIPELINE auto{flatten}\n#pragma ACCEL TILE FACTOR=auto{2}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for hardware acceleration, use the following comprehensive but concise natural language instruction for the specific #pragma values:\n\n1. Ensure the loop is not automatically pipelined:\n   `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process the loop in its original form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Automatically pipeline the innermost loop with a specific factor:\n   `#pragma ACCEL PIPELINE auto{__PIPE__L2}`\n\n4. Automatically tile the innermost loop with a specific factor:\n   `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`\n\n5. Run the iterations of the parallel loop with specific parallelization factor and reduction variable:\n   `#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}`\n\nApply these natural language instructions to the corresponding #pragma values in the C code to optimize it for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform automatic pipelining for this loop:\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\nProcess this loop in its original, full form without breaking it into smaller chunks:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\nRun the iterations of this loop one after the other, not in parallel:\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{8}\n```\n\nTreat the following function as a hardware kernel for acceleration:\n```c\n#pragma ACCEL kernel\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks and automatically pipeline the loops to optimize performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. For the loop, \"t = 0; t < 40; t++\": Automatically pipeline this loop without any assumption on factor - '#pragma ACCEL PIPELINE auto{off}'\n  \n2. For the loop, \"t = 0; t < 40; t++\" and the loop, \"for (i = 1; i < 120 - 1; i++)\": Process these loops in their original, full form without breaking them into smaller chunks - '#pragma ACCEL TILE FACTOR=auto{1}'\n\n3. For the loop, \"t = 0; t < 40; t++\": Run the iterations of this loop in parallel with a factor of 8 - '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n\n4. For the loop, \"for (i = 1; i < 120 - 1; i++)\": Run the iterations of this loop in parallel with a factor of 16 - '#pragma ACCEL PARALLEL FACTOR=auto{16}'\n\n5. For the loop, \"for (i = 1; i < 120 - 1; i++)\": Run the iterations of this loop in parallel with a factor of 12 - '#pragma ACCEL PARALLEL FACTOR=auto{12}'\n\nBy using these concise natural language instructions, the given HLS codes in C can be generated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n```\nMake the following function a hardware kernel for acceleration, do not automatically pipeline the loops, use a tile factor of 1 for loop optimization, and run the loop iterations in parallel with a factor of 8 for the outer loop and a factor of 4 for the inner loops.\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function in its original, full form without breaking it into smaller chunks. Run the iterations of the inner loop one after the other, not in parallel. Don't automatically pipeline the loops. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C for the given natural language instruction, you can use the following comprehensive but very concise instruction:\n\n'Process the loops in their original form without automatic pipelining, and use a tile factor of 1 for the outermost loop and a tile factor of 4 for the middle loop.'\n\nThis instruction will correspond to the following HLS code:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=1\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=4\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=1\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration. Use automatic pipeline balancing but turning off automatic pipelining for the main loop. Apply auto tiling and parallelization to the nested loops with specific pipeline and tile factors for each level.\"\n\nThis natural language instruction encompasses the process and the specific #pragma values to be used to generate the provided HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks and then automatically pipeline and flatten these loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Ensure the loop is not automatically pipelined: 'Don't automatically pipeline this loop'.\n2. Process the loop in its original, full form without breaking it into smaller chunks: 'Process this loop in its original, full form without breaking it into smaller chunks'.\n3. Run the iterations of this loop one after the other, not in parallel: 'Run the iterations of this loop one after the other, not in parallel'.\n4. Treat the following function as a hardware kernel for acceleration: 'Treat the following function as a hardware kernel for acceleration'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions:\n\n1. Enable automatic pipelining for this loop: \n   Use '#pragma ACCEL PIPELINE auto{flatten}'\n\n2. Set the tiling factor to 'auto' for optimal performance: \n   Use '#pragma ACCEL TILE FACTOR=auto{1}'\n\n3. Set the parallelization factor to 'auto' for the outer loop: \n   Use '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n\n4. Set the parallelization factor to 'auto' for the inner loops: \n   Use '#pragma ACCEL PARALLEL FACTOR=auto{8}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops sequentially, setting the pipeline off, and using a tile factor of 1 for each loop. Additionally, treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the kernel function `doitgen` with hardware acceleration. \nApply pipeline optimization with automatic settings.\nUse tiling with automatic factor settings for loop parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you need to follow these natural language instructions:\n\n1. Annotate the loop to be automatically pipelined with automatic flattening:\n   - Natural language instruction: 'Automatically pipeline and flatten this loop'\n   - Corresponding #pragma: '#pragma ACCEL PIPELINE auto{flatten}'\n\n2. Annotate the loop to be tiled with an automatic factor of 1:\n   - Natural language instruction: 'Automatically tile this loop with a factor of 1'\n   - Corresponding #pragma: '#pragma ACCEL TILE FACTOR=auto{1}'\n\n3. Annotate the loop to be parallelized with an automatic factor of 2:\n   - Natural language instruction: 'Automatically parallelize this loop with a factor of 2'\n   - Corresponding #pragma: '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n\n4. Annotate the nested loop to be parallelized with an automatic factor of 8:\n   - Natural language instruction: 'Automatically parallelize this nested loop with a factor of 8'\n   - Corresponding #pragma: '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n\n5. Annotate the second nested loop to be run sequentially, not in parallel:\n   - Natural language instruction: 'Run this nested loop sequentially, not in parallel'\n   - Corresponding #pragma: '#pragma ACCEL PARALLEL FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops. \n3. Focus on the original, full form of the loop without breaking it into smaller chunks.\n4. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n- For the loop: \"Don't automatically pipeline this loop\"\n  - Use: `#pragma ACCEL PIPELINE \"off\"`\n- For the loop: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n  - Use: `#pragma ACCEL TILE FACTOR=1`\n- For the loop: \"Run the iterations of this loop one after the other, not in parallel\"\n  - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n- For the function: \"Treat the following function as a hardware kernel for acceleration\"\n  - Use: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following concise natural language instruction: \n\n\"Optimize the loops for parallel execution and efficient resource utilization, and enable software pipelining for improved throughput.\"\n\nThis corresponds to the following pragma values:\n\n- `#pragma ACCEL kernel` treats the function as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto{flatten}` enables automatic pipelining with loop flattening.\n- `#pragma ACCEL TILE FACTOR=auto{1}` optimizes loop tiling with a factor of 1.\n- `#pragma ACCEL PIPELINE auto{flatten}` and `#pragma ACCEL TILE FACTOR=auto{8}` are used for the innermost loop to enable automatic pipelining with loop flattening and optimize loop tiling with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel and treat the following function as a hardware kernel for acceleration. Don't automatically pipeline this loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nSet the loop pipeline to manual.\nSet the loop tiling factor to 1.\nSet the pipeline of the innermost loop to manual with factor __PIPE__L1.\nSet the tiling factor of the innermost loop to __TILE__L1.\nSet the pipeline of the middle loop to manual with factor __PIPE__L2.\nSet the tiling factor of the middle loop to __TILE__L2.\nSet the loop parallelization with reduction and factor __PARA__L3.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code in C such that the loops are not automatically pipelined and are processed in their original form without automatic tiling:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction: \n\n\"Transform the loop into a hardware kernel for acceleration, pipeline the loop with automatic flattening, apply tiling with a factor of 1, and run the iterations of the loop in parallel with a factor of 8 for the outer loop and a factor of 1 for the inner loops.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the hardware acceleration of the given C code, use the following comprehensive but very concise natural language instructions for the specific #pragma values:\n\n1. For the outer loop, do not automatically pipeline it: \n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n2. Process the loop in its original, full form without breaking it into smaller chunks: \n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n3. Run the iterations of the following loops in parallel with a factor of 8: \n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{8}\n   ```\n\n4. Treat the following function as a hardware kernel for acceleration: \n   ```c\n   #pragma ACCEL kernel\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Apply pipeline directive with default settings to these loops\" describes the pragma directive '#pragma ACCEL PIPELINE auto{}'. \n\"Break the loops into smaller chunks and run them in parallel with default settings\" describes the pragma directive '#pragma ACCEL TILE FACTOR=auto{1}'.\n\"Apply pipeline directive with default settings and use loop flattening optimization\" describes the pragma directive '#pragma ACCEL PIPELINE auto{flatten}'.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the pragma values:\n\n1. Don't automatically pipeline this loop.\n   - #=> `#pragma ACCEL PIPELINE AUTO{off}`\n\n2. Process this loop in its original, full form without breaking it into smaller chunks.\n   - #=> `#pragma ACCEL TILE FACTOR=AUTO{1}`\n\n3. Run the iterations of this loop one after the other, not in parallel.\n   - #=> `#pragma ACCEL PARALLEL FACTOR=AUTO{1}`\n\n4. Treat the following function as a hardware kernel for acceleration.\n   - #=> `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. To disable automatic loop pipelining:\n   Use the instruction - 'Turn off automatic loop pipelining' to describe '#pragma ACCEL PIPELINE auto{off}'\n\n2. To process the loop in its original, full form without breaking into smaller chunks:\n   Use the instruction - 'Execute the loop with a tile factor of 1' to describe '#pragma ACCEL TILE FACTOR=auto{1}'\n\n3. To run the iterations of the loop one after the other, not in parallel:\n   Use the instruction - 'Execute the loop with a parallel factor of 1' to describe '#pragma ACCEL PARALLEL FACTOR=auto{3}'\n\n4. To treat the following function as a hardware kernel for acceleration:\n   Use the instruction - 'Designate the following function as a hardware kernel' to describe '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration and implement the specified loop optimizations. Enable automatic pipelining and tiling with a factor of 1 for the outermost loops, turn off automatic pipelining for the middle loop, and flatten the innermost loop for parallel execution with a tiling factor of 8.\"\n\nThis instruction covers the pragma values and their specific usage for the given code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the hardware acceleration of the given C code, you should:\n1. Specify that the loop should not be automatically pipelined with `#pragma ACCEL PIPELINE \"off\"`.\n2. Use `#pragma ACCEL TILE FACTOR=1` to ensure the loop is processed in its original, full form without breaking it into smaller chunks.\n3. Utilize `#pragma ACCEL PARALLEL FACTOR=1` to run the iterations of the loop one after the other, not in parallel.\n4. Lastly, declare the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code contains a kernel function 'doitgen' that performs matrix computations. To optimize this code for HLS synthesis, use the following comprehensive natural language instruction:\n\"Automatically pipeline the outer loops and use a tile factor of 4 for the outer loop and 1 for the inner loops. Flatten the innermost loop for parallel execution and mark the function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions:\n- For the first loop: \"Automatically pipeline this loop with loop flattening\"\n- For loop tiling: \"Use an automatic tile factor of 1\"\n- For the outer loop parallelization: \"Parallelize the loop with a parallel factor of automatic 3\"\n- For the inner loop parallelization: \"Parallelize the loop with a parallel factor of automatic 16, and run the iterations one after the other with a factor of 1\"\n- Treat the function as a hardware acceleration kernel: \"Mark the following function as a hardware kernel\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To parallelize hardware kernel 'kernel_doitgen' with loop optimizations, follow these steps:\n\n1. To disable automatic pipelining for the outer loop, use '#pragma ACCEL PIPELINE \"off\"'.\n2. Automatically tile the outer loop with a factor of 4, using '#pragma ACCEL TILE FACTOR=4'.\n3. Disable automatic pipelining for the middle loop with '#pragma ACCEL PIPELINE \"off\"'.\n4. Automatically tile the middle loop with a factor of 1, using '#pragma ACCEL TILE FACTOR=1'.\n5. Flatten the loop to improve data locality with '#pragma ACCEL PIPELINE \"flatten\"'.\n6. Disable automatic pipelining for the inner loop with '#pragma ACCEL PIPELINE \"off\"'.\n7. Automatically tile the inner loop with a factor of 1, using '#pragma ACCEL TILE FACTOR=1'.\n\nEnsure that kernel_doitgen function is annotated with '#pragma ACCEL kernel' to indicate it's a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the 'kernel_doitgen' function is treated as a hardware kernel for acceleration, pipeline the loops automatically except for the innermost loop, and tile the loops with a factor of 1 for the outer two loops and a factor of 4 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instruction:  \n- Apply hardware acceleration to the function `doitgen`  \n- Pipeline the outermost loop of the function  \n- Split the loop iterations into smaller chunks to enable parallel processing  \n- Stop automatic pipelining for the inner loop to process in its original form  \n- Flatten the inner loop for better resource utilization  \n\nThis will result in the provided HLS codes and their corresponding #pragma values in the C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following comprehensive but concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, automatically pipeline the outer loop, use the original full form of tiling without breaking it into smaller chunks, and parallelize the loops with appropriate factors, such as parallelizing the inner loops with a factor of 8 and the final loop with a factor of 1.\"\n\nThis instruction covers the use of the specific #pragma values in the given HLS code:\n\n1. Accelerate the function: '#pragma ACCEL kernel'\n2. Automatically pipeline the outer loop: '#pragma ACCEL PIPELINE auto{flatten}'\n3. Use the original full form of tiling: '#pragma ACCEL TILE FACTOR=auto{1}'\n4. Parallelize the loops with appropriate factors: \n   - Parallelize the inner loop with a factor of 8: '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n   - Parallelize the final loop with a factor of 1: '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n   \nThis single natural language instruction provides a comprehensive guide to generating the given HLS code in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops using hardware acceleration by automatically pipelining and tiling to increase parallelism. Use a parallel factor for reduction in the innermost loop. Treat the given function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should use the following comprehensive natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, automatically pipeline the loops, use a tiling factor of 4 for the outer loop, and a parallel factor of 13 for the outer loop. The inner loops should be parallelized with a reduction factor for x1 and x2 respectively.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{13}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration\n```c\n#pragma ACCEL kernel\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE off\n  \n#pragma ACCEL TILE FACTOR=1\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE off\n    \n#pragma ACCEL TILE FACTOR=1\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE flatten\n      \n#pragma ACCEL TILE FACTOR=1\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "In order to generate the given HLS codes, you would use the following natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration: \n   - \"Treat the following function as a hardware kernel for acceleration\"\n\n2. Disable automatic pipelining for the outermost loop: \n   - \"Don't automatically pipeline the outermost loop\"\n\n3. Set the tiling factor for the outermost loop to 2: \n   - \"Break the outermost loop into chunks of size 2\"\n\n4. Disable automatic pipelining for the second loop: \n   - \"Don't automatically pipeline the second loop\"\n\n5. Set the tiling factor for the second loop to 1: \n   - \"Process the second loop in its original, full form without breaking it into smaller chunks\"\n\n6. Enable loop flattening for the innermost loop: \n   - \"Flatten the innermost loop for parallel processing\"\n\n7. Set the tiling factor for the innermost loop to 1: \n   - \"Process the innermost loop in its original, full form without breaking it into smaller chunks\"\n\nThese natural language instructions would help to generate the corresponding HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, don't automatically pipeline the outer loop, and execute the loops in their original, full form without breaking them into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "- To stop the compiler from automatically pipelining the loop, use '#pragma ACCEL PIPELINE \"off\"'.\n- For the original, full form of the loop without breaking it into smaller chunks, use '#pragma ACCEL TILE FACTOR=1'.\n- To run the loop iterations one after the other, not in parallel, use '#pragma ACCEL PARALLEL FACTOR=1'.\n- Treat the function as a hardware kernel for acceleration by using '#pragma ACCEL kernel'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the following function as a hardware kernel, and apply automatic offloading for loop pipelining and tiling with specific factors. Additionally, parallelize the inner loop with reduction for optimization.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Process the loop without automatic pipelining: \n   '#pragma ACCEL PIPELINE auto{off}'\n   \n2. Keep the loop in its original, full form without tiling:\n   '#pragma ACCEL TILE FACTOR=auto{1}'\n   \n3. Run iterations of the loop in parallel with a factor of 8:\n   '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n   \n4. Treat the function as a hardware kernel for acceleration:\n   '#pragma ACCEL kernel'\n\nApply these instructions to the corresponding loops and function in your C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. For the kernel function `kernel_doitgen`, treat it as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n\n2. For the outer loop over variable `r`, apply tiling and define a tile factor using `#pragma ACCEL TILE FACTOR=8`.\n\n3. For the inner loop over variable `q`, disable automatic pipelining by specifying `#pragma ACCEL PIPELINE auto{off}`.\n\n4. For the innermost loop over variable `p`, flatten the loop and disable automatic pipelining using `#pragma ACCEL PIPELINE auto{flatten}`.\n\nBy following these natural language instructions, the given HLS codes can be generated in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Convert this function into a hardware kernel for acceleration and apply auto-pipelining, auto-tiling, and auto-parallelization with specific factors for the loops'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{13}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the outer loop with flattening. Tile the loop with a factor of 8 and parallelize it with a factor of 13. Parallelize the inner loop with a reduction of x1 and a specified factor. Automatically pipeline the outer loop with a specific factor and tile the loop with a specific factor. Parallelize the loop with a reduction of x2 and a specified factor.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{13}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, then for the nested loops of iterations, don't automatically pipeline the loops, and process each loop in its full form without breaking it into smaller chunks, and lastly run the iterations of the innermost loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration, automatically pipeline the outer loop, tile the loop with a factor of 1, parallelize the outer loop with a factor of 8, and parallelize the inner loops with a factor of 1 and 8 respectively.\"\n\nThis concise instruction covers the necessary pragma values and their corresponding process for the given HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Apply hardware acceleration to the function 'doitgen'. Disable automatic pipeline generation for all loops and process each loop in its full form without partitioning. Set the tile factor for each loop automatically to 1. Parallelize the innermost loop, with a parallelization factor determined automatically. Treat the entire function as a hardware kernel for acceleration.\"\n\nThese instructions will help to generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop\n   - `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process each loop in its original, full form without breaking it into smaller chunks\n   - `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of each loop one after the other, not in parallel\n   - `#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}`\n\nThese instructions will guide the compiler to apply the specified optimizations and transformations to the code, enabling hardware acceleration and improving performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following natural language instructions:\n\n- For the directive to turn off automatic pipelining for the loop, use \"Don't automatically pipeline this loop\".\n\n- To specify the tile factor for the loop as 1, use \"Process this loop with a tile factor of 1 without breaking it into smaller chunks\".\n\n- For the directive to flatten the loop, use \"Flatten the loop to improve parallelism\".\n\n- To treat the function as a hardware kernel for acceleration, use \"Mark the following function as a hardware kernel for acceleration\".", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply no pipeline to this loop and let the compiler decide the pipeline strategy '#pragma ACCEL PIPELINE auto{off}'. Use the tile factor 1 for this loop '#pragma ACCEL TILE FACTOR=auto{1}'. Use automatic pipeline strategy for this loop and flatten the loop hierarchy '#pragma ACCEL PIPELINE auto{flatten}'. Use the tile factor 4 for this loop '#pragma ACCEL TILE FACTOR=auto{4}'. This function should be treated as a hardware kernel for acceleration '#pragma ACCEL kernel'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, use the following comprehensive but concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the outermost loop of the function.\n\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of the middle loop one after the other, not in parallel.\n\n5. Treat the innermost loop as a candidate for automatic pipeline optimization.\n\n6. Process the middle loop in its original, full form without breaking it into smaller chunks.\n\n7. Run the iterations of the innermost loop in parallel and use automatic loop parallelization factor optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the #pragma values:\n- For loop flattening and automatic pipeline: \"Automatically flatten and pipeline this loop\"\n- For tiling with factor 1: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n- For parallelizing with factor 2: \"Run the iterations of this loop in parallel with a factor of 2\"\n- For parallelizing with factor 3: \"Run the iterations of this loop in parallel with a factor of 3\"\n- For parallelizing with factor 1: \"Run the iterations of this loop one after the other, not in parallel\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should follow the natural language instructions as follows:\n\n1. Identify the function as a hardware kernel for acceleration: Use the pragma '#pragma ACCEL kernel' before the function definition.\n\n2. Process the loop in its original, full form without breaking it into smaller chunks: Use the pragma '#pragma ACCEL TILE FACTOR=1' before the loop.\n\n3. Don't automatically pipeline this loop: Don't use any specific pragma for this case, meaning the loop will not be automatically pipelined.\n\n4. Run the iterations of this loop one after the other, not in parallel: Use the pragma '#pragma ACCEL PARALLEL FACTOR=1' before the loop.\n\n5. Process the loop in a way that allows it to be automatically pipelined: Use the pragma '#pragma ACCEL PIPELINE auto{}' before the loop.\n\n6. Run the iterations of this loop in parallel, with the given parallelization factor: Use the pragma '#pragma ACCEL PARALLEL FACTOR=8' before the loop.\n\nApplying these natural language instructions will generate the given HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline the loops. The loops should be tiled with a factor of 8 and run in parallel with a factor of 1. Inside the kernel, the first loop should be parallelized with a reduction for x1 using factor __PARA__L2, and the second loop should be parallelized with a reduction for x2 using factor __PARA__L3. Additionally, the second set of loops should be pipelined with factor __PIPE__L1 and tiled with factor __TILE__L1, running in parallel with factor __PARA__L1.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipeline off and factor 2 tiling for outer loops. Inside, process the loops with automatic pipeline and factor 1 tiling. Flatten the innermost loop to achieve parallelism. Also, mark the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code for the following C function using the following natural language instruction:\n\n\"Enable pipeline optimization for the entire loop. Do not allow auto-pipelining.\" \n- #pragma ACCEL PIPELINE auto{off}\n\n\"Break the loop into smaller chunks for processing in its original, full form. Auto-tile with a factor of 1.\"\n- #pragma ACCEL TILE FACTOR=auto{1}\n\n\"Enable pipeline optimization for the innermost loop. Auto-pipeline with a specific factor for the innermost loop.\"\n- #pragma ACCEL PIPELINE auto{__PIPE__L1}\n\n\"Auto-tile with a specific factor for the innermost loop. Ensure auto-pipelining for the innermost loop.\"\n- #pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n\n\"Enable pipeline optimization for the second innermost loop. Auto-pipeline with a specific factor for the second innermost loop.\"\n- #pragma ACCEL PIPELINE auto{__PIPE__L2}\n\n\"Auto-tile with a specific factor for the second innermost loop. Ensure auto-pipelining for the second innermost loop.\"\n- #pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n\n\"Run the iterations of the third innermost loop one after the other, not in parallel. Enable parallelism with a specific factor for the third innermost loop.\"\n- #pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\n\"Mark the function as a hardware kernel for acceleration.\"\n- #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the following loop. \n\n3. Process the following loop with a tile factor of 2 to break it into smaller chunks automatically.\n\n4. Run the iterations of the following loop in parallel with a factor of 13 automatically.\n\n5. Automatically pipeline the following loop with a factor that flattens the loop structure.\n\n6. Process the following loop with a tile factor that breaks it into smaller chunks automatically.\n\n7. Run the iterations of the following loop in parallel with a factor that reduces x1 automatically.\n\n8. Automatically pipeline the following loop with a factor that optimizes pipeline at level 1.\n\n9. Process the following loop with a tile factor that optimizes tiling at level 1.\n\n10. Run the iterations of the following loop in parallel with a factor that reduces x2 automatically.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{13}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the performance of the given code in hardware, use the following pragmas:\n- `#pragma ACCEL kernel`: Treat the following function as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto{}`: Automatically pipeline this loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}`: Break the loop into smaller chunks, each iterating once in its original, full form.\n- `#pragma ACCEL PARALLEL FACTOR=auto{16}`: Run the iterations of this loop in parallel with a factor of 16.\n- Inside the first loop:\n  - `#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}`: Run the iterations of this inner loop in parallel with a reduction operation for variable x1. The parallel factor is automatically determined.\n- `#pragma ACCEL PIPELINE auto{__PIPE__L1}`: Automatically pipeline this loop with specific pipeline settings.\n- `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`: Break the loop into smaller chunks with specific tiling settings.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`: Run the iterations of this loop in parallel with specific parallel settings.\n- Inside the second loop:\n  - `#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}`: Run the iterations of this inner loop in parallel with a reduction operation for variable x2. The parallel factor is automatically determined.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration, and apply the following directives: \n- 'Don't automatically pipeline this loop'\n- 'Process this loop in its original, full form without breaking it into smaller chunks'\n- 'Run the iterations of this loop one after the other, not in parallel'\n- 'Auto pipeline this loop'\n- 'Auto tile this loop'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   - Use the pragma: `#pragma ACCEL kernel`\n\n2. Automatically pipeline the outermost loop.\n   - Use the pragma: `#pragma ACCEL PIPELINE auto{}`\n\n3. Tile the outer loop with a factor determined automatically by the compiler.\n   - Use the pragma: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Turn off automatic pipelining for the middle loop.\n   - Use the pragma: `#pragma ACCEL PIPELINE auto{off}`\n\n5. Tile the middle loop with a factor determined automatically by the compiler and flatten it for better performance.\n   - Use the pragma: `#pragma ACCEL PIPELINE auto{flatten}`\n\n6. Tile the innermost loop with a factor determined automatically by the compiler.\n   - Use the pragma: `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Pipeline the innermost loop with a flattening factor of 8, and tile the outer loops with a factor of 1 to optimize for hardware acceleration.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n2. Don't automatically pipeline this loop\n3. Process this loop in its original, full form without breaking it into smaller chunks\n4. Process the loop with a tile factor of 4\n5. Automatically pipeline this loop with flattening\n6. Process the innermost loop with a tile factor of 1\n\nBy following these instructions, you'll be able to generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline the outermost loop. Process the loops using an automatic tiling factor of 1 for the outermost loop and 8 for the middle loop. Use automatic flattening for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you would use the following natural language instructions for the specific #pragma values:\n\n1. \"Treat the following loop as a hardware kernel for acceleration\": #pragma ACCEL kernel\n2. \"Don't automatically pipeline this loop\": #pragma ACCEL PIPELINE \"off\"\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\": #pragma ACCEL TILE FACTOR=1\n4. \"Run the iterations of this loop one after the other, not in parallel\": #pragma ACCEL PARALLEL FACTOR=1", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original full form without breaking them into smaller chunks, and implement hardware-specific optimization to pipeline and parallelize the loops according to their data dependencies and resource constraints.\n- For the outermost loop: `#pragma ACCEL PIPELINE \"off\"`\n- For the middle loop: `#pragma ACCEL TILE FACTOR=1`\n- For the innermost loop: `#pragma ACCEL TILE FACTOR=4`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Make the 'kernel_doitgen' function a hardware-accelerated kernel, and process the loops using pipeline and tiling with specific factors as follows: \n- Disable automatic pipelining for the outermost loop \n- Tile the outer loop with a factor of 1 \n- Disable automatic pipelining for the middle loop \n- Tile the middle loop with a factor of 2 \n- Flatten the parallel hierarchy for the innermost loop \n- Tile the innermost loop with a factor of 1.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the code for the function 'kernel_doitgen' is considered as a hardware kernel for acceleration.\n\nPerform the following optimizations:\n1. Disable automatic pipelining for the outermost loop.\n2. Use tiling with a factor of 1 for the outermost loop.\n3. Enable automatic pipelining for the first inner loop.\n4. Use automatic tiling for the first inner loop.\n5. Enable automatic pipelining for the second inner loop.\n6. Use automatic tiling for the second inner loop.\n7. Perform parallelization with reduction for the innermost loop.\n8. Ensure the iterations of the innermost loop are run in a sequence, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given C function using the following natural language instruction:\n\"Treat the following function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Use the following function as a hardware kernel for acceleration. Pipeline the outer loop and tile its computation into smaller chunks of size 2. Run the iterations of the outer loop in parallel with parallelization factor 1. Inside the outer loop, parallelize the inner loop with reduction of variable x1, and with a parallelization factor of __PARA__L2. Pipeline the second outer loop and tile its computation into smaller chunks of size __TILE__L1. Run the iterations of the second outer loop in parallel with parallelization factor 1. Inside the second outer loop, parallelize the inner loop with reduction of variable x2, and with a parallelization factor of __PARA__L3.\"\n\nThis instruction will result in the given HLS codes written in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, you can follow these comprehensive but very concise natural language instructions:\n\nProcess the following function as a hardware kernel for acceleration.\nFor the outermost loop, don't automatically pipeline this loop.\nFor the outermost loop, process this loop in its original, full form without breaking it into smaller chunks, and use a tile factor of 8.\nFor the middle loop, automatically pipeline the loop and process it with a tile factor of 1.\nFor the innermost loop, automatically pipeline the loop using 'flatten' and process it with a tile factor of 1.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function as a hardware kernel for acceleration and then for the loops with 'r', 'q', and 'p', apply auto{off} pipeline pragma, auto{2} tile factor pragma, and flatten pipeline pragma respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n1. Mark the function as a hardware kernel for acceleration by adding '#pragma ACCEL kernel' before the function definition.\n2. Turn off automatic loop pipelining by adding '#pragma ACCEL PIPELINE \"off\"' just before the outermost loop.\n3. Keep the loops in their original, full form without breaking them into smaller chunks by adding '#pragma ACCEL TILE FACTOR=1' before the outermost loop.\n4. Automatically pipeline the loop with a specific factor by adding '#pragma ACCEL PIPELINE auto{<factor>}' and '#pragma ACCEL TILE FACTOR=auto{<factor>}' before each loop that needs pipelining and tiling.\n5. Run the iterations of the innermost loop one after the other, not in parallel, with a reduction by adding '#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{<factor>}' before the innermost loop.\n\nFollowing these instructions will help generate the given HLS codes in C with the appropriate #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that this function is treated as a hardware kernel for acceleration, and process the nested loops using tiling with a factor of 1 for the outermost loop and 4 for the middle loop to improve data locality. Also, disable automatic loop pipelining to maintain the loop structure.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, you should follow the following concise instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outer loop.\n3. Process the outer loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of the outer loop in parallel with a factor of 8.\n5. Run the iterations of the first inner loop one after the other, not in parallel.\n6. Run the iterations of the second inner loop in parallel with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the specific #pragma values in C:\n\n1. 'Don't automatically pipeline this loop' - for '#pragma ACCEL PIPELINE auto{off}'\n2. 'Process this loop in its original, full form without breaking it into smaller chunks' - for '#pragma ACCEL TILE FACTOR=auto{1}'\n3. 'Run the iterations of this loop one after the other, not in parallel' - for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}', and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n4. 'Treat the following function as a hardware kernel for acceleration' - for '#pragma ACCEL kernel'\n\nUse these instructions to effectively generate the given HLS codes in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use hardware acceleration for the function 'kernel_doitgen' and apply automatic pipeline and tiling optimizations with default or calculated factors for loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, automatically pipeline the outer loop, tile it with a factor of 2, and run the outer loop iterations in parallel with a factor of 13. For the inner loops, automatically parallelize with reduction for x1 with a factor of __PARA__L2, and for x2 with a factor of __PARA__L3. Finally, pipeline the inner loops with a factor of __PIPE__L1 and tile them with a factor of __TILE__L1 and run them in parallel with factors of __PARA__L1.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{13}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original full form without breaking it into smaller chunks\n#pragma ACCEL TILE FACTOR=1\n\nDon't automatically pipeline this loop, let the tool decide whether to apply pipelining\n#pragma ACCEL PIPELINE \"off\"\n\nRun the iterations of this loop one after the other, not in parallel\n#pragma ACCEL PARALLEL FACTOR=1\n\nTreat the following function as a hardware kernel for acceleration\n#pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes as follows:\n\n- Enclose the following function as a hardware kernel for acceleration: '#pragma ACCEL kernel'\n\n- Disable automatic loop pipelining: '#pragma ACCEL PIPELINE auto{off}'\n\n- Set the tiling factor to 4 automatically for the loop: '#pragma ACCEL TILE FACTOR=auto{4}'\n\n- Run the iterations of the loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n\n- Perform parallel reduction on array x1 and set the factor automatically for the loop: '#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}'\n\n- Enable automatic loop pipelining for the following loop with label: '__PIPE__L1': '#pragma ACCEL PIPELINE auto{__PIPE__L1}'\n\n- Automatically set the tiling factor for the loop with label: '__TILE__L1': '#pragma ACCEL TILE FACTOR=auto{__TILE__L1}'\n\n- Automatically set the parallel factor for the loop with label: '__PARA__L1': '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'\n\n- Perform parallel reduction on array x2 and set the factor automatically for the loop: '#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original form and without breaking them into smaller chunks. Automatically pipeline the loops for potential performance improvement. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the loop with variable 'j', don't automatically pipeline it:\n   - Natural language instruction: \"Turn off automatic pipelining for this loop.\"\n\n2. Process the loop with an automatic tile factor of 1:\n   - Natural language instruction: \"Use an automatic tile factor of 1 for this loop.\"\n\n3. Run the iterations of the following loop one after the other, not in parallel:\n   - Natural language instruction: \"Execute this loop without parallelizing the iterations.\"\n\n4. Treat the 'kernel_covariance' function as a hardware kernel for acceleration:\n   - Natural language instruction: \"Designate the 'kernel_covariance' function as an acceleration kernel.\"\n\nI hope that helps! If you have any further questions, please, let me know.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration, pipeline all loops automatically, and tile the loops with a factor of 8 for the outer loop and a factor of 1 for the inner loops.\"\n\nThis instruction covers the specific #pragma values to use for the given HLS codes, including '#pragma ACCEL kernel', '#pragma ACCEL PIPELINE auto', and '#pragma ACCEL TILE FACTOR=auto{8}' for the outer loop and '#pragma ACCEL TILE FACTOR=auto{1}' for the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire kernel function 'doitgen' as a hardware acceleration kernel and enable automatic pipelining for the outer loops. Additionally, apply a tiling factor of 1 for each of the three nested loops to optimize memory access and parallelization. For the innermost loop, utilize flattening to optimize pipelining and resource utilization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Process this loop in its original, full form without breaking it into smaller chunks' would be a comprehensive but very concise natural language instruction used to generate '#pragma ACCEL TILE FACTOR=1'. \n\n'Run the iterations of this loop one after the other, not in parallel' would be a comprehensive but very concise natural language instruction used to generate '#pragma ACCEL PARALLEL FACTOR=1'.\n\n'Treat the following function as a hardware kernel for acceleration' would be a comprehensive but very concise natural language instruction used to generate '#pragma ACCEL kernel'.\n\n'Don't automatically pipeline this loop' would be a comprehensive but very concise natural language instruction used to generate '#pragma ACCEL PIPELINE off'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specified #pragma values:\n\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nThese instructions correspond to the following #pragma values:\n1. #pragma ACCEL kernel\n2. #pragma ACCEL PIPELINE auto{off}\n3. #pragma ACCEL TILE FACTOR=auto{1}\n4. #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}, #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}, #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}, #pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}, #pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}, #pragma ACCEL PIPELINE auto{__PIPE__L5}, #pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n\nBy following these instructions, the HLS codes can be generated in the C language.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following natural language instructions:\n\n1. For the function `kernel_jacobi_1d`, treat it as a hardware kernel for acceleration.\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. For the outer loop, don't automatically pipeline this loop.\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. For the outer loop, process it in its original, full form without breaking it into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. For the outer loop, run the iterations of this loop one after the other, not in parallel.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n5. For the first inner loop, run the iterations of this loop in parallel with a factor of 1.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n6. For the second inner loop, run the iterations of this loop in parallel with a factor of 8.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=8\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following pragmas:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop:\n   `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   `#pragma ACCEL PARALLEL FACTOR=1`\n\nYou can use these instructions to guide the code generation process.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration - '#pragma ACCEL kernel'\n2. Don't automatically pipeline this loop - '#pragma ACCEL PIPELINE \"off\"'\n3. Process this loop in its original, full form without breaking it into smaller chunks - '#pragma ACCEL TILE FACTOR=1'\n4. Run the iterations of this loop one after the other, not in parallel - '#pragma ACCEL PARALLEL FACTOR=1'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks and run the iterations of the loops one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration, but do not automatically pipeline the loops, and ensure they are not divided into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes in C using the following natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel\n\nThese instructions generate the following HLS codes in C:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE off\n  \n#pragma ACCEL TILE FACTOR=1\n  \n#pragma ACCEL PARALLEL FACTOR=1\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=__PARA__L2\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE __PIPE__L1\n  \n#pragma ACCEL TILE FACTOR=__TILE__L1\n  \n#pragma ACCEL PARALLEL FACTOR=__PARA__L1\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=__PARA__L3\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n```", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply the following pragma values for the given C code to leverage HLS acceleration:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   - `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop:\n   - `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop with its original, full form without breaking it into smaller chunks:\n   - `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   - `#pragma ACCEL PARALLEL FACTOR=1`\n\n5. Automatically pipeline this loop with the specific factor:\n   - `#pragma ACCEL PIPELINE auto{__PIPE__L5}`\n\n6. Automatically tile this loop with the specific factor:\n   - `#pragma ACCEL TILE FACTOR=auto{4}`\n\n7. Automatically parallelize this loop with the specific factor:\n   - `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` (and similar for other loops)", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: '#pragma ACCEL kernel'\n\n2. Don't automatically pipeline this loop: '#pragma ACCEL PIPELINE \"off\"'\n\n3. Process this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=1'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Process each loop in its original, full form without breaking it into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, you should:\n- Treat the following function as a hardware kernel for acceleration using `#pragma ACCEL kernel`\n- Disable automatic pipelining for the outermost loop using `#pragma ACCEL PIPELINE auto{off}`\n- Tile the outermost loop with a factor of 8 using `#pragma ACCEL TILE FACTOR=8`\n- Enable automatic pipelining for the inner loops using `#pragma ACCEL PIPELINE auto{}`\n- Tile the inner loops with a factor of 1 using `#pragma ACCEL TILE FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform automatic pipelining for the entire kernel function. For the outermost loop, break it into chunks of size 2, and don't automatically pipeline it. For the middle loop, process it without breaking it into smaller parts and flatten the pipeline. For the innermost loop, process it without breaking it into smaller parts and flatten the pipeline.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instruction:\n\n\"Designate the function as a hardware kernel for acceleration, pipeline the outer two loops automatically, and tile the loops with a factor of 1 and 2 respectively, while specifying pipeline off and flatten optimizations.\"\n\nThis instruction encompasses the specific #pragma values and process to be applied to the given HLS codes to achieve the desired hardware acceleration behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C, use the following comprehensive but concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Process this loop in its original, full form without breaking it into smaller chunks and run the iterations of this loop one after the other, not in parallel.\n\n3. Don't automatically pipeline this outer loop and process its iterations in a way that they are not broken into smaller chunks.\n\n4. Automatically pipeline the inner loop and flatten its iterations for efficient processing.\n\nThese instructions will map directly to the following #pragma values:\n\n1. #pragma ACCEL kernel\n\n2. #pragma ACCEL TILE FACTOR=1\n   #pragma ACCEL PARALLEL FACTOR=1\n\n3. #pragma ACCEL PIPELINE auto{off}\n   #pragma ACCEL TILE FACTOR=auto{1}\n\n4. #pragma ACCEL PIPELINE auto{}\n   #pragma ACCEL TILE FACTOR=auto{1}\n   #pragma ACCEL PIPELINE auto{flatten}", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes written in C for the given natural language instructions:\n\n1. Process the loop in its original, full form without breaking it into smaller chunks and don't automatically pipeline this loop.\n   ```c\n   void kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30]) {\n       int r;\n       int q;\n       int p;\n       int s;\n       // ...\n   }\n   ```\n\n2. Run the iterations of the loops one after the other, not in parallel and treat the following function as a hardware kernel for acceleration.\n   ```c\n   #pragma ACCEL kernel\n   void kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30]) {\n       // ...\n   }\n   ```\n\n3. Pipeline the loops automatically with a factor of 1.\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   // ...\n   ```\n\n4. Tile the loops automatically with a factor of 4 for the outer loop and a factor of 1 for the inner loops.\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{4}\n   // ...\n   #pragma ACCEL TILE FACTOR=auto{1}\n   // ...\n   ```\n\n5. Pipeline and flatten the innermost loop automatically.\n   ```c\n   #pragma ACCEL PIPELINE auto{flatten}\n   // ...\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and apply the following pragmas: For the outermost loop, don't automatically pipeline and process the loop in its original, full form without breaking it into smaller chunks. For the second loop, automatically pipeline the loop and process it in its original, full form without breaking it into smaller chunks. For the innermost loop, automatically pipeline, flatten the loop, and process it in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To compile this C code to HLS, you can use the following directives:\n- Use '#pragma ACCEL kernel' to indicate that the function 'doitgen' should be treated as a hardware kernel for acceleration.\n- Use '#pragma ACCEL PIPELINE auto{}' to enable automatic loop pipelining for the outermost loop.\n- Use '#pragma ACCEL PIPELINE auto{off}' to turn off automatic loop pipelining for the inner loop.\n- Use '#pragma ACCEL TILE FACTOR=auto{8}' to allow the compiler to determine the tiling factor for the middle loop.\n- Use '#pragma ACCEL PIPELINE auto{flatten}' to flatten the loop for better parallelization.\n- Use '#pragma ACCEL TILE FACTOR=auto{1}' to allow the compiler to determine the tiling factor for the inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and implement the following directives:\n1. Automatically pipeline the outermost loop, without dividing into smaller chunks\n2. Automatically tile the outermost loop into a factor of 1\n3. Flatten the middle loop for better parallelism\n4. Automatically tile the middle loop into a factor of 8", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   \n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n\n   `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n5. Don't automatically pipeline this loop.\n\n   `#pragma ACCEL PIPELINE auto{off}`\n\n6. Use a specific tile factor to optimize the loop.\n\n   `#pragma ACCEL TILE FACTOR=auto{2}`\n\n7. Run the iterations of this loop in parallel with a specific parallel factor.\n\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n8. Don't automatically pipeline this loop.\n\n   `#pragma ACCEL PIPELINE auto{}`\n\n9. Use a specific tile factor to optimize the loop.\n\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n10. Run the iterations of this loop in parallel with a specific parallel factor.\n\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:  \n   - Use `#pragma ACCEL kernel` before the function definition.\n\n2. Don't automatically pipeline this loop:  \n   - Use `#pragma ACCEL PIPELINE auto{off}` before the loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:  \n   - Use `#pragma ACCEL TILE FACTOR=auto{1}` for the loop to maintain its original form.\n\n4. Run the iterations of this loop one after the other, not in parallel:  \n   - Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` for sequential execution.\n\n5. Set the pipeline factor for the loop:  \n   - Use `#pragma ACCEL PIPELINE auto{flatten}` to set the pipeline factor for flattening the loop.\n\n6. Use the appropriate parallel factor for the loop:  \n   - Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` for different parallelization factors within the loops.\n\n7. Apply reductions within the loops:  \n   - Use `#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}` and `#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}` for the specified reductions.\n\n8. Use pipeline factors and parallel reductions within nested loops:  \n   - Use `#pragma ACCEL PIPELINE auto{__PIPE__L5}` and `#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}` for nested loop optimizations.\n\nBy following these natural language instructions and adding the corresponding #pragma values to the code, the HLS synthesis tool will generate the desired hardware design from the C code.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loops within the function, do not automatically pipeline them and process them in their original, full form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline the loops. Break the loops into smaller chunks with a factor of 8. Run the iterations of the loops in parallel with a factor of 1. Then, do not automatically pipeline the loops. Keep the loops in their original, full form. Run the iterations of the loops in parallel with a factor of 1. Flatten the loops automatically. Break the loops into smaller chunks with a factor of 1. Run the iterations of the loops in parallel with a factor of 1.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following comprehensive but concise natural language instruction for specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To pipeline the outer loop and flatten the inner loop, use the following pragma values:\n- For the outer loop: `#pragma ACCEL PIPELINE auto{}` and `#pragma ACCEL TILE FACTOR=1`\n- For the inner loop: `#pragma ACCEL PIPELINE auto{off}` and `#pragma ACCEL TILE FACTOR=2`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the outermost loop without automatic pipelining. Use a tile factor of 1 for the outermost loop and its inner loops. Pipeline the inner loops with specific pipeline factors. Use parallel execution with a reduction for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Optimize loop parallelism and data transfer for hardware acceleration using HLS pragmas.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. \"Treat the following function as a hardware kernel for acceleration\" for '#pragma ACCEL kernel'\n2. \"Don't automatically pipeline this loop\" for '#pragma ACCEL PIPELINE auto{off}'\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\" for '#pragma ACCEL TILE FACTOR=auto{1}'\n4. \"Run the iterations of this loop one after the other, not in parallel\" for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}', and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n   Note: The specific values for \"__PARA__L0\", \"__PARA__L1\", and \"__PARA__L2\" have not been provided, and may need to be determined based on the specific hardware and optimization requirements.\n5. \"Handle the pipeline automatically in a flattened manner\" for '#pragma ACCEL PIPELINE auto{flatten}' and '#pragma ACCEL PIPELINE auto{__PIPE__L5}'\n\nBy following these instructions, the given HLS codes in C can be generated.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes in C using the following natural language instructions:\n\n1. \"Treat the following function as a hardware kernel for acceleration.\"\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m, int n, double float_n, double data[100][80], double cov[80][80], double mean[80]) {\n  // code omitted for brevity\n}\n```\n\n2. \"Don't automatically pipeline this loop.\"\n\n```c\n#pragma ACCEL PIPELINE \"off\"\n  // code omitted for brevity\n```\n\n3. \"Process this loop in its original, full form without breaking it into smaller chunks.\"\n\n```c\n#pragma ACCEL TILE FACTOR=1\n  // code omitted for brevity\n```\n\n4. \"Run the iterations of this loop one after the other, not in parallel.\"\n\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n  // code omitted for brevity\n```", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following directives to the loops in the code: Turn off automatic loop pipelining for the outermost loop, set the tiling factor to 8 for the outermost loop, and turn off automatic loop pipelining and set the tiling factor to 1 for the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Pipeline the loops automatically and use a tiling factor of 1 for each loop, treating the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nDo not automatically pipeline the loops in this function.\n\nProcess the loops in their original, full form without breaking them into smaller chunks.\n\nRun the iterations of these loops one after the other, not in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS directives for the following C code:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n```\n\nConcise Instructions:\n- At the outermost loop, turn off automatic pipelining using `#pragma ACCEL PIPELINE auto{off}`\n- Use a tiling factor of 8 for the outer loop with `#pragma ACCEL TILE FACTOR=auto{8}`\n- For the middle loop, turn on automatic pipeline with `#pragma ACCEL PIPELINE auto{}`\n- Use a tiling factor of 1 for the middle loop with `#pragma ACCEL TILE FACTOR=auto{1}`\n- For the innermost loop, flatten the pipeline with `#pragma ACCEL PIPELINE auto{flatten}`\n- Use a tiling factor of 1 for the innermost loop with `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Describe the process of codes and which specific #pragma values to use respectively in natural languages:\n\nTo treat the following function as a hardware kernel for acceleration, use '#pragma ACCEL kernel'. \n\nTo turn off automatic pipelining for a certain loop, use '#pragma ACCEL PIPELINE auto{off}'.\n\nTo process a loop in its original, full form without breaking it into smaller chunks, use '#pragma ACCEL TILE FACTOR=auto{1}'.\n\nTo run the iterations of a loop one after the other, not in parallel, use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' for the first loop, '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' for the second loop, and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' for the third loop.\n\nPlease note that the specific values for pipeline, tile factor, and parallel factor were not given, so they are placeholders as '__PIPE__L5', '__PARA__L3', '__PARA__L4', '__PARA__L6' depending on the specific task.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, and pipeline the outer loop, tile the outer two loops with a factor of 2, and flatten the inner loop to execute it sequentially.\"\n\nThis instruction conveys the use of specific #pragma values, including '#pragma ACCEL kernel', '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{2}', and '#pragma ACCEL PIPELINE auto{flatten}' to achieve the desired HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. Disable automatic loop pipelining: '#pragma ACCEL PIPELINE auto{off}'\n2. Use an automatic tile factor of 1 to process the loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Enable automatic parallelization with a factor of 8 to run loop iterations in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nAvoid automatically pipelining the loops within this function.\n\nProcess the loops using their original, full form without breaking them into smaller chunks.\n\nRun the iterations of the loops one after the other, not in parallel for Factor L0; parallelize the reduction for the 'mean' variable.\n\nAvoid automatically pipelining the loops within this function.\n\nProcess the loops using their original, full form without breaking them into smaller chunks.\n\nRun the iterations of the loops one after the other, not in parallel for Factor L1; parallelize the reduction for the 'data' variable.\n\nAvoid automatically pipelining the loops within this function.\n\nProcess the loops with a tile factor of 2.\n\nRun the iterations of the loops in parallel for Factor L2; parallelize the reduction for the 'cov' variable.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and apply automatic pipelining, automatic tiling with a factor of 1, and automatic parallelization with a factor of 8.\"\n\nThis instruction encompasses all the required #pragma values mentioned in the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that this function is treated as a hardware kernel for acceleration and process the loops in their original, full form without breaking them into smaller chunks. Then, run the iterations of the loops in parallel, and automatically pipeline the loops where possible. Finally, apply specific factors and tile values for individual loops to optimize the hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. 'Do not automatically pipeline this loop' - Use '#pragma ACCEL PIPELINE auto{off}'\n\n2. 'Process this loop in its original, full form without breaking it into smaller chunks' - Use '#pragma ACCEL TILE FACTOR=auto{1}'\n\n3. 'Run the iterations of this loop one after the other, not in parallel' - Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n\n4. 'Treat the following function as a hardware kernel for acceleration' - Use '#pragma ACCEL kernel'\n\nBy following these natural language instructions, you can replicate the given HLS codes written in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Enable automatic pipelining for the loop: \n   Use the instruction \"Automatically apply pipeline optimization to this loop function.\"\n\n2. Set the tile factor to the default value of 1 for the loop: \n   Use the instruction \"Use the default tiling factor for this loop.\"\n\n3. Run the loop iterations in parallel with a factor value determined automatically:\n   Use the instruction \"Parallelize this loop with the parallel factor automatically determined.\"\n\n4. Turn off automatic pipelining for the loop: \n   Use the instruction \"Disable automatic pipeline optimization for this loop.\"\n\n5. Set the tiling factor to the default value of 1 for the loop: \n   Use the instruction \"Use the default tiling factor for this loop.\"\n\n6. Run the loop iterations in parallel with a factor value determined automatically:\n   Use the instruction \"Parallelize this loop with the parallel factor automatically determined.\"\n\n7. Flatten the loop for better pipelining optimization:\n   Use the instruction \"Flatten this loop for better pipeline optimization.\"\n\n8. Set the tile factor to the default value of 1 for the loop: \n   Use the instruction \"Use the default tiling factor for this loop.\"\n\n9. Run the loop iterations in parallel with a factor value determined automatically:\n   Use the instruction \"Parallelize this loop with the parallel factor automatically determined.\"\n\n10. Apply pipeline optimization with automatic determination of appropriate pipeline levels:\n    Use the instruction \"Apply automatic pipeline optimization with automatic determination of pipeline levels.\"\n\nFor the `kernel_covariance` function, use the instruction \"Treat the following function as a hardware kernel for acceleration.\"\n\nApplying these natural language instructions will generate the specified #pragma values in the given C code.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for processing the given C code with the following instructions:\n\n- Disable automatic loop pipeline\n- Set the loop tiling factor to 1\n- Set the parallelization factor to 1\n- Disable automatic loop pipeline\n- Set the loop tiling factor to 1\n- Set the parallelization factor to the default value\n- Set the loop tiling factor to 30\n- Enable automatic loop pipeline with a factor of 5", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following pragma directives:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   - Use '#pragma ACCEL kernel'\n\n2. Don't automatically pipeline the outermost loop:\n   - Use '#pragma ACCEL PIPELINE auto{off}'\n\n3. Process each loop in its original, full form without breaking it into smaller chunks:\n   - Use '#pragma ACCEL TILE FACTOR=auto{1}'\n\n4. Automatically pipeline the innermost loop without specifying a factor:\n   - Use '#pragma ACCEL PIPELINE auto{}'\n\n5. Use a tiling factor of 4 for the middle loop:\n   - Use '#pragma ACCEL TILE FACTOR=auto{4}'\n\n6. Automatically pipeline the innermost loop and flatten the loop hierarchy:\n   - Use '#pragma ACCEL PIPELINE auto{flatten}'\n\n7. Use a tiling factor of 1 for the innermost loop:\n   - Use '#pragma ACCEL TILE FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code by treating the following function as a hardware kernel for acceleration. Automatically pipeline the outer loops. Tile the loops with a factor determined automatically. Parallelize the outer loops with a factor of 32 and the inner loops with a factor of 2. Additionally, utilize pipeline, tiling, and parallelization with factors automatically determined for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS codes for the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n2. Don't automatically pipeline the outermost loop\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks\n4. Automatically flatten the innermost loop\n5. Process the innermost loop in its original, full form without breaking it into smaller chunks", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration,\ndisable automatic pipelining for the outer loop, \napply an automatic tiling factor of 8 for the outer loop, \ndisable automatic pipelining for the middle loop, \napply an automatic tiling factor of 1 for the middle loop, \ndisable automatic pipelining for the inner loop, \nand apply an automatic tiling factor of 1 for the inner loop.\"\n\nThis natural language instruction comprehensively covers the pragma values to use for the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize this loop for hardware acceleration by pipelining, tiling, and parallelizing as necessary. Consider using automatic optimizations for pipelining, tiling, and parallelization.\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30], double C4[30][30], double sum[30]) {\n  int r;\n  int q;\n  int p;\n  int s;\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n\n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n\n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n        for (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n        sum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate matrix multiplication using hardware kernel with auto pipeline, tiling factor, and parallelization factor. For the outer loop, automatically pipeline, set the tiling factor to 4, and parallelize with a factor of 32. For the inner loops, automatically pipeline with a specified L2 factor, set the L2 tiling factor, and parallelize with a specified L2 factor. Lastly, perform a reduction parallelization with a specified L3 factor.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code to mark the function as a hardware kernel for acceleration:\n\n```c\n#pragma ACCEL kernel\n```\n\nDon't automatically pipeline this loop:\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\nProcess this loop in its original, full form without breaking it into smaller chunks:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\nRun the iterations of this loop one after the other, not in parallel:\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n```\n\nTreat the following loop as a pipeline with iteration period controlled by hardware, not software:\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\nProcess this loop in its original, full form without breaking it into smaller chunks:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\nRun the iterations of this loop one after the other, not in parallel:\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code written in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the outermost loop, do not automatically pipeline the loop:\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n2. For the outermost loop, process it in its original full form without breaking it into smaller chunks:\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. For the middle loop, don't automatically pipeline it:\n   Use: `#pragma ACCEL PIPELINE auto{}`\n\n4. For the middle loop, process it in its original full form without breaking it into smaller chunks:\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n5. For the innermost loop, don't automatically pipeline it:\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n6. For the innermost loop, process it in its original full form without breaking it into smaller chunks:\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C as provided, use the following comprehensive but very concise natural language instruction:\n\"Define the following function as a hardware kernel for acceleration, turn off automatic pipelining for the main loops, and set the tiling factor to 2 for the outer loop and to 1 for the inner loops. Additionally, flatten the pipeline for the innermost loop to maximize parallelism.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Keep the loop running sequentially without parallelizing it automatically' to describe '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes for matrix multiplication in C, follow these concise natural language instructions:\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline the outermost loop.\n- Break the loops into chunks of size 2 for improved parallelism.\n- Run the iterations of the outermost loop in parallel with a factor of 32.\n- For the innermost loop:\n  - Run the iterations in parallel with a factor of 2 for j-loop.\n  - Pipeline the k-loop to increase throughput.\n  - Break the loop into tiles of size determined automatically for better memory utilization.\n  - Run the iterations in parallel and apply reduction for C with a factor determined automatically for better performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Apply automatic pipelining to this loop' would refer to '#pragma ACCEL PIPELINE auto{}'.\n'Tile the loop automatically based on the system's optimization factors' would refer to '#pragma ACCEL TILE FACTOR=auto{2}'.\n'Parallelize this loop automatically with a factor determined by the system' would refer to '#pragma ACCEL PARALLEL FACTOR=auto{32}'.\n'Reduce this loop's iteration output with an automatic system factor for parallelization' would refer to '#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the loop and handle it with the original size without breaking it into smaller parts. Run the iterations of this loop in parallel with a factor of 32. Also, pipeline the inner loop with a custom factor and tile the loop with a custom factor as well. Finally, run the innermost loop in parallel with a customized reduction factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, follow these natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n2. Automatically pipeline the loop and determine pipeline factor.\n3. Process the loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop in parallel with a specific parallel factor.\n\nFor specific #pragma values in the C code, use:\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the equivalent HLS codes to the given C code, use the following natural language instruction:\nProcess the following function as a hardware kernel for acceleration. Also, for the loops within the function, deactivate automatic pipelining and set the tiling factor to 1 in both dimensions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "**Optimize the loop to be fully unrolled without automatic pipelining**\n\nTo accomplish this, use the following pragma directives in your C code:\n```c\n#pragma ACCEL PIPELINE \"off\"\n#pragma ACCEL TILE FACTOR=1\n```\nThis will ensure that the loop is processed in its original, full form without breaking it into smaller chunks and that it is not automatically pipelined.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Pipeline the first loop automatically. Set the tiling factor for the first loop to 1. Run the iterations of the first loop in parallel. Turn off automatic loop pipelining for the second loop. Set the tiling factor for the second loop to 1. Run the iterations of the second loop in parallel. Turn off automatic loop pipelining for the third loop. Set the tiling factor for the third loop to 4. Run the iterations of the third loop in parallel using a factor of 8.\"\n\nThis natural language instruction should guide the process of adding specific #pragma values to the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes used in C:\n1. Treat the following function as a hardware kernel for acceleration.\n2. Automatically pipeline this loop.\n3. Break this loop into 4 smaller chunks.\n4. Run the iterations of this loop in parallel with auto-parallelization within a specified factor.\n5. Turn off automatic pipelining for this loop.\n6. Do not break this loop into smaller chunks.\n7. Run the iterations of this loop in parallel with auto-parallelization within a specified factor.\n8. Automatically pipeline this loop.\n9. Break this loop into 1 smaller chunk.\n10. Run the iterations of this loop in parallel with auto-parallelization within a specified factor.\n11. Automatically pipeline this loop.\n12. Break this loop into 1 smaller chunk.\n13. Run the iterations of this loop in parallel with auto-parallelization within a specified factor.\n14. Automatically pipeline this loop with a specific pipeline label.\n15. Break this loop into smaller chunks with a specific tile label.\n16. Run the iterations of this loop in parallel with specifying the parallelization factor.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the loops in the function without automatic parallelization. Use a tile factor of 1 for each loop to process in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the outermost loop.\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Don't automatically pipeline the second-level loop.\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n\n5. Divide the second-level loop into chunks of 4 iterations for parallel processing.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{4}`\n\n6. Flatten the innermost loop to maximize parallelism.\n   - Use: `#pragma ACCEL PIPELINE auto{flatten}`\n\n7. Process the innermost loop without further division.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code written in C, follow the natural language instructions below:\n\n\"Declare the following function as a hardware kernel for acceleration.\"\n\n\"Process this loop in its original, full form without breaking it into smaller chunks.\"\n\n\"Don't automatically pipeline this loop.\"\n\n\"Run the iterations of this loop one after the other, not in parallel.\"\n\nWhen writing the C code, remember to replace \"auto{off}\" with \"off\", \"auto{4}\" with the actual specific tile factor value, \"auto{32}\" with the actual specific parallel factor value, \"auto{__PIPE__L2}\" and \"auto{__TILE__L2}\" with their specific values, and \"auto{__PARA__L2}\" and \"auto{__PARA__L3}\" with their specific parallel factor values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, you need to use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - This is achieved using `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop\n   - Achieved with `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - This is done using `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Achieved with `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for HLS, the following #pragma values should be used:\n\n1. Ensure that the loop is not automatically pipelined:\n   \"#pragma ACCEL PIPELINE auto{off}\"\n\n2. Process the loop using its original, full form without breaking it into smaller chunks:\n   \"#pragma ACCEL TILE FACTOR=auto{1}\"\n\n3. Flatten the pipeline for the inner loop:\n   \"#pragma ACCEL PIPELINE auto{flatten}\"\n\n4. Treat the function as a hardware kernel for acceleration:\n   \"#pragma ACCEL kernel\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration with automatic loop pipelining and tiling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, you should specify the following natural language instruction:\n\n\"Declare the following function as a hardware kernel for acceleration, then process the loops as pipelines with factor of 1 to enhance parallelism, and also tile the loops with a factor of 4 for improved memory locality.\"\n\nBy providing this natural language instruction, you would use the following #pragma values respectively:\n- '#pragma ACCEL kernel'\n- '#pragma ACCEL PIPELINE auto{off}'\n- '#pragma ACCEL TILE FACTOR=auto{4}'\n- '#pragma ACCEL PIPELINE auto{off}'\n- '#pragma ACCEL TILE FACTOR=auto{1}'\n- '#pragma ACCEL PIPELINE auto{flatten}'\n- '#pragma ACCEL TILE FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the loops. Use automatic tiling for loops with a factor of 2. Run the iterations of the loops in parallel with the specified factor. Turn off automatic pipelining for the following loops. Use automatic tiling for loops with a factor of 1. Run the iterations of the loops in parallel with the specified factor. Turn off automatic pipelining for the following loops. Use automatic tiling for loops with a factor of 1. Run the iterations of the loops in parallel with the specified factor.\n", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n- 'Treat the following function as a hardware kernel for acceleration'\n   Use: `#pragma ACCEL kernel`\n\n- 'Don't automatically pipeline this loop'\n   Use: `#pragma ACCEL PIPELINE \"off\"`\n\n- 'Process this loop in its original, full form without breaking it into smaller chunks'\n   Use: `#pragma ACCEL TILE FACTOR=1`\n\n- 'Run the iterations of this loop one after the other, not in parallel'\n  Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\nThese instructions will guide the compiler to apply the specified optimization and parallelization techniques to the given C codes for hardware acceleration.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n1. Enable automatic pipelining for the entire kernel function.\n  \n   - Use: `#pragma ACCEL PIPELINE auto{}`\n\n2. Use an automatic tiling factor for the outermost loop.\n\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Use an automatic parallelization factor for the outermost loop.\n\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{25}`\n\n4. Enable automatic pipelining for the innermost loop.\n\n   - Use: `#pragma ACCEL PIPELINE auto{__PIPE__L2}`\n\n5. Use an automatic tiling factor for the innermost loop.\n\n   - Use: `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`\n\n6. Use an automatic parallelization factor for the innermost loop.\n\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n7. Use a reduction with an automatic parallelization factor for the nested inner loop.\n\n   - Use: `#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. For the first loop:\n   - \"Don't automatically pipeline this loop\"\n   - Use: #pragma ACCEL PIPELINE \"off\"\n\n2. For the second loop:\n   - \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   - Use: #pragma ACCEL TILE FACTOR=1\n\n3. For the third loop:\n   - \"Run the iterations of this loop one after the other, not in parallel\"\n   - Use: #pragma ACCEL PARALLEL FACTOR=1\n\n4. For the entire kernel function:\n   - \"Treat the following function as a hardware kernel for acceleration\"\n   - Use: #pragma ACCEL kernel", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Divide the loops into smaller chunks of size 2. Run the iterations of the loops in parallel, 3 instances at a time. Don't automatically pipeline the loops. Keep the loop in its original, full form without breaking it into smaller chunks. Run the iterations of the loop one after the other, not in parallel.\n\nThis can represent the HLS codes as:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n}\n```", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the `kernel_gemm` function as a hardware kernel for acceleration.\n\nDo not automatically pipeline the outer loop.\n\nProcess the outer loop in its original, full form without breaking it into smaller chunks.\n\nRun the iterations of the outer loop in parallel with a factor of 4.\n\nRun the iterations of the inner loop one after the other, not in parallel.\n\nPipeline the iterations of the inner loop using L2 pragma value.\n\nProcess the inner loop in tiled manner with a tile factor of L2.\n\nRun the iterations of the inner loop in parallel with a factor of L2 for reduction of variable C.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To iteratively process the given C code and generate the corresponding HLS pragma values, follow these concise natural language instructions:\n\n\"Designate the following function as a hardware kernel for acceleration.\"\n\n\"Automatically pipeline the following loop and let the tool decide when to enable/disable pipelining.\"\n\n\"Process this loop as a complete entity without breaking it down, and let the tool determine the tiling factor.\"\n\n\"Run the iterations of this loop in parallel, and let the tool decide the parallelization factor.\"\n\n\"Do not automatically pipeline the subsequent loop.\"\n\n\"Set the tiling factor of the next loop to 1 using the tool's automatic determination.\"\n\n\"Run the iterations of this loop in parallel with the factor determined automatically by the tool.\"\n\n\"Disable automatic pipelining for the next loop.\"\n\n\"Apply a tiling factor of 2 based on the tool's automatic determination.\"\n\n\"Parallelize this loop based on the factor automatically determined by the tool.\"\n\n\"Set the pipelining factor for the next loop automatically.\"\n\n\"Determine the tiling factor automatically for the subsequent loop.\"\n\n\"Parallelize this loop with a parallelization factor of 8 as per the automatic tool determination.\"\n\nThese instructions will guide the generation of the corresponding HLS pragma values for the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given C code with the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration: \n   '#pragma ACCEL kernel'\n\n2. Don't automatically pipeline the outer loop: \n   '#pragma ACCEL PIPELINE auto{off}'\n   \n3. Process the outer loop in its original, full form without breaking it into smaller chunks: \n   '#pragma ACCEL TILE FACTOR=auto{1}'\n\n4. Don't automatically pipeline the inner loop: \n   '#pragma ACCEL PIPELINE auto{off}'\n\n5. Process the inner loop in chunks of 8 iterations: \n   '#pragma ACCEL TILE FACTOR=auto{8}'\n\n6. Flatten the parallel loop for better performance: \n   '#pragma ACCEL PIPELINE auto{flatten}'\n\n7. Process the innermost loop in its original, full form without breaking it into smaller chunks: \n   '#pragma ACCEL TILE FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes in C:\n\nEnsure automatic pipelining for all loops:\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\nTile loops with a factor determined automatically:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\nAdditionally, you can specify the pipeline with a type of flattening for the innermost loop:\n```c\n#pragma ACCEL PIPELINE auto{flatten}\n```\n\nThis will allow the loops to be effectively parallelized and optimized for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes with the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outermost loop.\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n4. Automatically flatten the innermost loop for better performance.\n5. Process the innermost loop with a tile factor of 4 for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the loop. Use the original, full form of the loops without breaking them into smaller chunks. Run the iterations of the loops in parallel with a factor of '__PARA__L0'. Don't automatically pipeline this loop. Use a tile factor of 2. Run the iterations of this loop in parallel with a factor of '__PARA__L1'. Don't automatically pipeline this loop. Use the original, full form of the loops without breaking them into smaller chunks. Calculate the m * m correlation matrix with auto pipeline of '__PIPE__L3', tile factor of '__TILE__L3', and a parallel factor of 8.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Treat the following function as a hardware kernel for acceleration. Don't automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop in parallel with a factor of 4. For the outer loop, run the iterations of this loop in parallel with a factor of 10. For the middle loop, run the iterations of this loop one after the other, not in parallel. Pipeline the middle loop with a custom factor \"L2\". Process the middle loop in its original, full form without breaking it into smaller chunks with a custom factor \"L2\". Run the innermost loop in parallel with a custom reduction factor \"L3\" for variable C.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loop involving the calculation of mean and standard deviation, automatically pipeline it and process it in full form without breaking it into smaller chunks. Run the iterations of these loops in parallel without breaking it into smaller chunks. For the next loop, don't automatically pipeline it and run it in its original form, and run iterations in parallel. For the centering and reducing of column vectors loop, don't automatically pipeline it and process it by breaking it into smaller chunks. For the calculation of the correlation matrix, automatically pipeline it, break it into smaller chunks, and run iterations in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nFor the loop with variable \"j\", don't automatically pipeline this loop.\n\nFor the loop with variable \"j\", process the loop with a tiling factor of 25.\n\nFor the loop with variable \"j\", run the iterations one after the other, not in parallel.\n\nFor the loop with variable \"i\", pipeline this loop in a flattened manner.\n\nFor the loop with variable \"i\", process the loop in its original, full form without breaking it into smaller chunks.\n\nFor the loop with variable \"i\", run the iterations of this loop one after the other, not in parallel.\n\nFor the loop with variable \"i\", don't automatically pipeline this loop.\n\nFor the loop with variable \"i\", process the loop with a tiling factor of 1.\n\nFor the loop with variable \"i\", run the iterations one after the other, not in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nDisable automatic pipeline for this loop.\n\nProcess this loop in its original, full form without breaking it into smaller chunks.\n\nRun the iterations of this loop one after the other, not in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code written in C and include specific `#pragma` values:\n- Begin by indicating that the loop should be processed in its original, full form without breaking it into smaller chunks\n- Automatically pipeline the loop\n- Set the tile factor for the loop as automatic to determine the optimal tiling factor\n- Repeat the pipeline and tile factor settings as necessary for nested loops\n- Finally, specify that the following function should be treated as a hardware kernel for acceleration using the `#pragma ACCEL kernel` statement", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outermost loop in its original, full form without breaking it into smaller chunks, and achieve automatic pipelining for this loop using a pipeline with the default configuration. For the inner loops, perform automatic tiling with a factor of 1 and turn off automatic pipelining for the most deeply nested loop. This function should be treated as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To achieve the desired behavior in the given C code, use the following natural language instruction:\n\n\"Enable pipeline optimizations for the outermost loop and disable automatic pipeline for its inner loops. For tiling, use a tile factor of 8 for the outermost loop and a factor of 1 for the inner loops. Utilize flattening for inner loop pipelining.\"\n\nThis instruction uses natural language to describe the process and pragma values used in the provided HLS C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions:\n\n1. Generate the function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the outermost loop.\n\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n\n4. Don't automatically pipeline the middle loop.\n\n5. Process the middle loop in its original, full form without breaking it into smaller chunks.\n\n6. Flatten the middle loop to improve pipelining and throughput.\n\n7. Process the innermost loop in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n- Use the following function as a hardware kernel for acceleration.\n- Do not automatically pipeline the loop.\n- Use a tiling factor of 2 for loop optimization.\n- Run the loop iterations in parallel with a factor of 4.\n- Inside the loop, automatically pipeline with a specific factor for level 2.\n- Optimize the loop using tiling with a factor for level 2.\n- Perform parallel execution inside the loop with a reduction factor for level 3.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply pipeline, tiling, and parallelization to the loops in the `kernel_gemm` function, with appropriate factors for each pragma. Ensure that the nested loop for `k` is also parallelized with a reduction for variable `C`. Use auto values for pipeline, tile factor, and parallel factor for the outer loop with i, j, and the nested loop with k.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following C function as a hardware accelerator kernel. Then, don't automatically pipeline the outer loop, process it in its full form without breaking it into smaller chunks, and run the iterations of the outer loop in parallel with a factor of 4. Within the outer loop, run the iterations of the inner loop in parallel with a factor of 16, and don't automatically pipeline the inner loop. Also, process the inner loop in its full form without breaking it into smaller chunks and run its iterations in parallel with a factor of 3 using reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration.\n\nSet loop pipelining off.\n\nDivide the loop into chunks for tiling with a factor of 2.\n\nExecute the loop iterations in parallel with a factor of 4.\n\nExecute the innermost loop iterations in parallel with a factor of 16.\n\nPipeline the innermost loop with a specific pragmas.\n\nDivide the innermost loop into chunks for tiling with a specific pragmas.\n\nExecute the innermost loop iterations in parallel with a specific pragmas.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C codes into HLS compatible forms, use the following concise natural language instructions:\n\n- The first loop should not be automatically pipelined: `#pragma ACCEL PIPELINE auto{off}`\n- Process the loops in their original form without breaking them into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the iterations of the second loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following comprehensive but very concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, turn off automatic pipelining for the loops, tile with a factor of 1, and run loop iterations in parallel with a specific factor where applicable. Also, set specific pipeline, tiling, and parallel factors for each loop to optimize performance.\"\n\nThis instruction covers using '#pragma ACCEL kernel', '#pragma ACCEL PIPELINE auto{off}' for disabling automatic pipelining, '#pragma ACCEL TILE FACTOR=auto{1}' for tiling with a factor of 1, and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' to run the 'mean' loop in parallel with a specific factor. Similarly, corresponding pragma values are provided for the remaining loops to ensure proper optimization.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, you would need to:\n- Treat the function 'kernel_doitgen' as a hardware kernel for acceleration.\n- Disable automatic pipelining for the outermost loop over 'r'.\n- Specify a tile factor of 2 for the outermost loop over 'r'.\n- Disable automatic pipelining and specify a tile factor of 1 for the loop over 'q'.\n- Enable automatic pipelining with flattening and specify a tile factor of 1 for the innermost loop over 'p'.\n\nThe corresponding natural language instructions for the #pragma values would be:\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline the loop over 'r'.\n- Set the tile factor to 2 for the loop over 'r'.\n- Don't automatically pipeline the loop over 'q' and set the tile factor to 1.\n- Automatically pipeline the loop over 'p' with flattening and set the tile factor to 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\"Designate the function as a hardware kernel for acceleration. Split the loops into hardware-friendly chunks for parallel processing. Automatically pipeline the loops and optimize the parallel processing based on factors and auto-vectorization for acceleration.\"\n\nThis instruction covers the use of '#pragma ACCEL kernel' for kernel designation, '#pragma ACCEL TILE FACTOR=auto{8}' to split the loops into hardware-friendly chunks, and '#pragma ACCEL PIPELINE auto{flatten}' and '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loops for parallel processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire 'kernel_doitgen' function as a hardware kernel for acceleration. Then, for the nested loops, automatically pipeline the outer two loops and tile them with a factor of 1, while for the innermost loop, manually turn off automatic pipelining and tiling with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instructions for the specific #pragma values:\n\n- Mark the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline the outer loop.\n- Split the outer loop into smaller chunks with a factor of 4 for tiling.\n- Automatically pipeline the middle loop.\n- Perform tiling with a factor of 1 for the middle loop.\n- Automatically pipeline and flatten the innermost loop.\n- Perform tiling with a factor of 1 for the innermost loop.\n\nThese natural language instructions correspond to the specific #pragma values used in the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the 'doitgen' function as a hardware kernel.\n\n1. Apply automatic pipelining to the outer loop.\n2. Tile the outer loop with a factor of 8.\n3. Disable automatic pipelining for the middle loop.\n4. Tile the middle loop with a factor of 1.\n5. Apply loop flattening optimization to the inner loop.\n6. Tile the inner loop with a factor of 1.\n\nThese instructions will optimize the hardware acceleration of the 'doitgen' function using the specific pragma values for pipelining, tiling, and loop flattening optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Compile the following code using the following HLS pragmas:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Automatically pipeline this loop.\n\n6. Automatically tile the loop with the factor of 8.\n\n7. Automatically parallelize the loop with a factor of 4.\n\n8. Automatically parallelize the inner loop with a factor of 16.\n\n9. Automatically pipeline the inner loop using the L2 pipeline pragma.\n\n10. Automatically tile the inner loop using the L2 tile pragma.\n\n11. Automatically parallelize the inner loop with a factor specified using the L2 pragma.\n\n12. Automatically parallelize the reduction loop with a factor specified using the L3 pragma.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply default pipeline configuration to this loop.\nApply an auto tiling factor of 8 to this loop.\nApply an auto parallelization factor of 4 to this loop.\nApply a specific pipeline configuration to this inner loop.\nApply a specific tiling factor of L2 to this inner loop.\nApply a specific parallelization factor of L2 to this inner loop.\nApply a specific parallel reduction and parallelization factor of L3 to this innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, pipeline the outer loop with automatic settings, tile the loops with automatic factor, and parallelize the outer loop with a factor of 4 and the inner loop with a factor of 2. Also, pipeline, tile, and parallelize the innermost loop with automatic factors and apply reduction to the specified variable.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops using automatic pipeline optimization and a default tile factor optimization. Additionally, disable automatic pipeline for the inner loop and set the tile factor to 4 for the middle loop. Finally, flatten the pipeline for the innermost loop to achieve a better performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS codes in C, use the following natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Process the outermost loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of both inner loops one after the other, not in parallel.\n- Automatically pipeline the outermost loop, and flatten the innermost loop for better performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - ```#pragma ACCEL kernel```\n\n2. Don't automatically pipeline this loop\n   - ```#pragma ACCEL PIPELINE \"off\"```\n   \n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - ```#pragma ACCEL TILE FACTOR=1```\n   \n4. Run the iterations of this loop one after the other, not in parallel\n   - ```#pragma ACCEL PARALLEL FACTOR=1```", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes using the given concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline this loop:\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C codes, use the following comprehensive but concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   Corresponding #pragma value: '#pragma ACCEL kernel'\n\n2. Don't automatically pipeline this loop.\n   Corresponding #pragma value: '#pragma ACCEL PIPELINE \"off\"'\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   Corresponding #pragma value: '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   Corresponding #pragma value: '#pragma ACCEL PARALLEL FACTOR=1'\n", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_doitgen' function as a hardware kernel for acceleration, and enable pipeline optimization for the outermost loop by automatically determining the pipeline interval. Also, tile the loops with an automatic tiling factor of 1 for the outer loop and an automatic tiling factor of 4 for the inner loop to maximize parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the provided C function 'kernel_correlation', use the following natural language instructions:\n\n1. Treat the entire 'kernel_correlation' function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the loops in the function.\n\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n\n4. Run the iterations of the loops one after the other, not in parallel.\n\n5. For the first loop, don't automatically pipeline and process the loop in its original form without breaking it into smaller chunks. Run the iterations of the loop one after the other, not in parallel.\n\n6. For the second loop, don't automatically pipeline and process the loop in its original form without breaking it into smaller chunks. Run the iterations of the loop one after the other, not in parallel.\n\n7. For the third loop, don't automatically pipeline and process the loop in its original form without breaking it into smaller chunks. Run the iterations of the loop one after the other, not in parallel.\n\n8. For the fourth loop, automatically pipeline the loop.\n\n9. Process the loop with a tile factor of 30.\n\n10. Run the iterations of the loop in parallel with a factor of 8.\n\nThese natural language instructions correspond to the specific #pragma values to be used to generate the HLS codes for the C function 'kernel_correlation'.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C for acceleration, use the following comprehensive but concise natural language instruction for the specific #pragma values:\n\n1. For the kernel function:\n   \"Treat the following function as a hardware kernel for acceleration\"\n   Use: `#pragma ACCEL kernel`\n\n2. For the first loop to turn off automatic pipelining:\n   \"Don't automatically pipeline this loop\"\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n3. For the loop to process in its original full form without breaking into smaller chunks:\n   \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. For the loop iterations to run one after the other, not in parallel:\n   \"Run the iterations of this loop one after the other, not in parallel\"\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To compile the given C code into HLS, you can use the following natural language instructions for the specific #pragma values:\n1. Use '#pragma ACCEL kernel' to treat the 'kernel_correlation' function as a hardware kernel for acceleration.\n2. Use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loops in the code.\n3. Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the loops in their original, full form without breaking them into smaller chunks.\n4. Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' to parallelize the loop iterations and run them in parallel.\n5. Use '#pragma ACCEL PIPELINE auto{off}' to turn off automatic loop pipelining for the subsequent loops.\n6. Use '#pragma ACCEL TILE FACTOR=auto{8}' to specify a tile factor of 8 for the tiling optimization.\n7. Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' to parallelize the next loop with the specified parallel factor.\n8. Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the following loop without breaking it into smaller chunks.\n9. Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' to parallelize the loop iterations and run them in parallel.\n10. Use '#pragma ACCEL PIPELINE auto{__PIPE__L3}' to specify a pipeline factor for the subsequent loop.\n11. Use '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}' to specify a tile factor for the tiling optimization.\n12. Use '#pragma ACCEL PARALLEL FACTOR=auto{8}' to parallelize the final loop with the specified parallel factor.\n\nThese instructions will help you map the C code to HLS and optimize it for FPGA implementation.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the outer loop and do not pipeline the inner loop. Tile the loops with a factor of 8 for the outer loop and 1 for the inner loop. Flatten the pipelining for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for kernel_correlation function:\n\n- Treat the function 'kernel_correlation' as a hardware kernel for acceleration.\n- Turn off automatic pipelining for the following loops.\n- Divide the loops into smaller chunks with a factor of 2 for tiling.\n- Run the iterations of the loops in parallel with a parallel factor of 4 for the first loop and a parallel factor of 1 for the second loop.\n- Restore automatic pipelining for the following loops.\n- Use a tile factor of 1 for tiling in the subsequent loops.\n- Run the iterations of the loops in parallel with a parallel factor of 1 for the third and fourth loops.\n- Adjust the pipeline factor for the penultimate loop and use a tile factor and parallel factor of 8 for the last loop to optimize performance.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided C code using HLS pragmas, you could use the following natural language instruction:\n\n\"Declare the following function as a hardware kernel for acceleration and apply the specified optimizations:\n- Disable automatic pipelining for the outer loop.\n- Set the tiling factor for the outer loop to 1 to process it in its original, full form without breaking it into smaller chunks.\n- Flatten the inner loop for parallelization and set its tiling factor to 1.\"\n\nThis would result in the following HLS codes written in C:\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, you can use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Turn off automatic pipelining for the loop:\n   ```c\n   #pragma ACCEL PIPELINE off\n   ```\n\n2. Process the loop in its original, full form without breaking it into smaller chunks:\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n3. Run the iterations of this loop one after the other, not in parallel:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n4. Treat the following function as a hardware kernel for acceleration:\n   ```c\n   #pragma ACCEL kernel\n   ```\n\nBy following the above natural language instructions, you can generate the HLS codes accordingly.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use the following natural language instruction:\n\n- For the first pragma, use 'Pipeline this loop automatically' to describe '#pragma ACCEL PIPELINE auto'\n\n- For the second pragma, use 'Break this loop into 4 smaller chunks' to describe '#pragma ACCEL TILE FACTOR=4'\n\n- For the third pragma, use 'Run the iterations of the outer loop in parallel and break it into 4 smaller chunks' to describe '#pragma ACCEL PARALLEL FACTOR=4'\n\n- For the next pragma, use 'Pipeline the inner loop automatically using L2' to describe '#pragma ACCEL PIPELINE auto{__PIPE__L2}'\n\n- For the next pragma, use 'Break the iterations of the inner loop into 4 smaller chunks using L2' to describe '#pragma ACCEL TILE FACTOR=auto{__TILE__L2}'\n\n- For the last pragma, use 'Run the iterations of the inner loop in parallel and break it into smaller chunks using L3 for reduction with C' to describe '#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, the following comprehensive but very concise natural language instructions should be used:\n\n1. Enable the function as a hardware kernel for acceleration\n\n2. Disable automatic pipelining for the loop\n\n3. Specify a tile factor of 4 for loop tiling\n\n4. Use a parallel factor of 4 for parallelizing the loop iterations\n\n5. Use a parallel factor of 2 for the inner loop iterations\n\n6. Enable pipelining with specified pipeline factor for the innermost loop\n\n7. Specify a tile factor with a particular value for the innermost loop tiling\n\n8. Use a parallel factor with a particular value for the innermost loop parallelization\n\n9. Use parallel reduction with a particular factor for the specified accumulator in the innermost loop\n\nBy following these instructions, the given HLS codes in C can be generated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions with specific #pragma values:\n\n1. Mark the function as a hardware kernel for acceleration: '#pragma ACCEL kernel'\n\n2. Automatically pipeline the outermost loop: '#pragma ACCEL PIPELINE auto{}'\n\n3. Specify the tile factor for the outermost loop as 8: '#pragma ACCEL TILE FACTOR=auto{8}'\n\n4. Specify the parallel factor for the outermost loop as 4: '#pragma ACCEL PARALLEL FACTOR=auto{4}'\n\n5. Automatically pipeline the inner loop with specific pipeline level: '#pragma ACCEL PIPELINE auto{__PIPE__L2}'\n\n6. Specify the tile factor for the inner loop with a specific tile factor: '#pragma ACCEL TILE FACTOR=auto{__TILE__L2}'\n\n7. Specify the parallel factor for the inner loop with a specific parallel factor: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n\n8. Specify the parallel reduction factor for the innermost loop with a specific reduction factor: '#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can apply the following natural language instructions to the respective #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   - Add the directive '#pragma ACCEL kernel' before the function definition.\n\n2. Don't automatically pipeline this loop:\n   - Add the directive '#pragma ACCEL PIPELINE \"off\"' before the loops that should not be automatically pipelined.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   - Add the directive '#pragma ACCEL TILE FACTOR=1' before the loops that should not be tiled.\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   - Add the directive '#pragma ACCEL PARALLEL FACTOR=1' before the loops that should not be parallelized.\n\nBy applying these natural language instructions to the respective #pragma values in your C code, you will generate the intended HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C codes with specific #pragma values use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - `#pragma ACCEL kernel`\n\n2. Process the loop in its original, full form without breaking it into smaller chunks\n   - `#pragma ACCEL TILE FACTOR=1`\n\n3. Don't automatically pipeline the loops \n   - `#pragma ACCEL PIPELINE off`\n\n4. Automatically pipeline the loops \n   - `#pragma ACCEL PIPELINE auto`\n\n5. Automatically tile the loops with a specified factor \n   - `#pragma ACCEL TILE FACTOR=auto{}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process a function as a hardware kernel for acceleration.\nApply automatic pipelining to the outer loop.\nTile the outer loop with a factor of 8.\nApply manual pipelining with a factor of 1 to the middle loop.\nTile the middle loop with a factor of 1.\nApply loop flattening to the inner loop.\nTile the inner loop with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes:\n\n```c\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{4}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{8}\n```\n\nComprehensive but very concise natural language instructions are:\n\n1. Disable automatic loop pipelining for the following code block.\n2. Process this loop in its original, full form without breaking it into smaller chunks.\n3. Run the iterations of this loop one after the other, not in parallel.\n4. Enable automatic loop pipelining for the following code block.\n5. Process this loop in its original, full form without breaking it into smaller chunks.\n6. Run the iterations of this loop one after the other, not in parallel.\n7. Enable automatic loop pipelining for the following code block.\n8. Break this loop into smaller chunks with a size factor of 4 for improved parallel processing.\n9. Enable automatic loop pipelining for the following code block with a specific pipeline label (label __PIPE__L3).\n10. Break this loop into smaller chunks with a specific tile factor for improved parallel processing.\n11. Run the iterations of this loop in parallel with a factor of 8.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the `kernel_doitgen` function as a hardware kernel for acceleration, use `#pragma ACCEL kernel`. To automatically pipeline the outer loop, use `#pragma ACCEL PIPELINE auto{}`. To tile the outer loop with a factor of 4, use `#pragma ACCEL TILE FACTOR=auto{4}`. For the inner loops, use `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipelining and `#pragma ACCEL TILE FACTOR=auto{1}` to tile with a factor of 1. Additionally, use `#pragma ACCEL PIPELINE auto{flatten}` to flatten the innermost loop for better acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction: \n\n- Treat the following function as a hardware kernel for acceleration and process the loops with automatic pipeline and tiling factor optimizations, but with the innermost loop pipeline off:\n```c\n#pragma ACCEL kernel\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n    int r;\n    int q;\n    int p;\n    int s;\n\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL TILE FACTOR=auto{8}\n\nfor (r = 0; r < 25; r++) {\n    #pragma ACCEL PIPELINE auto{}\n    #pragma ACCEL TILE FACTOR=auto{1}\n    \n    for (q = 0; q < 20; q++) {\n        #pragma ACCEL PIPELINE auto{off}\n        #pragma ACCEL TILE FACTOR=auto{1}\n        \n        for (p = 0; p < 30; p++) {\n            sum[p] = 0.0;\n            for (s = 0; s < 30; s++) {\n                sum[p] += A[r][q][s] * C4[s][p];\n            }\n        }\n        for (p = 0; p < 30; p++) {\n            A[r][q][p] = sum[p];\n        }\n    }\n  }\n}\n```\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instruction:\n\n\"Designate the function 'kernel_doitgen' as a hardware kernel, and process the loops in their original form without breaking them into smaller chunks, treating the loops as non-pipelined, non-flattened, and with a tile factor of 1.\"\n\nThis instruction includes the following pragma values:\n- `#pragma ACCEL kernel`\n- `#pragma ACCEL PIPELINE auto{off}`\n- `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes written in C for a matrix multiplication hardware kernel:\n```c\n#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\"Mark the following function as a hardware kernel for acceleration, automatically pipeline the outer loop, tile the outer loop with a factor of 80, and parallelize the inner loop with a factor of 2.\"\n\nThis comprehensive instruction translates to the following specific #pragma values:\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=80\n\n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for specific #pragma values:\n\n1. For the first loop, **disable automatic pipelining**:\n   Use `#pragma ACCEL PIPELINE auto{off}` to ensure the loop is not automatically pipelined.\n\n2. For the loop iteration, **process in its original form without breaking it into smaller chunks**:\n   Apply `#pragma ACCEL TILE FACTOR=auto{1}` to keep the loop in its original, full form without tiling.\n\n3. For the outer loop, **run the iterations in parallel with a factor of 4**:\n   Utilize `#pragma ACCEL PARALLEL FACTOR=auto{4}` to parallelize the outer loop with a factor of 4.\n\n4. For the inner loop, **run the iterations in parallel with a factor of 2**:\n   Utilize `#pragma ACCEL PARALLEL FACTOR=auto{2}` to parallelize the inner loop with a factor of 2.\n\n5. For the innermost loop, **apply pipeline with specified configurations**:\n   Use `#pragma ACCEL PIPELINE auto{__PIPE__L2}` to apply auto-pipelining with specific pipeline configurations.\n   \n6. For the innermost loop iteration, **apply tiling with specified factor**:\n   Use `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}` to apply auto-tiling with specific tiling factor.\n   \n7. For the innermost loop iteration, **run the iterations in parallel with reduction**:\n   Apply `#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}` to parallelize the innermost loop with reduction and specified parallel factor.\n\nBy following these concise natural language instructions, the HLS codes can be generated as provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process a general matrix multiplication function as a hardware kernel for acceleration with the following optimizations:\n- Disable automatic pipelining for the main loop\n- Do not break the loop into smaller sections\n- Parallelize the outer loop with a factor of 32\n- Parallelize the inner loop with a factor of 16\n- Apply pipeline optimization for the innermost loop with L2 pipeline pragma\n- Use tiling with L2 tile factor for the innermost loop\n- Parallelize the innermost loop with a reduction variable and a factor determined by L3 parallel factor auto value.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you should:\n\n1. Treat the function as a hardware kernel for acceleration by using `#pragma ACCEL kernel`.\n\n2. Turn off automatic pipelining for the loops by using `#pragma ACCEL PIPELINE auto{off}`.\n\n3. Process the loops in their original, full form without breaking them into smaller chunks by using `#pragma ACCEL TILE FACTOR=auto{1}`.\n\n4. Run the iterations of the loops one after the other, not in parallel, by using `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n\n5. Specifically, for the 3rd loop, use `#pragma ACCEL PIPELINE auto{__PIPE__L3}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`, and `#pragma ACCEL PARALLEL FACTOR=auto{30}` to set pipeline, tiling factor, and parallel factor respectively.\n\nFollowing these instructions will enable hardware acceleration for the given codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes for a matrix multiplication function, use the following natural language instructions for the specific #pragma values:\n\n1. Use 'Enable pipeline optimization for this loop' to describe '#pragma ACCEL PIPELINE auto{off}'\n2. Use 'Break this loop into 2x2 chunks during hardware synthesis' to describe '#pragma ACCEL TILE FACTOR=auto{2}'\n3. Use 'Execute the loop iterations in parallel, each with a factor of 4' to describe '#pragma ACCEL PARALLEL FACTOR=auto{4}'\n4. Use 'Execute the inner loop iterations in parallel, each with a factor of 2' to describe '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n5. Use 'Enable software pipelining for this loop for L2 cache' to describe '#pragma ACCEL PIPELINE auto{__PIPE__L2}'\n6. Use 'Break this loop into L2-sized chunks during hardware synthesis' to describe '#pragma ACCEL TILE FACTOR=auto{__TILE__L2}'\n7. Use 'Execute the loop iterations in parallel, each with a factor determined by L2 parallelism' to describe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n8. Use 'Execute the inner loop iterations in parallel with reduction variable C, each with a factor determined by L3 parallelism' to describe '#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}'\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code:\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{on}\n  \n#pragma ACCEL TILE FACTOR=1\n  \n#pragma ACCEL PARALLEL FACTOR=8\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=4\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=1\n  \n#pragma ACCEL PARALLEL FACTOR=1\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=5\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j], (double)2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=1\n  \n#pragma ACCEL PARALLEL FACTOR=1\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=6\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{on}\n  \n#pragma ACCEL TILE FACTOR=8\n  \n#pragma ACCEL PARALLEL FACTOR=8\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{on}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=7_0\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\nNatural language instruction:\nCombine the data processing operation inside the function \"kernel_correlation\" into a single hardware kernel for acceleration. Pipeline the loops for efficient execution, and use a tile factor of 1 for maximizing parallelism. Additionally, parallelize loops to a factor of 8 to leverage hardware parallelism wherever possible. Finally, utilize additional pipelining and parallelization strategies as needed to optimize performance, considering factors such as loop dependencies and data access patterns.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the matrix computations in the following loop using hardware acceleration by specifying the appropriate pipelining, tiling, and flattening directives. \n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n\n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following concise natural language instruction:\n\n\"Designate the function as a hardware kernel for acceleration, and apply auto-pipelining with specified factors for tiling in each nested loop.\"\n\nThese instructions would translate to the following #pragma values in the C code:\n- Hardware kernel designation: #pragma ACCEL kernel\n- Auto-pipelining in all loops: #pragma ACCEL PIPELINE auto{}\n- Tiling factor of 4 in the outer loop: #pragma ACCEL TILE FACTOR=auto{4}\n- Explicitly turning off auto-pipelining in the second loop: #pragma ACCEL PIPELINE auto{off}\n- Tiling factor of 1 in the second and third loops: #pragma ACCEL TILE FACTOR=auto{1}", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C for hardware acceleration, use the following natural language instructions for specific #pragma values:\n1. For the loop that calculates the mean, turn off automatic pipelining: '#pragma ACCEL PIPELINE auto{off}'\n2. Process the loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Run the iterations of the mean loop in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n4. For the loop that calculates the standard deviation, use automatic pipelining: '#pragma ACCEL PIPELINE auto{}'\n5. Break down the standard deviation loop into smaller chunks with a factor of 20: '#pragma ACCEL TILE FACTOR=auto{20}'\n6. Run the iterations of the standard deviation loop in parallel with a factor of '__PARA__L1': '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'\n7. For the loop that centers and reduces the column vectors, use automatic pipelining: '#pragma ACCEL PIPELINE auto{}'\n8. Process the centering and reducing loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{1}'\n9. Run the iterations of the centering and reducing loop in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n10. For the loop that calculates the correlation matrix, use automatic pipelining with a specific factor: '#pragma ACCEL PIPELINE auto{__PIPE__L3}'\n11. Break down the correlation matrix loop into smaller chunks with a specific factor: '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'\n12. Run the iterations of the correlation matrix loop in parallel with a factor of 8: '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n13. Treat the 'kernel_correlation' function as a hardware kernel for acceleration: '#pragma ACCEL kernel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but concise natural language instructions:\n\n1. Use the function 'kernel_gemm' as a hardware kernel for acceleration.\n2. Do not automatically pipeline the outer loops of the function.\n3. Break the loops into smaller chunks using a tile factor of 8.\n4. Run the iterations of the outer loops in parallel with a factor of 4.\n5. Run the iterations of the inner loops in parallel with a factor of 2 and reduce the 'C' variable in parallel with a factor determined automatically.\n6. Pipeline the inner loops with a specific pipeline pragma value.\n7. Set tile factor for the inner loops with a specific tile factor pragma value.\n8. Run the iterations of the inner loops in parallel with a specific parallel factor pragma value.\n\nThese natural language instructions would correspond to the given HLS codes written in C with the specific #pragma values used.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the kernel_correlation function using HLS, follow these natural language instructions for the given C code:\n\n1. Annotate the loop to not automatically pipeline it\n   - Natural language: \"Don't automatically pipeline this loop\"\n   - HLS pragma: `#pragma ACCEL PIPELINE \"off\"`\n\n2. Annotate the loop to process in its original form without breaking into smaller chunks\n   - Natural language: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   - HLS pragma: `#pragma ACCEL TILE FACTOR=1`\n\n3. Annotate the loop to run iterations one after the other, not in parallel\n   - Natural language: \"Run the iterations of this loop one after the other, not in parallel\"\n   - HLS pragma: `#pragma ACCEL PARALLEL FACTOR=1`\n\n4. Annotate the function as a hardware kernel for acceleration\n   - Natural language: \"Treat the following function as a hardware kernel for acceleration\"\n   - HLS pragma: `#pragma ACCEL kernel`\n\nYou can apply these natural language instructions to the corresponding HLS pragmas in the provided C code to accelerate the kernel_correlation function.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Apply automatic pipelining to the outer loop and tiling with a factor of 80, automatic pipelining and tiling for the inner loop with tile and pipe factor values of __TILE__L1 and __PIPE__L1, and run the iterations of the innermost loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes: \n\n'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' \n\n'Treat the following loop as a single unit and don't break it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{1}' \n\n'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE auto{}' \n\n'Treat the following nested loop as a single unit and don't break it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}' \n\n'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}' \n\n'Treat the following function as a hardware kernel for acceleration' to describe '#pragma ACCEL kernel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, apply automatic pipelining to each loop to optimize the performance. For the first loop, set the tile factor to 1 to process it in its original, full form without breaking it into smaller chunks. Next, run the iterations of the second loop one after the other, not in parallel, setting the parallel factor to 1. For the third loop, use a tile factor of 20 to optimize processing, and set the parallel factor to 1. Lastly, for the final loop, use a tile factor of 1 and a parallel factor of 4 to enhance parallel processing.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process each loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Automatically pipeline this loop with the default settings.\n\n6. Break the loops into chunks of size 8 for improved parallelism.\n\n7. Run the iterations of this loop in parallel with a factor of 8.\n\n8. Turn off automatic loop pipelining for this loop.\n\n9. Process each loop in its original, full form without breaking it into smaller chunks.\n\n10. Run the iterations of this loop in parallel with factors defined by '__PARA__L2' and '__PARA__L6'.\n\n11. Automatically pipeline this loop with the specified factor.\n\n12. Break the loops into chunks of size defined by '__TILE__L3' for better performance.\n\n13. Run the iterations of this loop in parallel with a factor of 8.\n\nThese natural language instructions correspond to the specific #pragma values used in the given HLS codes to generate the hardware design.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n5. Automatically pipeline this loop to improve performance.\n\nThese instructions correspond to the following #pragma values:\n1. #pragma ACCEL kernel\n2. #pragma ACCEL PIPELINE \"off\"\n3. #pragma ACCEL TILE FACTOR=1\n4. #pragma ACCEL PARALLEL FACTOR=1\n5. #pragma ACCEL PIPELINE \"auto\"", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, follow these concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outer two loops.\n3. Process the outer two loops in their original, full form without breaking them into smaller chunks.\n4. Run the iterations of the outer two loops one after the other, not in parallel.\n5. Automatically pipeline the innermost loop with flattening enabled.\n6. Process the innermost loop with a tile factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nEnsure that the loop is not automatically pipelined.\nProcess the loop in its original, full form without breaking it into smaller chunks.\nRun the iterations of the loops in parallel with a factor of 32.\nFor the inner loop, parallelize it with a factor of 8.\nFor the innermost loop, parallelize it with a reduction on array C with a factor determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, the following natural language instructions can be used:\n\n1. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n2. Automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{}`\n  \n3. Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Turn off automatic pipelining for this loop: `#pragma ACCEL PIPELINE auto{off}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code using HLS pragmas, use the following concise natural language instruction:\n\n\"Translate the following function into a hardware kernel for acceleration, pipeline the outer loop automatically, tile the outer loop with a factor of 80, pipeline the inner loop with a specific factor, tile the inner loop with a specific factor, and parallelize the innermost loop automatically with a factor of 2.\"\n", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C with natural language instructions, follow these steps and use these specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the outermost loop of this function.\n\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n\n4. Automatically pipeline the middle loop of this function.\n\n5. Process the middle loop in its original full form without breaking it into smaller chunks.\n\n6. Automatically pipeline the innermost loop of this function and flatten the loop structure to improve parallelism.\n\n7. Process the innermost loop in chunks of size 8 for better hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for hardware acceleration, use the following natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n   - Instruction: \"Treat the following function as a hardware kernel for acceleration\" \n   - Pragma: #pragma ACCEL kernel\n\n2. Automatically pipeline the outer loop to enable concurrent execution of iterations\n   - Instruction: \"Automatically pipeline this loop\"\n   - Pragma: #pragma ACCEL PIPELINE auto{}\n\n3. Set the tiling factor for the outer loop to a value determined automatically\n   - Instruction: \"Automatically determine the tiling factor for the outer loop\"\n   - Pragma: #pragma ACCEL TILE FACTOR=auto{80}\n\n4. Automatically pipeline the inner loop with a specified factor\n   - Instruction: \"Automatically pipeline this loop with a specific factor\"\n   - Pragma: #pragma ACCEL PIPELINE auto{__PIPE__L1}\n\n5. Set the tiling factor for the inner loop to be determined automatically\n   - Instruction: \"Automatically determine the tiling factor for the inner loop\"\n   - Pragma: #pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n\n6. Parallelize the innermost loop with a specific parallelization factor\n   - Instruction: \"Automatically determine the parallelization factor for the innermost loop\"\n   - Pragma: #pragma ACCEL PARALLEL FACTOR=auto{2}", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following function is treated as a hardware kernel for acceleration, don't automatically pipeline the loops, process the loops using their full form without breaking them into smaller chunks, run the iterations of the loops one after the other (not in parallel), treat each loop as a separate unit, and pipeline the loops using the specified factors and parameters present in the code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To create hardware acceleration for the given function, use the following pragmas:\n\n- To automatically pipeline all loops: `#pragma ACCEL PIPELINE auto{}`\n- To process each loop in its original form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n- To run the iterations of each loop in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}`\n- And to treat the function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in natural language, you can use the following comprehensive but concise instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and then process the nested loops sequentially, without automatic pipelining and with a tile factor of 1 for the outermost loop, and a tile factor of 8 for the middle loop, while flattening the innermost loop for parallelism.\"\n\nThis instruction captures the essence of the given HLS codes and the specific #pragma values to use.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions for each pragma directive:\n\n1. ```#pragma ACCEL kernel```: Treat the following function as a hardware kernel for acceleration.\n\n2. ```#pragma ACCEL PIPELINE auto{off}```: Don't automatically pipeline this loop.\n\n3. ```#pragma ACCEL TILE FACTOR=auto{1}```: Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. ```#pragma ACCEL PARALLEL FACTOR=auto{25}```: Run the iterations of this loop in parallel with a factor of 25.\n\n5. In the nested loops:\n   - ```#pragma ACCEL PIPELINE auto{__PIPE__L2}```: Pipeline the loop at level 2 based on automatic analysis.\n   - ```#pragma ACCEL TILE FACTOR=auto{__TILE__L2}```: Apply tiling to the loop at level 2 based on automatic analysis.\n   - ```#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}```: Run iterations of the loop at level 2 in parallel based on automatic analysis.\n   - ```#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}```: Parallelize reductions in the innermost loop based on automatic analysis.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the `kernel_doitgen` function for acceleration, use the following comprehensive but concise natural language instruction: \"Turn off automatic loop pipelining and tile the loops with a factor of 1 and 4 respectively, while flattening the inner loop. Run in parallel with `auto` factor for pipelining and `auto` factor for tiling.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate generic HLS code template for C:\n```\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL TILE FACTOR=auto{}\n#pragma ACCEL PARALLEL FACTOR=auto{}\n```\n\nComprehensive concise natural language instruction:\n```\n1. Process the loop in pipeline mode with automatic optimizations.\n2. Break the loop into smaller chunks automatically for tiling.\n3. Run the iterations of the loop in parallel with automatic optimization.\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes: \n```\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n\n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{30}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n```\n\nNatural language instruction: \nPrepare the given function to be used as a hardware kernel for acceleration. Then, for each loop, enable automatic pipelining and tiling with a factor of 1, and run the iterations in parallel with an automated factor. For one of the loops, set the parallel factor to 4 and use automatic pipelining with a custom label provided.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS directives for the given C code, you can use the following comprehensive and concise natural language instructions:\n\n1. For the outermost loop that computes the means and standard deviations, use '#pragma ACCEL PIPELINE \"off\"'\n\n2. For the same outermost loop, enable tiling with a factor of 1: '#pragma ACCEL TILE FACTOR=1'\n\n3. For the loop computing the means in parallel, inline with the length of the innermost loop: '#pragma ACCEL PARALLEL FACTOR=100'\n\n4. For the innermost loop processing standard deviations in parallel, inline with the length of the innermost loop: '#pragma ACCEL PARALLEL FACTOR=100'\n\n5. For the loop that centers and reduces the column vectors, turn off pipelining: '#pragma ACCEL PIPELINE \"off\"'\n\n6. Again, enable tiling for the same loop, this time with a factor of 1: '#pragma ACCEL TILE FACTOR=1'\n\n7. For the loop calculating the correlation matrix, define a pipeline with a factor of 2: '#pragma ACCEL PIPELINE \"2\"'\n\n8. Enable tiling for the correlation matrix calculation loop with a factor of 2: '#pragma ACCEL TILE FACTOR=2'\n\n9. Finally, parallelize the innermost loop of the correlation computation, inline with the length of the innermost loop: '#pragma ACCEL PARALLEL FACTOR=100'\n\nThese instructions should guide you to correctly generate the corresponding HLS directives for the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop nest in its original form without automatically breaking it into smaller chunks and treat the following function as a hardware kernel for acceleration. Then use the following pragma values: \n- '#pragma ACCEL PIPELINE auto{off}' to disable automatic pipeline for the outermost loop\n- '#pragma ACCEL TILE FACTOR=auto{1}' to set the tiling factor of the outermost loop to 1\n- '#pragma ACCEL PIPELINE auto{}' to enable automatic pipeline for the middle loop\n- '#pragma ACCEL TILE FACTOR=auto{1}' to set the tiling factor of the middle loop to 1\n- '#pragma ACCEL PIPELINE auto{flatten}' to enable automatic pipeline with loop flattening for the innermost loop\n- '#pragma ACCEL TILE FACTOR=auto{1}' to set the tiling factor of the innermost loop to 1", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, and then process the nested loops while allowing the HLS tool to automatically decide whether to pipeline or tile the loops.\"\n\nThis instruction encompasses the use of the following #pragma values:\n- For marking the function as a hardware kernel: '#pragma ACCEL kernel'\n- For processing the loops with automatic pipeline optimization: '#pragma ACCEL PIPELINE auto'\n- For tiling the loops with automatic factor determination: '#pragma ACCEL TILE FACTOR=auto'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipeline off and factor of 2 tiling for outer loop, and with flatten pipeline and factor of 1 tiling for inner loops, treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific `#pragma` values:\n\n1. Treat the following function as a hardware kernel for acceleration `#pragma ACCEL kernel`\n2. Don't automatically pipeline the outer loop `#pragma ACCEL PIPELINE off`\n3. Process the outer loop in its original, full form without breaking it into smaller chunks `#pragma ACCEL TILE FACTOR=1`\n4. Automatically pipeline the first inner loop `#pragma ACCEL PIPELINE auto(__PIPE__L1)`\n5. Automatically tile the first inner loop with factor 1 `#pragma ACCEL TILE FACTOR=auto(__TILE__L1)`\n6. Automatically parallelize the second inner loop with a factor of 2 `#pragma ACCEL PARALLEL FACTOR=auto(2)`\n\nThese instructions will help in generating the HLS codes in C with the specified `#pragma` values.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without automatically breaking it into smaller chunks, and treat the following function 'kernel_doitgen' as a hardware kernel for acceleration. Also, don't automatically pipeline the outer loop and set the tile factor of the outer loop to 4, and for the inner loops, don't automatically pipeline them and set their tile factors to 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes in C as follows:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the `kernel_doitgen` function as a hardware kernel for acceleration. Apply automatic pipeline off for the outermost loop. Tile the loops with a factor of 1, and flatten the innermost loop for parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes with the following natural language instruction: \"Apply auto pipeline to all loops and auto tile factor to outer loop, auto pipeline and auto tile factor to inner loop, and auto parallel factor to innermost loop with two instances.\"", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, you can use the following natural language instructions:\n\n- Use automatic pipelining for the loop\n- Tile the loop with a factor of auto{1}\n- Parallelize the loop with a factor of auto{4}\n- Inside the nested loop, parallelize it with a factor of auto{10}\n- Auto pipeline the inner-most loop with a factor of auto{__PIPE__L2}\n- Tile the inner-most loop with a factor of auto{__TILE__L2}\n- Parallelize the inner-most loop with a factor of auto{__PARA__L2}\n- Use parallel reduction with a factor of auto{__PARA__L3}\n\nI hope this helps!", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. \nApply automatic pipelining with an off option for the outer loop. \nUse automatic tiling for the loops with a factor of 80. \nFor the innermost loop, apply automatic pipelining with a custom L1 option and automatic tiling with a custom L1 option. \nFinally, parallelize the loop with a factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function for hardware acceleration\"\n\nThis corresponds to the pragma directive:\n```c\n#pragma ACCEL kernel\n```\n\nAfter applying this directive, the function \"kernel_doitgen\" will be treated as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipeline off, and automatic tile factor of 80, and parallel factor of 2. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop with a tile factor of 4\n- Run the iterations of this loop one after the other, not in parallel\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel\n- Don't automatically pipeline this loop\n- Process this loop with a tile factor of 1\n- Run the iterations of this loop one after the other, not in parallel\n- Don't automatically pipeline this loop\n- Process this loop with a tile factor of 1\n- Run the iterations of this loop one after the other, not in parallel", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To parallelize the following 3 nested loops and apply specific factors, while also automatically determining pipeline configuration, use the following pragma values:\n\n1. Use `#pragma ACCEL kernel` to designate the function as a hardware kernel for acceleration.\n2. Utilize `#pragma ACCEL PIPELINE auto{off}` to disable automatic loop pipelining.\n3. Implement `#pragma ACCEL TILE FACTOR=auto{80}` to automatically determine and apply tiling factor for loop iteration. \n4. Finally, apply `#pragma ACCEL PARALLEL FACTOR=auto{2}` to automatically determine and set the parallel factor for the inner loop.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To write a natural language instruction for the given HLS codes, you can use the following:\n\n\"Accelerate the kernel_correlation function by interchanging its loops, automatically pipelining all loops, tiling with a factor of 25, and running the loops in parallel with a factor of 4 for mean computation, and with a factor of 1 for standard deviation and center-reduction computation, finally pipelining the correlation matrix computation with a factor of 3 and 7 for the inner loops.\"\n\nThis instruction references the specific pragma values used in the provided C code and describes the overall process and pragma values for accelerating the kernel_correlation function.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following steps to generate the given HLS codes in C:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the outer loop.\n\n3. Break the outer loop into smaller chunks with a factor of 4.\n\n4. Automatically pipeline the inner loop.\n\n5. Process the inner loop in its original, full form without breaking it into smaller chunks.\n\n6. Automatically pipeline and flatten the innermost loop.\n\n7. Process the innermost loop in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To succinctly describe the process of the given HLS codes and which specific #pragma values to use in natural language, you can use the following comprehensive but concise natural language instruction: \n\n\"Transform the specified loops for hardware acceleration. Utilize automatic pipelining and tiling for parallel processing optimization. Additionally, mark the function as a hardware kernel for acceleration.\"\n\nThis natural language instruction covers the key elements of the given HLS codes, including loop transformation, pipelining, tiling, and function marking for hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you can use the following comprehensive but very concise natural language instruction:\n\n\"Specify that the loop should not be automatically pipelined\"\n- To turn off auto-pipelining, use: `#pragma ACCEL PIPELINE auto{off}`\n\n\"Process this loop in its original, full form without breaking it into smaller chunks\"\n- To set the tile factor to 1, use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n\"Run the iterations of this loop one after the other, not in parallel\"\n- To set the parallel factor to 1 for the outer loop, use: `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n- To set the parallel factor to 1 for the inner loops, use: `#pragma ACCEL PARALLEL FACTOR=auto{12}`\n\nAdditionally, \"Treat the following function as a hardware kernel for acceleration\"\n- To indicate that the function is a hardware kernel, use: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the first loop, don't automatically pipeline it, break it into smaller chunks of size 2, and run the iterations in parallel with a factor of __PARA__L0. For the second loop, don't automatically pipeline it, keep it as it is, don't break it into smaller chunks, and run the iterations in parallel with a factor of __PARA__L1. For the third loop, don't automatically pipeline it, keep it as it is, don't break it into smaller chunks, and run the iterations in parallel with a factor of __PARA__L2. For the fourth loop, automatically pipeline it with a factor of __PIPE__L3, break it into smaller chunks, and run the iterations in parallel with a factor of 4.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for the following C code with the following natural language guide:\n\nProcess the following function as a hardware kernel for acceleration.\n\nFor the nested loops, automatically pipeline and use a tile factor of 1 for each loop.\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, then automatically pipeline the loops, tile the loops using a factor of 4, and run the iterations of the loops in parallel with a factor of __PARA__L0, __PARA__L4, __PARA__L1, __PARA__L5, __PARA__L2, __PARA__L6, 4, __PARA__L7_0 respectively. Finally, automatically pipeline, tile, and run the iterations in parallel with factors __PIPE__L3, __TILE__L3, 4, __PIPE__L7, and __PARA__L7_0 ", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To pipeline the outer loop automatically, use `#pragma ACCEL PIPELINE auto{}`. To tile the outer loop with a factor of `80` automatically, use `#pragma ACCEL TILE FACTOR=auto{80}`. To pipeline the inner loop with a specific label `__PIPE__L1` automatically, use `#pragma ACCEL PIPELINE auto{__PIPE__L1}`. To tile the inner loop with a specific label `__TILE__L1` automatically, use `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`. And to parallelize the innermost loop with a factor of `2` automatically, use `#pragma ACCEL PARALLEL FACTOR=auto{2}`.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code-to-text:\n\n\"Enable auto pipelining for this loop\" - #pragma ACCEL PIPELINE auto{}\n\n\"Use auto tiling factor for this loop\" - #pragma ACCEL TILE FACTOR=auto{1}\n\n\"Auto parallelize this loop\" - #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n\n\"Disable auto pipelining for this loop\" - #pragma ACCEL PIPELINE auto{off}\n\n\"Use auto tiling factor for this loop\" - #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n\"Set parallel factor to 4 for this loop\" - #pragma ACCEL PARALLEL FACTOR=auto{4}", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the first loop, don't automatically pipeline this loop: \n   Use '#pragma ACCEL PIPELINE \"off\"'\n\n2. For the first loop, process this loop in its original, full form without breaking it into smaller chunks:\n   Use '#pragma ACCEL TILE FACTOR=1'\n\n3. For the first loop, run the iterations of this loop one after the other, not in parallel:\n   Use '#pragma ACCEL PARALLEL FACTOR=1'\n\n4. Treat the following function as a hardware kernel for acceleration:\n   Use '#pragma ACCEL kernel'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use pragma: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n   - Use pragma: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use pragma: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use pragma: `#pragma ACCEL PARALLEL FACTOR=1`\n\n5. Auto pipeline this loop with automatic resource allocation.\n   - Use pragma: `#pragma ACCEL PIPELINE auto{}`\n\n6. Auto tile this loop with automatic resource allocation.\n   - Use pragma: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n7. Auto parallelize this loop with automatic resource allocation and a parallelization factor of 4.\n   - Use pragma: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n\n8. Auto parallelize the inner loop with automatic resource allocation and a parallelization factor of 25.\n   - Use pragma: `#pragma ACCEL PARALLEL FACTOR=auto{25}`\n\n9. Auto pipeline the inner loop with automatic resource allocation and the specific pipeline factor.\n   - Use pragma: `#pragma ACCEL PIPELINE auto{__PIPE__L2}`\n\n10. Auto tile the inner loop with automatic resource allocation and the specific tile factor.\n   - Use pragma: `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`\n\n11. Auto parallelize the inner loop with automatic resource allocation and the specific parallelization factor.\n   - Use pragma: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n12. Auto parallelize the nested inner loop with automatic resource allocation and the specific parallelization factor for reduction.\n   - Use pragma: `#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the given C code, use the following pragma directives:\n- Use '#pragma ACCEL kernel' to treat the `stencil3d` function as a hardware kernel for acceleration.\n- Use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the outer loop.\n- Use '#pragma ACCEL TILE FACTOR=auto{80}' to automatically tile the outer loop with a factor of 80.\n- Use '#pragma ACCEL PIPELINE auto{__PIPE__L1}' to automatically pipeline the inner loop.\n- Use '#pragma ACCEL TILE FACTOR=auto{__TILE__L1}' to automatically tile the inner loop.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{2}' to automatically parallelize the innermost loop with a factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function `kernel_doitgen` is treated as a hardware kernel for acceleration. Then, for each nested loop within the function:\n1. Do not automatically pipeline the loop.\n2. Process the loop in its original, full form without breaking it into smaller chunks.\n3. Run the iterations of the loop one after the other, not in parallel.\n4. Automatically flatten the loop and use a tile factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   Use: `#pragma ACCEL kernel`\n\n2. Automatically pipeline this loop.\n   Use: `#pragma ACCEL PIPELINE auto{}`\n\n3. Break the loop into smaller chunks with the auto-determined factor for tiling.\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of this loop in parallel with the auto-determined factor for parallelization.\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n5. Turn off automatic pipeline processing for the loop.\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n6. Tile the loop with a specific factor determined automatically.\n   Use: `#pragma ACCEL TILE FACTOR=auto{20}`\n\n7. Run the iterations of this loop in parallel with a specific factor determined automatically.\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n8. Turn off automatic pipeline processing for the loop.\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n9. Tile the loop into its original, full form without breaking it into smaller chunks.\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n10. Run the iterations of this loop in parallel with the auto-determined factor for parallelization.\n    Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n11. Automatically pipeline this loop with the specified factor.\n    Use: `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n12. Tile the loop with a specific factor determined automatically.\n    Use: `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n13. Run the iterations of this loop in parallel with the specified factor determined automatically.\n    Use: `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n\nJust use these natural language instructions to generate the provided HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given set of HLS codes written in C, use the following concise natural language descriptions for the specific #pragma values:\n\n1. Pipeline off the automatic pipelining for the loop:  \n   - Use `#pragma ACCEL PIPELINE auto{off}`.\n\n2. Use the original full form of the loop without breaking it into smaller chunks:  \n   - Implement `#pragma ACCEL TILE FACTOR=auto{1}`.\n\n3. Run the iterations of this loop one after the other, not in parallel:  \n   - Apply `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`.\n\n4. Treat the following function as a hardware kernel for acceleration:  \n   - Mark the function with `#pragma ACCEL kernel`.\n\nThe natural language instructions should suffice for generating the given HLS codes in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply automatic pipelining, tiling with a factor of 1 and flattening with a factor of 8 to the loops within the function \"kernel_doitgen\".", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes with the following instructions:\n\n1. Use hardware acceleration for the following function.\n   - Use: `#pragma ACCEL kernel`\n\n2. Do not automatically pipeline the outer and inner loops of the function.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process the outer and inner loops without breaking them into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1` for both loops\n\n4. Flatten the loop for automatic pipeline and optimization.\n   - Use: `#pragma ACCEL PIPELINE \"flatten\"` for the innermost loop\n   - Use: `#pragma ACCEL TILE FACTOR=4` for the innermost loop\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes from the given natural language instructions:\n\n1. The function 'kernel_correlation' should be marked as a hardware kernel for acceleration using the directive '#pragma ACCEL kernel'.\n2. For the loop calculating 'mean', apply automatic pipeline with '#pragma ACCEL PIPELINE auto{}'.\n3. Use automatic tiling with a factor of 8 for the 'mean' calculation loop using '#pragma ACCEL TILE FACTOR=auto{8}'.\n4. Parallelize the 'mean' calculation loop with a parallel factor '__PARA__L0' using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'.\n5. Disable automatic pipeline for the loop calculating 'stddev' using '#pragma ACCEL PIPELINE auto{off}'.\n6. Perform automatic tiling with a factor of 1 for the 'stddev' calculation loop with '#pragma ACCEL TILE FACTOR=auto{1}'.\n7. Parallelize the 'stddev' calculation loop with a parallel factor '__PARA__L1' using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'.\n8. Apply automatic pipeline to the loop for centering and reducing the column vectors with '#pragma ACCEL PIPELINE auto{}'.\n9. Use automatic tiling with a factor of 1 for the loop centering and reducing the column vectors with '#pragma ACCEL TILE FACTOR=auto{1}'.\n10. Parallelize the loop for centering and reducing the column vectors with a parallel factor '__PARA__L2' using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'.\n11. Apply automatic pipeline with a customized pipeline label for the calculation of the correlation matrix with '#pragma ACCEL PIPELINE auto{__PIPE__L3}'.\n12. Use automatic tiling with a customized tile label for the correlation matrix calculation loop with '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'.\n13. Parallelize the correlation matrix calculation loop with a parallel factor of 8 using '#pragma ACCEL PARALLEL FACTOR=auto{8}'.\n14. Set individual pipelining for the nested loop calculating the correlation matrix with '#pragma ACCEL PIPELINE auto{__PIPE__L7}'.\n15. Apply parallelization to the nested loop with labels '__PARA__L7_0' using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}'.\n\nThis set of natural language instructions should enable the generation of the given HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration, automatically pipeline the outer loop, and apply tiling with a factor of 80. For the inner loops, automatically pipeline with a factor of __PIPE__L1 and apply tiling with a factor of __TILE__L1. Additionally, run the iterations of the innermost loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n\n   Use the instruction: \"Treat the following function as a hardware kernel for acceleration\"\n\n2. Don't automatically pipeline this loop:\n\n   Use the instruction: \"Don't automatically pipeline this loop\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n\n   Use the instruction: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n4. Run the iterations of this loop one after the other, not in parallel:\n\n   Use the instruction: \"Run the iterations of this loop one after the other, not in parallel\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, and for the loops within it, turn off automatic pipelining and specify a tile factor of 1 for each loop. Also, flatten the loop pipeline and set a tile factor of 2 for the innermost loop.\"\n\nThis comprehensive but concise instruction covers the specific #pragma values to use for the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n- Treat the following function as a hardware kernel for acceleration.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Don't automatically pipeline this loop.\n- Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loops in their original form and do not break them into smaller chunks. Use automatic pipeline mode for the outermost loop. Set the tiling factor to automatically generate an appropriate value for both inner loops. Run iterations of the innermost loop in a flattened parallel pattern. Finally, mark the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the code, use the following natural language instructions:\n\n\"Designate the following function as a hardware kernel for acceleration.\"\n\n\"Turn off automatic pipelining for this loop.\"\n\n\"Divide this loop into smaller chunks using a factor of 4.\"\n\n\"Execute the iterations of this loop in parallel with a factor of 4.\"\n\n\"Execute the innermost loop iterations in parallel with a factor of 16.\"\n\n\"Apply a pipeline with a specific L2 setting to this innermost loop.\"\n\n\"Divide the innermost loop iterations into smaller chunks using a specific L2 tile factor.\"\n\n\"Execute the innermost loop iterations in parallel with a specific L2 factor.\"\n\n\"Execute the innermost loop iterations in parallel with a specific reduction factor for variable C, determined by a specific L3 parallel factor.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration. Pipeline the loops automatically. Do not tile the loops; process them in their original, full form without breaking into smaller chunks. Also, run the iterations of the loops one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C:\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]); // near-zero std. dev. handling\n  }\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{8}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nYou can use the following instruction:\n\n'For the kernel_correlation function, ensure that the loop for calculating mean and stddev are not pipelined automatically, use a tile factor of 1, and run the iterations of these loops in parallel. Then for the subsequent loop, turn off automatic pipelining, set a tile factor of 8, and run iterations in parallel. Finally, for the loop calculating the correlation matrix, use a pipeline factor and tile factor both set to auto, and run iterations in parallel.'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function 'kernel_doitgen' as a hardware kernel for acceleration. Pipeline the outermost loop with an automatically determined pipeline value. Tile the outermost loop with an automatically determined tile factor value of 8. Pipeline the middle loop and tile it with a factor of 1. Pipeline the innermost loop and tile it with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops using automatic pipelining and tiling with appropriate factors for parallelization and optimization. For the outer loop, automatically pipeline and tile with a factor of 1. For the second loop, manually enable pipelining, and automatically tile with a factor of 2. For the innermost loop, manually enable flattening pipeline and automatically tile with a factor of 1. Remember to treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given C codes, use the following natural language instructions:\n\n1. Apply pipeline at the outer loop with a defined factor, e.g., \"Turn off automatic loop pipelining\" for `#pragma ACCEL PIPELINE auto{off}`\n2. Apply tiling at the outer loop with a defined factor, e.g., \"Apply tiling with factor 80 to the outer loop\" for `#pragma ACCEL TILE FACTOR=80`\n3. Apply pipeline at the inner loop with a defined factor, e.g., \"Apply automatic loop pipelining at the inner loop with factor L1\" for `#pragma ACCEL PIPELINE auto{__PIPE__L1}`\n4. Apply tiling at the inner loop with a defined factor, e.g., \"Apply tiling with a factor L1 to the inner loop\" for `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`\n5. Run the iterations of the innermost loop one after the other, not in parallel, e.g., \"Run the iterations of the innermost loop sequentially\" for `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n6. Treat the `stencil3d` function as a hardware kernel for acceleration, e.g., \"Designate the `stencil3d` function as a hardware kernel\" for `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the loop processing without automatic pipelining: \n    Use the instruction \"Don't automatically pipeline this loop\" to describe `#pragma ACCEL PIPELINE auto{off}`\n\n2. For processing the loop in its original, full form without breaking it into smaller chunks:\n    Use the instruction \"Process this loop as a single unit\" to describe `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. For running the iterations of the loop one after the other, not in parallel: \n    Use the instruction \"Run the iterations of this loop sequentially, not in parallel\" to describe `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n4. For treating the following function as a hardware kernel for acceleration: \n    Use the instruction \"Designate the following function as a hardware kernel\" to describe `#pragma ACCEL kernel`\n\nBy following these instructions, the given HLS codes written in C can be generated to control the hardware synthesis of the program.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following loop executes in its original, full form without breaking it into smaller chunks:  \n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr, int nq, int np, double A[25][20][30], double C4[30][30], double sum[30]) {\n  int r;\n  int q;\n  int p;\n  int s;\n  \n  #pragma ACCEL PIPELINE auto{}\n  \n  #pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n    #pragma ACCEL PIPELINE auto{}\n    \n    #pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n      #pragma ACCEL PIPELINE auto{off}\n      \n      #pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and process the loops with the specified optimizations. Disable automatic pipelining for the outermost loop and set the tile factor to 4. Disable automatic pipelining and set the tile factor to 1 for the nested loops. Flatten the pipeline for the innermost loop. Run the iterations of all loops sequentially, not in parallel.\"\n\nThe respective #pragma values to use are:\n- `#pragma ACCEL kernel`\n- `#pragma ACCEL PIPELINE auto{off}`\n- `#pragma ACCEL TILE FACTOR=auto{4}`\n- `#pragma ACCEL PIPELINE auto{off}`\n- `#pragma ACCEL TILE FACTOR=auto{1}`\n- `#pragma ACCEL PIPELINE auto{flatten}`\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, you can use the following natural language instructions:\n- Use hardware acceleration for the following function\n- Automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel\n- Turn off automatic pipelining for this loop\n- Set the tile factor for this loop to 1\n- Set the parallel factor for this loop to the given value\n- Set the parallel factor for this loop to the given value\n- Automatically pipeline this loop with a factor of 2\n- Set the tile factor for this loop to 2\n- Set the parallel factor for this loop to the given value\n- Set the parallel factor for this loop to the given value\n- Automatically pipeline this loop with a given factor\n- Set the tile factor for this loop to the given value\n- Set the parallel factor for this loop to the given value", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration and pipeline the outer loop with auto scheduling. Tile the outer loop with a factor of 8 and also pipeline with auto scheduling. Inside the outer loop, process the iterations in parallel with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction: \n\n\"Optimize the loop by automatically pipelining, and apply tiling with a factor of 8 for the outermost loop and a factor of 1 for the inner loops, while turning off automatic pipelining for the innermost loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the outer loop\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process the outer loop with a tile factor of 80\n   - Use: `#pragma ACCEL TILE FACTOR=80`\n\n4. Automatically pipeline the middle loop with a specific label for the pipeline\n   - Use: `#pragma ACCEL PIPELINE auto{__PIPE__L1}`\n\n5. Process the middle loop with a tile factor determined automatically\n   - Use: `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`\n\n6. Run the iterations of the inner loop in parallel with a factor of 2\n   - Use: `#pragma ACCEL PARALLEL FACTOR=2`\n\nThese natural language instructions will guide the generation of the HLS codes written in C with the necessary `#pragma` values.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Run the iterations of the inner loop in parallel with a reduction operation, calculate 's' and 'q' without breaking the loop into smaller chunks, and do not pipeline the loop automatically. Treat the function as a hardware kernel for acceleration.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but very concise natural language instructions:\n\nFor the loop over variable `r`, automatically pipeline it:\n\n```\n#pragma ACCEL PIPELINE auto{}\n```\n\nFor the loop over variable `r`, use a tile factor of 1:\n\n```\n#pragma ACCEL TILE FACTOR=1\n```\n\nFor the loop over variable `q`, automatically pipeline it:\n\n```\n#pragma ACCEL PIPELINE auto{}\n```\n\nFor the loop over variable `q`, use a tile factor of 2:\n\n```\n#pragma ACCEL TILE FACTOR=2\n```\n\nFor the innermost loop over variable `p`, automatically pipeline it:\n\n```\n#pragma ACCEL PIPELINE auto{}\n```\n\nFor the innermost loop over variable `p`, use a tile factor of 1:\n\n```\n#pragma ACCEL TILE FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, pipeline the outer loop with automatic settings, tile the outer loop with automatic factor, pipeline the inner loop with automatic L1 settings, tile the inner loop with automatic L1 factor, and run the iterations of the innermost loop one after the other in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration \n\n```c\n#pragma ACCEL kernel\n\nvoid stencil3d(long C0, long C1, long orig[39304], long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_doitgen' function as a hardware kernel for acceleration. \nPipeline the loops automatically, and tile the loops with a factor of 8 for the outermost loop and a factor of 1 for the inner loops. Additionally, pipeline the inner loops with a 'flatten' option.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the function 'stencil3d' as a hardware kernel for acceleration, automatically pipeline the outer loop, tile it using factor 8, pipeline the inner loop with a specific factor, and run the innermost loop iterations in serial rather than in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, and then pipeline the outermost loop with auto pipelining, tile the middle loop with a factor of 2, and disable pipelining for the innermost loop while tiling it with a factor of 1.\"\n\nWhen following this instruction, you would use the following #pragma values:\n1. #pragma ACCEL kernel\n2. #pragma ACCEL PIPELINE auto{}\n3. #pragma ACCEL TILE FACTOR=2\n4. #pragma ACCEL PIPELINE auto{off}\n5. #pragma ACCEL TILE FACTOR=1", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you should use the following comprehensive but concise natural language instruction:\n\n\"Treat the following function as a hardware kernel for acceleration and don't automatically pipeline the loops; process each loop in its original form, without breaking it into smaller chunks; run the iterations of the loops in parallel with the factor value set to the loop index; for the second set of loops, pipeline them manually and set the tile factor to 1; center and reduce the column vectors using a tile factor of 30 and run the iterations in parallel with the factor value set to the loop index; finally, calculate the correlation matrix with manual pipelining, a specific tile factor, and parallel execution with a factor of 4 for inner loops.\"\n\nThis natural language instruction covers the pragma values for the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Convert this function into an HLS hardware kernel for acceleration, enable auto pipelining for the outer loop and its innermost loop, and set the tile factor to auto for both loops as well.\"", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Use pragma ACCEL PIPELINE to turn off automatic pipelining, ACCEL TILE with a factor of 1 to process the loop in its original form, and ACCEL PARALLEL with a factor of 1 to run loop iterations sequentially.\"", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate hardware kernel for acceleration and automatically pipeline the outer loop and inner loop. Use a tile factor of 8 for both loops, and parallelize the innermost loop with a factor of 2. The pragma values to use are:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{8}\n\n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate high-level synthesis (HLS) code for the given C function 'stencil3d' with the following instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n   Use: #pragma ACCEL kernel\n\n2. Automatically pipeline the outer loop for performance optimization.\n   Use: #pragma ACCEL PIPELINE auto{}\n\n3. Apply tiling optimization with an automatically determined tile factor for the outer loop.\n   Use: #pragma ACCEL TILE FACTOR=auto{80}\n\n4. Automatically pipeline the inner loop with a specific pipeline level for further optimization.\n   Use: #pragma ACCEL PIPELINE auto{__PIPE__L1}\n\n5. Apply tiling optimization with an automatically determined tile factor for the inner loop.\n   Use: #pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n\n6. Parallelize the innermost loop with an automatically determined parallel factor for efficient hardware execution.\n   Use: #pragma ACCEL PARALLEL FACTOR=auto{2}", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instruction:\n\n**Utilize the function as a hardware kernel for acceleration. Pipeline the outermost loop automatically with no specific pipeline factor. Tile the outermost loop automatically with a factor of 1. Pipeline the middle loop with a manual control for pipelining off. Tile the middle loop automatically with a factor of 8. Pipeline the innermost loop automatically with the default pipeline factor. Tile the innermost loop automatically with a factor of 1.**\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use the following natural language instructions for pragma values:\n\n1. For disabling automatic loop pipelining: \n   Use `#pragma ACCEL PIPELINE auto{off}` to process this loop without automatic pipeline optimization.\n\n2. For processing the loop in its original, full form without breaking it into smaller chunks:\n   Use `#pragma ACCEL TILE FACTOR=auto{1}` to ensure the loop is not split into smaller chunks.\n\n3. For running the loop iterations one after the other, not in parallel:\n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` for the outer loop and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}` for the inner loop to disable parallel processing.\n\n4. For re-enabling automatic loop pipelining:\n   Use `#pragma ACCEL PIPELINE auto{}` to allow the compiler to automatically pipeline the loop.\n\n5. For setting the tile factor to 2 for loop optimization: \n   Use `#pragma ACCEL TILE FACTOR=auto{2}` to enable tiling with a factor of 2 for loop optimization.\n\n6. For parallelizing the loop with a specified factor:\n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` to parallelize the outer loop and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}` for the inner loop with the specified factor.\n\n7. For subsequent loop optimization and parallelization:\n   Use `#pragma ACCEL PIPELINE auto{}` and `#pragma ACCEL TILE FACTOR=auto{1}` for the following loops to enable automatic pipelining and tiling with a factor of 1.\n\n8. For parallelizing the loop with a factor of 8:\n   Use `#pragma ACCEL PARALLEL FACTOR=auto{8}` to parallelize the loop with a factor of 8.\n\nFor the given C code, use these natural language instructions to guide the insertion of the relevant #pragma values to achieve the desired HLS optimization and parallelization.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "- Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{off}`\n- Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=25`\n- Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=__PARA__L0`\n- Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{1}`\n- Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n- Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=__PARA__L1`\n- Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{1}`\n- Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n- Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=__PARA__L2`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loops, use automatic pipeline and tiling, and parallelize with factor auto{__PARA__L0}, auto{__PARA__L1}, auto{__PARA__L2}, and auto{4} respectively.\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]){\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    stddev[j] = (stddev[j] <= eps?1.0:stddev[j]);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, and for the loops within this function:\n1. Enable automatic pipelining.\n2. Use a tile factor of 1 for the loops, allowing them to run in their original, full form without breaking into smaller chunks.\n3. Run the loop iterations one after another, not in parallel according to the code's structure.\n4. Disable automatic pipelining for a specific loop.\n5. Set a tile factor of 20.\n6. Allow loop iterations to run in parallel for optimized performance.\n7. Enable automatic pipelining for another specific loop.\n8. Use a tile factor of 1.\n9. Run loop iterations one after another, not in parallel.\n10. Enable automatic pipelining for a specific loop.\n11. Set a tile factor based on the specific loop parameters.\n12. Allow loop iterations to run in parallel using a factor of 4.\"\n\nUsing these instructions should lead to the generation of the provided HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Use hardware acceleration for the following function, disable automatic pipelining for the loop, set the loop tile factor to 1, and run loop iterations sequentially rather than in parallel.'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Do not automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Center and reduce the column vectors. Calculate the m * m correlation matrix.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the respective HLS codes, use the following natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n   - For the function `kernel_correlation`, use `#pragma ACCEL kernel`.\n\n2. Don't automatically pipeline the loops.\n   - For all loops, use `#pragma ACCEL PIPELINE \"off\"`.\n\n3. Process the loops in their original, full form without breaking into smaller chunks.\n   - For all loops, use `#pragma ACCEL TILE FACTOR=1`.\n\n4. Run the iterations of the loops one after the other, not in parallel.\n   - For the first loop, use `#pragma ACCEL PARALLEL FACTOR=1`.\n   - For the second loop, use `#pragma ACCEL PARALLEL FACTOR=1`.\n   - For the third loop, use `#pragma ACCEL PARALLEL FACTOR=1`.\n   - For the fourth loop, use `#pragma ACCEL PARALLEL FACTOR=1`.\n   - For the fifth loop, use `#pragma ACCEL PARALLEL FACTOR=1`.\n\n5. Set the specific auto pragmas where indicated by the placeholders.\n   - Set the auto pragma values for `PIPELINE`, `TILE` and `PARALLEL` where specified in the code. Use the placeholders `auto{off}`, `auto{1}`, `auto{__PARA__L0}`, `auto{__PARA__L4}`, `auto{__PARA__L1}`, `auto{__PARA__L5}`, `auto{__PARA__L2}`, `auto{__PARA__L6}`, `auto{__PIPE__L3}`, `auto{__TILE__L3}`, `auto{__PARA__L7_0}`, `auto{__PIPE__L7}` accordingly.\n\nFollowing these natural language instructions will generate the respective HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Treat the 'stencil3d' function as a hardware kernel for acceleration.\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline the outer loop of the stencil3d function.\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. Process the outer loop of the stencil3d function in its original, full form without breaking it into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. Run the inner loop iterations of the stencil3d function one after the other, not in parallel.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\nThese instructions provide a comprehensive natural language description of the specific #pragma values to be used for generating the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n1. Treat the 'kernel_correlation' function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops.\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n4. Run the iterations of the loops one after the other, not in parallel.\n5. Enable automatic pipelining for the loops.\n6. Break the loops into smaller chunks with a factor of 4.\n7. Run the iterations of the loops in parallel with a factor of 1.\n8. Break the loops into smaller chunks with a factor of 1.\n9. Run the iterations of the loops in parallel with a factor of 1 for the first set of loops.\n10. Enable automatic pipelining for the loops.\n11. Break the loops into smaller chunks with a factor of 1.\n12. Run the iterations of the loops in parallel with a factor of 1 for the second set of loops.\n13. Enable automatic pipelining for the loops.\n14. Break the loops into smaller chunks with a factor of 1.\n15. Run the iterations of the loops in parallel with a factor of 1 for the third set of loops.\n16. Enable automatic pipelining for the loops with a specific pipeline and tile factor for each respective loop.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel with a factor decided automatically. Pipeline the loop after that. Then, apply tiling to the loop. Finally, parallelize the last loop with an automatic factor and reduce the nested loop parallelization with another automatic factor. Mark the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outermost loop.\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n4. Automatically pipeline the innermost loop.\n5. Process the innermost loop in its original form without breaking it into smaller chunks.\n\nThese instructions will result in the corresponding #pragma values in the given C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Pipeline the loops automatically. Set the tile factor to 1 to process the loops in their original form. Run the loop iterations in parallel with a factor of 1. Disable automatic pipelining for the following loops. Set the tile factor to 4 for the specified loops. Run the loop iterations in parallel with a factor of 1. Reset automatic pipelining for the next loops. Modify the tile factor to 1 for these loops. Run the loop iterations in parallel with specified factors. Adjust pipeline settings and tile factor for the specific loop. Set the tile factor and parallel factor to 4 for the last loop.\"\n\nThis instruction covers all the pragma values and their corresponding usage as seen in the provided code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code, use the following natural language instruction:\n\"Designate the following function as a hardware kernel for acceleration, parallelize the loop for variable i and unroll it automatically, pipeline the following loop automatically with generated factors, and tile the loop for variable i with a factor calculated automatically.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C codes, you should use the following natural language instruction:\n\n\"Mark the entire function as a hardware kernel for acceleration. Also, turn off automatic pipeline optimization for the outer loop and set the tiling factor to 80. For the inner loops, use automatic pipeline with a specific offloading level, set the tiling factor, and set the parallelization factor to 2.\"\n\nThis instruction summarizes the use of #pragma values in the given HLS C codes.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\"For the loop iterating over 's', run the iterations in parallel with auto-parallelization factor of 1; pipeline the loop for optimization without specifying a specific auto-pipeline factor; tile the loop for optimization with a factor of 1; for the nested loop iterating over 'q', run the outer loop iterations in parallel with auto-parallelization factor of 2, and run the inner loop iterations in parallel with reduction auto-parallelization factor of 1.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_bicg' function as a hardware kernel for acceleration. Then, for the first loop, run the iterations one after the other, and don't pipeline it. For the second loop, parallelize with a factor of 2, and implement reduction parallelization for the inner loop with a factor of 1. Set tiling factor to 1 for the second loop as well.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided C code, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Utilize auto-pipelining for the outermost loop and factorize tiling for both outer loops with a factor of 1 for coherent execution.\"\n\nThis instruction entails using '#pragma ACCEL kernel' to mark the function as a hardware kernel, '#pragma ACCEL PIPELINE auto{}' for auto-pipelining the outermost loop, and '#pragma ACCEL TILE FACTOR=auto{1}' for factorizing tiling for both outer loops with a factor of 1 for coherent execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction for the specific #pragma values:\n\n1. Simplify the code by not automatically pipelining the loop: use `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process the loop in its original, full form without breaking it into smaller chunks: use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Run the iterations of this loop one after the other, not in parallel: use `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n4. Treat the following function as a hardware kernel for acceleration: use `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`  \nRun the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`  \nDon't automatically pipeline this loop: `#pragma ACCEL PIPELINE \"off\"`  \nTreat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Don't automatically pipeline this loop. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   - Description: Designate the function as a hardware kernel for acceleration\n   - Generated #pragma: #pragma ACCEL kernel\n\n2. Automatically pipeline this loop:\n   - Description: Enable automatic pipelining for the loop\n   - Generated #pragma: #pragma ACCEL PIPELINE auto{}\n\n3. Break this loop into smaller chunks to process it in its full form:\n   - Description: Use a tile factor of 1 to avoid breaking the loop into smaller chunks\n   - Generated #pragma: #pragma ACCEL TILE FACTOR=auto{1}\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   - Description: Use a parallel factor of 1 to process the loop sequentially\n   - Generated #pragma: #pragma ACCEL PARALLEL FACTOR=auto{1}\n\n5. Turn off automatic pipelining for this loop:\n   - Description: Disable automatic pipelining for the loop\n   - Generated #pragma: #pragma ACCEL PIPELINE auto{off}\n\n6. Set the tile factor for this loop to a value determined automatically:\n   - Description: Automatically determine the tile factor for this loop\n   - Generated #pragma: #pragma ACCEL TILE FACTOR=auto{20}\n\n7. Set the parallel factor for this loop to a value determined automatically:\n   - Description: Automatically determine the parallel factor for this loop\n   - Generated #pragma: #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n\n8. Disable automatic pipelining for this loop:\n   - Description: Turn off automatic pipelining for this specific loop\n   - Generated #pragma: #pragma ACCEL PIPELINE auto{off}\n\n9. Automatically determine the tile factor for this loop:\n   - Description: Use an automatically determined tile factor for this loop\n   - Generated #pragma: #pragma ACCEL TILE FACTOR=auto{1}\n\n10. Automatically determine the parallel factor for this loop:\n    - Description: Use an automatically determined parallel factor for this loop\n    - Generated #pragma: #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n\n11. Set the pipelining characteristic for this loop to a value determined automatically:\n    - Description: Automatically determine the pipelining characteristic for this loop\n    - Generated #pragma: #pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n12. Set the tile factor for this loop to a value determined automatically:\n    - Description: Automatically determine the tile factor for this loop\n    - Generated #pragma: #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n13. Set the parallel factor for this loop to 4:\n    - Description: Use a specific parallel factor of 4 for this loop\n    - Generated #pragma: #pragma ACCEL PARALLEL FACTOR=auto{4}", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, don't automatically pipeline the loops, and use a tile factor of 1 for the outer loop, 2 for the middle loop, and 1 for the inner loop. Additionally, flatten the pipeline for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function as a hardware kernel for acceleration, disable automatic loop pipelining, set tile factor to 1 for loop blocking, and set parallel factor to 1 for certain loops.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nThen, for different loops within the function:\n- Don't automatically pipeline the loops\n- Process the loops in their original, full form without breaking them into smaller chunks\n- Run the iterations of the loops one after the other, not in parallel\n- Determine the specific factors for tiling and parallelization based on the naming convention provided within each pragma.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow the natural language instruction as described below:\n\n- Treat the function as a hardware kernel for acceleration.\n- Don't automatically pipeline the loops.\n- Process the loops in their original, full form without breaking them into smaller chunks by setting the TILE FACTOR to 1.\n- Run the iterations of the loops in parallel with a factor of 80 for the first loop, and a factor of 100 for the second loop.\n- Don't automatically pipeline the loops.\n- Process the loops in their original, full form without breaking them into smaller chunks by setting the TILE FACTOR to 1.\n- Run the iterations of the loops in parallel with a factor of 80 for the first loop, and a factor of 100 for the second loop.\n- Don't automatically pipeline the loops.\n- Process the loops with a TILE FACTOR of 2.\n- Run the iterations of the loops in parallel with a factor of 80 for the first loop, and a factor of 100 for the second loop.\n- Automatically pipeline the loop with a factor of 3.\n- Process the loops with a TILE FACTOR of 3.\n- Run the iterations of the loops in parallel with a factor of 8.\n\nFollowing these comprehensive but concise instructions will generate the given HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform parallel processing for the first loop with a factor of 1, disable automatic pipelining for this code, set tile factor to 1 for optimization, and perform parallel processing for the second loop with a factor of 2, reduce further parallelism to factor 1 for inner loop operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions:\n\n1. Treat the `kernel_correlation` function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops.\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n4. Run the iterations of the loops one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\nOptimize the loop so that it is not automatically pipelined: \n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\nProcess the loop in its original, full form without breaking it into smaller chunks:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\nRun the iterations of this loop one after the other, not in parallel:\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```\n\nTreat the following function as a hardware kernel for acceleration:\n```c\n#pragma ACCEL kernel\n```", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n- First, designate the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n- Then, for the loop `spmv_1`, turn off automatic pipelining with `#pragma ACCEL PIPELINE \"off\"`.\n- Next, process the loop in its original, full form without breaking into smaller chunks using `#pragma ACCEL TILE FACTOR=1`.\n- Finally, run the iterations of the loop one after the other, not in parallel, with `#pragma ACCEL PARALLEL FACTOR=1`.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in parallel for the second loop iteration, automatically pipeline the loop, and tile the loop with the automatic factor for the first loop iteration. Make sure to treat this function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the hardware kernel for spmv function, use the following pragma directives:\n- 'Don't automatically pipeline this loop' for '#pragma ACCEL PIPELINE auto{off}'\n- 'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=auto{1}'\n- 'Run the iterations of this loop one after the other, not in parallel' for '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n- 'Treat the following function as a hardware kernel for acceleration' for '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the outer loop or the inner loop. Use a tile factor of 80 for the outer loop and the inner loop. Run the iterations of the innermost loop with a parallel factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, follow these natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Run the iterations of the first nested loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the first nested loop.\n\n3. Don't automatically pipeline the loops.\n   - Use: `#pragma ACCEL PIPELINE auto{}`\n\n4. Process the second nested loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}` for the second nested loop.\n\n5. Run the iterations of the second nested loop in parallel, and divide the iterations into two sets for parallel execution.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the second nested loop.\n   - Use: `#pragma ACCEL PARALLEL reduction FACTOR=auto{1}` for the innermost nested loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions:\n\n1. Treat the 'kernel_correlation' function as a hardware kernel for acceleration.(\"#pragma ACCEL kernel\")\n\n2. Don't automatically pipeline the loops.(\"#pragma ACCEL PIPELINE \"off\"\")\n\n3. Process the loops in their original, full form without breaking them into smaller chunks. Use an auto factor of 8 for tiling.(\"#pragma ACCEL TILE FACTOR=8\")\n\n4. Run the iterations of the loops one after the other, not in parallel. Use an auto factor.(\"#pragma ACCEL PARALLEL FACTOR=__PARA__L0\")\n\n5. Don't automatically pipeline the loops.(\"#pragma ACCEL PIPELINE \"off\"\")\n\n6. Process the loops in their original, full form without breaking them into smaller chunks. Use an auto factor of 1 for tiling.(\"#pragma ACCEL TILE FACTOR=1\")\n\n7. Run the iterations of the loops one after the other, not in parallel. Use an auto factor.(\"#pragma ACCEL PARALLEL FACTOR=__PARA__L1\")\n\n8. Disable automatic pipelining for this loop.(\"#pragma ACCEL PIPELINE off\")\n\n9. Process the loop in its original, full form without breaking it into smaller chunks. Use an auto factor of 1 for tiling.(\"#pragma ACCEL TILE FACTOR=1\")\n\n10. Run the iterations of this loop one after the other, not in parallel. Use an auto factor.(\"#pragma ACCEL PARALLEL FACTOR=__PARA__L2\")\n\n11. Set the pipeline directive to a specific value. Use \"auto\" for the pipeline factor.(\"#pragma ACCEL PIPELINE auto\")\n\n12. Set the tiling directive to a specific value. Use \"auto\" for the tile factor.(\"#pragma ACCEL TILE FACTOR=auto\")\n\n13. Run the iterations of the loop in parallel with a factor of 4.(\"#pragma ACCEL PARALLEL FACTOR=4\")\n\n14. Set the pipeline directive to a specific value. Use \"__PIPE__L7\" for the pipeline factor.(\"#pragma ACCEL PIPELINE auto{__PIPE__L7}\")\n\n15. Run the iterations of the loop in parallel with a specific factor. Use \"__PARA__L7_0\" for the parallel factor.(\"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\")", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, the following natural language instruction can be used:\n\n\"Designate the following function as a hardware kernel for acceleration, then process the loops with a tiling factor of 8 and pipeline them automatically. Within the innermost loop, turn off automatic pipelining and flatten the pipeline to improve performance.\"\n\nThis natural language instruction intends to convey the use of the following HLS pragmas:\n- '#pragma ACCEL kernel' for designating the function as a hardware kernel.\n- '#pragma ACCEL TILE FACTOR=8' for processing the loops with a tiling factor of 8.\n- '#pragma ACCEL PIPELINE auto' for automatically pipelining the loops.\n- '#pragma ACCEL PIPELINE auto{off}' for turning off automatic pipelining within the innermost loop.\n- '#pragma ACCEL PIPELINE auto{flatten}' for flattening the pipeline within the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To fully accelerate the `kernel_doitgen` function in C, you can follow these concise natural language instructions:\n\n\"Automatically pipeline all applicable loops\" - This will use `#pragma ACCEL PIPELINE auto{}` to enable automatic loop pipelining.\n\n\"Process each loop in its original form without breaking it into smaller chunks\" - Use `#pragma ACCEL TILE FACTOR=auto{1}` to apply the original loop tiling factor.\n\n\"Turn off automatic loop pipelining for the outer loop\" - Add `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipelining for the outer loop.\n\n\"Flatten the innermost loop to improve parallelism\" - Apply `#pragma ACCEL PIPELINE auto{flatten}` to flatten the innermost loop.\n\nFollowing these comprehensive natural language instructions will efficiently accelerate the designated function in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To achieve hardware acceleration, apply the following pragmas to the given C code:\n\n1. Mark the function as a hardware kernel for acceleration:\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Automatically pipeline the outer two loops and process them in their original, full form without breaking into smaller chunks:\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n3. Turn off automatic pipelining for the second loop:\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n4. Flatten the innermost loop and process it in its original, full form without breaking into smaller chunks:\n   ```c\n   #pragma ACCEL PIPELINE auto{flatten}\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\nBy following these instructions, the code will be optimized for hardware acceleration using HLS pragmas.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the spmv function as a hardware kernel for acceleration\nUse manual pipeline control, do not automatically pipeline this loop\nUse the original, full form of the loop without breaking it into smaller chunks\nRun the iterations of the loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code, use the following natural language instruction: \n\n1. Define the function as a hardware kernel for acceleration.\n2. Run the iterations of the first loop in parallel but limit the parallelism to a factor determined automatically.\n3. Pipeline the loop without specifying a specific factor for the pipeline stages.\n4. Process the second loop in its original, full form without breaking it into smaller chunks and limit the tiling factor to a value determined automatically.\n5. Run the outer loop iterations in parallel with a factor determined automatically, and within this loop, run the inner loop iterations in parallel for reduction with a factor also determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C for the given natural language instruction, here are the comprehensive but very concise instructions:\n\n1. For the first loop, \"Do not automatically pipeline this loop\" using `#pragma ACCEL PIPELINE \"off\"`\n2. For breaking the loop into its original, full form without smaller chunks, \"Process this loop with a tile factor of 1\" using `#pragma ACCEL TILE FACTOR=1`\n3. For running loop iterations one after the other and not in parallel, \"Run the iterations of this loop sequentially, not in parallel\" using `#pragma ACCEL PARALLEL FACTOR=1`\n4. For treating the function as a hardware kernel for acceleration, \"Treat the following function as a hardware kernel for acceleration\" using `#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions:\n1. Mark the function as a hardware kernel for acceleration using '#pragma ACCEL kernel'.\n2. Enable automatic pipelining for all loops using '#pragma ACCEL PIPELINE auto{}'.\n3. Use automatic tiling with a factor of 1 for the outermost loop and a factor of 2 for the middle loop using '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL TILE FACTOR=auto{2}' respectively.\n4. Disable automatic pipelining for the innermost loop using '#pragma ACCEL PIPELINE auto{off}'.\n5. With these instructions, your HLS codes will be generated to achieve the desired hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C codes, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration and then apply the following optimizations:\n- Turn off automatic pipelining for the outer loop.\n- Set the tiling factor to 1 for the outer loop.\n- Enable automatic pipelining with an unspecified factor for the middle loop.\n- Set the tiling factor to 2 for the middle loop.\n- Enable automatic pipelining with an unspecified factor for the inner loop.\n- Set the tiling factor to 1 for the inner loop.\"\n\nThis comprehensive instruction encapsulates the necessary pragma values and the process for optimizing the HLS C codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Transform this function into a hardware kernel, and apply off pipelining, automatic tiling with factor 1, and automatic parallelization with corresponding factors for each loop.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in parallel with a factor determined automatically, and run the iterations of the inner loops in parallel with a factor computed automatically. For the outer loop, do not automatically pipeline it. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following #pragma values:\n1. Use '#pragma ACCEL kernel' to indicate that the following function is a hardware kernel for acceleration.\n2. Use '#pragma ACCEL PIPELINE auto{off}' to disable automatic pipeline optimization for the outer loop.\n3. Use '#pragma ACCEL TILE FACTOR=auto{1}' to apply tiling with a factor of 1 to the outer loop.\n4. Use '#pragma ACCEL PIPELINE auto{}' and '#pragma ACCEL TILE FACTOR=auto{1}' for the inner loops to apply default pipeline and tiling with a factor of 1 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outermost loop.\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n4. Don't automatically pipeline the middle loop.\n5. Process the middle loop with a tile factor of 4.\n6. Don't automatically pipeline the innermost loop.\n7. Process the innermost loop in its original, full form without breaking it into smaller chunks.\n\nThese natural language instructions correspond to the following #pragma values respectively:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE off\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PIPELINE off\n\n#pragma ACCEL TILE FACTOR=4\n\n#pragma ACCEL PIPELINE off\n\n#pragma ACCEL TILE FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. Process the loop with hardware acceleration\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline this loop\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of this loop one after the other, not in parallel\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```\n\nBy incorporating these natural language instructions, you can accurately generate the given HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{200}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following comprehensive but concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and apply the following optimizations: pipeline the outer loop, disable automatic loop pipelining, tile the loop with a factor of 1 for the outermost loop, and flatten the innermost loop with automatic pipelining.\"\n\nThis instruction explicitly defines the process and specifies the pragma values to use for the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Run the iterations of the first loop of the function in parallel with a factor automatically determined. Do not automatically pipeline the first loop. Process the second loop in its original, full form without breaking it into smaller chunks and run it in parallel with a factor automatically determined. Also, use reduction operation in parallel with a factor automatically determined for the nested loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Parallelize the loop with a factor of 2, then pipeline the loop, and finally tile the loop with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but concise natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration, use automatic pipeline for each loop, set tile factor to auto for each loop, and enable automatic parallelization with factor values based on loop iterations.\"\n\nThese instructions cover the use of #pragma ACCEL kernel, #pragma ACCEL PIPELINE auto{}, #pragma ACCEL TILE FACTOR=auto{}, and #pragma ACCEL PARALLEL FACTOR=auto{__PARA__Lx} for each loop, where x is the specific loop number.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and for the loops inside: don't automatically pipeline them and process them in their original, full form without breaking them into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in its original, full form without breaking it into smaller chunks. \nRun the iterations of this loop one after the other, not in parallel. \nDon't automatically pipeline this loop. \nTreat the 'spmv' function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations of the second for loop in parallel with a factor of 8, and perform reduction for the inner loop in parallel with a factor of 1. Pipeline the loop and tile it with a factor of 1. Treat the entire function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "## Task: Generate HLS code for specific natural language instructions\nTo process the loop in its original, full form without breaking it into smaller chunks, use '#pragma ACCEL TILE FACTOR=1'.\n\nTo automatically pipeline this loop with off mode, use '#pragma ACCEL PIPELINE auto{off}'.\n\nTo run the iterations of this loop one after the other, not in parallel, use '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n\nTo treat the following function as a hardware kernel for acceleration, use '#pragma ACCEL kernel'.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\nProcess the entire `kernel_doitgen` function as a hardware kernel for acceleration and optimize the loops with the following pragma values:\n- Use auto pipelining for the outer loop: `#pragma ACCEL PIPELINE auto{}`\n- Tile the loops with a factor of 8 for `r` and a factor of 1 for `q` and `p`: `#pragma ACCEL TILE FACTOR=auto{8}`, `#pragma ACCEL TILE FACTOR=auto{1}`\n- Turn off auto pipelining for the inner loops: `#pragma ACCEL PIPELINE auto{off}`\n- Tile the inner loops with a factor of 1: `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function as a hardware kernel for acceleration. Pipeline the loops with auto{off} directive to avoid automatic pipelining. Tile the loops with a factor of 1 using the auto{1} directive. Parallelize the loops with a factor of __PARA__L0, __PARA__L1, __PARA__L2, and 8 respectively using the auto directive.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply pipeline off to the outer loop and flatten the pipelining in the inner loops to avoid auto-pipelining and flatten the nested loops respectively. Use tile factor 8 for the outer loop and tile factor 1 for the inner loops to enable loop tiling. Additionally, mark the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the C program, using natural language instructions, you can use the following comprehensive but concise instructions for each #pragma directive:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Describe as: \"Mark this function as a hardware kernel\"\n   - HLS code: #pragma ACCEL kernel\n\n2. Don't automatically pipeline this loop\n   - Describe as: \"Turn off automatic pipelining for this loop\"\n   - HLS code: #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Describe as: \"Set the tile factor of this loop to 1\"\n   - HLS code: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Describe as: \"Set the parallel factor of this loop to 1\"\n   - HLS code: #pragma ACCEL PARALLEL FACTOR=1\n\n5. Apply these natural language instructions to the given C program, and the HLS codes will be generated accordingly.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks and pipeline them without automatic optimization. The function \"kernel_doitgen\" should be treated as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code, use the following concise natural language instructions for the specific #pragma values:\n\n1. For the kernel_correlation function:\n   - Treat the following function as a hardware kernel for acceleration\n   - #pragma ACCEL kernel\n\n2. For the loop for calculating mean:\n   - Run the iterations of this loop one after the other, not in parallel\n   - Process this loop in its original, full form without breaking it into smaller chunks\n   - Don't automatically pipeline this loop\n   - #pragma ACCEL PARALLEL FACTOR=1\n   - #pragma ACCEL TILE FACTOR=1\n   - #pragma ACCEL PIPELINE \"off\"\n\n3. For the loop for calculating stddev:\n   - Run the iterations of this loop in parallel, using a parallel factor of auto{__PARA__L1}\n   - Break this loop into smaller chunks to optimize for parallel execution\n   - Automatically pipeline this loop with a pipeline factor specific to the loop\n   - #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n   - #pragma ACCEL TILE FACTOR=auto{8}\n   - #pragma ACCEL PIPELINE auto{off}\n\n4. For the loop for centering and reducing column vectors:\n   - Run the iterations of this loop in parallel, using a parallel factor of auto{__PARA__L2}\n   - Process this loop in its original, full form without breaking it into smaller chunks\n   - Don't automatically pipeline this loop\n   - #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n   - #pragma ACCEL TILE FACTOR=auto{1}\n   - #pragma ACCEL PIPELINE \"off\"\n\n5. For the loop for calculating the correlation matrix:\n   - Run the iterations of this loop in parallel, using a parallel factor of 4\n   - Break this loop into smaller chunks to optimize for data transfer\n   - Automatically pipeline this loop with a pipeline factor specific to the loop\n   - #pragma ACCEL PARALLEL FACTOR=4\n   - #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n   - #pragma ACCEL PIPELINE auto{__PIPE__L3}\n\nThese natural language instructions correspond to the specific #pragma values used in the given HLS code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the outer loop with automatic pipelining. Tile the loops with an auto factor of 80. Parallelize the innermost loop with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you would use the following natural language instructions for the specific #pragma values:\n\n1. Don't automatically pipeline this loop: '#pragma ACCEL PIPELINE auto{off}'\n2. Process this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Run the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n4. Treat the following function as a hardware kernel for acceleration: '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Automatically pipeline all loops in a shallow and wide manner, and break them into smaller chunks to maximize throughput'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Treat the following function as a hardware kernel for acceleration, turn off auto-pipelining for the outer loop, set automatic tiling factor to 80 for both loops, and execute the innermost loop iterations in parallel with a factor of 1.\"\n\nThis instruction encompasses the pragma values and their respective behaviors in a comprehensive but concise manner.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes using natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   \n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline these loops.\n\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n3. Process these loops in their original, full form without breaking them into smaller chunks.\n\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n4. Run the iterations of these loops one after the other, not in parallel.\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops using hardware kernel acceleration, and apply the following optimizations:\n1. Pipeline the outermost loop\n2. Tile the loops with a tile factor of 1", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes in C that carry out hardware acceleration. Ensure that the loop is not automatically pipelined, is processed in its original form without breaking into smaller chunks, and runs iterations sequentially without parallelization. Tag the function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n1. Do not automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{off}`.\n2. Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{25}`.\n3. Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`.\n4. Do not automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{off}`.\n5. Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`.\n6. Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`.\n7. Do not automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{}`.\n8. Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`.\n9. Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`.\n\nUse these natural language instructions to generate the corresponding HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n- Treat the function 'kernel_doitgen' as a hardware kernel for acceleration.\n- Turn off automatic pipelining for the outermost loop.\n- Break down the outer loop into smaller chunks with a factor of 2.\n- Turn off automatic pipelining for the middle loop.\n- Process the middle loop in its original, full form without breaking it into smaller chunks.\n- Flatten the innermost loop for better parallelism.\n- Process the innermost loop in its original, full form without breaking it into smaller chunks.\n\nThis comprehensive but concise natural language instruction will guide the generation of the given HLS codes in C using the specific #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive and concise natural language instructions for the specific #pragma values:\n\n1. For the first loop, do not automatically pipeline it:\n   Use `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process the first loop in its original, full form without breaking it into smaller chunks:\n   Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Run the iterations of the first loop one after the other, not in parallel:\n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n4. For the second loop, do not automatically pipeline it:\n   Use `#pragma ACCEL PIPELINE auto{off}`\n\n5. Process the second loop with a tile factor of 4:\n   Use `#pragma ACCEL TILE FACTOR=auto{4}`\n\n6. Run the iterations of the second loop in parallel:\n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n7. For the third loop, do not automatically pipeline it:\n   Use `#pragma ACCEL PIPELINE auto{}`\n\n8. Process the third loop in its original, full form without breaking it into smaller chunks:\n   Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n9. Run the iterations of the third loop in parallel:\n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n10. For the fourth loop, pipeline it with a pipeline factor of 3:\n    Use `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n11. Process the fourth loop with a tile factor determined automatically:\n    Use `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n12. Run the iterations of the fourth loop in parallel with a factor of 8:\n    Use `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n\nApply these instructions accordingly to achieve the desired HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C:\n\nUse the directive 'pragma ACCEL kernel' to indicate that the following function should be treated as a hardware kernel for acceleration.\n\nTo disable automatic pipelining for the loops, use 'pragma ACCEL PIPELINE auto{off}'.\n\nFor the outer loop that iterates over 'r', use 'pragma ACCEL TILE FACTOR=auto{2}' to break the loop into smaller chunks.\n\nFor the inner loops that iterate over 'q' and 'p', use 'pragma ACCEL PIPELINE auto{flatten}' to automatically flatten the loops and 'pragma ACCEL TILE FACTOR=auto{1}' to process them in their original, full form without breaking into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the specific #pragma values:\n\n1. For the outer loop 'for (r = 0; r < 25; r++)', use '#pragma ACCEL PIPELINE \"off\"' to prevent automatic pipelining.\n2. For the middle loop 'for (q = 0; q < 20; q++)', use '#pragma ACCEL TILE FACTOR=4' to enable tiling with a factor of 4.\n3. For the innermost loop 'for (p = 0; p < 30; p++)', use '#pragma ACCEL PIPELINE \"flatten\"' to flatten the pipeline for this loop.\n\nAdditionally, use '#pragma ACCEL kernel' to declare the 'kernel_doitgen' function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Ensure this function is considered as a hardware kernel for acceleration using '#pragma ACCEL kernel'\n2. Disable automatic pipeline optimization for the loop using '#pragma ACCEL PIPELINE auto{off}'\n3. Keep the loop in its original, full form without breaking it into smaller chunks by setting tile factor to 1 using '#pragma ACCEL TILE FACTOR=auto{1}'\n4. Run the iterations of the loop one after the other, not in parallel, by setting parallel factor to 1 using '#pragma ACCEL PARALLEL FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C code can be described with the following natural language instruction: \n\n\"Pipeline the outer loop, tile the inner loop with a factor of 80, and parallelize the innermost loop with a factor of 1. Treat the entire function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in parallel with a factor of 2. Pipeline the loop automatically. Treat the loop with a tile factor of 1. Process the inner loop in parallel with a factor of 1 for reduction. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration'\n\n2. 'Don't automatically pipeline this loop'\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks'\n\n4. 'Run the iterations of this loop one after the other, not in parallel'\n\n5. 'Automatically pipeline this loop'\n\n6. 'Set the tile factor automatically for this loop'\n\n7. 'Set the parallelization factor automatically for this loop'\n\n8. 'Set the pipeline factor explicitly for this loop'\n\n9. 'Set the tile factor explicitly for this loop'\n\n10. 'Set the parallelization factor explicitly for this loop'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\nRun the iterations of this loop one after the other, not in parallel\nDon't automatically pipeline this loop", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops nested within the function 'kernel_doitgen' for hardware acceleration with the following optimizations:\n- Disable automatic pipelining for the outermost loop\n- Set the tiling factor to 1 for the outermost loop\n- Apply automatic pipelining for the middle loop\n- Set the tiling factor to 4 for the middle loop\n- Disable automatic pipelining for the innermost loop\n- Set the tiling factor to 1 for the innermost loop", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   This corresponds to: `#pragma ACCEL kernel`\n   \n2. Don't automatically pipeline this loop.\n   This corresponds to: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   This corresponds to: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   This corresponds to: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the given C function, use the following comprehensive but concise natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the loops.\n\n3. Process the loops with a tile factor of 25.\n\n4. Run the iterations of certain loops one after the other, not in parallel.\n\n5. Don't automatically pipeline the loops.\n\n6. Process the loops with a tile factor of 1.\n\n7. Run the iterations of certain loops one after the other, not in parallel.\n\n8. Don't automatically pipeline the loops.\n\n9. Process the loops with a tile factor of 1.\n\n10. Run the iterations of certain loops in parallel with a factor of 8.\n\nThese natural language instructions correspond to the given HLS pragma values in the C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate HLS codes for the given C function 'kernel_correlation', with specific pragma values:\n1. Disable automatic loop pipelining:\n   Use pragma '#pragma ACCEL PIPELINE off' for the loop that calculates the mean and standard deviation.\n\n2. Set the loop tiling factor to 1 for the original, full form:\n   Use pragma '#pragma ACCEL TILE FACTOR=1' for the loop that centers and reduces the column vectors.\n\n3. Run the iterations of the loop one after the other, not in parallel (factor = 1):\n   Use pragma '#pragma ACCEL PARALLEL FACTOR=1' for the loop that calculates the m * m correlation matrix.\n\nEach of these instructions corresponds to the specific #pragma values in the given C code for 'kernel_correlation' function.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following actions to generate the given HLS codes in C:\n\n1. Treat the function 'kernel_bicg' as a hardware kernel for acceleration.\n2. Run iterations of the loop for variable 'i' in parallel with a factor of two.\n3. Pipeline the loop for variable 'i' automatically.\n4. Process the loop in its original, full form without breaking it into smaller chunks.\n5. Run iterations of the inner loop for variable 'j' in parallel with a factor of one.\n\nThis set of natural language instructions corresponds to the provided HLS codes and indicates the specific #pragma values to use for each action.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "- For Loop Pipeline: Use the pragma '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loop for acceleration.\n- For Loop Tiling: Use the pragma '#pragma ACCEL TILE FACTOR=auto{2}' to split the loop into smaller chunks for optimization.\n- For Loop Parallelization: Use the pragma '#pragma ACCEL PARALLEL FACTOR=auto{4}' to parallelize the loop for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Pipeline this loop automatically\" - used to describe the pragma '#pragma ACCEL PIPELINE auto{}' which is needed for automatic pipeline optimization based on the hardware platform.\n\n\"Break the loop into smaller chunks with a factor of 80 for tiling\" - used to describe the pragma '#pragma ACCEL TILE FACTOR=auto{80}' which is necessary for generating tiled hardware designs.\n\n\"Run the iterations of this loop in parallel with a factor of 2\" -  used to describe the pragma '#pragma ACCEL PARALLEL FACTOR=auto{2}' which is needed to enable parallel execution of loop iterations based on the hardware platform.\n\n\"Treat the following function as a hardware kernel for acceleration\" - used to describe the pragma '#pragma ACCEL kernel' which marks the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C for the given natural language instructions, use the following:\n\n1. 'Don't automatically pipeline this loop' -> #pragma ACCEL PIPELINE \"off\"\n2. 'Process this loop in its original, full form without breaking it into smaller chunks' -> #pragma ACCEL TILE FACTOR=1\n3. 'Run the iterations of this loop one after the other, not in parallel' -> #pragma ACCEL PARALLEL FACTOR=1\n4. 'Treat the following function as a hardware kernel for acceleration' -> #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can follow the following comprehensive but concise natural language instructions:\n\n1. Treat the 'kernel_correlation' function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loop containing the computation for 'mean' and 'stddev'.\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n4. Run the iterations of some loops in parallel, with the specific parallel factor values '__PARA__L0', '__PARA__L4', '__PARA__L1', '__PARA__L5', '__PARA__L2', '__PARA__L6', '__PARA__L7_0'.\n5. Adjust the tile factor for some loops automatically.\n6. Perform some loops in a pipeline with specific pipeline factor values, such as 'off', '__PIPE__L3', '__PIPE__L7'.\n7. For the last loop, set the specific parallel factor value to '4'.\n\nBy following these instructions, you can generate the given HLS codes in C with the appropriate #pragma values for hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Use hardware acceleration for this function, pipeline the loops with default settings, arrange the loops in their original form, and run the iterations one after the other, in parallel where appropriate.'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following comprehensive natural language instruction for the #pragma values:\n\n- 'Treat the following function as a hardware kernel for acceleration' for '#pragma ACCEL kernel'\n- 'Don't automatically pipeline this loop' for '#pragma ACCEL PIPELINE \"off\"'\n- 'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=1'\n- 'Run the iterations of this loop one after the other, not in parallel' for '#pragma ACCEL PARALLEL FACTOR=1'\n\nThese instructions apply to specific points within the provided code to direct the HLS compilation process.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: \n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop:\n   `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   `#pragma ACCEL PARALLEL FACTOR=1`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following steps to accelerate the 'kernel_bicg' function:\n- Parallelize the inner loop with a factor of 2\n- Pipeline the outer loop\n- Do not break the loop into smaller chunks\n- Parallelize the outer loop with a factor of 2 and the reduction with a factor of 1", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n\"Optimize the loop unrolling and pipelining manually for better performance\" - for '#pragma ACCEL PIPELINE auto{off}'\n\n\"Keep the loop in its original, full form without breaking it into smaller chunks\" - for '#pragma ACCEL TILE FACTOR=auto{1}'\n\n\"Disable automatic pipelining for this loop to improve resource utilization\" - for '#pragma ACCEL PIPELINE auto{off}'\n\n\"Keep the loop in its original, full form without breaking it into smaller chunks\" - for '#pragma ACCEL TILE FACTOR=auto{1}'\n\n\"Manually pipeline the loop for better performance\" - for '#pragma ACCEL PIPELINE auto{}'\n\n\"Keep the loop in its original, full form without breaking it into smaller chunks\" - for '#pragma ACCEL TILE FACTOR=auto{1}'\n\n\"Treat the following function as a hardware kernel for acceleration\" - for '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following steps to generate the given HLS codes:\n\n1. Treat the 'kernel_bicg' function as a hardware kernel for acceleration using '#pragma ACCEL kernel'.\n2. Run the iterations of the first loop one after the other, not in parallel, using '#pragma ACCEL PARALLEL FACTOR=auto{2}'.\n3. Don't automatically pipeline the first loop using '#pragma ACCEL PIPELINE auto{}'.\n4. Process the second loop in its original, full form without breaking it into smaller chunks using '#pragma ACCEL TILE FACTOR=auto{1}'.\n5. Run the iterations of the second loop in parallel using '#pragma ACCEL PARALLEL FACTOR=auto{2}', and apply reduction to the inner loop using '#pragma ACCEL PARALLEL reduction FACTOR=auto{1}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop with a parallel factor of auto{2}, and automatically pipeline the loop in its entirety, and treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n\n\"Modify the loop to enable automatic pipelining\" - This generates '#pragma ACCEL PIPELINE auto{}'  \n\"Treat the loop to be broken into smaller chunks with auto factor\" - This generates '#pragma ACCEL TILE FACTOR=auto{4}'  \n\"Automatically pipeline and flatten the inner most loop\" - This generates '#pragma ACCEL PIPELINE auto{flatten}'  \n", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code in C, use the following natural language instruction:\n\"Optimize this loop for acceleration by automatically managing the pipeline and tiling factors, ensuring compatibility with hardware implementation.\"\n\nThe corresponding #pragma values to use:\n- '#pragma ACCEL kernel' to mark the function as a hardware kernel for acceleration.\n- '#pragma ACCEL PIPELINE auto{off}' to disable automatic loop pipelining.\n- '#pragma ACCEL TILE FACTOR=auto{1}' to set the tiling factor automatically to 1 for efficient hardware processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loop iterating over 'r', apply automatic pipeline optimization. For the loop iterating over 'q', apply automatic pipeline optimization and a tiling factor of 1. For the innermost loop iterating over 'p', turn off automatic pipeline optimization and apply a tiling factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nDo not automatically pipeline the loops.\nProcess each loop in its original, full form without breaking it into smaller chunks.\nRun the iterations of each loop in parallel with the specified factor values for parallelization.\nEnsure the iterations of the innermost loop are processed one after the other, not in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the corresponding #pragma values:\n\n1. For the loop containing the calculation of mean and standard deviation, use \"Don't automatically pipeline this loop\" to describe '#pragma ACCEL PIPELINE auto{off}'\n2. For the loop calculating mean, use \"Process this loop in its original, full form without breaking it into smaller chunks\" to describe '#pragma ACCEL TILE FACTOR=auto{1}'\n3. For the mean and standard deviation calculations, use \"Run the iterations of this loop one after the other, not in parallel\" to describe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}'\n4. For the loop calculating standard deviation and handling near-zero standard deviation values, use \"Don't automatically pipeline this loop\" to describe '#pragma ACCEL PIPELINE auto{off}'\n5. For the loop centering and reducing the column vectors, use \"Process this loop in smaller chunks to enhance parallelism\" to describe '#pragma ACCEL TILE FACTOR=auto{8}'\n6. For the loop calculating the correlation matrix, use \"Automatically pipeline this loop with a specific factor\" to describe '#pragma ACCEL PIPELINE auto{__PIPE__L3}', and \"Process this loop in smaller chunks to enhance parallelism\" to describe '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}', and \"Run the iterations of this loop in parallel\" to describe '#pragma ACCEL PARALLEL FACTOR=auto{8}' and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}'\n\nBy following these instructions, you can generate the given HLS codes in C with the corresponding #pragma values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the first loop, 'Disable automatic pipelining for this loop'\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n2. For the second loop, 'Avoid tiling and process this loop in its original, full form'\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. For the third loop, 'Run the iterations of this loop sequentially, not in parallel'\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\nLastly, 'Treat the following function as a hardware kernel for acceleration'\n   Use: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS code written in C:\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nThe natural language instruction for the comprehensive but very concise #pragma values to use is:\n\n\"Automatically pipeline all eligible loops, use auto tiling with factor 2, and parallelize eligible loops with a custom parallel factor for different loop levels as specified in the code.\"\n", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outer loop in parallel with a factor of 2, then pipeline the inner loop, and finally tile the outer loop with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel with a factor of 2, then pipeline it. Process the second loop with a tile factor of 1, and also in parallel with a factor of 2. Additionally, use reduction for the inner loop parallelization with a factor of 1. Finally, consider the entire function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the kernel_correlation function using HLS pragmas as follows:\n\n1. Use automatic pipeline optimization for the loops.\n   `#pragma ACCEL PIPELINE auto{}`\n   \n2. Process the loops in their original, full form without breaking them into smaller chunks.\n   `#pragma ACCEL TILE FACTOR=auto{2}`\n   \n3. Run the iterations of the loops in parallel.\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n4. Disable automatic pipeline optimization for the following loop.\n   `#pragma ACCEL PIPELINE auto{off}`\n\n5. Process the loop in its original, full form without breaking it into smaller chunks.\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n6. Run the iterations of the loop in parallel.\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n7. Disable automatic pipeline optimization for the following loop.\n   `#pragma ACCEL PIPELINE auto{off}`\n\n8. Process the loop in its original, full form without breaking it into smaller chunks.\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n9. Run the iterations of the loop in parallel.\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n10. Perform pipeline optimization for a specific iteration factor.\n   `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n11. Process the loop in its original, full form without breaking it into smaller chunks.\n   `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n12. Run the iterations of the loop in parallel with a factor of 4.\n   `#pragma ACCEL PARALLEL FACTOR=auto{4}`\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code processes a sparse matrix-vector multiplication using hardware acceleration.\n\n1. To prevent automatic pipelining of the loop, use `#pragma ACCEL PIPELINE auto{off}`.\n2. For achieving the original full form without breaking into smaller chunks, apply `#pragma ACCEL TILE FACTOR=auto{1}`.\n3. To run the loop iterations one after the other, not in parallel, employ `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n4. Finally, to treat the `spmv` function as a hardware kernel for acceleration, add `#pragma ACCEL kernel` at the beginning of the function.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outer loop in parallel with a factor of 2 for automatic parallelization. Pipeline the inner loop automatically. Use a tile factor of 1 for the loops. Process the innermost loop in parallel with a factor of 1 for automatic parallelization and perform automatic reduction for parallelization. Additionally, treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n\n\"Define the following function as a hardware kernel for acceleration and apply the following pragma values to the loops:\n1. Disable automatic pipelining for the outermost loop.\n2. Tile the outer loop with a factor of 1.\n3. Disable automatic pipelining for the middle loop.\n4. Tile the middle loop with a factor of 8.\n5. Enable automatic pipelining for the innermost loop.\n6. Tile the innermost loop with a factor of 1.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Compile the loop into hardware kernel for acceleration.\nAutomatically pipeline this loop.\nDivide the loop into its original, full form without breaking it into smaller chunks.\nRun the iterations of this loop in parallel.\nAutomatically pipeline this loop and flatten it.\nDivide the loop into smaller chunks with a tile factor of 1.\nRun the iterations of this loop in parallel with a factor of 1.\n\n(please, note that \"__PARA__L0\" and \"__PIPE__L5\" are non-standard and seem to be placeholders; they should be replaced with the actual factors and tile values as appropriate for the specific application)", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you can use the following natural language instructions with specific #pragma values:\n\n1. For the `kernel_correlation` function, treat it as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n\n2. For the loop that calculates the mean values, process it in its original form without breaking it into smaller chunks using `#pragma ACCEL TILE FACTOR=1` and run the iterations one after the other, not in parallel using `#pragma ACCEL PARALLEL FACTOR=1`.\n\n3. For the same loop, don't automatically pipeline it using `#pragma ACCEL PIPELINE off`.\n\n4. For the loop that calculates the standard deviation, automatically pipeline it using `#pragma ACCEL PIPELINE auto{}` and break it into chunks using `#pragma ACCEL TILE FACTOR=2`. Run the iterations in parallel using `#pragma ACCEL PARALLEL FACTOR=__PARA__L1`.\n\n5. Center and reduce the column vectors in the subsequent loop by using `#pragma ACCEL TILE FACTOR=1` and executing the iterations in parallel using `#pragma ACCEL PARALLEL FACTOR=__PARA__L2`. Also, don't automatically pipeline this loop using `#pragma ACCEL PIPELINE auto{}`.\n\n6. Finally, calculate the correlation matrix by breaking the loop into chunks using `#pragma ACCEL TILE FACTOR=__TILE__L3`, running the iterations in parallel using `#pragma ACCEL PARALLEL FACTOR=4`, and automatically pipelining it using `#pragma ACCEL PIPELINE auto{__PIPE__L3}`.\n\nBy following these instructions, you will be able to generate and implement the given HLS codes effectively.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline this loop and process it in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following function is treated as a hardware kernel for acceleration and then process the loops without automatic pipelining or tiling, and run the iterations one after the other without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Create a hardware kernel for acceleration. Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Don't automatically pipeline this loop.\"", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Set the following function as a hardware kernel for acceleration. Then, ensure that the loops are not automatically pipelined and are processed in their original, full form without breaking them into smaller chunks. Also, run the iterations of the innermost loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Parallelize the outer loop with a factor of 2. Pipeline the loop to increase throughput. Tile the loop with a factor of 1 to optimize data locality. Parallelize the inner loop with a factor of 2 and utilize reduction for the inner loop. Also, treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C HLS code, you need to add the following natural language instructions as #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration\n   ```\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline this loop\n   ```\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   ```\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel\n   ```\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nApply auto{off} pipeline pragma to the outer loop to disable automatic pipelining.\nApply auto{1} tile factor pragma to the outer loop for the original, full form processing.\nApply auto{off} pipeline pragma to the inner loop to disable automatic pipelining.\nApply auto{4} tile factor pragma to the inner loop for breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire function as a hardware kernel for acceleration. Parallelize the outer loop with a factor of 2, pipeline the inner loop, and tile it with a factor of 1. Also, parallelize the innermost loop with a reduction factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Combine the following loops and execute them in parallel in hardware:\n```c\n#pragma ACCEL kernel\n\nvoid spmv(double val[1666], int cols[1666], int rowDelimiters[495], double vec[494], double out[494]) {\n  #pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (int i = 0; i < 494; i++) {\n    double sum = 0;\n    double Si = 0;\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    for (int j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS Code Directives: \n\n1. Indicate that the following function should be synthesized as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Disable automatic pipelining for the loop to prevent the compiler from automatically breaking it into smaller chunks.\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n3. Set the tiling factor to 1 to process the loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n4. Set the parallel factor to 1 to run the iterations of the loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\nUse #pragma ACCEL kernel to indicate that the following function should be treated as a hardware kernel for acceleration.\n\nFor the outer loop, use #pragma ACCEL PIPELINE auto{off} to prevent automatic pipelining.\n\nFor the outer loop, use #pragma ACCEL TILE FACTOR=auto{1} to process the loop in its original, full form without breaking it into smaller chunks.\n\nFor the middle loop, use #pragma ACCEL PIPELINE auto{off} to prevent automatic pipelining.\n\nFor the middle loop, use #pragma ACCEL TILE FACTOR=auto{2} to process the loop in smaller chunks with a factor of 2.\n\nFor the inner loop, use #pragma ACCEL PIPELINE auto{flatten} to enable flattening for the loop iterations.\n\nFor the inner loop, use #pragma ACCEL TILE FACTOR=auto{1} to process the loop in its original form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nTreat the iterations of the 3rd loop as a non-parallel continuous sequence. Set the tile factor to 1 and flatten the pipeline.\n\nProcess the iterations of the 2nd loop in parallel with a factor of 1. Set the tile factor to 8.\n\nProcess the iterations of the 1st loop in parallel with a factor of the specified value to achieve parallelism. Set the tile factor to 1.\n\nAutomatically pipeline all the loops in the function.\n", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration \n```c\n#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, the following natural language instruction can be used:\n\n\"Designate the following function as a hardware kernel for acceleration, and apply auto pipelining to all loops, auto tiling with factor 2 for the first loop, auto parallelization with reduction for mean calculations in the first loop, auto tiling with factor 1 for the second loop, auto parallelization with reduction for data in the second loop, flatten the pipeline for the third loop, auto tiling with factor 1 for the third loop, auto parallelization with reduction for covariances in the third loop.\"\n\nThis instruction covers the necessary pragma values to be used for the given HLS codes in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that this function is treated as a hardware kernel for acceleration. Then, run the iterations of the second loop in parallel with a factor of 2. Pipeline the loops automatically for efficient processing. Finally, process the second loop in its full, original form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{2}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Process the loops in their original form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration and apply the following optimizations:\n1. Disable automatic pipelining for the outer loop over 'r'.\n2. Do not break the outer loop into smaller chunks; keep it in its original, full form.\n3. Flatten the innermost loop over 'p', and set the tile factor automatically to '1'.\n4. For the middle loop over 'q', do not automatically pipeline it and set the tile factor to '4'.\"\n\nThis comprehensive instruction covers the pragma values used in the provided HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the `kernel_bicg` function as a hardware kernel for acceleration. Apply automatic parallelism with a factor of 2 for the first loop iterating up to 116. Then, pipeline the loop operations automatically. Additionally, apply automatic tiling with a factor of 1, and automatic parallelism with a factor of 2 for the second loop iterating up to 124. Lastly, perform reduction parallelism automatically with a factor of 1 for the innermost loop iterating up to 116.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in parallel with the auto factor of 2 for both the outer and inner loops, and automatically pipeline the loops. Also, treat the function 'kernel_bicg' as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes in C by including the following natural language instructions:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Don't automatically pipeline this loop.\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n\n7. Run the iterations of this loop one after the other, not in parallel.\n\n8. Automatically pipeline this loop with the option to flatten the resulting pipeline.\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n\n10. Run the iterations of this loop in parallel with the specified parallel factor.\n\n11. Automatically pipeline this loop with the option to use a specific pipeline factor.\n\n12. Process this loop using tiling with the specified tile factor.\n\n13. Run the iterations of this loop in parallel with the specified parallel factor.\n\n14. Automatically pipeline this loop with the option to flatten the resulting pipeline.\n\n15. Process this loop using tiling with the specified tile factor.\n\n16. Run the iterations of this loop in parallel with the specified parallel factor.\n\n17. Automatically pipeline this loop with the option to use a specific pipeline factor.\n\n18. Run the iterations of this loop in parallel with the specified parallel factor.\n\n19. Automatically pipeline this loop with the option to flatten the resulting pipeline.\n\n20. Run the iterations of this loop in parallel with the specified parallel factor.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations of the first for loop in parallel in 2 groups, then pipeline the loop but don't automatically break it into smaller chunks. For the second for loop, process the iterations in parallel in 2 groups and reduce the result value in parallel in 2 groups as well. Treat the function 'kernel_bicg' as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{2}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with specific #pragma values:\n\nUse the instruction 'Apply pipelining to the following loop' for '#pragma ACCEL PIPELINE auto{off}', and use 'Divide the following loop into its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=auto{1}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks by using \"#pragma ACCEL TILE FACTOR=1\". Don't automatically pipeline this loop by using \"#pragma ACCEL PIPELINE auto{off}\". Run the iterations of this loop one after the other, not in parallel, by using \"#pragma ACCEL PARALLEL FACTOR=auto{1}\". Treat the following function as a hardware kernel for acceleration using \"#pragma ACCEL kernel\".", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code written in C, use the following natural language instruction:\n\n\"Process the following function as a hardware kernel for acceleration. Then, don't automatically pipeline the loops and don't tile or break them into smaller chunks. Finally, run the iterations of the loops one after the other, not in parallel, with specific factor values given for each loop as follows: 25 for the outer loop, 1 for the second loop, 1 for the third loop, and 4 for the last loop.\"\n\nThis comprehensive but concise instruction will guide the generation of the given HLS codes using #pragma values in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   \n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n\n   `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n   `#pragma ACCEL PARALLEL FACTOR=1`\n\n5. Use the provided script as a guide to automatically determine pipeline and parallelization factors for this loop.\n\n   `#pragma ACCEL PIPELINE auto`\n  \n   `#pragma ACCEL PARALLEL FACTOR=auto`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in parallel where possible, using hardware acceleration for optimized performance.\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_bicg(int m, int n, double A[124][116], double s[116], double q[124], double p[116], double r[124]) {\n  int i;\n  int j;\n\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double)0);\n  }\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n\n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration\n```c\n#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code, use the following comprehensive but very concise natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration and disable automatic pipelining for the outer loop and tile the loop with a factor of 8, then disable automatic pipelining for the middle loop and tile the loop with a factor of 1, finally flatten the inner loop with automatic pipelining and a tile factor of 1.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations in parallel using a factor determined automatically, and treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and process the loops using the specified #pragma values, turning off automatic loop pipelining and setting the tiling factor to 4 for the outer loop, and to 1 for the inner loops. Additionally, flatten the pipeline and parallelize the inner most loop.\"\n\nThis comprehensive but concise instruction covers the process of the given codes and which specific #pragma values to apply: \n1. '#pragma ACCEL kernel' for specifying the function as a hardware kernel\n2. '#pragma ACCEL PIPELINE auto{off}' for turning off automatic loop pipelining\n3. '#pragma ACCEL TILE FACTOR=auto{4}' for setting the tiling factor to 4 for the outer loop\n4. '#pragma ACCEL PIPELINE auto{flatten}' for flattening the pipeline\n5. '#pragma ACCEL PARALLEL FACTOR=auto{1}' for parallelizing the innermost loop", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive but concise natural language instruction for the given HLS codes written in C:\n\n\"Optimize the following function for hardware acceleration by specifying pipeline and parallel processing. Use pragma ACCEL kernel to mark the function as a hardware kernel for acceleration. Then, apply pragma ACCEL PIPELINE off to disable automatic pipelining. Next, use pragma ACCEL TILE FACTOR=1 to process the loop in its original, full form without breaking it into smaller chunks. Finally, apply pragma ACCEL PARALLEL FACTOR=__PARA__L0, __PARA__L1, __PARA__L2, and __PARA__L7_0 to parallelize specific loops and sections for efficient hardware acceleration.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code, follow these comprehensive but concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration: Use '#pragma ACCEL kernel' before the function declaration.\n\n2. Don't automatically pipeline the outermost loop in its full original form: Use '#pragma ACCEL PIPELINE auto{off}' before the outermost loop.\n\n3. Process the outermost loop in smaller chunks of size 2: Use '#pragma ACCEL TILE FACTOR=auto{2}' before the outermost loop.\n\n4. Don't automatically pipeline the middle loop in its full original form: Use '#pragma ACCEL PIPELINE auto{off}' before the middle loop.\n\n5. Process the middle loop in its original, full form without breaking it into smaller chunks: Use '#pragma ACCEL TILE FACTOR=auto{1}' before the middle loop.\n\n6. Automatically flatten the innermost loop for parallel processing: Use '#pragma ACCEL PIPELINE auto{flatten}' before the innermost loop.\n\n7. Process the innermost loop in its original, full form without breaking it into smaller chunks: Use '#pragma ACCEL TILE FACTOR=auto{1}' before the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop for initializing 's' array in parallel with a factor of 2. Then pipeline the loop for initializing 's' array to improve throughput. Next, process the second loop in its original, full form without breaking it into smaller chunks, using a tile factor of 1, and in parallel with a factor of 2. Finally, within the second loop, perform reduction operations in parallel with a factor of 25.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{25}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Automatically pipeline all loops and break them into smaller chunks for parallel execution, and treat the function as a hardware kernel for acceleration.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Processes the first loop in parallel, then processes the iterations of the second loop in parallel with a reduction, maintains the original loop and doesn't automatically pipeline it. Treats the function as a hardware kernel for acceleration.\n\nInstructions:\n- First loop: #pragma ACCEL PARALLEL FACTOR=auto{1}\n- Second loop: #pragma ACCEL PARALLEL FACTOR=auto{4}, #pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n- Pipelining: #pragma ACCEL PIPELINE auto{off}\n- Function: #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. For the function 'kernel_covariance', treat it as a hardware kernel for acceleration:\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. For the loop processing of 'mean' calculation, automatically pipeline the loop:\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   ```\n\n3. For the loop processing of 'mean' calculation, use a tile factor of 1:\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n4. For the loop processing of 'mean' calculation, run the iterations one after the other, not in parallel:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n   ```\n\n5. For the loop processing of 'data' calculation, automatically pipeline the loop and use a tile factor of 4:\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   #pragma ACCEL TILE FACTOR=auto{4}\n   ```\n\n6. For the loop processing of 'data' calculation, run the iterations one after the other, not in parallel:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n   ```\n\n7. For the loop processing of 'cov' calculation, automatically pipeline the loop with flattening and use a tile factor of 1:\n   ```c\n   #pragma ACCEL PIPELINE auto{flatten}\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n8. For the loop processing of 'cov' calculation, run the iterations one after the other, not in parallel, and use pipeline flattening:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n   #pragma ACCEL PIPELINE auto{__PIPE__L5}\n   ```\n\nI hope this helps! Let me know if you need further assistance.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\"Treat the following function as a hardware kernel for acceleration. Automatically pipeline the loops and tile them with a factor of 1. For the innermost loop, tile with a factor of 4.\"\n\nThis instruction encompasses the specific #pragma values to use in natural languages for the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions with specific #pragma values:\n\n1. Ensure that this loop is not automatically pipelined - '#pragma ACCEL PIPELINE auto{off}'.\n\n2. Process this loop in its original, full form without breaking it into smaller chunks - '#pragma ACCEL TILE FACTOR=auto{1}'.\n\n3. For each of the nested loops, do not automatically pipeline - '#pragma ACCEL PIPELINE auto{off}'.\n\n4. Also, process each nested loop in its original, full form without breaking it into smaller chunks - '#pragma ACCEL TILE FACTOR=auto{1}'.\n\nBy following these instructions, the provided HLS codes for the kernel_doitgen function will be generated in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, then run the first loop in parallel with a factor of 2, pipeline the loop, and tile the loop with a factor of 1. Finally, parallelize the second loop using a reduction factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{4}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Parallelize the loop over variable \"i\" with a factor of 2, by added directive \"#pragma ACCEL PARALLEL FACTOR=auto{2}\". Pipeline the following loop automatically using \"#pragma ACCEL PIPELINE auto{}\". Finally, parallelize the inner loop with a factor of 2 and perform reduction operations parallelly with a factor of 4 using \"#pragma ACCEL PARALLEL FACTOR=auto{2}\" and \"#pragma ACCEL PARALLEL reduction FACTOR=auto{4}\" directives, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{4}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize loops by automatically pipelining and tiling them for acceleration with hardware kernels using HLS pragmas. Use the following pragma values: '#pragma ACCEL PIPELINE auto', '#pragma ACCEL TILE FACTOR=auto{1}', and '#pragma ACCEL PIPELINE flatten'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that loops are not automatically pipelined and remain in their original form by setting the pragma `#pragma ACCEL PIPELINE auto{off}`. Also maintain the original loop structure without breaking it into smaller chunks by setting the pragma `#pragma ACCEL TILE FACTOR=auto{1}` for each loop. And finally, designate the function `kernel_doitgen` as a hardware kernel for acceleration using `#pragma ACCEL kernel`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Parallelize the loops, automatically determining the parallelization factor. Pipeline the loops automatically. Use a tiling factor of 1 for the loops. Reduce the inner loop operations in parallel automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Run the iterations of the first loop in parallel with a factor of 2. Pipeline the second loop automatically. Tile the second loop automatically with a factor of 1. Run the iterations of the second loop in parallel with a factor of 2 and apply reduction automatically with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{4}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n- Treat the following function as a hardware kernel for acceleration by adding `#pragma ACCEL kernel` at the beginning of the function.\n- Don't automatically pipeline the loops by adding `#pragma ACCEL PIPELINE \"off\"` before the loops.\n- Process the loops in their original, full form without breaking them into smaller chunks by adding `#pragma ACCEL TILE FACTOR=1` before the loops.\n- Ensure that the iterations of the loops run one after the other, not in parallel, by adding `#pragma ACCEL PARALLEL FACTOR=1` before the loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "### Generate HLS Codes\nTo generate HLS codes for the given C function `kernel_correlation`, you can use the following comprehensive but concise natural language instructions:\n\n1. **Treat the following function as a hardware kernel for acceleration**: `#pragma ACCEL kernel`\n\n2. **Don't automatically pipeline this loop**: `#pragma ACCEL PIPELINE \"off\"`\n\n3. **Process this loop in its original, full form without breaking it into smaller chunks**: `#pragma ACCEL TILE FACTOR=1`\n\n4. **Run the iterations of this loop one after the other, not in parallel**: `#pragma ACCEL PARALLEL FACTOR=1`\n\nThese natural language instructions correspond to the HLS code pragmas used in the given C function for hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but very concise natural language instruction:\n\n### \"Analyze and implement hardware acceleration for the correlation function, using automatic pipeline, tiling with factor=1, and parallelization with factor=__PARA__L0, __PARA__L4, __PARA__L1, __PARA__L5, __PARA__L2, __PARA__L6, 8, and __PARA__L7_0 for the respective loops.\"\n\nThis instruction will guide the implementation of hardware acceleration using HLS pragmas for pipeline, tiling, and parallelization with the specified factors for each loop in the correlation function.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire kernel function as a hardware kernel for acceleration.\n\nFor the loops inside the kernel:\n1. Turn off automatic pipelining for each loop.\n2. Do not break the loops into smaller chunks.\n3. Run the loop iterations in parallel with a parallel factor of 1.\n4. For the final loop, set specific pipeline and tile factors, and parallelization factor of 8.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To enable hardware acceleration for the given C code, follow these instructions:\n1. Utilize hardware parallelism for the loop over variable 'i' by adding `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n2. Enable pipelining for the loops to improve the throughput by adding `#pragma ACCEL PIPELINE auto{}`.\n3. Allow the compiler to automatically determine the tiling factor for loop optimization by adding `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. Apply hardware parallelism for the loops over variables 'i' and 'j' by adding `#pragma ACCEL PARALLEL FACTOR=auto{2}` and `#pragma ACCEL PARALLEL reduction FACTOR=auto{4}` respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{4}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration.\n\nApply automatic pipelining to the outer loop.\n\nTile the outer loop with a factor of 80.\n\nInside the outer loop, apply automatic pipelining with a specific level.\n\nInside the outer loop, tile the inner loop with a specific factor.\n\nInside the inner loop, run iterations in parallel with a specific factor.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Run the outer loop iterations in parallel with a factor of 8, then pipeline the subsequent inner loops, and finally perform a parallel reduction within the innermost loop.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks, and automatically pipeline the loops for acceleration. Use a tile factor of 8 for the outer loops and a tile factor of 1 for the inner loops in a nested loop structure.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the function 'doitgen' as a hardware kernel for acceleration, disable automatic pipeline optimization for the outer loop, set the tiling factor to 1 for both outer and inner loops, and disable automatic pipeline optimization for the innermost loop\"\n\nThis covers the following pragma values:\n- #pragma ACCEL kernel\n- #pragma ACCEL PIPELINE auto{off}\n- #pragma ACCEL TILE FACTOR=auto{1}\n- #pragma ACCEL PIPELINE auto{off}\n- #pragma ACCEL TILE FACTOR=auto{1}", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the given C function, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Automatic pipeline, tiling, and parallelism optimizations are already enabled based on the context of the loops and data dependencies. No specific manual tuning is required.\"\n\nThis comprehensive instruction encompasses the pragma values to be used in the HLS codes and conveys the automatic nature of the optimizations without the need for specific manual tuning.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function to be treated as a hardware kernel for acceleration and disable automatic loop pipelining, set tile factor to 1 and parallel factor to original for the first loop. Disable loop pipelining, set tile factor to 4 and parallel factor to original for the second loop. Disable loop pipelining, set tile factor to 1 and parallel factor to original for the third loop. Set pipeline factor to original, tile factor to original and parallel factor to 8 for the final loop.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the function as a hardware kernel for acceleration:\n   - Use the instruction 'Treat the following function as a hardware kernel for acceleration'\n\n2. Don't automatically pipeline these loops:\n   - Use the instruction 'Don't automatically pipeline these loops'\n\n3. Process these loops in their original, full form without breaking them into smaller chunks:\n   - Use the instruction 'Process these loops in their original, full form without breaking them into smaller chunks'\n\n4. Run the iterations of these loops one after the other, not in parallel:\n   - Use the instruction 'Run the iterations of these loops one after the other, not in parallel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the 'kernel_bicg' function in C, use the following comprehensive pragma instructions: \n\n1. Parallelize the loop for variable i with a factor of 4: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n2. Pipeline the loop automatically: `#pragma ACCEL PIPELINE auto{}`\n3. Keep the original loop form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Parallelize the nested loop for variable j with a factor of 2 and apply reduction: `#pragma ACCEL PARALLEL FACTOR=auto{2}`, `#pragma ACCEL PARALLEL reduction FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process and optimize the iterations of the following loop with pipeline, tiling and parallel execution enabled:\n```c\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n\nvoid spmv(double val[1666], int cols[1666], int rowDelimiters[495], double vec[494], double out[494])\n{\n    // code block\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use these natural language instructions:\n\n1. Disable automatic loop pipelining: `#pragma ACCEL PIPELINE auto{off}`\n2. Process the loop without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n3. Run loop iterations one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n4. Disable automatic loop pipelining: `#pragma ACCEL PIPELINE auto{off}`\n5. Set the tile factor for loop processing: `#pragma ACCEL TILE FACTOR=auto{20}`\n6. Run loop iterations one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n7. Disable automatic loop pipelining: `#pragma ACCEL PIPELINE auto{off}`\n8. Process the loop without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n9. Run loop iterations one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n10. Set the pipeline factor for loop processing: `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n11. Set the tile factor for loop processing: `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n12. Run loop iterations in parallel with a factor of 4: `#pragma ACCEL PARALLEL FACTOR=auto{4}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but concise natural language instruction:\n\n\"Generate hardware kernel for acceleration, don't automatically pipeline all loops, process loops in their original form without breaking into smaller chunks, run loop iterations in parallel with factor auto, set tile factor to 1 for some loops, and vary the parallel factor based on loop context.\"\n\nThis description captures the necessary #pragma values for the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you would use the following natural language instructions for the respective #pragma values:\n\n1. Don't automatically pipeline this loop\n   - Meaning: Turn off auto-pipelining for the loop\n   - HLS Code: `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process this loop in its original, full form without breaking it into smaller chunks\n   - Meaning: Set the tile factor to 1 for the loop\n   - HLS Code: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Run the iterations of this loop one after the other, not in parallel\n   - Meaning: Set the parallel factor to 1 for the loop\n   - HLS Code: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n4. Treat the following function as a hardware kernel for acceleration\n   - Meaning: Mark the function as a hardware kernel for acceleration\n   - HLS Code: `#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outer loop (r-loop).\n3. Process the outer loop in its original, full form without breaking it into smaller chunks (TILE FACTOR=1).\n4. Don't automatically pipeline the inner loop (q-loop).\n5. Process the inner loop in its original, full form without breaking it into smaller chunks (TILE FACTOR=2).\n6. Automatically flatten the loop for better acceleration.\n7. Process the innermost loop in its original, full form without breaking it into smaller chunks (TILE FACTOR=1).", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C:\n\nFor the loop where 's[i] = ((double)0)', use '#pragma ACCEL PARALLEL FACTOR=auto{2}' to run iterations of this loop in parallel.\n\nFor the loop where 'for (i = 0; i < 124; i++)', use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline this loop.\n\nFor the inner loop within the second loop, use '#pragma ACCEL PARALLEL reduction FACTOR=auto{1}' to parallelize the reduction operation.\n\nAnd finally, for the function 'kernel_bicg', use '#pragma ACCEL kernel' to treat it as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the nested loops in their original form without breaking them into smaller chunks and pipeline them automatically. For the outermost loop, automatically tile the iterations to factor 1 in order to fully utilize the hardware resources. Inside the nested loops, pipeline the iterations automatically and flatten the loop to factor 8 to achieve better parallelism. This function is meant to be treated as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the `kernel_correlation` function, perform the following actions:\n1. Mark the function as a hardware kernel:\n   Use `#pragma ACCEL kernel` to designate the function for acceleration.\n2. Apply pipelining, tiling, and parallelization directives as follows:\n   - For the first loop, apply auto pipelining, factor=1 tiling, and factor=__PARA__L0 parallelization.\n   - For the second loop, apply auto pipelining, factor=1 tiling, and factor=__PARA__L1 parallelization.\n   - For the third loop, apply auto pipelining, factor=1 tiling, and factor=__PARA__L2 parallelization.\n   - For the final loop, apply auto pipelining, factor=__PIPE__L3 tiling, and factor=1 parallelization.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is treated as a hardware kernel for acceleration:\n\n```c\n#pragma ACCEL kernel\n```\n\n- Disable automatic pipelining for the loop:\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n- Process the loop in its original, full form without breaking it into smaller chunks:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n- Run the iterations of this loop one after the other, not in parallel:\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n```\n\n- Disable automatic pipelining for the loop:\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n- Process the loop in its original, full form without breaking it into smaller chunks:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n- Run the iterations of this loop one after the other, not in parallel:\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n```\n\n- Disable automatic pipelining for the loop:\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n- Apply a specific tiling factor for the loop:\n```c\n#pragma ACCEL TILE FACTOR=auto{30}\n```\n\n- Run the iterations of this loop in parallel according to the specified factor:\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n```\n\n- Apply a specific pipelining factor for the loop:\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n```\n\n- Apply a specific tiling factor for the loop:\n```c\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n```\n\n- Run the iterations of this loop in parallel according to the specified factor:\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS codes written in C language using the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   \n   ```\n   #pragma ACCEL kernel\n   ```\n\n2. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   ```\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n3. Run the iterations of this loop one after the other, not in parallel.\n\n   ```\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n4. Don't automatically pipeline this loop.\n\n   ```\n   #pragma ACCEL PIPELINE \"off\"\n   ```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given code for hardware acceleration, use the following concise natural language instruction:\n\"Convert the following function into a hardware kernel and specify the loop parallelization and pipelining factors. Tile the loop with a factor of 80, pipeline the outer loop with off, pipeline the inner loop with L1, and parallelize the innermost loop with a factor of 2.\"\nThis describes the process of adding the specific #pragma values to the given C code.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Automatically pipeline the outer and inner loops, but do not pipeline the middle loop. Tile the loops with a factor of 1 for the outer and inner loops, and a factor of 4 for the middle loop. Treat this function as a hardware kernel for acceleration.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the compiler to treat the following function as a hardware kernel for acceleration.\n\nUse automatic pipelining for loops.\n\nUse automatic tiling with a factor of 1 for loops.\n\nParallelize the loops with automatic parallel factor determined by the compiler.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the loop that calculates the mean of the data:\n   - `Don't automatically pipeline this loop` - to describe `#pragma ACCEL PIPELINE auto{off}`\n   - `Process this loop in its original, full form without breaking it into smaller chunks` - to describe `#pragma ACCEL TILE FACTOR=auto{1}`\n   - `Run the iterations of this loop one after the other, not in parallel` - to describe `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n2. For the loop that calculates the standard deviation of the data:\n   - `Don't automatically pipeline this loop` - to describe `#pragma ACCEL PIPELINE auto{off}`\n   - `Process this loop in its original, full form without breaking it into smaller chunks` - to describe `#pragma ACCEL TILE FACTOR=auto{1}`\n   - `Run the iterations of this loop one after the other, not in parallel` - to describe `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n3. For the loop that centers and reduces the column vectors:\n   - `Don't automatically pipeline this loop` - to describe `#pragma ACCEL PIPELINE auto{off}`\n   - `Break this loop into chunks of 4 iterations` - to describe `#pragma ACCEL TILE FACTOR=auto{4}`\n   - `Run the iterations of this loop in parallel` - to describe `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n4. For the loop that calculates the correlation matrix:\n   - `Automatically pipeline this loop with a custom factor` - to describe `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n   - `Break this loop into chunks of 4 iterations` - to describe `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n   - `Run the iterations of this loop in parallel` - to describe `#pragma ACCEL PARALLEL FACTOR=auto{4}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following concise natural language instruction: \n\n\"Mark the function as a hardware kernel for acceleration and configure the loop optimizations as follows: disable automatic loop pipelining, set the tile factor of the outer loop to 1, pipeline the inner loops automatically, and set the tile factor of the inner loops to 8 and 1 respectively.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. \n- Treat the first loop as parallel with a factor of 2.\n- Pipeline the loop automatically. \n- Do not parallelize the iterations of the second loop individually, instead use a parallel factor of 2 for the outer loop, and a reduction factor of 1 for the inner loop. \n- Do not break the second loop into smaller chunks, instead process it in its original, full form.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in parallel for the given factor (determined automatically) to improve performance, pipeline this loop automatically, and treat the following function as a hardware kernel for acceleration.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code using HLS pragmas:\n1. Apply pipeline optimization to the outermost loop by using `#pragma ACCEL PIPELINE auto{}`\n2. Apply tiling optimization with a factor of 8 to the outer loop and a factor of 1 to the inner loops using `#pragma ACCEL TILE FACTOR=auto{8}` and `#pragma ACCEL TILE FACTOR=auto{1}`, respectively. Use `#pragma ACCEL PIPELINE auto{flatten}` within the innermost loop to flatten data paths.\n3. Mark the function as a hardware kernel with `#pragma ACCEL kernel` before its declaration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a C function 'kernel_correlation' using the following natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Automatically pipeline the loops for optimization.\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n4. Run the iterations of the loops in parallel with a factor optimized for each loop.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the `kernel_doitgen` function as a hardware kernel for acceleration. Inside the function, automatically pipeline the outermost loops and use a tiling factor of 4 for the outer loop, and a tiling factor of 1 for the inner loops. Additionally, flatten the innermost loop and pipeline it automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. \nPipeline this loop automatically. \nProcess this loop in its original, full form without breaking it into smaller chunks.\nRun the iterations of this loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following steps to generate the given HLS codes written in C:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following comprehensive but very concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Don't automatically pipeline this loop.\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n\n7. Run the iterations of this loop one after the other, not in parallel.\n\n8. Don't automatically pipeline this loop.\n\n9. Process this loop with a tile factor of 30.\n\n10. Run the iterations of this loop in parallel with a factor of 1.\n\n11. Process this loop with a tile factor of 80.\n\n12. Run the iterations of this loop one after the other, not in parallel.\n\n13. Process this loop with a tile factor of 80.\n\n14. Run the iterations of this loop in parallel with a factor of 1.\n\n15. Process this loop with a tile factor of 80.\n\n16. Run the iterations of this loop in parallel with a factor of 1.\n\n17. Perform this loop iteration with automatic pipelining.\n\n18. Process this loop with a tile factor of 80.\n\n19. Run the iterations of this loop in parallel with a factor of 1.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but very concise natural language instruction:\n\"For the loops, automatically pipeline for optimization, utilize auto tiling with factor 25, and parallelize with factor defined by __PARA__L0 in the first loop; then auto pipeline for optimization, tile with factor 1, and parallelize with factor defined by __PARA__L1 in the second loop; auto pipeline for optimization, tile with factor 1, and parallelize with factor defined by __PARA__L2 in the third loop; and finally auto pipeline for optimization, tile with factor defined by __TILE__L3, and parallelize with factor 8 in the last loop. Additionally, treat the function as a hardware kernel for acceleration.\"\n\nThis should guide the process of applying the specific #pragma values for HLS code generation.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To summarize the HLS codes in C:\n- For the entire function, treat it as a hardware kernel for acceleration.\n- For the loops involving mean calculations, automatically pipeline the loop and divide it into smaller chunks with a factor of __PARA__L0.\n- For the same loops, process the iterations in parallel with a factor of __PARA__L4.\n- For the loops involving standard deviation calculations, automatically pipeline the loop and do not divide it into smaller chunks, process the iterations in parallel with a factor of __PARA__L1.\n- For the loops involving centering and reducing column vectors, automatically pipeline the loop, do not divide it into smaller chunks, process the iterations in parallel with a factor of __PARA__L2.\n- For the loop involving correlation matrix calculation, automatically pipeline the loop with specific pipeline and tile factors, and process the iterations in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Declare the function as a hardware kernel for acceleration.\n2. Apply automatic pipeline on the outer two loops.\n3. Apply tiling with a factor of 1 on the outer two loops.\n4. Disable automatic pipeline on the innermost loop.\n5. Apply tiling with a factor of 8 on the innermost loop.\n\nThis natural language instruction summarises the process and pragma values used in the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Apply automatic pipeline optimization to all the loops. For the first loop, break it into smaller chunks of size 4 as well as run the iterations in parallel. For the second loop, keep it in its original, full form without breaking it into smaller chunks and run the iterations one after the other, not in parallel. For the third loop, break it into smaller chunks of size 1, and run the iterations in parallel. For the final loop, apply automatic pipeline optimization, break it into smaller chunks of size 1, and run the iterations in parallel with a factor of 8.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration' to describe '#pragma ACCEL kernel'\n2. 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"'\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1'\n4. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1'\n\nI hope that helps!", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive natural language instructions for the specific #pragma values:\n\n1. Indicate that the loop should not be automatically pipelined:\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\nNatural language instruction: \"Do not automatically pipeline this loop.\"\n\n2. Specify that the loop should be processed in its original, full form without breaking it into smaller chunks:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\nNatural language instruction: \"Process this loop in its original, full form without breaking it into smaller chunks.\"\n\n3. Instruct to turn off automatic pipelining for a specific loop:\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\nNatural language instruction: \"Do not automatically pipeline this loop.\"\n\n4. Specify that the pragma should determine the tile factor automatically with a factor of 1:\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\nNatural language instruction: \"Automatically determine the tile factor for this loop with a factor of 1.\"\n\nThese natural language instructions correspond to the specific #pragma values used in the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions for pragma values:\n\n1. `#pragma ACCEL kernel` - Treat the following function as a hardware kernel for acceleration\n\n2. `#pragma ACCEL PIPELINE auto{}` - Automatically pipeline this loop\n\n3. `#pragma ACCEL TILE FACTOR=auto{1}` - Break this loop into smaller chunks of size 1 to process in its original, full form\n\n4. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` - Run the iterations of this loop in parallel with the specified factor\n\n5. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` - Run the iterations of this loop in parallel with the specified factor\n\n6. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` - Run the iterations of this loop in parallel with the specified factor", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following loop operations are processed as a hardware kernel for acceleration, automatically pipeline the loop, and then process in its original, full form without breaking it into smaller chunks. Next, run the iterations of the loops one after the other, not in parallel, and for the final loop, pipeline the operation, process in its original form, and run the iterations in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outermost loop.\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n4. Don't automatically pipeline the middle loop.\n5. Process the middle loop in its original, full form without breaking it into smaller chunks.\n6. Automatically pipeline the innermost loop and flatten the loop to improve parallelism.\n7. Process the innermost loop in smaller chunks, with a tile factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To automatically pipeline the loop use:\n\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\nTo tile the loop with an automatic factor use:\n\n```c\n#pragma ACCEL TILE FACTOR=auto{25}\n```\n\nTo parallelize the loop with an automatic factor use:\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n```\n\nTo automatically pipeline the loop use:\n\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\nTo tile the loop with a factor of 1 use:\n\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\nTo parallelize the loop with an automatic factor use:\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n```\n\nTo automatically pipeline the loop use:\n\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\nTo tile the loop with a factor of 1 use:\n\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\nTo parallelize the loop with an automatic factor use:\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but concise natural language instruction:\n\n\"Identify and mark the following function as a hardware kernel for acceleration, then apply automatic pipeline optimization, factorize tiling automatically, and parallelize the loops with relevant factors. Later, conduct element-wise operations with automatic pipeline and tiling, and finally parallelize the final loops for optimal hardware acceleration.\"\n\nThis instruction covers the process of marking the function as a hardware kernel and using automatic optimization, tiling, and parallelization with relevant factors for hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Automatically pipeline the loops, then break them into their smallest chuncks. Run the loop iterations in parallel. Apply automatic loop pipelining and smaller chunking. Lastly, calculate the correlation matrix with parallel iterations.\"\n\nThis instruction encompasses the process of the given HLS codes and specifies the pragma values to use.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, you should use the following comprehensive but very concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, then for the loop iterations: automatically pipeline the outermost loop, process the loop in its original, full form without breaking it into smaller chunks, pipeline the middle loop without automatic parallelization, and finally, flatten the innermost loop and tile it with a factor of 8.\"\n\nThese natural language instructions correspond to the specific #pragma values as follows:\n1. '#pragma ACCEL kernel'\n2. '#pragma ACCEL PIPELINE auto{}'\n3. '#pragma ACCEL TILE FACTOR=1'\n4. '#pragma ACCEL PIPELINE auto{off}'\n5. '#pragma ACCEL TILE FACTOR=8'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the first loop: don't automatically pipeline this loop, process this loop in its original, full form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel. For the second loop: do automatically pipeline this loop, process this loop in its original, full form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel. For the third loop: don't automatically pipeline this loop, process this loop in its original, full form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel. For the fourth loop: automatically pipeline this loop, process this loop by factor of 8, and run the iterations of this loop in parallel with factor of 8.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and do not automatically pipeline or break the loops. Use the original, full form for each loop without breaking them into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C with concise natural language instructions, you can use the following guidelines:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don\u2019t automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks and treat the function as a hardware kernel for acceleration. Then, automatically pipeline the outer loop and do not pipeline the middle loop; use a tile factor of 8 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, then apply automatic pipelining and tiling optimizations to the loops with appropriate factors and settings.\"\n\nThis single comprehensive but concise instruction covers the key pragma values to use in the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you need to annotate the loops and functions with specific #pragma values to guide the hardware acceleration process. Below is a concise natural language instruction for the provided codes:\n\n\"Annotate the outermost loops with 'AUTO' pipeline and auto-factorization for tiling and parallelization. Mark the entire function as a hardware kernel for acceleration.\"\n\nThis description encompasses the use of #pragma directives such as PIPELINE, TILE, PARALLEL, and kernel, and gives an overview of their application in the C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is considered as a hardware kernel for acceleration. Process each loop with the specified pragmas according to their respective types:\n\n1. For the loop with index `j` from 0 to 80:\n   a. Do not automatically pipeline this loop.\n   b. Break this loop into smaller chunks of size 8.\n   c. Run the iterations of this loop in parallel.\n\n2. For the loop with index `j` from 0 to 80:\n   a. Do not automatically pipeline this loop.\n   b. Process this loop in its original, full form without breaking it into smaller chunks.\n   c. Run the iterations of this loop in parallel.\n\n3. For the loop with index `i` from 0 to 100:\n   a. Do not automatically pipeline this loop.\n   b. Process this loop in its original, full form without breaking it into smaller chunks.\n   c. Run the iterations of this loop in parallel.\n\n4. For the loop with index `i` from 0 to 80 - 1:\n   a. Automatically pipeline this loop.\n   b. Break this loop into smaller chunks of size 8.\n   c. Run the iterations of this loop in parallel.\n\nAlso, it's important to ensure that the values for `auto` and `__PARA__L0`, `__PARA__L4`, `__PARA__L1`, `__PARA__L5`, `__PARA__L2`, `__PARA__L6`, `__PIPE__L3`, `__TILE__L3`, `__PIPE__L7`, `__PARA__L7_0` are set accordingly as per the specific requirements for the factor values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Generate hardware kernel for acceleration, with pipelining and tiling factor set to 1 for all loops inside the function.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": " To generate the given HLS codes written in C, use the following natural language instructions:\n1. For the function `kernel_correlation`, treat it as a hardware kernel for acceleration by using the pragma `#pragma ACCEL kernel`.\n\n2. For the loop that calculates the mean of each column vector, process it in parallel by using the pragma `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`.\n\n3. For the loop that calculates the standard deviation of each column vector, process it in parallel as well by using the pragma `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`.\n\n4. For the loop that centers and reduces the column vectors, do not automatically pipeline it by using the pragma `#pragma ACCEL PIPELINE auto{off}`.\n\n5. For the loop that calculates the correlation matrix, run it in parallel with a factor of 8 by using the pragma `#pragma ACCEL PARALLEL FACTOR=auto{8}`.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the given C functions:\n\n- Use '#pragma ACCEL kernel' to treat the following function as a hardware kernel for acceleration.\n\n- Use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loop.\n\n- Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the loop in its original, full form without breaking it into smaller chunks.\n\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' for running the iterations of the loop in parallel.\n\n- Use '#pragma ACCEL PIPELINE auto{off}' to disable automatic pipelining for the loop.\n\n- Use '#pragma ACCEL TILE FACTOR=auto{4}' to set the tile factor for the loop.\n\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' for running the iterations of the loop in parallel.\n\n- Use '#pragma ACCEL PIPELINE auto{__PIPE__L3}' to set the pipeline factor for the loop.\n\n- Use '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}' to set the tile factor for the loop.\n\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{8}' for running the iterations of the loop in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following natural language instruction:\n\n- Treat the following function as a hardware kernel for acceleration\n- Do not automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, and specify that the loops within it should not automatically pipeline or be tiled. Manually set the pipeline and tiling factors for each loop to optimize the performance and resource usage as desired.\"\n\nThis instruction encompasses the pragma values used in the given HLS codes, including \"#pragma ACCEL kernel\" for hardware kernel designation, and the manual setting of pipeline and tiling factors for each loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline all loops, then tile the outermost loop with a factor of 8. For the inner loops, turn off automatic pipelining and tile with a factor of 1. Ensure the innermost loops are flattened for efficient acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations in parallel with a factor of 2, and then automatically pipeline the loop for better performance. Use a tile factor of 1 for the loop to optimize memory access. Lastly, parallelize the nested loop with a reduction factor of 4 for improved efficiency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{4}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions as #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   Use '#pragma ACCEL kernel' at the beginning of the kernel function.\n\n2. Don't automatically pipeline the outermost loop:\n   Use '#pragma ACCEL PIPELINE auto{off}' just before the outermost loop.\n\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks:\n   Use '#pragma ACCEL TILE FACTOR=auto{1}' just before the outermost loop.\n\n4. Automatically pipeline the inner loops:\n   Use '#pragma ACCEL PIPELINE auto{}' just before the inner loops.\n\n5. Automatically flatten the loop iterations for better parallelism:\n   Use '#pragma ACCEL PIPELINE auto{flatten}' just before the innermost loop.\n\n6. Process the innermost loop with a tiling factor of 4:\n   Use '#pragma ACCEL TILE FACTOR=auto{4}' just before the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process both loops in parallel, each with a factor of 2. For the outer loop, pipelining is automatically applied. The inner loop should have a reduction applied to it for parallel processing. The function should be treated as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To convert the loop to hardware, treat the function as a hardware kernel using '#pragma ACCEL kernel'.\nFor the first loop, disable automatic pipelining using '#pragma ACCEL PIPELINE \"off\"'.\nFor the second loop, keep it as a single unit without tiling using '#pragma ACCEL TILE FACTOR=1'.\nFor the third loop, run the iterations in sequence without parallel processing using '#pragma ACCEL PARALLEL FACTOR=1'.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following instructions:\n\n1. Mark the \"doitgen\" as a hardware kernel for acceleration.\n2. Do not automatically pipeline the loops.\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n4. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "In order to generate the provided HLS codes, the following natural language instruction should be used for the respective #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Don't automatically pipeline this loop.\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n\n7. Run the iterations of this loop one after the other, not in parallel.\n\n8. Don't automatically pipeline this loop.\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n\n10. Run the iterations of this loop in parallel with a factor of 4.\n\nThese instructions provide a clear and concise guide for applying the specific #pragma values to the given C codes for HLS.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Then, for each loop: pipeline the loop automatically, set the tile factor to 1, and parallelize it with a factor determined automatically. In specific cases, turn off automatic pipelining, set tile factor to 30, and use specified parallelization factor. Finally, for the last loop, set pipeline directive and tile factor to specific values, and parallelize it with a factor of 4.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the code above in C, you would use the following concise natural language instruction:\n\n'Utilize hardware acceleration for the function, and automatically pipeline, tile with factor 1, and parallelize with appropriate factors for the specified loops.'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration. Turn off automatic pipelining for all loops. Use the original full form without breaking loops into smaller chunks. Run loop iterations one after the other, not in parallel. For the loops processing column vectors, apply a tile factor of 8. For the correlation matrix calculation, apply a pipeline factor of 3 and a tile factor of 4. Utilize parallelism for the inner loop and handle the loop variables accordingly.\"\n\nThis natural language instruction encompasses the specific #pragma values and their respective uses in the given HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code utilizing HLS, use the following pragma values for natural language instructions:\n\n1. Use parallel processing optimization for successive iterations with automatic factorization of 2.\n2. Don't automatically pipeline the loop for efficient resource utilization.\n3. Keep the loop in its original form without dividing it into smaller chunks.\n4. Utilize parallel processing optimization for successive iterations with automatic factorization of 2, and apply reduction in parallel with automatic factorization of 4 for nested loop.\n\nUtilize the above described pragma values in the given C code to enhance its hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{4}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you can follow these natural language instructions for the specific #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration' - This instruction is already specified in the given code as '#pragma ACCEL kernel'. No additional instruction is needed for this.\n\n2. 'Don't automatically pipeline this loop' - Use the pragma '#pragma ACCEL PIPELINE \"off\"'\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' - Use the pragma '#pragma ACCEL TILE FACTOR=1'\n\n4. 'Run the iterations of this loop one after the other, not in parallel' - Use the pragma '#pragma ACCEL PARALLEL FACTOR=1'\n\n5. Please note that the \"__PARA__L0\", \"__PARA__L4\", \"__PARA__L1\", \"__PARA__L5\", \"__PARA__L2\", \"__PARA__L6\", \"__PIPE__L3\", \"__TILE__L3\", and \"__PIPE__L7\" are variable values and should be replaced with actual values based on the specific implementation and design considerations.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Pipeline the loops with auto pipelining. Tile the loops with a factor of 4. Parallelize the loops with a factor determined by __PARA__L0, __PARA__L4, __PARA__L1, __PARA__L5, __PARA__L2, and __PARA__L7_0. Turn off automatic pipelining for the following loop. Tile the loop with a factor of 1. Parallelize the loop with a factor determined by __PARA__L6. Pipeline the loop with auto pipelining. Tile the loop with a factor of 1. Parallelize the loop with a factor determined by 4.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n \n1. \"Treat the following function as a hardware kernel for acceleration\": This describes '#pragma ACCEL kernel'\n2. \"Don't automatically pipeline this loop\": This describes '#pragma ACCEL PIPELINE \"off\"'\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\": This describes '#pragma ACCEL TILE FACTOR=1'\n4. \"Run the iterations of this loop one after the other, not in parallel\": This describes '#pragma ACCEL PARALLEL FACTOR=1'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the following function as a hardware kernel. Pipeline the loops automatically, tile the loops with a factor of 8, and parallelize the loops with a factor of __PARA__L0. Then, turn off automatic pipelining, tile the loops with a factor of 1, and parallelize the loops with a factor of __PARA__L1. After that, pipeline the loops automatically, tile the loops with a factor of 1, and parallelize the loops with a factor of __PARA__L2. Finally, pipeline the loops with a specific factor, tile the loops with a specific factor, and parallelize the loops with a factor of 4.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate HLS codes for the given C function, use the following comprehensive yet concise natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Automatically pipeline the loop.\n   - Use: `#pragma ACCEL PIPELINE auto{}`\n  \n3. Break the loop into smaller chunks to process in its original form.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n  \n4. Process the loop iterations in parallel, with a specific parallel factor.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` (for the first loop), `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` (for the second loop), `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` (for the third loop), `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}` (for the nested loop)\n  \n5. Disable automatic pipelining for a specific loop.\n   - Use: `#pragma ACCEL PIPELINE auto{off}` (for the fourth loop)\n  \n6. Set a specific tile factor for the loop.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{8}` (for the second loop), `#pragma ACCEL TILE FACTOR=auto{1}` (for the fourth loop), `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}` (for the sixth loop)\n  \n7. Enable specific pipeline and parallel factors for a loop.\n   - Use: `#pragma ACCEL PIPELINE auto{__PIPE__L3}` (for the sixth loop), `#pragma ACCEL PIPELINE auto{__PIPE__L7}` (for the nested loop), `#pragma ACCEL PARALLEL FACTOR=auto{8}` (for the sixth loop)", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration using the \"#pragma ACCEL kernel\" directive. While processing the loops, automatically pipeline them and use a tile factor of 1 in order to maintain their original, full form without breaking them into smaller chunks. Then, run the iterations of the loops using a parallel factor \"__PARA__L0\", \"__PARA__L4\", \"__PARA__L1\", \"__PARA__L5\", \"__PARA__L2\", and \"__PARA__L6\", respectively. Finally, for specific loops, control the pipeline, tile factor, and parallel factor using the respective auto values and named labels \"__PIPE__L3\", \"__TILE__L3\", and \"__PARA__L7_0\".", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code instructions:\n\n- Treat the function as a hardware kernel for acceleration\n- Automatically pipeline this loop\n- Tile this loop using automatic factor\n- Run the iterations of this loop in parallel using automatic factor\n- Automatically pipeline this loop with off setting\n- Tile this loop with a factor of 1\n- Run the iterations of this loop in parallel using automatic factor with specific parameter __PARA__L0\n- Automatically pipeline this loop with off setting\n- Tile this loop with a factor of 1\n- Run the iterations of this loop in parallel using automatic factor with specific parameter __PARA__L1\n- Automatically pipeline this loop with off setting\n- Tile this loop with a factor of 2\n- Run the iterations of this loop in parallel using automatic factor with specific parameter __PARA__L2\n- Pipeline this loop with a factor of __PIPE__L3\n- Tile this loop with a factor of __TILE__L3\n- Run the iterations of this loop in parallel using specific factor 4", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C for the given function, use the following natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Automatically pipeline the loop for optimization.\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\n3. Break the loop into smaller chunks and process it in its original, full form.\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n4. Run the iterations of this loop in parallel with a specific factor.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n```\n\n5. Center and reduce the column vectors without breaking the loop into smaller chunks.\n```c\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n```\n\n6. Pipeline the loop and define specific pipeline values.\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n```\n\n7. Break the loop into smaller chunks and process it with a defined tile factor.\n```c\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n```\n\n8. Run the iterations of this loop in parallel with a specific factor for optimization.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{8}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To enable hardware acceleration for the given C code, use the following natural language instructions for the specified #pragma values:\n\n1. For the outermost loops in the `kernel_correlation` function, automatically pipeline the loops without specifying the factor: \n   Use the instruction: \"Automatically pipeline this loop\".\n\n2. For the outermost loops in the `kernel_correlation` function, process the loop in its original form without breaking it into smaller chunks:\n   Use the instruction: \"Tile this loop with a factor of 1\".\n\n3. For the nested loop over variable `j`: \n   Run the iterations of this loop in parallel, do not break the loop into smaller chunks:\n   Use the instruction: \"Parallelize this loop\" with the balancing factor `__PARA__L0`.\n\n4. For the nested loop over variable `j`: \n   Run the iterations of this loop in parallel, do not break the loop into smaller chunks:\n   Use the instruction: \"Parallelize this loop\" with the balancing factor `__PARA__L1`.\n\n5. For the nested loop over variables `i` and `j`: \n   Run the iterations of this loop in parallel, do not break the loop into smaller chunks:\n   Use the instruction: \"Parallelize this loop\" with the balancing factor `__PARA__L2`.\n\n6. For the last loop over variables `i`, automatically pipeline the loop without specifying the factor: \n   Use the instruction: \"Automatically pipeline this loop\".\n\n7. For the last loop over variables `i`, tile the loop with a factor of 4: \n   Use the instruction: \"Tile this loop with a factor of 4\".\n\n8. For the last loop over variables `i`, run the iterations of this loop one after the other, not in parallel: \n   Use the instruction: \"Process this loop serially\".\n\n9. For the last loop over variables `i`, tile the loop with a factor of 1: \n   Use the instruction: \"Tile this loop with a factor of 1\".", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the given C function, use the following instructions:\n\n- Use 'Turn off automatic pipelining for this loop' for '#pragma ACCEL PIPELINE auto{off}'\n- Use 'Keep this loop in its original form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=auto{1}'\n- Use 'Flatten this loop without automatically pipelining it' for '#pragma ACCEL PIPELINE auto{flatten}'\n- Use 'Automatically pipeline this loop without specifying a specific factor' for '#pragma ACCEL PIPELINE auto{}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following comprehensive but concise natural language instruction for the specific #pragma values:\n\n1. Treat the function as a hardware kernel for acceleration: \n   \"#pragma ACCEL kernel\"\n\n2. Automatically pipeline the loop: \n   \"#pragma ACCEL PIPELINE auto{}\"\n\n3. Process the loop in its original, full form without breaking it into smaller chunks: \n   \"#pragma ACCEL TILE FACTOR=auto{1}\"\n\n4. Run the iterations of the loop one after the other, not in parallel: \n   \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\" (for the first loop)\n   \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\" (for the second loop)\n   \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\" (for the third loop)\n   \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\" (for the fourth loop)\n\n5. Automatically pipeline and flatten the loop: \n   \"#pragma ACCEL PIPELINE auto{flatten}\" (for the fourth loop)\n   \"#pragma ACCEL PIPELINE auto{__PIPE__L3}\" (for the fifth loop)\n   \"#pragma ACCEL PIPELINE auto{__PIPE__L7}\" (for the sixth loop)\n\n6. Automatically tile the loop: \n   \"#pragma ACCEL TILE FACTOR=auto{20}\" (for the second loop)\n   \"#pragma ACCEL TILE FACTOR=auto{1}\" (for the third and fifth loops)\n   \"#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\" (for the fourth loop)\n\n7. Automatically parallelize the loop: \n   \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\" (for the first loop)\n   \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\" (for the second loop)\n   \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\" (for the third loop)\n   \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" (for the fifth loop)\n   \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\" (for the sixth loop)", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following concise natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration, turn off automatic pipelining for the outermost loop, apply a tile factor of 1 for the outermost loop, use automatic pipelining with flattening for the middle loop, and apply a tile factor of 2 for the middle loop.\"\n\nThis instruction can be translated to the following #pragma values:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PIPELINE auto{flatten}\n\n#pragma ACCEL TILE FACTOR=auto{2}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the kernel function header, use the instruction: \"Treat the following function as a hardware kernel for acceleration\"\n\n2. For the outer loop, use the instruction: \"Don't automatically pipeline this loop\"\n\n3. For the middle loop, use the instruction: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n4. For the innermost loop, use the instruction: \"Run the iterations of this loop one after the other, not in parallel\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nThese instructions correspond to the following #pragma values:\n1. '#pragma ACCEL kernel'\n2. '#pragma ACCEL PIPELINE auto{off}'\n3. '#pragma ACCEL TILE FACTOR=1'\n4. '#pragma ACCEL PARALLEL FACTOR=1'\n\nBy applying these instructions to the code, the corresponding HLS codes in C can be generated.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions and corresponding #pragma values:\n\n1. **Direct the compiler to not automatically pipeline the loops:**\n   Use the directive `#pragma ACCEL PIPELINE auto{off}`.\n\n2. **Process the loops in their original, full form without breaking them into smaller chunks:**\n   Use the directive `#pragma ACCEL TILE FACTOR=auto{1}`.\n\n3. **Automatically pipeline the loops with a flatten mode option:**\n   Use the directive `#pragma ACCEL PIPELINE auto{flatten}`.\n\nAdditionally, for the `kernel_doitgen` function, use the directive `#pragma ACCEL kernel` to treat it as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but very concise natural language instruction for the specific #pragma values:\n\n- `#pragma ACCEL PIPELINE auto{off}`: Do not automatically pipeline this loop.\n- `#pragma ACCEL TILE FACTOR=auto{2}`: Automatically break this loop into smaller chunks with a factor of 2.\n- `#pragma ACCEL PIPELINE auto{flatten}`: Automatically flatten the pipeline of this loop.\n- `#pragma ACCEL kernel`: Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C with the specific #pragma values for acceleration, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, process the loops using automatic pipelining with pipeline off, and employ a tile factor of 1 for each loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{2}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nYou can use the following concise natural language instruction for each #pragma directive:\n\n1. ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n   Instruction: Do not automatically pipeline this loop.\n\n2. ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n   Instruction: Process this loop in its original, full form without breaking it into smaller chunks.\n\n3. ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n   ```\n   Instruction: Run the iterations of this loop one after the other, not in parallel.\n\n4. ```c\n   #pragma ACCEL PIPELINE auto{__PIPE__L3}\n   ```\n   Instruction: Automatically pipeline this loop with a factor value determined by the tool.\n\n5. ```c\n   #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n   ```\n   Instruction: Automatically break this loop into smaller chunks for processing.\n\n6. ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{4}\n   ```\n   Instruction: Execute this loop in parallel, with a factor of 4.\n\nRemember to replace \"auto\" and \"__PARA__L0\" with the actual values which may be generated by the tool and refer to the tool's documentation for proper usage.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Turn off automatic pipelining for this loop and set the tiling factor to 2. Process this loop in parallel with factor 1. Turn off automatic pipelining for this loop and process in its original, full form. Automatically tile the loop with factor 1. Run the iterations of this loop in parallel with factor 1. Automatically pipeline the loop with factor 3. Automatically tile the loop with factor 3. Run the iterations of this loop in parallel with factor 4.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Use automatic pipelining for loops. Tile the loops with automatic factor. Run loops in parallel with automatic factor.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, the following natural language instruction can be used:\n\n\"Define the following function as a hardware kernel for acceleration. Then, for the loops within the function, turn off automatic pipelining, and process each loop in its original form without breaking it into smaller chunks. For the outer loops, automatically pipeline without flattening and use a tile factor of 1. For the inner loops, flatten the loops and apply an automatic pipeline with a tile factor of 8.\"\n\nThis natural language instruction corresponds to the following #pragma values in the C code:\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PIPELINE auto{} \n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PIPELINE auto{flatten}\n#pragma ACCEL TILE FACTOR=auto{8}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C codes, use the natural language instructions as follows:\n- Apply automatic pipelining to the loops by using `#pragma ACCEL PIPELINE auto{}`\n- Tile the loops with a factor of 1 by using `#pragma ACCEL TILE FACTOR=1` for the outermost loop and `#pragma ACCEL TILE FACTOR=2` for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should:\n- Treat the function as a hardware kernel for acceleration.\n- Automatically pipeline the first loop and use default settings for the pipeline factor.\n- Factorize the loop using \"auto{2}\" for tiling.\n- Parallelize the loop with the default parallel factor for the outer loop.\n- Calculate the mean and the standard deviation for the data arrays while automatically pipelining and tiling the loops and using the default parallel factor.\n- Center and reduce the column vectors using automatic pipelining, tiling, and default parallel factor.\n- Calculate the correlation matrix with a specific pipeline factor, tiling factor, and parallel factor.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS codes, you would instruct as follows:\n\n\"Accelerate the following function as a hardware kernel\"\n- Use: `#pragma ACCEL kernel`\n\n\"Turn off automatic pipelining for all loops within the function\"\n- Use: `#pragma ACCEL PIPELINE auto{off}`\n\n\"Process the loops using automatic tiling with a factor of 8 for the outer loop\"\n- Use: `#pragma ACCEL TILE FACTOR=auto{8}`\n\n\"Use auto pipelining and flattening for the second nested loop\"\n- Use: `#pragma ACCEL PIPELINE auto{flatten}`\n\n\"Process the remaining loops with automatic tiling and no pipelining\"\n- Use: `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the innermost loop, don't automatically pipeline it and process it in its original, full form without breaking it into smaller chunks. Run the outer loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C for the given natural language instructions, you can use the following concise but comprehensive instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline this loop.\n\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```\n\n5. Auto pipeline this loop with flattening.\n\n```c\n#pragma ACCEL PIPELINE auto{flatten}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire function as a hardware kernel for acceleration.\n\nFor each of the following loops:\n1. Don't automatically pipeline this loop.\n2. Process this loop in its original, full form without breaking it into smaller chunks.\n3. Run the iterations of this loop one after the other, not in parallel.\n\n  a. Center and reduce the column vectors:\n    - Don't automatically pipeline this loop.\n\n  b. Calculate the m * m correlation matrix:\n    - Pipeline this loop automatically with a factor of 4.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code with the following natural language instruction:\n\"Apply pipeline without auto optimization to the outermost loop and disable pipeline for the inner loops. Use tiling with factor 1 at all levels.\"\n\nYou can use the following HLS pragma values in C:\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=1", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and then apply the following directives to the loop: Turn off automatic pipelining for the outer loop, set tile factor to 1 for both the outer and middle loops, and set tile factor to 8 for the inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Compile this function as a hardware kernel for acceleration, then specify that the loops should not be automatically pipelined and should be processed in their original form without breaking into smaller chunks and that the iterations of the loop should not run in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, don't automatically pipeline this loop, process the loop in its original, full form, without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you would use the following comprehensive but very concise natural language instructions:\n\n- Mark the function as a hardware kernel: `Treat the following function as a hardware kernel for acceleration`\n- Pipe-lining: `Automatically pipeline this loop` and `Don't automatically pipeline this loop`\n- Loop tiling: `Set the tile factor of this loop to a recommended value` \n- Loop parallelization: `Run the iterations of this loop in parallel with a recommended factor`\n\nThese instructions will guide the use of specific #pragma values to achieve the desired HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions and #pragma values:\n\n1. For the outermost loop, set pipeline to \"off\" to disable automatic pipelining:\n   - Natural language instruction: \"Don't automatically pipeline this loop\"\n   - #pragma value: #pragma ACCEL PIPELINE \"off\"\n\n2. For the middle loop, set the tile factor to 4 to break it into smaller chunks for parallel processing:\n   - Natural language instruction: \"Break this loop into smaller chunks with a factor of 4\"\n   - #pragma value: #pragma ACCEL TILE FACTOR=4\n\n3. For the innermost loop, flatten the pipeline to enable parallel processing without breaking it into smaller chunks:\n   - Natural language instruction: \"Flatten the pipeline for this loop\"\n   - #pragma value: #pragma ACCEL PIPELINE \"flatten\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, please follow these natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n\n2. Automatically pipeline the loop.\n\n3. Process the loop with its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop in parallel.\n\n5. Turn off automatic pipeline for this loop.\n\n6. Set the tile factor for automatic tiling to 8.\n\n7. Run the iterations of this loop in parallel with a factor of 4.\n\n8. Turn off automatic pipeline for this loop.\n\n9. Process the loop with its original, full form without breaking it into smaller chunks.\n\n10. Automatically pipeline the loop.\n\n11. Set the tile factor for automatic tiling to 1.\n\n12. Run the iterations of this loop in parallel with a factor of 4.\n\nPlease note that the placeholders like auto, __PARA__L0, __PIPE__L3, and __TILE__L3 should be replaced with actual values and configurations based on the specific hardware and design requirements.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Keep the loops in their original, full form without breaking them into smaller chunks. Run the iterations of each loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_correlation' function as a hardware kernel for acceleration.\n\nUse automatic pipeline optimization for the loops.\n\nTile the loops with an automatic factor of 2, 1, 1, and 1 respectively.\n\nRun the iterations of the loops in parallel with automatic factors for parallelization.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with specific #pragma values, you can use the following natural language instruction:\n\n\"Declare the following function as a hardware kernel for acceleration and apply the following optimizations: Disable automatic loop pipelining, set tile factor to 1 for the outermost loop, disable automatic loop pipelining for the middle loop and set tile factor to 2, flatten the innermost loop and set tile factor to 1.\"\n\nThis instruction would translate to the following #pragma values:\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{2}\n\n#pragma ACCEL PIPELINE auto{flatten}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the compiler automatically pipelines this loop to optimize performance by applying '#pragma ACCEL PIPELINE auto'.\n\nBreak this loop into smaller chunks for efficient processing by using '#pragma ACCEL TILE FACTOR=25'.\n\nExecute the iterations of this loop in parallel with a specific parallelism factor by applying '#pragma ACCEL PARALLEL FACTOR=__PARA__L0'.\n\nDisable automatic pipelining for this loop to reduce resource usage by applying '#pragma ACCEL PIPELINE auto{off}'.\n\nProcess this loop in its original, full form without breaking it into smaller chunks by using '#pragma ACCEL TILE FACTOR=1'.\n\nRun the iterations of this loop in parallel with a specific parallelism factor by applying '#pragma ACCEL PARALLEL FACTOR=__PARA__L1'.\n\nDisable automatic pipelining for this loop to reduce resource usage by applying '#pragma ACCEL PIPELINE auto{off}'.\n\nProcess this loop in its original, full form without breaking it into smaller chunks by using '#pragma ACCEL TILE FACTOR=1'.\n\nRun the iterations of this loop in parallel with a specific parallelism factor by applying '#pragma ACCEL PARALLEL FACTOR=__PARA__L2'.\n\nEnable automatic pipelining for this loop with a specific pipeline factor by using '#pragma ACCEL PIPELINE auto{__PIPE__L3}'.\n\nBreak this loop into smaller chunks for efficient processing by using '#pragma ACCEL TILE FACTOR=__TILE__L3'.\n\nExecute the iterations of this loop in parallel with a specific parallelism factor by applying '#pragma ACCEL PARALLEL FACTOR=__PARA__L3'.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration, and apply automatic pipeline, tiling, and parallelization with appropriate factors for loops as specified in the comments.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_fdtd_2d(int tmax,int nx,int ny,double ex[200][240],double ey[200][240],double hz[200][240],double _fict_[100])\n{\n  int t;\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 0; t < 100; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 240; j++) {\n      ey[0][j] = _fict_[t];\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 200; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n      for (j = 0; j < 240; j++) {\n        ey[i][j] = ey[i][j] - 0.5 * (hz[i][j] - hz[i - 1][j]);\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 200; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n      for (j = 1; j < 240; j++) {\n        ex[i][j] = ex[i][j] - 0.5 * (hz[i][j] - hz[i][j - 1]);\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L4}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 200 - 1; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7}\n      for (j = 0; j < 240 - 1; j++) {\n        hz[i][j] = hz[i][j] - 0.7 * (ex[i][j + 1] - ex[i][j] + ey[i + 1][j] - ey[i][j]);\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loops.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of the loops one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C:\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n```\nYou would use the following natural language instruction for each #pragma:\n\n1. 'Turn off automatic pipelining for this loop' for `#pragma ACCEL PIPELINE auto{off}`\n2. 'Don't break this loop into smaller chunks' for `#pragma ACCEL TILE FACTOR=auto{1}`\n3. 'Turn off automatic pipelining for this loop' for `#pragma ACCEL PIPELINE auto{off}`\n4. 'Break this loop into 8 smaller chunks' for `#pragma ACCEL TILE FACTOR=auto{8}`\n5. 'Turn off automatic pipelining for this loop' for `#pragma ACCEL PIPELINE auto{off}`\n6. 'Don't break this loop into smaller chunks' for `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration.\n\n- For the outer loop, deactivate automatic pipelining.\n- Use a tile factor of 1 for the outer loop.\n- Run the iterations of the outer loop in parallel with factor \\_\\_PARA\\_\\_L0.\n- For the first inner loop, activate automatic pipelining.\n- Use a tile factor of 4 for the first inner loop.\n- Run the iterations of the first inner loop in parallel with factor \\_\\_PARA\\_\\_L1.\n- For the second inner loop, deactivate automatic pipelining.\n- Use a tile factor of 1 for the second inner loop.\n- Run the iterations of the second inner loop in parallel with factor \\_\\_PARA\\_\\_L2.\n- For the last inner loop, activate automatic pipelining with factor \\_\\_PIPE\\_\\_L3, use a tile factor of \\_\\_TILE\\_\\_L3, and run the iterations in parallel with factor 4.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS codes using these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nThe result will be the HLS codes you are trying to write with the appropriate #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Additionally, do not automatically pipeline the loops, use a tile factor of 1, and run the iterations of the loops in parallel with a factor of 1.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nFor the first loop, don't automatically pipeline this loop, process it in its original, full form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel.\n\nFor the second loop, automatically pipeline it without specifying a pipeline off value, process it in smaller chunks with a factor of 4, and run the iterations of this loop in parallel with a factor of 1.\n\nFor the third loop, automatically pipeline it with an unspecified pipeline value, process it in its original form without tiling, and run the iterations of this loop in parallel with a factor of 1.\n\nFor the fourth loop, automatically pipeline it with a pipeline factor of 3, process it in smaller chunks with a factor of 4, and run the iterations of this loop in parallel with a factor of 4.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform automatic pipelining on all applicable loops using `#pragma ACCEL PIPELINE auto{}` and enable tiling with a factor of 2 on the outermost loop and a factor of 1 on the inner loops using `#pragma ACCEL TILE FACTOR=auto{2}` for the outer loop and `#pragma ACCEL TILE FACTOR=auto{1}` for the inner loops. Additionally, flatten the pipeline hierarchy for the innermost loop using `#pragma ACCEL PIPELINE auto{flatten}`. Finally, mark the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loops in their original, full form without breaking them into smaller chunks and don't automatically pipeline them. Treat the function 'doitgen' as a hardware kernel for acceleration. Use a tile factor of 8 for the outer loop and 1 for the inner loops. Finally, use flatten pipeline for the innermost loop to optimize for area.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, for the loops within the function:\n- Disable automatic pipelining for the outer loop\n- Tile the outer loop with a factor of 4\n- Disable automatic pipelining for the middle loop\n- Tile the middle loop with a factor of 1\n- Flatten the middle loop\n- Tile the innermost loop with a factor of 1", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. \"Treat the following function as a hardware kernel for acceleration\" - Use '#pragma ACCEL kernel' before the function definition.\n\n2. \"Don't automatically pipeline this loop\" - Use '#pragma ACCEL PIPELINE auto{off}' to prevent automatic pipelining of the loop.\n\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\" - Use '#pragma ACCEL TILE FACTOR=auto{1}' to maintain the original loop form without tiling.\n\n4. \"Run the iterations of this loop one after the other, not in parallel\" - Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' to set the parallel factor to 1 for the specified loop.\n\n5. \"Center and reduce the column vectors without automatically pipelining\" - Use '#pragma ACCEL PIPELINE auto{}' before the loop to disable automatic pipelining.\n\n6. \"Center and reduce the column vectors without breaking it into smaller chunks\" - Use '#pragma ACCEL TILE FACTOR=auto{8}' to avoid tiling and process the loop in its original form.\n\n7. \"Calculate the m * m correlation matrix with specific pipeline and tile factors\" - Use '#pragma ACCEL PIPELINE auto{__PIPE__L3}' and '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}' to set the pipeline and tile factors for the specified loop.\n\n8. \"Calculate the m * m correlation matrix with a parallel factor of 4\" - Use '#pragma ACCEL PARALLEL FACTOR=auto{4}' to run the iterations in parallel with the specified factor.\n\nBy following these natural language instructions and incorporating the associated #pragma values into the C code, you can generate the corresponding HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Don't automatically pipeline this loop: \n   - Use `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process this loop in its original, full form without breaking it into smaller chunks:\n   - Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Run the iterations of this loop one after the other, not in parallel:\n   - Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` for the first loop and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` for the second loop and so on\n\n4. Treat the following function as a hardware kernel for acceleration:\n   - Use `#pragma ACCEL kernel` for the function \"kernel_correlation\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following natural language instruction:\n\n\"Mark the function 'kernel_correlation' as a hardware kernel for acceleration. Disable automatic pipelining for the loops. Set the loop tiling factor to 1 and parallel factor to the number found in the code comments for each loop iteration.\"\n\nWith this comprehensive instruction, you can generate the provided HLS codes written in C using the correct #pragma values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C:\n\n```c\n#pragma ACCEL kernel\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n    // ...\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL TILE FACTOR=auto{8}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n    for (j = 0; j < 80; j++) {\n        // ...\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n        for (i = 0; i < 100; i++) {\n            // ...\n        }\n    }\n    // ...\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 80; j++) {\n        // ...\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n        for (i = 0; i < 100; i++) {\n            // ...\n        }\n    }\n    // ...\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (i = 0; i < 100; i++) {\n        // ...\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n        for (j = 0; j < 80; j++) {\n            // ...\n        }\n    }\n    // ...\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (i = 0; i < 80 - 1; i++) {\n        // ...\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n        for (j = i + 1; j < 80; j++) {\n            // ...\n        }\n    }\n}\n```\n\nYou should add the following comprehensive but very concise natural language instruction used to generate the codes:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop with a tile factor calculated automatically with a value of 8.\n4. Run the iterations of this loop in parallel, using the `__PARA__L0` factor.\n5. Don't automatically pipeline this loop.\n6. Process this loop with a tile factor calculated automatically with a value of 1.\n7. Run the iterations of this loop in parallel, using the `__PARA__L1` factor.\n8. Treat the following loops as a single iteration and don't parallelize them.\n9. Don't automatically pipeline this loop.\n10. Process this loop with a tile factor calculated automatically with a value of 1.\n11. Run the iterations of this loop in parallel, using the `__PARA__L2` factor.\n12. Automatically pipeline this loop.\n13. Process this loop with a tile factor calculated automatically.\n14. Run the iterations of this loop in parallel, using a factor of 8.\n15. Automatically pipeline this loop.\n16. Run the iterations of this loop in parallel, using the `__PARA__L7_0` factor.\n17. Automatically pipeline this loop.\n\nThese instructions will help generate the provided HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given natural language instructions:\n\n1. Use pipeline optimization for the loops.\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\n2. Tile the loops with a factor of 1.\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n3. Parallelize the loops with the given factor (__PARA__L0, __PARA__L1, __PARA__L2, __PARA__L4, __PARA__L5, __PARA__L6, __PARA__L7, __PARA__L7_0).\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n```\n\n4. Use pipeline optimization for specific loops.\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes, follow these concise natural language instructions:\n- Treat the `kernel_correlation` function as a hardware kernel for acceleration.\n- For the first loop, don't automatically pipeline it.\n- Process the loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of the loop one after the other, not in parallel.\n- For the second loop, don't automatically pipeline it.\n- Process the loop in tiles with a factor of 8.\n- Run the iterations of the loop in parallel.\n- For the third loop, don't automatically pipeline it.\n- Process the loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of the loop in parallel.\n- Calculate the m * m correlation matrix using pipeline with automatic factor, tiling with automatic factor, and running the iterations in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Don't automatically pipeline this loop -> #pragma ACCEL PIPELINE \"off\"\n2. Process this loop in its original, full form without breaking it into smaller chunks -> #pragma ACCEL TILE FACTOR=1\n3. Run the iterations of this loop one after the other, not in parallel -> #pragma ACCEL PARALLEL FACTOR=1\n4. Treat the following function as a hardware kernel for acceleration -> #pragma ACCEL kernel\n\nSimply follow these natural language instructions and corresponding #pragma values to generate the given HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the above HLS codes written in C, use the following comprehensive but very concise natural language instruction:\n\n\"Define the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Process each loop in its original, full form without breaking it into smaller chunks. Run the iterations of each loop one after the other, not in parallel. For specific loops, pipeline with a factor of 8, tile with a factor of 1, and run in parallel with a factor of 4.\"\n\nThis instruction covers the necessary process of the codes and the specific #pragma values to use in natural language.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you should use the following natural language instruction for the specific #pragma values:\n\n- For the pipeline off: \"Disable automatic pipelining for this loop\"\n- For tile factor=1: \"Do not break this loop into smaller chunks, process it as a whole\"\n- For parallel factor=1: \"Run the iterations of this loop one after the other, not in parallel\"\n- For treating the function as a hardware kernel: \"Designate the following function as a hardware kernel for acceleration\"\n\nBy following these natural language instructions, you will be able to generate the given HLS codes in C with the appropriate #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, don't automatically pipeline the first loop, process the loops in their original form without breaking them into smaller chunks, and run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the outer loop and tile the loop with the specified factor, then process the inner loops in parallel with the specified reduction factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: \n   - Use `#pragma ACCEL kernel` to designate the function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop: \n   - Use `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipelining for the loops.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks: \n   - Use `#pragma ACCEL TILE FACTOR=auto{1}` to specify that the loop should not be broken into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel: \n   - Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to ensure that the loop iterations are not run in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the natural language instruction below:\n\n\"Designate the following function as a hardware kernel for acceleration and process the loops using specific optimizations: Turn off automatic pipelining for the outermost loop, apply a tiling factor of 2 for the outer loop, turn off automatic pipelining for the middle loop, apply a tiling factor of 1 for the middle loop, flatten the pipelining for the innermost loop, and apply a tiling factor of 1 for the innermost loop.\"\n\nThis natural language instruction corresponds to the following pragma values in C:\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{2}\n\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PIPELINE auto{flatten}\n#pragma ACCEL TILE FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration. \n- Don't automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To treat the following function as a hardware kernel for acceleration, use the following pragmas for the given loops:\n\n- Disable automatic loop pipelining: `#pragma ACCEL PIPELINE \"off\"`\n- Process each loop in its full form without tiling: `#pragma ACCEL TILE FACTOR=1`\n- Run the iterations of the loops sequentially, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Run the iterations of the second loop one after the other, not in parallel by using '#pragma ACCEL PIPELINE auto{}', Divide the iteration of the second loop into chunks to process in parallel by using '#pragma ACCEL TILE FACTOR=auto{116}', and treat the reduction of the second loop as a separate parallel process by using '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}' and '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}' respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the first loop automatically. Tile the second loop with a factor of 4. Run the iterations of the second loop one after the other, not in parallel. Utilize reduction for the second loop for variable tmp and y separately with a factor of 1 for both.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loops in their original, full form without breaking them into smaller chunks and treat the function as a hardware kernel for acceleration.\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr, int nq, int np, double A[25][20][30], double C4[30][30], double sum[30]) {\n  int r;\n  int q;\n  int p;\n  int s;\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n\n#pragma ACCEL PIPELINE auto{flatten}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given C code, you would use the following natural language instructions for the specific #pragma values:\n\n1. 'Don't automatically pipeline the outer loop' to describe '#pragma ACCEL PIPELINE auto{off}'\n2. 'Process the outer loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{1}'\n3. 'Run the iterations of the outer loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n4. 'Perform the reduction operation on the inner loop and update the tmp array' to describe '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}'\n5. 'Perform the reduction operation on the inner loop and update the y array' to describe '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Turn off auto-pipelining for all loops. Set the tile factor to 2 and parallel factor to 0 for the first nested loop. Then, set auto-pipelining and tile factor to 1 for the next loop. Finally, configure the parallel factor to 0 for the second nested loop and set specific pipeline, tile, and parallel factors for the last loop to enable parallel execution, optimized pipelining, and tiling.\"\n\nThis natural language instruction encompasses all the necessary #pragma values to use in order to generate the provided HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Automatically pipeline the first loop with offloading disabled, tile the second loop with a factor of 116, and run the iterations of the third loop one after the other, not in parallel, with factors 1 and 1 for reduction variables tmp and y, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate hardware-accelerated code for the function \"kernel_doitgen\" with specific loop optimizations:\n- Use automatic pipelining for all loops\n- Tile the loops with a factor of 8 for the outermost loop and 1 for the inner loops", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the function `kernel_correlation` as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loop.\n3. Process each loop in its original, full form without breaking it into smaller chunks (TILE FACTOR=1).\n4. Run the iterations of the loop one after the other, not in parallel (PARALLEL FACTOR=1 for outer loop).\n5. Treat the inner loop similarly as the outer loop (PARALLEL FACTOR=1 for inner loop).\n6. Don't automatically pipeline the loop.\n7. Process each loop in its original, full form without breaking it into smaller chunks (TILE FACTOR=1).\n8. Run the iterations of the loop one after the other, not in parallel (PARALLEL FACTOR=1 for outer loop).\n9. Treat the inner loop similarly as the outer loop (PARALLEL FACTOR=1 for inner loop).\n10. Don't automatically pipeline the loop.\n11. Break the loop into smaller chunks with factor 4.\n12. Run the outer loop iterations in parallel.\n13. Run the inner loop iterations in parallel (PARALLEL FACTOR=30).\n14. Automatically pipeline the outer loop with a specified pipeline pattern.\n15. Break the loop into smaller chunks with a specified tile pattern.\n16. Run the iterations of the loop in parallel with a specific parallelization factor for the inner loop.\n\nFollowing these instructions will generate the provided HLS codes in C with the appropriate #pragma values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline this loop.\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n   ```\n\n5. Don't automatically pipeline this loop.\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{4}\n   ```\n\n7. Run the iterations of this loop one after the other, not in parallel.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n   ```\n\n8. Don't automatically pipeline this loop.\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n10. Run the iterations of this loop one after the other, not in parallel.\n    ```c\n    #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    ```\n\n11. Automatically pipeline this loop with specific properties.\n    ```c\n    #pragma ACCEL PIPELINE auto{__PIPE__L3}\n    ```\n\n12. Process this loop in its original, full form with specific properties.\n    ```c\n    #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    ```\n\n13. Run the iterations of this loop in parallel with specific properties.\n    ```c\n    #pragma ACCEL PARALLEL FACTOR=auto{4}\n    ```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instructions for HLS codes in C:\n\n- \"Don't automatically pipeline this loop\"\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n- \"Process this loop in its original, full form without breaking it into smaller chunks\"\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n- \"Run the iterations of this loop one after the other, not in parallel\"\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n```\n\n- \"Treat the following function as a hardware kernel for acceleration\"\n```c\n#pragma ACCEL kernel\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given C code:\n\n1. Treat the function as a hardware kernel for acceleration: '#pragma ACCEL kernel'\n2. Automatically pipeline the outermost loop: '#pragma ACCEL PIPELINE auto{}'\n3. Tile the outermost loop with the factor set to auto: '#pragma ACCEL TILE FACTOR=auto{1}'\n4. Turn off automatic pipelining for the middle loop: '#pragma ACCEL PIPELINE auto{off}'\n5. Tile the middle loop with the factor set to auto: '#pragma ACCEL TILE FACTOR=auto{4}'\n6. Flatten the innermost loop by automatically pipelining it: '#pragma ACCEL PIPELINE auto{flatten}'\n7. Tile the innermost loop with the factor set to auto: '#pragma ACCEL TILE FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "In the given C code, to generate HLS codes, use the following natural language instructions for the #pragma values:\n\n1. 'Enable automatic pipelining for this loop'\n   - HLS code: '#pragma ACCEL PIPELINE auto{}'\n\n2. 'Divide the loop into its original, full form without breaking it into smaller chunks'\n   - HLS code: '#pragma ACCEL TILE FACTOR=auto{1}'\n\n3. 'Run the iterations of this loop one after the other, not in parallel'\n   - HLS code: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n\n4. 'Disable automatic pipelining for this loop'\n   - HLS code: '#pragma ACCEL PIPELINE auto{off}'\n\n5. 'Divide the loop into its original, full form without breaking it into smaller chunks'\n   - HLS code: '#pragma ACCEL TILE FACTOR=auto{2}'\n\n6. 'Run the iterations of this loop in parallel with a factor of 8'\n   - HLS code: '#pragma ACCEL PARALLEL FACTOR=auto{8}'\n\nNote: Ensure to replace '__PARA__L0', '__PARA__L1', '__PARA__L2', '__PARA__L4', '__PARA__L5', '__PARA__L6', '__PARA__L7_0', '__PIPE__L3', and '__TILE__L3' with appropriate loop labels or constants as required in the actual use case.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration. Apply automatic pipeline optimization with the pipeline off, tile the loops with a factor of 1, and run the parallel loops with a factor of 30.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following natural language instructions:\n\n1. `#pragma ACCEL PIPELINE auto{off}`: \n   - Don't automatically pipeline the following loops.\n\n2. `#pragma ACCEL TILE FACTOR=auto{1}`: \n   - Process the following loops in their original, full form without breaking them into smaller chunks.\n\n3. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`: \n   - Run the iterations of the following loop in parallel.\n\n4. `#pragma ACCEL PIPELINE auto{}`: \n   - Treat the following loop as normal behavior.\n\n5. `#pragma ACCEL TILE FACTOR=auto{30}`: \n   - Break down the following loop into smaller chunks with a factor of 30.\n\n6. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`: \n   - Run the iterations of the following loop in parallel.\n\n7. `#pragma ACCEL PIPELINE auto{__PIPE__L3}`: \n   - Automatically pipeline the following loop.\n\n8. `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`: \n   - Break down the following loop into smaller chunks with an automatic factor.\n\n9. `#pragma ACCEL PARALLEL FACTOR=auto{4}`: \n   - Run the iterations of the following loop in parallel with a factor of 4.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. For the first loop, treat it as a hardware kernel for acceleration: \n   Use `#pragma ACCEL kernel` to mark the function as a hardware kernel for acceleration.\n\n2. For the second loop over the 'y' array, don't automatically pipeline the loop:\n   Use `#pragma ACCEL PIPELINE \"off\"` to turn off automatic loop pipelining.\n\n3. For the third loop over the 'tmp' array, process it in its original, full form without breaking it into smaller chunks:\n   Use `#pragma ACCEL TILE FACTOR=1` to specify a tiling factor of 1 for the loop.\n\n4. For the fourth loop over the 'tmp' array, run the iterations one after the other, not in parallel:\n   Use `#pragma ACCEL PARALLEL FACTOR=1` to specify a parallel factor of 1 for the loop to run the iterations sequentially.\n\n5. For the innermost loop over 'j' and the 'tmp' array, perform parallel reduction with a factor of 1 for 'tmp':\n   Use `#pragma ACCEL PARALLEL reduction=tmp FACTOR=1` to parallelize the reduction operation for the 'tmp' array.\n\n6. For the innermost loop over 'j' and the 'y' array, perform parallel reduction with a factor of 1 for 'y':\n   Use `#pragma ACCEL PARALLEL reduction=y FACTOR=1` to parallelize the reduction operation for the 'y' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code with HLS pragmas:\n1. Use '#pragma ACCEL kernel' to specify a hardware acceleration kernel function.\n2. Use '#pragma ACCEL PIPELINE auto{}' to enable automatic pipelining of the loop.\n3. Use '#pragma ACCEL TILE FACTOR=auto{2}' to specify automatic tiling with a factor of 2.\n4. Use '#pragma ACCEL PARALLEL FACTOR=auto{1}' to enable automatic parallelization of the outer loop.\n5. Use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}' to automatically parallelize and perform reduction for the first inner loop.\n6. Use '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}' to automatically parallelize and perform reduction for the second inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Use parallelism on the following loop with a reduction and execute iterations sequentially' to describe '#pragma ACCEL PARALLEL reduction=tmp FACTOR=1 and #pragma ACCEL PARALLEL reduction=y FACTOR=1'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following comprehensive but concise natural language instruction:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel\n\nThis natural language instruction captures all the necessary #pragma values to use in the HLS codes for the given C function.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To summarize the task:\n\n\"Use pragma ACCEL kernel for the following function to treat it as a hardware kernel. Turn off automatic pipelining for the first loop. Then, apply tiling with a factor of 8 and parallelism with a factor of __PARA__L0. For the second loop, turn off automatic pipelining, set the tiling factor to 1, and apply parallelism with a factor of __PARA__L1. For the next loop, turn off automatic pipelining, set the tiling factor to 1, and apply parallelism with a factor of __PARA__L2. Finally, use specific pipeline, tiling, and parallel factors for the last loop as auto{__PIPE__L3}, auto{__TILE__L3}, and auto{30} respectively.\"\n\nI hope this helps! Let me know if you need further assistance!", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given loop in its original form without breaking it into smaller chunks, then automatically pipeline the loop and set the pipeline factor to 1. Also, run the iterations of this loop one after the other, not in parallel, and treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To enable hardware acceleration and optimize the given C code, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Enable hardware acceleration for the function: 'Treat the following function as a hardware kernel for acceleration'\n   - Use: #pragma ACCEL kernel\n\n2. Pipeline the outermost loop with automatic settings: 'Automatically pipeline the outermost loop'\n   - Use: #pragma ACCEL PIPELINE auto{}\n\n3. Disable automatic loop pipelining for the outermost loop: 'Manually turn off loop pipelining for the outermost loop'\n   - Use: #pragma ACCEL PIPELINE auto{off}\n\n4. Flatten the loop for parallel processing with automatic settings: 'Automatically flatten the loop for parallel processing'\n   - Use: #pragma ACCEL PIPELINE auto{flatten}\n\n5. Set the tile factor for the outermost and inner loops automatically to 1: 'Automatically set the tile factor to 1 for the outermost and inner loops'\n   - Use: #pragma ACCEL TILE FACTOR=auto{1}\n\nThese instructions clarify how to apply the specific #pragma directives in natural language to optimize the given C code for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but very concise natural language instruction for the specific #pragma values:\n1. Treat the following function as a hardware kernel for acceleration - `#pragma ACCEL kernel`\n2. Don't automatically pipeline this loop - `#pragma ACCEL PIPELINE auto{off}`\n3. Process this loop in its original, full form without breaking it into smaller chunks - `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Run the iterations of this loop one after the other, not in parallel - `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` (and similarly for other parallel factors)\n5. Utilize appropriate pipelining for this loop - `#pragma ACCEL PIPELINE auto{__PIPE__L3}` (and similarly for other pipeline factors)", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive natural language instructions for the pragma values:\n\n- Use '#pragma ACCEL kernel' to treat the following function as a hardware kernel for acceleration.\n- Use '#pragma ACCEL PIPELINE auto{off}' to manually turn off automatic loop pipelining for the following loop.\n- Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the following loop in its original, full form without breaking it into smaller chunks.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run the iterations of the following loop one after the other, not in parallel, with a parallel factor of 1.\n- Use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}' to parallelize the reduction variable 'tmp' in the inner loop with a parallel factor of 1.\n- Use '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}' to parallelize the reduction variable 'y' in the outer loop with a parallel factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C: \nUse '#pragma ACCEL PIPELINE \"off\" to disable automatic loop pipelining, '#pragma ACCEL TILE FACTOR=4' to parallelize the loop with chunk factor 4, '#pragma ACCEL PARALLEL FACTOR=__PARA__L0' to run iterations of a loop in parallel as a full, '#pragma ACCEL PIPELINE ' to re-enable automatic loop pipelining, '#pragma ACCEL TILE FACTOR=1' to process loop in its original, full form, '#pragma ACCEL PARALLEL FACTOR=__PARA__L1' to run iterations of a loop in parallel, '#pragma ACCEL PIPELINE \"off\"' to disable automatic loop pipelining, '#pragma ACCEL TILE FACTOR=1' to process loop in its original, full form, '#pragma ACCEL PARALLEL FACTOR=__PARA__L2' to run iterations of a loop in parallel, '#pragma ACCEL PIPELINE '__PIPE__L3'' to enable loop pipelining, '#pragma ACCEL TILE FACTOR=__TILE__L3' to parallelize the loop with specified tile factor, and '#pragma ACCEL PARALLEL FACTOR=30' to run iterations of a loop in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the pragma values as described:\n\n1. For the outermost loop, enable automatic pipelining:  \n   `#pragma ACCEL PIPELINE auto`\n\n2. Break the outermost loop into smaller chunks, with a factor determined automatically:  \n   `#pragma ACCEL TILE FACTOR=auto{2}`\n\n3. Parallelize the outermost loop at the highest level, with a factor determined automatically:  \n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n4. After inner loop, enable automatic pipelining:  \n   `#pragma ACCEL PIPELINE auto`\n\n5. After inner loop, keep it as a single chunk without breaking:  \n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n6. Parallelize the inner loop at the specified level, with a factor determined automatically:  \n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`,  \n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`,  \n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}`,  \n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}`,  \n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}`,  \n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}`\n\n7. For specific loops and factors, use custom pragma values e.g.,  \n   `#pragma ACCEL PIPELINE auto{__PIPE__L3}`  \n   `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`  \n\nEnsure to treat the function as a hardware kernel for acceleration with:  \n`#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the given C function `kernel_doitgen`, use the following natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n   - Use `#pragma ACCEL kernel` before the function declaration.\n\n2. Enable automatic pipelining for the outermost loop.\n   - Use `#pragma ACCEL PIPELINE auto{}` before the outermost loop.\n\n3. Disable automatic pipelining for the mid-level loop.\n   - Use `#pragma ACCEL PIPELINE auto{off}` before the mid-level loop.\n\n4. Flatten the mid-level loop.\n   - Use `#pragma ACCEL PIPELINE auto{flatten}` before the mid-level loop.\n\n5. Set the tile factor automatically for both the outermost and mid-level loops.\n   - Use `#pragma ACCEL TILE FACTOR=auto{2}` before the outermost loop, and `#pragma ACCEL TILE FACTOR=auto{1}` before the mid-level loop.\n\nBy following these instructions, you can generate the HLS codes for the given C function using the specific #pragma values provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code to accelerate the `kernel_atax` function by using the following natural language instructions:\n1. Mark the function as a hardware kernel for acceleration.\n2. Automatically pipeline the first loop to improve throughput.\n3. Automatically determine the tile factor for loop tiling.\n4. Run the iterations of the outer loop one after the other, not in parallel.\n5. Use parallelization with reduction for the inner loops to improve performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the corresponding #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration' for '#pragma ACCEL kernel'\n\n2. 'Don't automatically pipeline this loop' for '#pragma ACCEL PIPELINE \"off\"'\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=1'\n\n4. 'Run the iterations of this loop one after the other, not in parallel' for '#pragma ACCEL PARALLEL FACTOR=1'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Use hardware acceleration for the function and pipeline the first loop, tile the second loop by a factor of 116, and run the iterations of the third loop in parallel.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform pipelining for the loop, tile it with a factor of 116, and run iterations of the loop in parallel with a parallel factor of 1. Use reduction for the inner loop with variable 'tmp', and another reduction for the outer loop with variable 'y'. Then treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with HLS pragmas:\n\nFor the function `kernel_correlation`:\n1. Treat the following function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n2. Pipeline the loops automatically using `#pragma ACCEL PIPELINE auto{}`.\n3. Process the loops in their original, full form without breaking them into smaller chunks using `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. Run the iterations of the loops in parallel based on the factor `__PARA__L0` using `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`.\n\nFor the subsequent loops in the function:\n5. Pipeline the loops automatically using `#pragma ACCEL PIPELINE auto{}`.\n6. Process the loops in their original, full form without breaking them into smaller chunks using `#pragma ACCEL TILE FACTOR=auto{1}`.\n7. Run the iterations of the loops in parallel based on the factor `__PARA__L1`, `__PARA__L2`, `__PARA__L4`, `__PARA__L5`, `__PARA__L6`, `__PARA__L7_0` using `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}`.\n\n8. The keyword `auto` and `__PIPE__L3`, `__TILE__L3`, and `flatten` will need to be clarified as their meaning could not be inferred from the document.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but very concise natural language instructions:\n\n- Treat the 'kernel_correlation' function as a hardware kernel for acceleration.\n- Don't automatically pipeline the loops.\n- Process the loops in their original, full form without breaking them into smaller chunks.\n- Run the iterations of the loops one after the other, not in parallel.\n- Center and reduce the column vectors.\n- Calculate the m*m correlation matrix with specific tiling, pipelining, and parallelization factors.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Natural language instruction: \"Mark the following function as a hardware kernel for acceleration.\"\n   - HLS code: #pragma ACCEL kernel\n\n2. Automatically pipeline this loop.\n   - Natural language instruction: \"Automatically pipeline this loop to maximize throughput.\"\n   - HLS code: #pragma ACCEL PIPELINE auto{}\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Natural language instruction: \"Do not tile this loop, keep it in its original form.\"\n   - HLS code: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Natural language instruction: \"Run the iterations of this loop sequentially, not in parallel.\"\n   - HLS code: #pragma ACCEL PARALLEL FACTOR=1\n\n5. Turn off automatic pipeline for this loop.\n   - Natural language instruction: \"Disable automatic pipelining for this loop.\"\n   - HLS code: #pragma ACCEL PIPELINE auto{off}\n\n6. Set tile factor to 1 for this loop, no tiling.\n   - Natural language instruction: \"Do not split this loop into smaller tiles, keep it as a single unit.\"\n   - HLS code: #pragma ACCEL TILE FACTOR=1\n\n7. Run iterations of this loop one after the other, not in parallel.\n   - Natural language instruction: \"Execute the loop iterations sequentially, not in parallel.\"\n   - HLS code: #pragma ACCEL PARALLEL FACTOR=1\n\n8. Turn off automatic pipelining for this loop.\n   - Natural language instruction: \"Disable automatic pipelining for this loop.\"\n   - HLS code: #pragma ACCEL PIPELINE auto{off}\n\n9. Set tile factor to 1 for this loop, no tiling.\n   - Natural language instruction: \"Disable loop tiling, keep it as a single unit.\"\n   - HLS code: #pragma ACCEL TILE FACTOR=1\n\n10. Run iterations of this loop one after the other, not in parallel.\n    - Natural language instruction: \"Execute the loop iterations sequentially, not in parallel.\"\n    - HLS code: #pragma ACCEL PARALLEL FACTOR=1\n\n11. Enable pipelining for this loop with custom pipeline attributes.\n    - Natural language instruction: \"Enable custom pipelining for this loop with specific pipeline attributes.\"\n    - HLS code: #pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n12. Set tile factor to a specific value for this loop.\n    - Natural language instruction: \"Adjust the loop tiling factor to a specific value.\"\n    - HLS code: #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n13. Run iterations of this loop in parallel with a specific parallelization factor.\n    - Natural language instruction: \"Execute the loop iterations in parallel with a specific factor for parallelization.\"\n    - HLS code: #pragma ACCEL PARALLEL FACTOR=auto{4}", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   Use '#pragma ACCEL kernel' before the function definition.\n\n2. Disable automatic pipelining for this loop:\n   Use '#pragma ACCEL PIPELINE \"off\"' before the loop that requires disable auto-pipelining.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   Use '#pragma ACCEL TILE FACTOR=1' before the loop that needs to be processed without tiling.\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   Use '#pragma ACCEL PARALLEL FACTOR=1' before the loop that should not run in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes in C using natural language instructions as follows:\n\n1. For the kernel_correlation function, treat it as a hardware kernel for acceleration using the pragma: '#pragma ACCEL kernel'\n\n2. Use the pragma '#pragma ACCEL PIPELINE off' to turn off automatic loop pipelining for the first loop.\n\n3. Keep the second loop in its original, full form without breaking it into smaller chunks using the pragma: '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of the third loop one after the other, not in parallel, by using: '#pragma ACCEL PARALLEL FACTOR=1'\n\n5. Disable automatic loop pipelining for the fourth loop with: '#pragma ACCEL PIPELINE off'\n\n6. Set the tile factor for the fifth loop to 20 using: '#pragma ACCEL TILE FACTOR=20'\n\n7. Run the iterations of the fifth loop in parallel with a factor of '__PARA__L1' using: '#pragma ACCEL PARALLEL FACTOR=__PARA__L1'\n\n8. Turn off automatic loop pipelining for the sixth loop using: '#pragma ACCEL PIPELINE off'\n\n9. Keep the original form for the sixth loop with: '#pragma ACCEL TILE FACTOR=1'\n\n10. Run the iterations of the seventh loop in parallel with a factor of '__PARA__L2' using: '#pragma ACCEL PARALLEL FACTOR=__PARA__L2'\n\n11. Set the pipeline factor for the eighth loop to '__PIPE__L3' with: '#pragma ACCEL PIPELINE __PIPE__L3'\n\n12. Set the tile factor for the eighth loop to '__TILE__L3' using: '#pragma ACCEL TILE FACTOR=__TILE__L3'\n\n13. Run the iterations of the eighth loop in parallel with a factor of 30 by using: '#pragma ACCEL PARALLEL FACTOR=30'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Specify the factors for parallelizing the reduction operations.\n\nThese instructions correspond to the specific #pragma values used in the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following comprehensive but very concise natural language instruction for the specific #pragma values:\n\n1. Disable automatic pipelining for the loop: \n   ```\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n2. Process the loop in its original, full form without breaking it into smaller chunks:\n   ```\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n3. Run the iterations of this loop one after the other, not in parallel:\n   ```\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n   ```\n\n4. Enable automatic pipelining for the loop:\n   ```\n   #pragma ACCEL PIPELINE auto{}\n   ```\n\n5. Break the loop into smaller chunks of size 8 for optimization:\n   ```\n   #pragma ACCEL TILE FACTOR=auto{8}\n   ```\n\n6. Run the iterations of this loop in parallel, breaking into specified smaller chunks:\n   ```\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n   ```\n\n7. Disable automatic pipelining for the loop:\n   ```\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n8. Process the loop in its original, full form without breaking it into smaller chunks:\n   ```\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n9. Run the iterations of this loop in parallel, breaking into specified smaller chunks:\n   ```\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n   ```\n\n10. Enable automatic pipelining for the loop:\n    ```\n    #pragma ACCEL PIPELINE auto{__PIPE__L3}\n    ```\n   \n11. Break the loop into smaller chunks for optimization:\n    ```\n    #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    ```\n\n12. Run the iterations of this loop in parallel, breaking into specified smaller chunks:\n    ```\n    #pragma ACCEL PARALLEL FACTOR=auto{30}\n    ```\n\nFinally, treat the following function as a hardware kernel for acceleration:\n    ```\n    #pragma ACCEL kernel\n    ```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nAlso, add the following pragma values for specific loops:\n- Do not automatically pipeline the first loop.\n- Break the loop into smaller chunks with a factor of 116.\n- Run the iterations of the second loop one after the other, without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following C code with hardware acceleration and parallelization:\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n    int i;\n    int j;\n    int k;\n    double eps = 0.1;\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n    for (j = 0; j < 80; j++) {\n        mean[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n        for (i = 0; i < 100; i++) {\n            mean[j] += data[i][j];\n        }\n        mean[j] /= float_n;\n    }\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{8}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 80; j++) {\n        stddev[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n        for (i = 0; i < 100; i++) {\n            stddev[j] += pow(data[i][j] - mean[j], (double)2);\n        }\n        stddev[j] /= float_n;\n        stddev[j] = sqrt(stddev[j]);\n        stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n    }\n\n#pragma ACCEL PIPELINE auto{flatten}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n        for (j = 0; j < 80; j++) {\n            data[i][j] -= mean[j];\n            data[i][j] /= sqrt(float_n) * stddev[j];\n        }\n    }\n\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 0; i < 80 - 1; i++) {\n        corr[i][i] = 1.0;\n\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n        for (j = i + 1; j < 80; j++) {\n            corr[i][j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n            for (k = 0; k < 100; k++) {\n                corr[i][j] += data[k][i] * data[k][j];\n            }\n            corr[j][i] = corr[i][j];\n        }\n    }\n    corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nThe comprehensive but concise natural language instruction for using the specific #pragma values are as follows:\n\n1. For the first loop, use automatic pipelining: `#pragma ACCEL PIPELINE auto{}`.\n2. For the first loop, use automatic tiling with a factor of 1: `#pragma ACCEL TILE FACTOR=auto{1}`.\n3. Parallelize the first loop with automatic factor: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`.\n4. For the second loop, use automatic pipelining: `#pragma ACCEL PIPELINE auto{}`.\n5. For the second loop, use automatic tiling with a factor of 8: `#pragma ACCEL TILE FACTOR=auto{8}`.\n6. Parallelize the second loop with automatic factor: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`.\n7. For the center and reduce loop, use automatic pipelining with flattening: `#pragma ACCEL PIPELINE auto{flatten}`.\n8. For the center and reduce loop, use automatic tiling with a factor of 1: `#pragma ACCEL TILE FACTOR=auto{1}`.\n9. Parallelize the center and reduce loop with automatic factor: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`.\n10. For the correlation matrix loop, use automatic pipelining with a custom name: `#pragma ACCEL PIPELINE auto{__PIPE__L3}`.\n11. For the correlation matrix loop, use automatic tiling with a custom name: `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`.\n12. Parallelize the correlation matrix loop with a factor of 1: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop, process the loop in its original, full form without breaking it into smaller chunks, and run the iterations of the second loop one after the other, not in parallel. Also, treat the inner loops as parallel with a reduction factor of 1 for both 'tmp' and 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes:\n\n1. For the first loop, instruct to turn off automatic pipelining: \n   --> \"Don't automatically pipeline this loop\"\n\n2. For the second loop, instruct to process in its original form without breaking into smaller chunks:\n   --> \"Process this loop in its original, full form without breaking into smaller chunks\"\n\n3. For the third loop, instruct to run iterations one after the other, not in parallel:\n   --> \"Run the iterations of this loop one after the other, not in parallel\"\n\n4. Finally, treat the following function as a hardware kernel for acceleration:\n   --> \"Treat the following function as a hardware kernel for acceleration\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code with the following instructions:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n\n  #pragma HLS PIPELINE off\n  #pragma HLS ARRAY_PARTITION variable=A dim=1 complete\n  #pragma HLS ARRAY_PARTITION variable=x dim=1 complete\n  #pragma HLS ARRAY_PARTITION variable=y dim=1 complete\n  #pragma HLS ARRAY_PARTITION variable=tmp dim=1 complete\n\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in its original, full form without breaking it into smaller chunks, and don't automatically pipeline it. Run the iterations of this loop one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and automatically pipeline the loop, break it into smaller chunks with a factor of 116, and process the iterations in parallel with a factor of 1 for the outer loop and factor of 1_0 for the reduction with 'tmp' and a factor of 1_1 for the reduction with 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with HLS pragma values in natural language, consider the following:\n\n1. Mark the following function as a hardware kernel for acceleration - Use `#pragma ACCEL kernel` to indicate that the function `kernel_atax` is to be treated as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the first loop - Use `#pragma ACCEL PIPELINE \"off\"` to prevent the automatic pipelining of the first loop.\n\n3. Process the first loop without breaking it into smaller chunks - Use `#pragma ACCEL TILE FACTOR=1` to process the first loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of the second loop one after the other, not in parallel - Use `#pragma ACCEL PARALLEL FACTOR=1` to specify that the iterations of the second loop should not run in parallel.\n\n5. Define the specific reduction factor for the first nested parallel loop - Use `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}` to define the specific reduction factor for the first nested parallel loop.\n\n6. Define the specific reduction factor for the second nested parallel loop - Use `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}` to define the specific reduction factor for the second nested parallel loop.\n\nBy following these instructions, the given C code with the corresponding HLS pragma values can be generated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the outer loop. Use a tile factor of 1 for both the outer and middle loops. Use loop flattening for the innermost loop and a tile factor of 8 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform pipeline optimization for loops to maximize throughput and minimize initiation interval by adding `#pragma ACCEL PIPELINE auto{off}`.\n\nDivide loops into smaller chunks to increase parallelism and optimize resource utilization using `#pragma ACCEL TILE FACTOR=25` and `#pragma ACCEL TILE FACTOR=1` for different loops respectively.\n\nEnable parallel processing with a factor of 80 for the outer loop and 100 for the inner loop using `#pragma ACCEL PARALLEL FACTOR=__PARA__L0` and `#pragma ACCEL PARALLEL FACTOR=__PARA__L4` for the first loop, and `#pragma ACCEL PARALLEL FACTOR=__PARA__L1` and `#pragma ACCEL PARALLEL FACTOR=__PARA__L5` for the second loop.\n\nDisable automatic pipeline optimization to process a specific loop in its original form with `#pragma ACCEL PIPELINE auto{off}` and `#pragma ACCEL PIPELINE auto{}`.\n\nFurther divide loops into smaller chunks for parallel processing with `#pragma ACCEL TILE FACTOR=1` and `#pragma ACCEL TILE FACTOR=__TILE__L3` to optimize resource allocation.\n\nFinally, mark the function as a hardware kernel for acceleration with `#pragma ACCEL kernel`.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C:\n\n1. Mark the function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the first loop.\n3. Process the first loop without breaking it into smaller chunks using the default tile factor.\n4. Run the iterations of the second loop one after the other, not in parallel. Use parallel factor 1.\n   Reduce the variable \"tmp\" in parallel using a parallel factor of 1.\n   Reduce the variable \"y\" in parallel using a parallel factor of 1.\n\nThese instructions correspond to the given HLS codes with the respective #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\nDetailed explanation for the #pragma values can be found in the HLS C codes provided above.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the natural language instruction:\n\n\"Indicate that the following function should be treated as a hardware kernel for acceleration, then for the loops, turn off automatic pipelining, set the tiling factor to 1, and set the parallel factor of both inner loops to 1 with reduction of 'tmp' and 'y' variables.\"\n\nBy following this comprehensive instruction, you can generate the given HLS codes in C with the required #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Automatically pipeline the loop, process it in its original form without breaking it into smaller chunks, and run the iterations of this loop in parallel. Treat the following function as a hardware kernel for acceleration.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n1. Treat the function \"kernel_correlation\" as a hardware kernel for acceleration.\n\n2. Process loops in their original, full form without breaking them into smaller chunks.\n\n3. Run the iterations of loops one after the other, not in parallel.\n\n4. Don't automatically pipeline loops; turn off automatic pipelining.\n\n5. When implementing loop tiling, use a tile factor of 1 for most loops except for one where the tile factor can be 30.\n\n6. For the designated loop, use a parallelization factor for acceleration provided as \"__PARA__L0\", \"__PARA__L1\", \"__PARA__L2\", \"__PARA__L4\", \"__PARA__L5\", \"__PARA__L6\", \"__PARA__L7_0\".\n\n7. When required, use specific pipeline and parallelization factors labeled as \"__PIPE__L3\", \"__PIPE__L7\".", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n'\n1. Mark the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the outermost loop.\n\n3. Process the outermost loop in its original, full form without breaking it into smaller chunks.\n\n4. Don't automatically pipeline the middle loop.\n\n5. Process the middle loop in its original, full form without breaking it into smaller chunks.\n\n6. Run the iterations of the innermost loop one after the other, not in parallel.\n\n7. Break the iterations of the innermost loop into smaller chunks of size 4 for optimization.\n'", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{flatten}\n      \n#pragma ACCEL TILE FACTOR=auto{4}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should use the following natural language instructions for the specific #pragma values:\n\n1. For the kernel function, use the instruction 'Treat the following function as a hardware kernel for acceleration'.\n\n2. For the loop calculating the mean, use the instruction 'Don't automatically pipeline this loop', 'Process this loop in its original, full form without breaking it into smaller chunks', and 'Run the iterations of this loop one after the other, not in parallel'.\n\n3. For the loop calculating the standard deviation, use the same instructions as above for mean calculation.\n\n4. For the loop centering and reducing the column vectors, use 'Don't automatically pipeline this loop' and 'Process this loop in chunks with a factor of 4', and 'Run the iterations of this loop in parallel with a factor of 6'.\n\n5. For the loop calculating the correlation matrix, use 'Automatically pipeline this loop in a way that supports dependencies', 'Process this loop in chunks', and 'Run the iterations of this loop in parallel with a factor of 30'.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\"Treat the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Process the first loop with its original, full form without breaking it into smaller chunks. Run the iterations of the second loop one after the other, not in parallel. For the second loop, treat the inner loops as reductions and run them sequentially, not in parallel\"\n\nThis natural language instruction encapsulates the specific #pragma values to use for the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To use HLS codes to describe the given C code, you can use the following concise natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration by adding the pragma '#pragma ACCEL kernel'.\n\n2. Don't automatically pipeline the loop by adding the pragma '#pragma ACCEL PIPELINE \"off\"'.\n\n3. Process each loop in its original, full form without breaking it into smaller chunks by adding the pragma '#pragma ACCEL TILE FACTOR=1'.\n\n4. Run the iterations of the loop one after the other, not in parallel, by adding the pragma '#pragma ACCEL PARALLEL FACTOR=1'.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and then apply the following specific directives: \n- Do not automatically pipeline the loop\n- Break the loop into smaller chunks with a factor of 116\n- Run the iterations of the loop in parallel with a factor of 1 for the outer loop and automatic factor for inner loops with reduction operations on 'tmp' and 'y'\n\nThis would result in the following HLS code:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n    int i;\n    int j;\n    for (i = 0; i < 124; i++) \n        y[i] = ((double )0);\n    #pragma ACCEL PIPELINE auto{off}\n    #pragma ACCEL TILE FACTOR=auto{116}\n    #pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 0; i < 116; i++) {\n        tmp[i] = 0.0;\n        #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n        for (j = 0; j < 124; j++) {\n            tmp[i] += A[i][j] * x[j];\n        }\n        #pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n        for (j = 0; j < 124; j++) {\n            y[j] += A[i][j] * tmp[i];\n        }\n    }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these steps:\n\n1. Treat the function as a hardware kernel for acceleration: ```#pragma ACCEL kernel```\n\n2. Don't automatically pipeline the loops: ```#pragma ACCEL PIPELINE \"off\"```\n\n3. Process the loops in their original, full form without breaking them into smaller chunks: ```#pragma ACCEL TILE FACTOR=1```\n\n4. Run the iterations of the loops one after the other, not in parallel: ```#pragma ACCEL PARALLEL FACTOR=1```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes using natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Process this loop in its original, full form without breaking it into smaller chunks, and don't automatically pipeline this loop.\n\n3. Run the iterations of this loop one after the other, not in parallel, and don't automatically pipeline this loop.\n\n4. Repeat the same instructions provided in point 3.\n\n5. Repeat the same instructions provided in point 3.\n\nThe natural language instructions above correspond to the following HLS pragma values in the given C code:\n\n1. `#pragma ACCEL kernel`\n\n2. `#pragma ACCEL PIPELINE auto{off}`\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. `#pragma ACCEL PIPELINE auto{off}`\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. `#pragma ACCEL PIPELINE auto{off}`\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n5. `#pragma ACCEL PIPELINE auto{off}`\n   `#pragma ACCEL TILE FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instructions for the specific #pragma values:\n\n1. For the first loop, turn off automatic pipelining: \n   - Natural Language Instruction: 'Don't automatically pipeline this loop'\n   - Corresponding #pragma: #pragma ACCEL PIPELINE auto{off}\n\n2. For the first loop, process in its original, full form without breaking it into smaller chunks: \n   - Natural Language Instruction: 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - Corresponding #pragma: #pragma ACCEL TILE FACTOR=25\n\n3. For the first loop, run the iterations sequentially, not in parallel: \n   - Natural Language Instruction: 'Run the iterations of this loop one after the other, not in parallel'\n   - Corresponding #pragma: #pragma ACCEL PARALLEL FACTOR=__PARA__L0\n\n4. For the second loop, enable automatic pipelining: \n   - Natural Language Instruction: 'Automatically pipeline this loop'\n   - Corresponding #pragma: #pragma ACCEL PIPELINE auto{}\n\n5. For the second loop, process in its original, full form without breaking it into smaller chunks: \n   - Natural Language Instruction: 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - Corresponding #pragma: #pragma ACCEL TILE FACTOR=1\n\n6. For the second loop, run the iterations sequentially, not in parallel: \n   - Natural Language Instruction: 'Run the iterations of this loop one after the other, not in parallel'\n   - Corresponding #pragma: #pragma ACCEL PARALLEL FACTOR=__PARA__L1\n\n7. For the third loop, turn off automatic pipelining: \n   - Natural Language Instruction: 'Don't automatically pipeline this loop'\n   - Corresponding #pragma: #pragma ACCEL PIPELINE auto{off}\n\n8. For the third loop, process in its original, full form without breaking it into smaller chunks: \n   - Natural Language Instruction: 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - Corresponding #pragma: #pragma ACCEL TILE FACTOR=1\n\n9. For the third loop, run the iterations sequentially, not in parallel: \n   - Natural Language Instruction: 'Run the iterations of this loop one after the other, not in parallel'\n   - Corresponding #pragma: #pragma ACCEL PARALLEL FACTOR=__PARA__L2\n\n10. For the fourth loop, use specific pipeline and tiling factors and run the iterations in parallel: \n   - Natural Language Instruction: 'Use specific factors for pipelining and tiling, and run the iterations in parallel'\n   - Corresponding #pragma: #pragma ACCEL PIPELINE auto{__PIPE__L3}, #pragma ACCEL TILE FACTOR=auto{__TILE__L3}, #pragma ACCEL PARALLEL FACTOR=30", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use automatic pipeline, automatic tiling with factor=4, and automatic parallelization with factor=1 for both tmp reduction and y reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following natural language instructions for the specific #pragma values:\n\n1. The kernel_correlation function should be treated as a hardware kernel for acceleration: \n   - Use `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loops in the code: \n   - Use `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process the loops in their original, full form without breaking them into smaller chunks:\n   - Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of the loops one after the other, not in parallel: \n   - Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}`\n\n5. The last three loops should have their TILE and PIPELINE factors set as follows: \n   - `#pragma ACCEL TILE FACTOR=auto{8}`, `#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL TILE FACTOR=auto{1}`, `#pragma ACCEL PIPELINE auto{__PIPE__L3}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`, `#pragma ACCEL PARALLEL FACTOR=auto{30}`, `#pragma ACCEL PIPELINE auto{__PIPE__L7}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and then apply specific optimizations to the loops within the function as follows:\n- Disable automatic pipelining for the first loop iteration.\n- Tile the loop with a factor of 8 for efficient memory access.\n- Allow parallelism with a factor of 1 for the outer loop and reduce the inner loops for efficient execution.\"\n\nYou can use these instructions to apply the respective #pragma values in the C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply hardware kernel acceleration to the function and ensure the loop is not automatically pipelined, use a tile factor of 1, and run iterations of the loop without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that this function is treated as a hardware kernel for acceleration. \nApply automatic pipeline off for the following loop. \nFactor the tiling factor as 1 for the following loop. \nSet the parallel factor as 1 for the following loop. \nApply reduction with a factor of 1 for the 'tmp' variable in the nested loop. \nApply reduction with a factor of 1 for the 'y' variable in the nested loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Break the loop into smaller chunks with a factor of 116. Run the iterations of the second loop one after the other, not in parallel. Use parallelism with a reduction variable for the inner loops to factor 1 for both 'tmp' and 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you can use the following natural language instructions:\n\n1. Enable manual pipelining for the following loop to control initiation interval: \n   `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process the loop in its original, complete form without breaking it into smaller chunks: \n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Run the iterations of this loop one after the other, not in parallel: \n   `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n4. Treat the following function as a hardware kernel for acceleration: \n   `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loop, process the loop in its original, full form without breaking it into smaller chunks, and run the iterations of the loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code for the given C code with the following natural language instruction:\n\n\"Designate the function as a hardware kernel for acceleration. Then, automatically pipeline the loop, process it in its full form without breaking into smaller chunks, and run the iterations one after the other in parallel. Next, turn off automatic pipelining for the loop, keep the loop in its original form, and run the iterations in parallel. Finally, for the last three loops, disable automatic pipelining, process the loop in its original form without breaking it into smaller chunks, and run the iterations in parallel.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should:\n\n1. Treat the function 'kernel_correlation' as a hardware kernel for acceleration.\n   --> Use: \n      '#pragma ACCEL kernel'\n\n2. Don't automatically pipeline the loops in the code.\n   --> Use:\n      '#pragma ACCEL PIPELINE \"off\"'\n\n3. Process the loops in their original, full form without breaking into smaller chunks.\n   --> Use:\n      '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of certain loops one after the other, not in parallel.\n   --> Use:\n      '#pragma ACCEL PARALLEL FACTOR=1'\n\n5. Automatically determine the pipeline strategy for the pipeline-enabled loop.\n   --> Use:\n      '#pragma ACCEL PIPELINE auto{}'\n\n6. Automatically determine the tile factor for the tiled loops.\n   --> Use:\n      '#pragma ACCEL TILE FACTOR=auto{value}'\n\n7. Automatically determine the parallel factor for the parallelized loops.\n   --> Use:\n      '#pragma ACCEL PARALLEL FACTOR=auto{value}'\n\nEnsure to substitute the 'value' with the appropriate value determination approach or specific values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, the following natural language instructions for specific #pragma values can be used:\n\n1. For the first loop, to prevent automatic pipeline optimization:  \n   \"Don't automatically pipeline this loop\"\n\n2. For the second loop, to keep the loop in its original, full form without breaking it into smaller chunks:  \n   \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n3. For the third loop, to run the iterations one after the other, not in parallel:  \n   \"Run the iterations of this loop one after the other, not in parallel\"\n\n4. For the fourth loop, with two steps to follow:  \n    a. \"Don't automatically pipeline this loop\"  \n    b. \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n5. For the fifth loop, with two steps to follow:  \n    a. \"Run the iterations one after the other, not in parallel\"  \n    b. \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n6. For the sixth loop, with two steps to follow:  \n    a. \"Run the iterations one after the other, not in parallel\"  \n    b. \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n7. For the seventh loop:  \n   a. \"Pipeline this loop\" with automatic pipeline setting  \n   b. \"Break this loop into smaller, tiled chunks with the automatic tiling factor of 1\"  \n   c. \"Run the iterations in parallel, with a factor of 30\"\n\n8. For the eighth loop:  \n   a. \"Pipeline this loop\" with automatic pipeline setting  \n   b. \"Pipeline this loop\" with automatic pipeline setting  \n   c. \"Run the iterations in parallel, with a specific factor\"\n\nEach instruction corresponds to a specific #pragma value in the given HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive natural language instructions for the specific #pragma values:\n\n1. \"Don't automatically pipeline the outer loop\" - for the loop with variable 'i'\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n\n2. \"Process the outer loop in its original, full form without breaking it into smaller chunks\" - for the loop with variable 'i'\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. \"Run the iterations of the outer loop one after the other, not in parallel\" - for the loop with variable 'i'\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n4. \"Run the inner loop in parallel without breaking it into smaller chunks and apply a reduction operation on the variable 'tmp'\" - for the inner loop with variable 'j'\n   - Use: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}`\n\n5. \"Run the inner loop in parallel without breaking it into smaller chunks and apply a reduction operation on the variable 'y'\" - for the inner loop with variable 'j'\n   - Use: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}`\n\n6. Optionally, \"Treat the following function as a hardware kernel for acceleration\"\n   - Use: `#pragma ACCEL kernel` at the beginning of the function declaration", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code for the given C code, use the following comprehensive and concise natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n2. Do not automatically pipeline the loop for j in the mean calculation.\n3. Process the mean calculation loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of the mean calculation loop one after the other, not in parallel.\n5. Turn off automatic loop pipelining for the loop calculating standard deviation.\n6. Process the standard deviation calculation loop in its original, full form without breaking it into smaller chunks.\n7. Run the iterations of the standard deviation calculation loop one after the other, not in parallel.\n8. Turn off automatic loop pipelining for the loop centering and reducing the column vectors.\n9. Process the loop for centering and reducing the column vectors using a tile factor of 2.\n10. Run the iterations of the loop for centering and reducing the column vectors in parallel with a factor of 6.\n11. Set the pipeline depth for the loop calculating the correlation matrix.\n12. Use a tile factor for the loop calculating the correlation matrix to enable data parallelism.\n13. Run the iterations of the loop calculating the correlation matrix in parallel with a factor of 30.\n\nThese natural language instructions correspond to the respective HLS pragma values used in the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in its original, full form without breaking it into smaller chunks. Run the iterations of the loop in parallel with a factor of 1. Treat the following function as a hardware kernel for acceleration. Also, don't automatically pipeline this loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the following C code:\n\n\"treat the following function as a hardware kernel for acceleration\ndon't automatically pipeline this loop\nprocess this loop in its original, full form without breaking it into smaller chunks\nrun the iterations of this loop one after the other, not in parallel\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the given C function, use the following natural language instructions:\n\n1. Enable automatic pipelining for this loop\n   `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process this loop in its original, full form without breaking it into smaller chunks\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Run the iterations of this loop one after the other, not in parallel\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n4. Enable automatic pipelining for this loop\n   `#pragma ACCEL PIPELINE auto{off}`\n\n5. Break this loop into chunks of 20 iterations, if possible\n   `#pragma ACCEL TILE FACTOR=auto{20}`\n\n6. Run the iterations of this loop in parallel if possible\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n7. Enable automatic pipelining for this loop\n   `#pragma ACCEL PIPELINE auto{off}`\n\n8. Process this loop in its original, full form without breaking it into smaller chunks\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n9. Run the iterations of this loop in parallel if possible\n   `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n10. Enable automatic pipelining for this loop\n   `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n11. Break this loop into chunks of 30 iterations, if possible\n   `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n12. Run the iterations of this loop in parallel if possible\n   `#pragma ACCEL PARALLEL FACTOR=auto{30}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should:\n1. Treat the function 'kernel_correlation' as a hardware kernel for acceleration.\n2. Automatically pipeline the loops, both inner and outer.\n3. Set the tile factor to be automatically determined.\n4. Automatically parallelize the loops, with specific parallel factor values for each loop.\n\nMake sure to adjust the 'auto' and '__PARA__L' values based on your specific hardware and optimization requirements.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use hardware acceleration to optimize the function 'kernel_correlation'. \n- Turn off automatic loop pipelining.\n- Process the loops in their original form without breaking them into smaller chunks.\n- Run the iterations of the loops in parallel with the specified factor.\n- Center and reduce the column vectors.\n- Calculate the correlation matrix with specified loop optimizations.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire `kernel_correlation` function as a hardware kernel for acceleration. Automatically pipeline the loops. Use the original full form of the loops without breaking them into smaller chunks. Run the iterations of the loops in parallel, with specific factors for each loop. Adjust the pipeline, tiling, and parallelization factors automatically based on the hardware and software settings.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To turn this C code into HLS code, you can use the following comprehensive but concise natural language instructions.\n\n```plaintext\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Use the original full form for this loop without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Automatically pipeline this loop.\n\n6. Use a tile factor of 1 for this loop.\n\n7. Run this loop in parallel with the specified parallel factor.\n\n8. Turn off automatic pipelining for this loop.\n\n9. Use a tile factor of 1 for this loop.\n\n10. Run this loop in parallel with the specified parallel factor.\n\n11. Turn off automatic pipelining for this loop.\n\n12. Use a tile factor of 8 for this loop.\n\n13. Run this loop in parallel with the specified parallel factor.\n\n14. Pipeline this loop with the specified pipeline factor.\n\n15. Use a tile factor of the specified value for this loop.\n\n16. Run this loop in parallel with the specified parallel factor.\n\n17. Pipeline this loop with the specified pipeline factor.\n\n18. Run this loop in parallel with the specified parallel factor.\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To parallelize and optimize the kernel_atax function for hardware acceleration, use the following natural language instruction for the C code:\n\n\"Apply automatic pipeline optimization for loops\" - #pragma ACCEL PIPELINE auto{}\n\n\"Automatically tile the loop with a factor of 8\" - #pragma ACCEL TILE FACTOR=auto{8}\n\n\"Run the iterations of the outer loop one after the other, and apply full parallelization with a reduction for variable 'tmp' and variable 'y'\" - #pragma ACCEL PARALLEL FACTOR=auto{1}, #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}, #pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n\n\"Treat the following function as a hardware kernel for acceleration\" - #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes:\n\n1. Identify the function as a hardware kernel for acceleration using the `#pragma ACCEL kernel`.\n\n2. Disable automatic loop pipelining with `#pragma ACCEL PIPELINE \"off\"`.\n\n3. Process each loop in its original, full form without breaking it into smaller chunks using `#pragma ACCEL TILE FACTOR=1`.\n\n4. Run the iterations of the loops in parallel using `#pragma ACCEL PARALLEL FACTOR=__PARA__L0`, `#pragma ACCEL PARALLEL FACTOR=__PARA__L1`, `#pragma ACCEL PARALLEL FACTOR=__PARA__L2`, and `#pragma ACCEL PARALLEL FACTOR=__PARA__L7_0`.\n\n5. Reset the pipeline and tiling with `#pragma ACCEL PIPELINE auto{}` and `#pragma ACCEL TILE FACTOR=auto{8}` respectively.\n\n6. Finally, optimize the last loop by setting the parallel factor as `#pragma ACCEL PARALLEL FACTOR=4`.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To enable hardware acceleration, first define the function as a kernel using the '#pragma ACCEL kernel'. Then, pipeline the loops automatically with '#pragma ACCEL PIPELINE auto{}', set the tiling factor to the default value using '#pragma ACCEL TILE FACTOR=auto{2}', and parallelize the outer loop with a factor of '__PARA__L0' using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'. For the subsequent loops, set the tiling factor to the default value with '#pragma ACCEL TILE FACTOR=auto{1}', and parallelize each loop with suitable reduction factors using '#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}', '#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}', and '#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}'. Additionally, pipeline the innermost loop with '#pragma ACCEL PIPELINE auto{__PIPE__L5}'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loops.\n   - Use: `#pragma ACCEL PIPELINE auto{off}` \n\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\nIt's important to note that the \"auto\" in the natural language instruction indicates that the specific value can be automatically determined by the HLS tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Identify the function as a hardware kernel for acceleration, then for each loop:\n- Disable automatic loop pipelining\n- Keep the loop in its original form without breaking it into smaller chunks\n- Run loop iterations one after the other, not in parallel\n  - Adjust tile factor automatically\n- Disable automatic loop pipelining\n  - Set tile factor to 20 automatically\n  - Adjust parallel factor automatically\n- Disable automatic loop pipelining\n  - Keep the loop in its original form without breaking it into smaller chunks\n  - Run loop iterations one after the other, not in parallel\n  - Set tile factor to 1 automatically\n  - Adjust parallel factor automatically\n- Adjust pipeline, tile factor, and parallel factor automatically\"\n\nThis natural language instruction is for generating the provided HLS codes in C using specific #pragma values for hardware acceleration and loop optimizations.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code using HLS pragmas, you can apply the following comprehensive but concise natural language instructions:\n\n1. Ensure the loop is not automatically pipelined: \n   Use `#pragma ACCEL PIPELINE \"off\"`\n\n2. Process the loop using the original full form without breaking it into smaller chunks: \n   Utilize `#pragma ACCEL TILE FACTOR=1`\n\n3. Run the iterations of the loop sequentially, not in parallel: \n   Implement `#pragma ACCEL PARALLEL FACTOR=1`\n\n4. Treat the function as a hardware kernel for acceleration: \n   Use `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks, with a tile factor of 8.\n4. Don't automatically pipeline this loop.\n5. Process this loop in its original, full form without breaking it into smaller chunks, with a tile factor of 1.\n6. Don't automatically pipeline this loop.\n7. Process this loop in its original, full form without breaking it into smaller chunks, with a tile factor of 1.\n\nUsing these instructions, you can generate the HLS codes written in C with the respective #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{off}\n      \n#pragma ACCEL TILE FACTOR=auto{1}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive natural language instruction for the pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Break the loop into smaller chunks to process it in its original, full form.\n4. Run the iterations of this loop one after the other, not in parallel.\n5. Process this loop in its original, full form without breaking it into smaller chunks.\n6. Treat this loop as a single iteration, not in parallel.\n7. Center and reduce the column vectors in this loop.\n8. Calculate the correlation matrix taking advantage of parallelism and pipelining for optimization.\n\nThese instructions correspond to the specific #pragma values used in the provided HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks. Automatically pipeline the loops for optimization. Run the iterations of the loops one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply hardware acceleration on the function 'kernel_atax' and ensure the loop is not automatically pipelined, use a tile factor of 1 and run the iterations of the outer loop and inner loops in parallel with a factor of 1. Reduce the 'tmp' and 'y' arrays using parallelization with a factor of 1 for each reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and for the loop iterations, don't automatically pipeline this loop, process in its original form without breaking it into smaller chunks, and run the iterations one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for kernel_correlation function with specific pragma values:\n- Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{off}`\n- Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n- Center and reduce the column vectors: `#pragma ACCEL PIPELINE auto{}; #pragma ACCEL TILE FACTOR=auto{1}; #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n- Calculate the m * m correlation matrix: `#pragma ACCEL PIPELINE auto{__PIPE__L3}; #pragma ACCEL TILE FACTOR=auto{__TILE__L3}; #pragma ACCEL PARALLEL FACTOR=auto{4}`\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration and pipeline the outermost loop with auto scheduling. Tile the outermost loop with a factor of 1, then pipeline the middle loop with off and tile it with a factor of 1. Lastly, pipeline the innermost loop with auto scheduling and tile it with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{}\n      \n#pragma ACCEL TILE FACTOR=auto{8}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, use manual pipelining, tile factor of 1, and parallel factor of 1 for the loops and their reductions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the following natural language instructions for using #pragma values:\n\n1. Mark the function as a hardware kernel for acceleration: \n   Use `#pragma ACCEL kernel` before the function declaration.\n\n2. Don't automatically pipeline the following loops: \n   Use `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipelining.\n\n3. Process the loops with their original size and form, without breaking them into smaller chunks: \n   Use `#pragma ACCEL TILE FACTOR=auto{1}` to set the tiling factor to 1.\n\n4. Run the iterations of the following loops one after the other, not in parallel: \n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}` to set the parallel factors accordingly.\n\n5. Customize pipelining specific loops based on their identification tags:\n   Use `#pragma ACCEL PIPELINE auto{__PIPE__L3}` and `#pragma ACCEL PIPELINE auto{__PIPE__L7}` to set the pipeline factors for specific identified loops.\n\n6. Set the tiling factor for specific loops accordingly with the identification tags: \n   Use `#pragma ACCEL TILE FACTOR=auto{2}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}` to set the tiling factors for specific identified loops.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you should:\n\n1. Mark the function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Automatically pipeline the loop.\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\n3. Break the loop into smaller chunks of 25 elements for optimization.\n```c\n#pragma ACCEL TILE FACTOR=25\n```\n\n4. Run iterations of the loop in parallel with the given parallel factor (__PARA__L0).\n```c\n#pragma ACCEL PARALLEL FACTOR=__PARA__L0\n```\n\n5. Turn off automatic pipelining for the loop.\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n6. Process the loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n7. Run the iterations of the loop in parallel with the given parallel factor (__PARA__L1).\n```c\n#pragma ACCEL PARALLEL FACTOR=__PARA__L1\n```\n\n8. Automatically pipeline the loop.\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\n9. Process the loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n10. Run the iterations of the loop in parallel with the given parallel factor (__PARA__L2).\n```c\n#pragma ACCEL PARALLEL FACTOR=__PARA__L2\n```\n\n11. Pipelining the loop with a specific pipeline factor (__PIPE__L3).\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n```\n\n12. Break the loop into smaller chunks for optimization with the specified tile factor (__TILE__L3).\n```c\n#pragma ACCEL TILE FACTOR=__TILE__L3\n```\n\n13. Run iterations of the loop in parallel with the given parallel factor (30).\n```c\n#pragma ACCEL PARALLEL FACTOR=30\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive natural language instruction for each corresponding #pragma value:\n\n1. To automatically pipeline the loop, use \"#pragma ACCEL PIPELINE auto{}\".\n\n2. To process the loop in its original, full form without breaking it into smaller chunks, use \"#pragma ACCEL TILE FACTOR=auto{1}\".\n\n3. To run the iterations of the loop one after the other, not in parallel, use \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\".\n\n4. To turn off automatic pipelining for the loop, use \"#pragma ACCEL PIPELINE auto{off}\".\n\n5. To define the tile factor of the loop as auto{20}, use \"#pragma ACCEL TILE FACTOR=auto{20}\".\n\n6. To parallelize the loop with factor auto{__PARA__L1}, use \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\".\n\n7. To set the pipeline of the loop to auto{__PIPE__L3}, use \"#pragma ACCEL PIPELINE auto{__PIPE__L3}\".\n\n8. To define the tile factor of the loop as auto{__TILE__L3}, use \"#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\".\n\n9. To parallelize the loop with factor auto{30}, use \"#pragma ACCEL PARALLEL FACTOR=auto{30}\".", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the above HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Process each loop in its original, full form without breaking it into smaller chunks.\n\n3. Run the iterations of the following loop in parallel.\n\n4. Don't automatically pipeline the following loop.\n\n5. Process this loop with automatic tiling enabled with a factor of 2.\n\n6. Process each loop sequentially, not in parallel.\n\n7. Don't automatically pipeline the following loop.\n\n8. Process this loop without any tiling.\n\n9. Run the iterations of the following loop in parallel.\n\n10. Don't automatically pipeline the following loop.\n\n11. Process this loop without any tiling.\n\n12. Run the iterations of the following loop in parallel.\n\n13. Automatically pipeline the following loop.\n\n14. Process this loop with automatic tiling enabled.\n\n15. Run the iterations of the following loop in parallel.\n\n16. Automatically pipeline the following loop with a factor of 30.\n\nNote: \"__PARA__L0\", \"__PARA__L1\", \"__PARA__L2\", \"__PARA__L4\", \"__PARA__L5\", \"__PARA__L6\", \"__PARA__L7\", and \"__PARA__L7_0\" are placeholders for any specific parallel factors indicated by the HLS tool.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function 'kernel_atax' as a hardware kernel for acceleration. Pipeline the first loop, tile the second loop with a factor of 1, and run the iterations of the second nested loop one after the other in parallel with a factor of 1. Also, perform reduction operations for the 'tmp' array and 'y' array in the respective nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use hardware acceleration for the function, turn off automatic loop pipelining, tile the loop by a factor of 2, and run the outer loop iterations in parallel with a parallelization factor of 1. Additionally, parallelize the inner loops with reduction for tmp and y variables each with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks.\nRun the iterations of this loop one after the other, not in parallel.\nTreat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline the first loop. Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of the loop one after the other, not in parallel. Ensure the inner loops are automatically parallelized with a reduction clause for both 'tmp' and 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply a hardware acceleration kernel to the given function. Then, for each of the three primary loops, disable automatic pipelining, set the tiling factor to 2, and set the parallelization factor to 1. Additionally, for the final loop, set the tiling factor to 1, and set the parallelization factor to 30, dividing the work across 30 parallel instances. Remember to adjust the auto values based on your specific hardware and optimization requirements.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the following C code by using the following pragma directives:\n\n1. Treat the function 'kernel_atax' as a hardware kernel for acceleration.\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline the first loop.\n   ```c\n   #pragma ACCEL PIPELINE off\n   ```\n\n3. Process the first loop in its original, full form without breaking it into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. Run the iterations of the second nested loop one after the other, not in parallel.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n5. Run the iterations of the third nested loop one after the other, not in parallel, and perform reduction for the variable 'tmp'.\n   ```c\n   #pragma ACCEL PARALLEL reduction=tmp FACTOR=1\n   ```\n\n6. Run the iterations of the fourth nested loop one after the other, not in parallel, and perform reduction for the variable 'y'.\n   ```c\n   #pragma ACCEL PARALLEL reduction=y FACTOR=1\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process and optimize the 'atax' function as a hardware accelerator kernel by using the following pragmas in C:\n- Begin by disabling automatic pipelining for the loop\n- Apply tiling with a factor of 1 to maintain the loop in its original, full form\n- Make sure the iterations of the loop run one after the other, not in parallel, using a parallel factor of 1", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: \n   - Description: Specify that the function should be mapped to hardware for acceleration.\n   - Instruction: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop:\n   - Description: Disable automatic pipelining for the loop to prevent the compiler from automatically breaking it into smaller chunks.\n   - Instruction: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   - Description: Keep the loop in its original, full form without dividing it into smaller chunks for better performance.\n   - Instruction: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   - Description: Specify that the iterations of the loop should be executed sequentially rather than in parallel.\n   - Instruction: `#pragma ACCEL PARALLEL FACTOR=1`\n\nNote: The \"__PARA__L0\", \"__PARA__L4\", \"__PARA__L1\", \"__PARA__L5\", \"__PARA__L2\", \"__PARA__L6\", \"__PARA__L7_0\" are placeholders in the original code, and the specific values or expressions to be used should be determined based on the context and optimization goals.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow this concise natural language instruction:\n\n1. Mark the following function as a hardware kernel for acceleration using the pragma: ```#pragma ACCEL kernel```\n2. Disable automatic loop pipelining using the pragma: ```#pragma ACCEL PIPELINE \"off\"```\n3. Keep the loop in its original, full form without breaking it into smaller chunks using the pragma: ```#pragma ACCEL TILE FACTOR=1```\n4. Run the iterations of the loop one after the other, not in parallel, using the pragma: ```#pragma ACCEL PARALLEL FACTOR=1```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the corresponding #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration. This should be applied to the function\n\n2. Don't automatically pipeline the first loop.\n\n3. Process the first loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of the second loop one after the other, not in parallel, with a parallel reduction of variable 'tmp'.\n\n5. Run the iterations of the third loop one after the other, not in parallel, with a parallel reduction of variable 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration. Pipeline the loops with an off setting. Set the tile factor to 4 for parallel processing. For the subsequent loops, ensure pipeline is set to its default setting and the tile factor to 1. Finally, parallelize the loops with a factor of 30.\"\n\nThis single instruction encompasses the necessary #pragma values and their specific settings to generate the given HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, turn off automatic loop pipelining, set the tiling factor to 116, and run the outer loop iterations in parallel with a factor of 1. Inside the outer loop, parallelize the inner loops for the reductions of 'tmp' and 'y' with factors of 1_0 and 1_1 respectively.\"\n\nHope that helps!", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the C code provided, use the following concise natural language instruction to generate the corresponding HLS codes:\n\n- Use the function as a hardware kernel for acceleration.\n- Turn off automatic pipelining for this loop.\n- Factor the loop into its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.\n- Implement parallel reduction for the 'tmp' and 'y' variables with a factor of 1 for each.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is treated as a hardware kernel for acceleration.\nProcess the first loop in its original, full form without breaking it into smaller chunks and run the iterations of this loop one after the other, not in parallel.\nDon't automatically pipeline the loops and adjust the tile factor automatically to optimize performance.\nRun the iterations of the inner loops in parallel, but with reduction operations for variables 'tmp' and 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To provide a comprehensive but concise natural language instruction for the given HLS codes:\n\n\"Optimize this code for acceleration by treating the 'kernel_correlation' function as a hardware kernel, disabling automatic pipeline, tiling the loops with a factor of 1, and running parallel iterations with a factor determined by the compiler. Apply specific pipeline, tiling, and parallelization factors as well as specific loop setup to optimize the hardware implementation.\"\n\nThis instruction encompasses the use of the relevant HLS #pragma values in the provided C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, you may use the following natural language instruction:\n\nTreat the following function as a hardware kernel for acceleration. For the loop that calculates the mean and standard deviation, turn off automatic loop pipelining. Use a tile factor of 1 for this loop and run the iterations one after the other, not in parallel. For the loop that centers and reduces the column vectors, turn off automatic loop pipelining, use a tile factor of 4, and run the iterations in parallel with a parallel factor of 2. For the loop that calculates the correlation matrix, use a pipeline factor of 3, a tile factor of 3, and run the iterations in parallel with a parallel factor of 30.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes:\n- For the loop over variable `j`, automatically pipeline the loop with pragma `#pragma ACCEL PIPELINE auto{}`\n- For the loop over variable `j`, use a tile factor of 1 with pragma `#pragma ACCEL TILE FACTOR=auto{1}`\n- For the loop over variable `j`, process the iterations in sequence, not in parallel, with pragma `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n- Stop automatic loop pipelining for the loop over variable `j` with pragma `#pragma ACCEL PIPELINE auto{off}`\n- For the loop over variable `j`, use a tile factor of 2 with pragma `#pragma ACCEL TILE FACTOR=auto{2}`\n- For the loop over variable `j`, process the iterations in sequence, not in parallel, with pragma `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n- Stop automatic loop pipelining for the loop over variable `j` with pragma `#pragma ACCEL PIPELINE auto{off}`\n- For the loop over variable `i`, use a tile factor of 1 with pragma `#pragma ACCEL TILE FACTOR=auto{1}`\n- For the loop over variable `i`, process the iterations in sequence, not in parallel, with pragma `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n- Pipeline the loop over variable `i` with pragma `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n- Use a tile factor for the loop over variable `i` with pragma `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n- Process the iterations of the loop over `i` in parallel, with a parallelization factor of 30, with pragma `#pragma ACCEL PARALLEL FACTOR=auto{30}`\n- Pipeline the inner loop over variable `j` with pragma `#pragma ACCEL PIPELINE auto{__PIPE__L7}`\n- Process the iterations of the inner loop over `j` in parallel, with a parallelization factor of `__PARA__L7_0`, with pragma `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following natural language instruction:\n- Treat the function `kernel_correlation` as a hardware kernel for acceleration\n- Don\u2019t automatically pipeline the loops\n- Process the loops in their original, full form without breaking into smaller chunks\n- Run the iterations of the loops in parallel with a factor of 1\n- Automatically pipeline the loops\n- Break the loops into smaller chunks with a factor of 8\n- Run the iterations of the last loop in parallel with a factor of 30", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the given C function, use the following comprehensive but concise natural language instruction for each specific #pragma value:\n\n1. \"Treat the following function as a hardware kernel for acceleration\" - Use `#pragma ACCEL kernel`\n2. \"Don't automatically pipeline this loop\" - Use `#pragma ACCEL PIPELINE \"off\"`\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\" - Use `#pragma ACCEL TILE FACTOR=1`\n4. \"Run the iterations of this loop one after the other, not in parallel\" - Use `#pragma ACCEL PARALLEL FACTOR=1`\n\nYou can use these natural language instructions to guide the generation of the corresponding HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks, and treat the following function as a hardware kernel for acceleration. \n\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Convert this loop into hardware accelerator and apply specified optimizations: turn off automatic pipelining, factor the loop tile automatically, and run iterations in parallel with specified factors for reduction operations'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Declare the following function as a hardware kernel for acceleration.\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Automatically pipeline the loop to improve throughput.\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   ```\n\n3. Break the loop into smaller chunks to improve parallelism with automatic tiling factor.\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{116}\n   ```\n\n4. Execute the loop iterations one after the other, not in parallel, with automatic parallel factor.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{1}\n   ```\n\n5. Perform reduction operation on the variable 'tmp' in parallel with automatic factor.\n   ```c\n   #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n   ```\n\n6. Perform reduction operation on the variable 'y' in parallel with automatic factor.\n   ```c\n   #pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Enable manual pipeline for this loop' for '#pragma ACCEL PIPELINE auto{off}'.\n'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=1'.\n'Run the iterations of this loop one after the other, not in parallel, with parallel factor being dynamic' for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'.\n'Enable automatic pipeline for this loop' for '#pragma ACCEL PIPELINE auto{}'.\n'Utilize automatic tiling with factor 4 for this loop' for '#pragma ACCEL TILE FACTOR=auto{4}'.\n'Run the iterations of this loop in parallel, with parallel factor being dynamic' for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'.\n'Enable manual pipeline for this loop' for '#pragma ACCEL PIPELINE auto{off}'.\n'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=1'.\n'Run the iterations of this loop in parallel, with parallel factor being dynamic' for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'.\n'Enable manual pipeline with specific pipeline factor for this loop' for '#pragma ACCEL PIPELINE auto{__PIPE__L3}'.\n'Utilize automatic tiling with specific tiling factor for this loop' for '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'.\n'Run the iterations of this loop in parallel, with specific parallel factor' for '#pragma ACCEL PARALLEL FACTOR=auto{30}'.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nFor the loop with index 'j', don't automatically pipeline it.\nProcess the loop in its original, full form without breaking it into smaller chunks.\nRun the iterations of the loop one after the other, not in parallel, and for iterations of loop index 'i', also not in parallel.\nRe-pipeline the loop automatically.\nBreak the loop into smaller chunks with a factor of 4.\nRun the iterations of the loop in parallel, broken into chunks of size 5.\nRe-pipeline the loop automatically.\nBreak the loop into smaller chunks with a factor of 1.\nRun the iterations of the loop in parallel, broken into chunks of size 6.\nRe-pipeline the loop with a factor of 3.\nBreak the loop into smaller chunks with a factor of 3.\nRun the iterations of the loop in parallel, broken into chunks of size 30.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Use a tile factor of 1 and run iterations of the second loop one after the other without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in its original, full form without breaking it into smaller chunks.\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Treat the following loop as a hardware kernel for acceleration.\n\n6. Don't automatically pipeline this loop.\n\n7. Process each loop in its original, full form without breaking it into smaller chunks.\n\n8. Run the iterations of this loop one after the other, not in parallel.\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n\n10. Run the iterations of this loop one after the other, not in parallel.\n\n11. Don't automatically pipeline this loop.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C:\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n    int i;\n    int j;\n    int k;\n    double eps = 0.1;\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{4}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\nfor (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n        mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n}\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\nfor (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n        stddev[j] += pow(data[i][j] - mean[j], (double)2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    /* The following in an inelegant but usual way to handle\n            near-zero std. dev. values, which below would cause a zero-\n            divide. */\n    stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n}\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\nfor (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n        data[i][j] -= mean[j];\n        data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n}\n\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{30}\nfor (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n        corr[i][j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n        for (k = 0; k < 100; k++) {\n            corr[i][j] += data[k][i] * data[k][j];\n        }\n        corr[j][i] = corr[i][j];\n    }\n}\ncorr[80 - 1][80 - 1] = 1.0;\n}\n```\nThe comprehensive natural language instruction to generate these HLS codes would be:\n\n\"Generate hardware accelerated code for the function 'kernel_correlation'. Disable automatic loop pipelining for the loops. Break the loops into smaller chunks with a factor of 4 for tiling. Run the iterations of the loops in parallel with a factor of 6. Disable automatic loop pipelining for the next set of loops. Process the loops without breaking them into smaller chunks. Run the iterations of the loops in parallel with a factor of 1. Center and reduce the column vectors. Enable loop pipelining for the following loops. Break the loops into smaller chunks with an auto-calculated tiling factor. Run the iterations of the loops in parallel with a factor of 30.\"\n\nThis natural language instruction provides a detailed description of how to generate the HLS codes in C and which specific #pragma values to use for hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following optimizations: \n- Don't automatically pipeline the loops\n- Break the loops into smaller chunks with a factor of 4\n- Run the inner loop iterations one after the other, not in parallel with a factor of 1\n- Run the first inner loop parallel with reduction of 'tmp' and a factor of 1 and the second inner loop parallel with reduction of 'y' and a factor of 1", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loop. Tile the loop with a factor of 2. Run the iterations of the loop in parallel with a factor of 1. Use parallel reduction for the inner loop with a factor of 1 for the variable 'tmp'. Use parallel reduction for the inner loop with a factor of 1 for the variable 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code from C using the following concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops.\n3. Process the loops with an auto tile factor of 4.\n4. Run the iterations of the loops one after the other, not in parallel, with a reduction factor of 1 for the \"tmp\" array.\n5. Run the iterations of the loops one after the other, not in parallel, with a reduction factor of 1 for the \"y\" array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\nUse '#pragma ACCEL kernel' to mark the following function as a hardware kernel for acceleration.\nThen, use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loop.\nAfter that, use '#pragma ACCEL TILE FACTOR=auto{8}' to automatically break the loop into smaller chunks with a factor of 8.\nFinally, use '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run the loop iterations in parallel with a factor of 1.\n\nThis set of instructions will guide the generation of the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes, use the following concise natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Then, turn off automatic loop pipelining, set loop tiling factor to 1, and perform loop parallelization with a factor equal to the specified value for each loop.\"\n\nThis instruction corresponds to the following #pragma values:\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j], (double)2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    /* The following is an inelegant but usual way to handle near-zero std. dev. values, which below would cause a zero-divide. */\n    stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n  }\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{8}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Use hardware acceleration for the following function and apply pipeline, tiling, and parallelization optimizations with specific auto values for each pragma.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Concurrently process the outer loop iterations in parallel with a reduction on variable tmp and y, then pipeline the loops with optimization of pipeline factor of 8 and parallel factor of 1. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n- 'Treat the following function as a hardware kernel for acceleration'\n- 'Don't automatically pipeline this loop'\n- 'Process this loop in its original, full form without breaking it into smaller chunks'\n- 'Run the iterations of this loop one after the other, not in parallel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, you should use the following natural language instruction for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: \n   Use '#pragma ACCEL kernel' to specify the function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop: \n   Use '#pragma ACCEL PIPELINE auto{off}' to disable automatic pipelining for a specific loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks: \n   Use '#pragma ACCEL TILE FACTOR=1' to specify original full-form processing for a loop without tiling.\n\n4. Run the iterations of this loop one after the other, not in parallel: \n   Use '#pragma ACCEL PARALLEL FACTOR=1' to specify running loop iterations sequentially, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with HLS pragmas, use the instructions:\n\"Treat the following function as a hardware kernel for acceleration and disable automatic loop pipelining. Tile the outer loop by a factor of 116. Run the outer loop iterations in parallel and reduce the inner loops with factors of 1 for tmp and y arrays.\"\n\nThis corresponds to the following HLS pragma values:\n1. `#pragma ACCEL kernel`\n2. `#pragma ACCEL PIPELINE auto{off}`\n3. `#pragma ACCEL TILE FACTOR=116`\n4. `#pragma ACCEL PARALLEL FACTOR=1`\n5. `#pragma ACCEL PARALLEL reduction=tmp FACTOR=1_0`\n6. `#pragma ACCEL PARALLEL reduction=y FACTOR=1_1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following natural language instruction:\n\n1. 'Treat the following function as a hardware kernel for acceleration' to describe '#pragma ACCEL kernel'.\n2. 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE auto{off}'.\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{1}'.\n4. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'.\n5. 'No specific instruction to be added for pipeline in this context'.\n6. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{1}'.\n7. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'.\n8. 'No specific instruction to be added for pipeline in this context'.\n9. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{30}'.\n10. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'.\n11. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'.\n12. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{30}'.\n\nApply these instructions accordingly to generate the HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Keep the loop from i=0 to 124 running sequentially without automatic pipelining. Use a tile factor of 1 and run the iterations of the loop one after the other, not in parallel. Use parallelization with a reduction operation to calculate the 'tmp' array and another reduction to update the 'y' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline this loop.\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n```\n\n5. Don't automatically pipeline this loop.\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n7. Run the iterations of this loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n```\n\n8. Don't automatically pipeline this loop.\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=auto{2}\n```\n\n10. Run the iterations of this loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n```\n\n11. Automatically pipeline this loop with a factor of 3.\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n```\n\n12. Process this loop in smaller chunks using a factor of 3.\n```c\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n```\n\n13. Run the iterations of this loop in parallel with a factor of 30.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{30}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks. Run the iterations of the loops in parallel. Treat the following function as a hardware kernel for acceleration. Automatically pipeline the loops. Disable automatic pipeline for this loop. Use a tile factor of 1 for these loops. Set the tile factor to 4 for this loop. Do not automatically pipeline this loop, but set a custom pipeline value.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and then automatically pipeline the loop, use a tile factor of 1 and run the iterations of the loop in parallel. Also, apply automatic parallelization with a reduction factor of 1 for 'tmp' and 'y' respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j], (double)2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n  }\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nYou can use the following natural language instruction:\n\n\"Apply hardware acceleration to the function `kernel_correlation`. Pipeline the loops and tile factor with automatic calculation, and parallelize the loops with the corresponding factors provided in the pragma values.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the corresponding #pragma values:\n\n1. For the function `kernel_correlation`, treat it as a hardware kernel for acceleration: \n   - Use: `#pragma ACCEL kernel`\n\n2. For the loop processing with original, full form without breaking it into smaller chunks:\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n3. For the loop iterations to run one after the other, not in parallel:\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\n4. For not automatically pipelining a loop:\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n\n5. For automatic pipelining of a loop:\n   - Use: `#pragma ACCEL PIPELINE auto`\n\n6. For flattening a loop for pipelining:\n   - Use: `#pragma ACCEL PIPELINE auto{flatten}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Break the second loop into smaller chunks with a factor of 116. Run the iterations of the second loop one after the other, not in parallel, with a reduction variable 'tmp' and a factor of 1.0. Also, run the iterations of the second inner loop one after the other, not in parallel, with a reduction variable 'y' and a factor of 1.1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_correlation' function as a hardware kernel for acceleration.\nMake sure to automatically pipeline all loops.\nEnsure that the loops are processed in their original, full form without breaking them into smaller chunks.\n\nFor the first loop, run its iterations one after the other, not in parallel.\nFor the second loop, pipeline it manually to prevent automatic pipelining.\nFor the third loop, break it into smaller chunks with a tile factor of 30, and run the iterations in parallel.\nFinally, for the last loop, manually set the pipeline and tile factors and run the iterations in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, automatically pipeline the first loop, and apply tiling, but do not parallelize the outer loop. Apply parallel reduction for the inner loops on the temporary array and the output array 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, then don't automatically pipeline the first loop, apply tiling with factor=2, and run the iterations of the second loop one after the other, not in parallel.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the second loop with a tile factor of 4 to break it into smaller chunks, and run the iterations of both inner loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions for the specific #pragma values:\n\n1. For the first loop, set the directive to 'Don't automatically pipeline this loop'.\n\n2. Then for the loop over 'i', set the directive to 'Process this loop in its original, full form without breaking it into smaller chunks'. \n\n3. Next, for the same loop over 'i', set the directive to 'Run the iterations of this loop one after the other, not in parallel'. \n\nOverall, treat the given function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration' describes '#pragma ACCEL kernel'\n\n2. 'Automatically pipeline this loop' describes '#pragma ACCEL PIPELINE auto{}'\n\n3. 'Break this loop into smaller chunks and process it in its original, full form' describes '#pragma ACCEL TILE FACTOR=1'\n\n4. 'Run the iterations of this loop one after the other, not in parallel' describes '#pragma ACCEL PARALLEL FACTOR=1'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you can use the following natural language instructions for the specific #pragma values:\n\n1. \"Treat the following function as a hardware kernel for acceleration.\"\n   Use: `#pragma ACCEL kernel`\n\n2. \"Automatically pipeline this loop.\"\n   Use: `#pragma ACCEL PIPELINE auto{}`\n\n3. \"Break this loop into smaller chunks using a tile factor of 4.\"\n   Use: `#pragma ACCEL TILE FACTOR=auto{4}`\n\n4. \"Run the iterations of this loop one after the other, not in parallel.\"\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n5. \"Turn off automatic pipelining for this loop.\"\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n6. \"Process this loop in its original, full form without breaking it into smaller chunks.\"\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n7. \"Run the iterations of this loop one after the other, not in parallel again.\"\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n8. \"Turn off automatic pipelining for the following code block.\"\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n9. \"Process this loop in its original, full form without breaking it into smaller chunks again.\"\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n10. \"Run the iterations of this loop one after the other, not in parallel once more.\"\n    Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n11. \"Automatically pipeline this loop with a value for pipeline factor.\"\n    Use: `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n12. \"Break this loop into smaller chunks using a specific tile factor.\"\n    Use: `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n13. \"Run a specific number of iterations of this loop in parallel.\"\n    Use: `#pragma ACCEL PARALLEL FACTOR=auto{30}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C codes, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop manually.\n3. Process this loop in its original, full form without breaking it into smaller chunks automatically.\n4. Run the iterations of this loop one after the other, not in parallel automatically \n5. Automatically pipeline this loop\n6. Process this loop in its original, full form without breaking it into smaller chunks manually \n7. Run the iterations of this loop one after the other, not in parallel manually \n8. Automatically pipeline this loop with an off state\n9. Process this loop in its original, full form without breaking it into smaller chunks manually \n10. Run the iterations of this loop one after the other, not in parallel automatically \n11. Automatically pipeline this loop using the flatten option \n12. Process this loop in its original, full form without breaking it into smaller chunks manually \n13. Run the iterations of this loop one after the other, not in parallel automatically \n14. Automatically pipeline this loop with the specified custom pipeline label\n15. Process this loop in its original, full form without breaking it into smaller chunks with the specified custom tile factor\n16. Run the iterations of this loop one after the other, not in parallel automatically.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the following function as a hardware kernel and apply automatic pipeline, tiling with a factor of 4, and parallelization with a reduction on tmp and y variables, using auto factors for optimization.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following concise natural language instructions for specific #pragma values:\n1. To prevent automatic pipelining for the loop, use `#pragma ACCEL PIPELINE auto{off}`\n2. For processing the loop in its original, full form without breaking it into smaller chunks, use `#pragma ACCEL TILE FACTOR=auto{1}`\n3. To run the iterations of the loop one after the other, not in parallel, use `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n4. To treat the following function as a hardware kernel for acceleration, simply add `#pragma ACCEL kernel` before the function definition.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in its original, full form without breaking it into smaller chunks. Don't automatically pipeline this loop. Run the iterations of this loop one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use hardware acceleration to compute the correlation matrix and optimize the loops:\n- Use hardware kernel acceleration: `#pragma ACCEL kernel`\n- Automatically pipeline the loops: `#pragma ACCEL PIPELINE auto`\n- Tile the loops for optimization: `#pragma ACCEL TILE FACTOR=auto`\n- Parallelize the loops for concurrency: `#pragma ACCEL PARALLEL FACTOR=auto`\n- Turn off automatic pipelining for specific loop: `#pragma ACCEL PIPELINE auto{off}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the code, consider the following concise natural language instruction for each #pragma value:\n\n1. To disable automatic pipelining for the loop: \"Turn off automatic loop pipelining\"\n2. To set the tiling factor to 4: \"Use tiling with a factor of 4 for loop optimization\"\n3. To set the parallelization factor to 1: \"Parallelize the loop with a factor of 1 to run iterations sequentially\"\n4. To specify the first reduction variable: \"Perform reduction for the variable 'tmp' with a factor of 1\"\n5. To specify the second reduction variable: \"Perform reduction for the variable 'y' with a factor of 1\"\n\nThese concise instructions provide a natural language interpretation of each #pragma value used in the given HLS C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To enable hardware acceleration for the given C code, use the following pragma values:\n1. Use `#pragma ACCEL kernel` to treat the `kernel_correlation` function as a hardware kernel for acceleration.\n2. For the first loop, use `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipelining.\n3. For the second loop, use `#pragma ACCEL TILE FACTOR=2` to set the tile factor to 2 for breaking it into smaller chunks.\n4. For the third loop, use `#pragma ACCEL PARALLEL FACTOR=1` to run the iterations of this loop one after the other, not in parallel. Also, change `auto{__PARA__L0}` to `1` for the parallel factor value.\n5. For the fourth loop, use `#pragma ACCEL PIPELINE auto{flatten}` to flatten pipelining.\n6. For the fifth loop, use `#pragma ACCEL TILE FACTOR=1` to maintain the original form without breaking it into smaller chunks.\n7. For the sixth loop, use `#pragma ACCEL PIPELINE auto{__PIPE__L3}` to enable automatic pipelining for that specific pipeline level. Also, `auto{__TILE__L3}` should be changed to `1` for the tile factor value.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and apply the following optimizations:\n- Do not automatically pipeline the loops.\n- Process the loops in their original full form without breaking them into smaller chunks.\n- Run the iterations of the loops one after the other, not in parallel.\"\n\nThis comprehensive but concise instruction encompasses the specific #pragma values to use in natural language for the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and apply the following pragma values:\n1. Don't automatically pipeline the loop.\n2. Process the loop with its original, full form without breaking it into smaller chunks.\n3. Run the iterations of the loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following natural language instructions:\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.\n- Automatically pipeline this loop.\n- Process this loop as a single tile, not breaking it into smaller chunks.\n- Run the iterations of this loop in parallel with the specified factor.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire 'kernel_correlation' function as a hardware kernel for acceleration and apply automatic pipelining to all loops and parallelism with a factor of '__PARA__L0' for the first loop, '__PARA__L4' for the second loop, '__PARA__L1' for the third loop, '__PARA__L5' for the fourth loop, '__PARA__L2' for the fifth loop, '__PARA__L6' for the sixth loop, '__PARA__L7_0' for the innermost loop. Additionally, use a tile factor of 1 for the loops and 8 for the fifth loop, and specific pipeline off for the third and sixth loops.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following loop is not automatically pipelined: '#pragma ACCEL PIPELINE auto{off}'\n\nProcess this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{1}'\n\nRun the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n\nEnsure the following loop is not automatically pipelined: '#pragma ACCEL PIPELINE auto{}'\n\nProcess this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{1}'\n\nRun the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'\n\nEnsure the following loop is not automatically pipelined: '#pragma ACCEL PIPELINE auto{}'\n\nProcess this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=auto{4}'\n\nRun the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n\nPerform pipeline with specified factors: '#pragma ACCEL PIPELINE auto{__PIPE__L3}'\n\nProcess this loop in smaller chunks based on specified factor: '#pragma ACCEL TILE FACTOR = auto{__TILE__L3}'\n\nRun the iterations of this loop in parallel with the specified parallel factor: '#pragma ACCEL PARALLEL FACTOR=auto{30}'\n", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following comprehensive but concise natural language instruction for the specified #pragma values:\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n\n#pragma ACCEL PIPELINE off\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=1\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=1\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n\n#pragma ACCEL PIPELINE off\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=1\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=1\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j], (double)2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    // The following in an inelegant but usual way to handle near-zero std. dev. values, which below would cause a zero-divide.\n    stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n  }\n\n#pragma ACCEL PIPELINE\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=1\n  for (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=1\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n\n#pragma ACCEL PIPELINE\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=30\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n\n#pragma ACCEL PIPELINE\n\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=1\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nThe corresponding natural language instructions for the specific #pragma values are as follows:\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Automatically pipeline the loop without any optimization. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Automatically pipeline the loop without any optimization. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop in parallel with a factor of 30.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, you need to process the code and use specific #pragma values as follows:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use #pragma ACCEL kernel\n\n2. Don't automatically pipeline this loop.\n   - Replace #pragma ACCEL PIPELINE auto{} with #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Replace #pragma ACCEL TILE FACTOR=auto{1} with #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Replace #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0} with #pragma ACCEL PARALLEL FACTOR=1\n\n5. Don't automatically pipeline this loop and flatten nested loops.\n   - Replace #pragma ACCEL PIPELINE auto{flatten} with #pragma ACCEL PIPELINE \"off\" and #pragma ACCEL TILE FACTOR=auto{1} with #pragma ACCEL TILE FACTOR=1\n\n6. Flatten the pipeline for the loop.\n   - Replace #pragma ACCEL PIPELINE auto{__PIPE__L3} with #pragma ACCEL PIPELINE \"flatten\"\n\n7. Tile this loop with a specific factor.\n   - Replace #pragma ACCEL TILE FACTOR=auto{__TILE__L3} with #pragma ACCEL TILE FACTOR=3\n\n8. Run this parallel loop with a specific parallel factor.\n   - Replace #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0} with #pragma ACCEL PARALLEL FACTOR=1\n\nBy following these instructions, you can adjust the HLS codes in C to achieve the desired behavior using the specific #pragma values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following pragma values and concise natural language instructions:\n\n1. `#pragma ACCEL PIPELINE auto{off}`\n   - Natural Language: \"Don't automatically pipeline this loop\" - This pragma turns off automatic pipelining for the loop.\n\n2. `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Natural Language: \"Process this loop in its original, full form without breaking it into smaller chunks\" - This pragma sets the tiling factor to 1, allowing the loop to be processed in its original form.\n\n3. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n   - Natural Language: \"Run the iterations of this loop one after the other, not in parallel\" - This pragma specifies that the loop iterations should not run in parallel.\n\n4. `#pragma ACCEL PIPELINE auto{}`\n   - Natural Language: \"Process this loop with automatic pipelining\" - This pragma enables automatic pipelining for the loop.\n\n5. `#pragma ACCEL TILE FACTOR=auto{2}`\n   - Natural Language: \"Break this loop into smaller chunks, with a tiling factor of 2\" - This pragma sets the tiling factor to 2, enabling the loop to be broken into smaller chunks for processing.\n\n6. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n   - Natural Language: \"Run the iterations of this loop in parallel\" - This pragma specifies that the loop iterations should run in parallel.\n\n7. `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n   - Natural Language: \"Automatically pipeline this loop with a specific pipeline type\" - This pragma enables automatic pipelining for the loop with a specific pipeline type.\n\n8. `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n   - Natural Language: \"Process this loop with a specific tiling factor\" - This pragma sets a specific tiling factor for processing the loop.\n\n9. `#pragma ACCEL PARALLEL FACTOR=auto{30}`\n   - Natural Language: \"Run the iterations of this loop in parallel with a specific factor\" - This pragma specifies running the loop iterations in parallel with a specific factor.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Treat the following function as a hardware kernel for acceleration: '#pragma ACCEL kernel'\n2. Don't automatically pipeline this loop: '#pragma ACCEL PIPELINE \"off\"'\n3. Process this loop in its original, full form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=1'\n4. Run the iterations of this loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=1'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Process this loop in its original, full form without breaking it into smaller chunks and run the iterations of this loop one after the other, not in parallel, with parallel factor set to the specific value '__PARA__L0' for the outer loop.\n3. Don't automatically pipeline this loop and process this loop in its original, full form without breaking it into smaller chunks with tile factor set to 'auto' for the outer loop.\n4. Run the iterations of this loop one after the other, not in parallel, with parallel factor set to the specific value '__PARA__L4' for the inner loop.\n5. Automatically pipeline this loop with the specific factor 'off'.\n6. Process this loop in its original, full form without breaking it into smaller chunks with tile factor set to the specific value '4' for the outer loop.\n7. Run the iterations of this loop one after the other, not in parallel, with parallel factor set to the specific value '__PARA__L1' for the outer loop and the specific value '__PARA__L5' for the inner loop.\n8. Automatically pipeline this loop with the specific factor 'flatten'.\n9. Process this loop in its original, full form without breaking it into smaller chunks and run the iterations of this loop one after the other, not in parallel, with parallel factor set to the specific value '__PARA__L6' for the inner loop.\n10. Automatically pipeline this loop with the specific factor set to '__PIPE__L3'.\n11. Process this loop in its original, full form without breaking it into smaller chunks with tile factor set to the specific value '__TILE__L3' and run the iterations of this loop one after the other, not in parallel, with parallel factor set to '1' for the outer loop and the specific value '__PARA__L7_0' for the innermost loop.\n\nThis set of instructions comprehensively guides the generation of the provided HLS C codes using the appropriate #pragma values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Break the second loop into 2 smaller chunks. Run the iterations of the second loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the following function as a hardware kernel. Disable automatic pipeline for the first loop. Use a tile factor of 4 for loop tiling. Run the iterations of the outer loop one after the other, and treat the inner loops as parallel with reduction variables for both 'tmp' and 'y'.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To create hardware acceleration code in C, use the following concise natural language instructions to generate the specific #pragma values:\n\n1. For the kernel function, indicate that it should be treated as a hardware kernel for acceleration:\n   Instruction: \"Treat the following function as a hardware kernel for acceleration\"\n   HLS code: #pragma ACCEL kernel\n\n2. For automatically pipelining a loop:\n   Instruction: \"Automatically pipeline this loop\"\n   HLS code: #pragma ACCEL PIPELINE auto{}\n\n3. For tiling a loop with a specific factor:\n   Instruction: \"Tile this loop with a specific factor\"\n   HLS code: #pragma ACCEL TILE FACTOR=auto{2}\n   \n4. For running the iterations of a loop sequentially, not in parallel:\n   Instruction: \"Run the iterations of this loop one after the other, not in parallel\"\n   HLS code: #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n\n5. For turning off automatic pipelining for a loop:\n   Instruction: \"Don't automatically pipeline this loop\"\n   HLS code: #pragma ACCEL PIPELINE auto{off}\n\n6. For tiling a loop with a factor of 1:\n   Instruction: \"Process this loop with a tile factor of 1\"\n   HLS code: #pragma ACCEL TILE FACTOR=auto{1}\n\n7. For running the iterations of a loop in parallel with a specific factor:\n   Instruction: \"Run the iterations of this loop in parallel with a specific factor\"\n   HLS code: #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following instructions to optimize the given C code using HLS pragmas:\n1. Mark the function as a hardware kernel for acceleration.\n2. Pipeline the loop automatically to maximize throughput.\n3. Tile the loop with an automatic factor to enhance data locality and reduce memory access time.\n4. Execute the iterations of the outer loop one after the other, not in parallel, using a factor of 1.\n5. Perform the inner loops in parallel to maximize computation using an auto-calculated factor for both the reduction operations.\n\nYou can use the following HLS pragmas in your code:\n```c\n#pragma ACCEL kernel\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL TILE FACTOR=auto{4}\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function in its original, full form without breaking it into smaller chunks, treat it as a hardware kernel for acceleration, don't automatically pipeline the first loop, tile it with a factor of 116, and run the iterations of the second loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use the following instructions:\n\n1. `#pragma ACCEL PIPELINE auto{off}`: Do not automatically pipeline this loop.\n2. `#pragma ACCEL TILE FACTOR=auto{1}`: Process this loop in its original, full form without breaking it into smaller chunks.\n3. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`: Run the iterations of this loop in parallel with the specified factor.\n\nHope this helps!", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and ensure the following optimizations:\n- Turn off automatic pipelining for the loop\n- Use a tile factor of 1 for the loop\n- Run the iterations of the loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions to apply specific #pragma values:\n1. For the function 'kernel_correlation', treat it as a hardware kernel for acceleration.\n2. Process the loops with j and i indices, for mean and stddev calculations, in their original, full form without breaking them into smaller chunks, by using #pragma ACCEL TILE FACTOR=1.\n3. Run the iterations of the loops one after the other, not in parallel, for mean and stddev calculations using #pragma ACCEL PARALLEL FACTOR=1.\n4. Don't automatically pipeline the loops for mean and stddev calculations by specifying #pragma ACCEL PIPELINE \"off\".\n5. Limit the parallelization to auto{__PARA__L0} for the loop over j, and auto{__PARA__L4} for the loop over i, when calculating mean.\n6. Limit the parallelization to auto{__PARA__L1} for the loop over j, and auto{__PARA__L5} for the loop over i, when calculating stddev.\n7. Flatten the loop for centering and reducing the column vectors, using #pragma ACCEL PIPELINE auto{flatten}.\n8. Limit the parallelization to auto{__PARA__L2} for the loop over i, and auto{__PARA__L6} for the loop over j, when centering and reducing the column vectors.\n9. Pipelining for the loop over i, followed by tiling and parallelization for the inner loops, can be achieved by #pragma ACCEL PIPELINE auto{__PIPE__L3}, followed by #pragma ACCEL TILE FACTOR=auto{__TILE__L3}, and #pragma ACCEL PARALLEL FACTOR=auto{1}, for calculating the m * m correlation matrix.\n10. Lastly, avoid automatic pipelining of the loop for the i indices when calculating the m * m correlation matrix by using #pragma ACCEL PIPELINE auto{off}.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for hardware acceleration, use the following comprehensive but very concise natural language instructions for the specific #pragma values:\n\n1. Enable automatic pipelining for the loops:  \n   Use `#pragma ACCEL PIPELINE auto{}`\n   \n2. Set the tile factor for loop optimization to automatic:  \n   Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Parallelize the loops with a specific factor:  \n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` for the outer loop, and similarly for the other parallelized loops.\n\n4. Disable automatic pipelining for specific loops:  \n   Use `#pragma ACCEL PIPELINE auto{off}` for the specified loops\n\n5. Set the specific tile factor for the loops:  \n   Use `#pragma ACCEL TILE FACTOR=auto{20}` for the specified loop\n\n6. Set specific pipelining for the loops:  \n   Use `#pragma ACCEL PIPELINE auto{__PIPE__L3}` for the specified loop, and similarly for other pipelined loops.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instructions for the specific #pragma values:\n\n1. 'Set pipeline directive to manual mode for loop optimization' for '#pragma ACCEL PIPELINE \"off\"'\n\n2. 'Apply tiling with a factor of 1 for full and original loop processing' for '#pragma ACCEL TILE FACTOR=1'\n\n3. 'Run loop iterations in sequence without parallelization' for '#pragma ACCEL PARALLEL FACTOR=1'\n\n4. 'Declare the following function as a hardware kernel for acceleration' for '#pragma ACCEL kernel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. \nEnsure the loop inside the function is not automatically pipelined. \nDivide the loop into its original, full form without breaking it into smaller chunks.\nExecute the loop iterations one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in its full form without breaking it into smaller chunks, and treat the iterations of this loop separately, not in parallel. Also, do not automatically pipeline this loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following optimizations:\n- Pipeline the loop automatically\n- Break the loop into smaller chunks with a factor of 4\n- Run iterations of the loop in parallel with a factor of 1 for both the 'tmp' and 'y' reductions", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the following C code using the following natural language instructions:\n1. Treat the function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loop.\n3. Process the loop using a tiling factor of 2.\n4. Run the iterations of the loop one after the other, not in parallel. Use a reduction factor of 1 for variable tmp. Use a reduction factor of 1 for variable y.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks, treat the loop as a hardware kernel for acceleration, don't automatically pipeline this loop, run the iterations of the loop one after the other, not in parallel, treat the following function as a hardware kernel for acceleration, don't automatically pipeline this loop, process the loop in its original, full form without breaking it into smaller chunks, run the iterations of this loop one after the other, not in parallel, treat the following function as a hardware kernel for acceleration, don't automatically pipeline this loop, process the loop in its original, full form without breaking it into smaller chunks, run the iterations of this loop one after the other, not in parallel, don't automatically pipeline this loop, process the loop in its original, full form without breaking it into smaller chunks, run the iterations of this loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. For the kernel_correlation function, treat it as a hardware kernel for acceleration by using the directive: '#pragma ACCEL kernel'\n\n2. Process and automatically pipeline the loops in their original, full form without breaking into smaller chunks: '#pragma ACCEL PIPELINE auto{}'\n\n3. Run the iterations of the loops in parallel, accelerating the computation: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}'\n\n4. Treat the loops individually to reduce the resource usage and optimize performance: '#pragma ACCEL TILE FACTOR=auto{1}', '#pragma ACCEL TILE FACTOR=auto{2}', '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'\n\n5. Turn off automatic pipeline for specific loops to control the initiation interval manually: '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL PIPELINE auto{__PIPE__L3}', '#pragma ACCEL PIPELINE auto{__PIPE__L7}'\n\nBy following these instructions, you can efficiently generate the given HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the given HLS codes written in C, you can use the following comprehensive yet concise natural language instructions to generate the necessary #pragma values:\n\n1. Don't automatically pipeline this loop: \n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n2. Process this loop in its original, full form without breaking it into smaller chunks: \n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n3. Run the iterations of this loop one after the other, not in parallel:\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```\n\n4. Treat the following function as a hardware kernel for acceleration:\n```c\n#pragma ACCEL kernel\n```\n\nBy following the above natural language instructions and applying the corresponding #pragma values, you can ensure the accurate processing of the given HLS codes in C for hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, follow these concise natural language instructions for the specific #pragma values:\n\n1. For the kernel function, use the instruction 'Treat the following function as a hardware kernel for acceleration'.\n   Use: `#pragma ACCEL kernel`\n\n2. To turn off automatic pipeline for a loop, use the instruction 'Don't automatically pipeline this loop'.\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n3. For processing a loop in its original, full form without breaking it into smaller chunks, use the instruction 'Process this loop in its original, full form without breaking it into smaller chunks'.\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of a loop one after the other, not in parallel, by using the instruction 'Run the iterations of this loop one after the other, not in parallel'.\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`, and so on, for different loops with different factors.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, you need to use the following specific #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration' -> Use: `#pragma ACCEL kernel`\n\n2. 'Don't automatically pipeline this loop' -> Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' -> Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. 'Run the iterations of this loop one after the other, not in parallel' -> Use: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, you can use the following natural language instructions for the specific #pragma values:\n\n1. For the main loop:\n   - \"Don't automatically pipeline this loop\" -> #pragma ACCEL PIPELINE \"off\"\n   - \"Process this loop in its original, full form without breaking it into smaller chunks\" -> #pragma ACCEL TILE FACTOR=1\n   - \"Run the iterations of this loop one after the other, not in parallel\" -> #pragma ACCEL PARALLEL FACTOR=1\n\n2. For inner loops:\n   - \"Run the iterations of this loop one after the other, not in parallel and perform iterative reduction on 'tmp'\" -> #pragma ACCEL PARALLEL reduction=tmp FACTOR=1\n   - \"Run the iterations of this loop one after the other, not in parallel and perform iterative reduction on 'y'\" -> #pragma ACCEL PARALLEL reduction=y FACTOR=1\n\n3. For the entire function:\n   - \"Treat the following function as a hardware kernel for acceleration\" -> #pragma ACCEL kernel\n\nBy incorporating these natural language instructions into the source code, you will be able to generate the desired HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the kernel function 'atax', automatically pipeline the loop.\n   Use: `#pragma ACCEL PIPELINE auto{}`\n\n2. For the loop iterating over 'i', break it into smaller chunks with a factor determined automatically.\n   Use: `#pragma ACCEL TILE FACTOR=auto{2}`\n\n3. For the loop iterating over 'i', run the iterations one after the other, not in parallel, with a reduction operation on 'tmp'.\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}`, `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}`\n\n4. For the loop iterating over 'j' inside the loop over 'i', also run the iterations one after the other, not in parallel, with a reduction operation on 'y'.\n   Use: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you'd use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration. For the first loop, disable automatic pipelining. Split the second loop into smaller chunks with a tile factor of 116. Process the iterations of the third loop one after the other, not in parallel, with a parallel factor of 1. For the inner loops within the third loop, use parallelism and perform reduction operations for array 'tmp' and 'y' with a parallel factor of 1_0 and 1_1 respectively.\"\n\nThis comprehensive but concise natural language instruction covers the specific #pragma values and their usage to generate the provided HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you may use the following natural language instruction:\n\n\"Define the following function as a hardware kernel for acceleration and then use automatic pipelining, tiling with factor 8, and parallelization with factor 1 for the specified loops in the code.\"\n\nThis natural language instruction captures the pragma values to use in the HLS code and their respective processes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following comprehensive but concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration. (Use '#pragma ACCEL kernel')\n\n2. Don't automatically pipeline this loop. (Use '#pragma ACCEL PIPELINE \"off\"')\n\n3. Process this loop in its original, full form without breaking it into smaller chunks. (Use '#pragma ACCEL TILE FACTOR=1')\n\n4. Run the iterations of this loop one after the other, not in parallel. (Use '#pragma ACCEL PARALLEL FACTOR=1')\n\nThese instructions should guide the process of writing the HLS codes in C using the specific #pragma values mentioned.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loop and use the original loop size without breaking it into smaller chunks. Run the loop iterations one after the other, not in parallel, and treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following concise natural language instruction for each pragma:\n\n1. `#pragma ACCEL PIPELINE auto{}`: Automatically pipeline the loop.\n2. `#pragma ACCEL TILE FACTOR=auto{1}`: Break the loop into smaller chunks with automatic tiling.\n3. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`: Run the iterations of this loop in parallel with a specified factor.\n4. `#pragma ACCEL PIPELINE auto{off}`: Don't automatically pipeline this loop.\n5. `#pragma ACCEL TILE FACTOR=auto{8}`: Break the loop into smaller chunks with automatic tiling and a specific factor.\n6. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`: Run the iterations of this loop in parallel with a specified factor.\n7. `#pragma ACCEL PIPELINE auto{off}`: Don't automatically pipeline this loop.\n8. `#pragma ACCEL TILE FACTOR=auto{1}`: Break the loop into smaller chunks with automatic tiling.\n9. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`: Run the iterations of this loop in parallel with a specified factor.\n10. `#pragma ACCEL PIPELINE auto{__PIPE__L3}`: Automatically pipeline the loop with a specific factor.\n11. `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`: Break the loop into smaller chunks with automatic tiling and a specific factor.\n12. `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Run the iterations of this loop in parallel with a specified factor.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To create hardware acceleration code in C, use the following natural language instructions for the given HLS codes:\n\n1. Mark the function as a hardware kernel for acceleration.\n\n2. Pipeline the loop automatically for improved throughput.\n\n3. Break the loop into smaller chunks using the specified factor for tiling.\n\n4. Run the loop iterations in parallel for efficient execution.\n\nRemember to replace \"auto\" with appropriate values based on the design requirements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but very concise natural language instruction:\n\n```plaintext\nConvert the function to a hardware kernel for acceleration and apply the following pragmas: \n1. Disable automatic pipelining for all loops\n2. Keep the original loop structure without breaking it into smaller chunks\n3. Run loop iterations sequentially, not in parallel\n4. Enable automatic pipelining for all loops\n5. Keep the original loop structure without breaking it into smaller chunks\n6. Run loop iterations sequentially, not in parallel\n7. Disable automatic pipelining for all loops\n8. Set tile factor to 4 for loop tiling\n9. Run loop iterations sequentially, not in parallel\n10. Set tile factor to 2 for loop tiling\n11. Run loop iterations sequentially, not in parallel\n```\nThis natural language instruction translates to the appropriate `#pragma` directives in the HLS C code to achieve the desired hardware acceleration and optimization.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions for the specific #pragma values:\n\n1. '#pragma ACCEL PIPELINE auto{off}' - Don't automatically pipeline the loop.\n\n2. '#pragma ACCEL TILE FACTOR=auto{1}' - Process the loop in its original, full form without breaking it into smaller chunks.\n\n3. '#pragma ACCEL PARALLEL FACTOR=auto{1}' - Run the iterations of this loop one after the other, not in parallel.\n\n4. '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}' - Run the inner loop iterations in parallel, reducing the variable 'tmp'.\n\n5. '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}' - Run the inner loop iterations in parallel, reducing the variable 'y'.\n\nIncorporating these instructions will generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, turn off automatic pipelining for the first loop, apply a tiling factor of 4, and run iterations of the main loop in parallel with a parallel factor of 1. Use the auto directive to determine factor values for reductions involving 'tmp' and 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C: \n1. Treat the entire function as a hardware kernel for acceleration,\n2. Don't automatically pipeline the first loop,\n3. Process the loop in its original, full form without breaking it into smaller chunks,\n4. Run the iterations of the loop one after the other, not in parallel,\n5. Parallelize the inner loop with a reduction on variable 'tmp',\n6. Parallelize the inner loop with a reduction on variable 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form, breaking them into smaller chunks for optimization is not required; Run the iterations of the outer loop one after the other, not in parallel; Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, then apply the following directives to manage parallelism and pipelining: 'Don't automatically pipeline this loop but manage it manually', 'Process this loop in its original, full form without breaking it into smaller chunks', 'Run the iterations of this loop in parallel with a specific factor', 'Don't automatically pipeline this loop but manage it manually', 'Process this loop in its original, full form without breaking it into smaller chunks', 'Run the iterations of this loop in parallel with a specific factor', 'Don't automatically pipeline this loop but manage it manually', 'Process this loop in its original, full form without breaking it into smaller chunks', 'Run the iterations of this loop in parallel with a specific factor', 'Don't automatically pipeline this loop but manage it manually', 'Process this loop in its original, full form without breaking it into smaller chunks', 'Run the iterations of this loop in parallel with a specific factor'.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following comprehensive but very concise natural language instructions:\n\n1. Generate the following function as a hardware kernel for acceleration:\n\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline this loop:\n\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n4. Run the iterations of this loop one after the other, not in parallel:\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n```\n\n5. Treat the following loop as a hardware kernel for acceleration:\n\n```c\n#pragma ACCEL kernel\n```\n\n6. Automatically pipeline this loop:\n\n```c\n#pragma ACCEL PIPELINE auto{}\n```\n\n7. Process this loop in its original, full form without breaking it into smaller chunks:\n\n```c\n#pragma ACCEL TILE FACTOR=auto{8}\n```\n\n8. Run the iterations of this loop one after the other, not in parallel:\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n```\n\n9. Automatically pipeline this loop:\n\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n```\n\n10. Process this loop in its original, full form without breaking it into smaller chunks:\n\n```c\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n```\n\n11. Run the iterations of this loop one after the other, not in parallel:\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```\n\nThese natural language instructions should help generate the specified HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with the specified pragma values as follows:\n1. Apply automatic pipeline optimization to the loops.\n2. Tile the loops with an automatic factor of 8.\n3. Process the loops in parallel with an automatic factor of 1 for the outer loop and auto-reduction pragma for both the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks.\nTreat the iterations of this loop as dependent and run them sequentially, not in parallel.\nTreat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you should:\n\n1. Treat the following function as a hardware kernel for acceleration by adding the pragma '#pragma ACCEL kernel'.\n2. Turn off automatic pipelining for the loop using '#pragma ACCEL PIPELINE auto{off}'.\n3. Keep the loop in its original, full form without breaking it into smaller chunks using '#pragma ACCEL TILE FACTOR=auto{1}'.\n4. Run the iterations of the loop one after the other, not in parallel, by using '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n5. Utilize parallelism for reduction of the 'tmp' variable with '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}'.\n6. Utilize parallelism for reduction of the 'y' variable with '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}'.\n\nFollowing these instructions, you can generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Convert the following function into a hardware kernel for acceleration. Then, turn off automatic pipelining for the loop, apply a tile factor of 1, and run the iterations in the loop one after the other, not in parallel. Calculate the mean and standard deviation in parallel and pipeline the loops for efficiency. Finally, calculate the correlation matrix with specified pipelining, tiling, and parallelization factors.\"\n\nThis would translate to the following #pragma values in the C code:\n```\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=1\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL TILE FACTOR=4\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL TILE FACTOR=1\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Mark the 'kernel_correlation' function as a hardware kernel for acceleration.\n2. Disable automatic pipeline optimization for the following loop.\n3. Execute the loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n5. Disable automatic pipeline optimization for the subsequent loop.\n6. Execute the loop in its original, full form without breaking it into smaller chunks.\n7. Run the iterations of this loop one after the other, not in parallel.\n8. Disable automatic pipeline optimization for the next loop.\n9. Execute the loop in its original, full form without breaking it into smaller chunks.\n10. Run the iterations of this loop one after the other, not in parallel.\n\nFollowing these instructions will help generate the desired HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions with specific #pragma values:\n\n1. Define the following function as a hardware kernel for acceleration:\n   Natural language instruction: \"Treat this function as a hardware kernel for acceleration\"\n   Corresponding #pragma value: #pragma ACCEL kernel\n\n2. Don't automatically pipeline the first loop:\n   Natural language instruction: \"Don't automatically pipeline this loop\"\n   Corresponding #pragma value: #pragma ACCEL PIPELINE \"off\"\n\n3. Process the first loop in its original, full form without breaking it into smaller chunks:\n   Natural language instruction: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   Corresponding #pragma value: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of the first loop one after the other, not in parallel:\n   Natural language instruction: \"Run the iterations of this loop one after the other, not in parallel\"\n   Corresponding #pragma value: #pragma ACCEL PARALLEL FACTOR=1\n\n5. Use auto parallelization with reduction for the second loop:\n   Natural language instruction: \"Use auto parallelization with reduction for this loop\"\n   Corresponding #pragma values:\n   - #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n   - #pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n\nBy following these natural language instructions and corresponding #pragma values, you can generate the HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in its original form and do not break it into smaller chunks. For the outer loop, run the iterations one after the other and not in parallel. Automatically pipeline the loops and parallelize the inner loops with reductions for both 'tmp' and 'y' variables. Lastly, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Treat the following function as a hardware kernel for acceleration. For the first loop, don't automatically pipeline it. Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. For the second loop, don't automatically pipeline it. Break the loop into chunks of 2 for processing. Run the chunks in parallel. For the third loop, don't automatically pipeline it. Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. For the last loop, automatically pipeline it with a factor of 3. Break the loop into chunks of 3 for processing. Run the chunks in parallel.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\"Treat the function as a hardware kernel. Turn off automatic loop pipelining. Process the loop in its original form without breaking it into smaller chunks. Run the iterations of the loops one after the other, not in parallel. Then, automatically pipeline the loop. Process the loop in smaller chunks for optimization. Run the loop iterations in parallel. Center and reduce the column vectors. Calculate the correlation matrix with optimization for parallelism and pipelining.\"\n", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n- Mark the function as a hardware kernel for acceleration.\n- Disable automatic loop pipelining for the entire kernel function.\n- Set loop tiling factor to 1 for the calculations within the kernel.\n- Use loop parallelization factor of 1 for the outer loop that calculates the mean and standard deviation.\n- Disable automatic loop pipelining for the subsequent calculations after the mean and standard deviation.\n- Set loop tiling factor to 1 for the subsequent calculations.\n- Use loop parallelization factor of 1 for the subsequent calculations.\n- Disable automatic loop pipelining for the centering and reducing calculations after the mean and standard deviation calculations.\n- Set loop tiling factor to 4 for the centering and reducing calculations.\n- Use loop parallelization factor of 2 for the subsequent calculations.\n- Set loop tiling factor to 1 for the last correlation matrix calculation.\n- Use loop parallelization factor of 1 for the last correlation matrix calculation.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but very concise natural language instructions:\n\n1. Use pragma ACCEL kernel to treat the following function as a hardware kernel for acceleration.\n\n2. Apply pragma ACCEL PIPELINE auto{} to automatically pipeline the loop.\n\n3. Utilize pragma ACCEL TILE FACTOR=8 to break the loop into smaller chunks of size 8.\n\n4. Employ pragma ACCEL PARALLEL FACTOR=__PARA__L0 to run the iterations of the loop in parallel.\n\n5. Disable automatic loop pipelining using pragma ACCEL PIPELINE auto{off}.\n\n6. Set pragma ACCEL TILE FACTOR=1 to process the loop in its original, full form without breaking it into smaller chunks.\n\n7. Use pragma ACCEL PARALLEL FACTOR=__PARA__L1 to run the iterations of the loop in parallel.\n\n8. Disable automatic loop pipelining using pragma ACCEL PIPELINE auto{off}.\n\n9. Set pragma ACCEL TILE FACTOR=1 to process the loop in its original, full form without breaking it into smaller chunks.\n\n10. Use pragma ACCEL PARALLEL FACTOR=__PARA__L2 to run the iterations of the loop in parallel.\n\n11. Enable loop pipelining using pragma ACCEL PIPELINE auto{__PIPE__L3} for the specific pipeline level.\n\n12. Set pragma ACCEL TILE FACTOR=__TILE__L3 to break the loop into smaller chunks based on the specific tile level.\n\n13. Utilize pragma ACCEL PARALLEL FACTOR=1 to run the iterations of the loop in parallel.\n\n14. Employ pragma ACCEL PIPELINE auto{__PIPE__L7} to enable loop pipelining at the specified pipeline level.\n\n15. Use pragma ACCEL PARALLEL FACTOR=__PARA__L7_0 to run the iterations of the loop in a parallel manner.\n\nBy following these natural language instructions, the provided HLS codes in C can be generated effectively.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. Don't automatically pipeline this loop:\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n   \n2. Process this loop in its original, full form without breaking it into smaller chunks:\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n   \n3. Run the iterations of this loop one after the other, not in parallel:\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n   \n4. Treat the following function as a hardware kernel for acceleration:\n   Use: `#pragma ACCEL kernel`\n\nThese natural language instructions correspond to the given HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the 'kernel_correlation' function in C, use the following natural language instructions for the specified #pragma values:\n\n1. 'Concurrently calculate the sum of data and store in mean' - for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n2. 'Compute standard deviation serially, without parallelization' - for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}'\n3. 'Disable automatic pipelining for the subsequent loop' - for '#pragma ACCEL PIPELINE auto{off}'\n4. 'Flatten loop processing for efficient acceleration' - for '#pragma ACCEL PIPELINE auto{flatten}'\n5. 'Optimize processing of the correlation matrix' - for '#pragma ACCEL PIPELINE auto{__PIPE__L3}'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for each specific #pragma values:\n\n1. 'Don't automatically pipeline this loop' for '#pragma ACCEL PIPELINE auto{off}'\n   \n2. 'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=auto{1}'\n\n3. 'Run the iterations of this loop one after the other, not in parallel' for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' \n\n4. 'Leave the decision of pipeline factor to be automatic' for '#pragma ACCEL PIPELINE auto{}'\n   \n5. 'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=auto{1}'\n\n6. 'Run the iterations of this loop one after the other, not in parallel' for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n\n7. 'Flatten the pipeline, treating all stages as equally critical' for '#pragma ACCEL PIPELINE auto{flatten}'\n\n8. 'Process this loop using a tile factor that is automatically decided' for '#pragma ACCEL TILE FACTOR=auto{30}'\n   \n9. 'Run the iterations of this loop one after the other, not in parallel' for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n\n10. 'Allow the pipeline factor and tile factor to be determined automatically' for '#pragma ACCEL PIPELINE auto{__PIPE__L3}' and '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'\n\nThese comprehensively concise natural language instructions should be used to generate the given HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following directives to the respective loops: \n- Turn off the automatic pipelining for the loop\n- Divide the iteration space of the loop into chunks of 4\n- Run the iterations of the loop in parallel with a factor of 1, and reduce the variables with associated reduction factors for both the inner loops", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions:\n\n1. Treat the 'kernel_correlation' function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops within the function.\n3. Process the loops using their original, full form without breaking them into smaller chunks.\n4. Run the iterations of the loops one after the other, not in parallel.\n5. Use a tile factor of 1 for the loops.\n6. Use a tile factor of 30 for a specific loop within the function.\n7. Apply parallelization with a factor of 2 for a specific loop within the function.\n\nThese instructions correspond to the specific #pragma values in the C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should use the following natural language instructions for the specific #pragma values:\n\n1. For the line declaring the kernel function:\n   - Use 'Treat the following function as a hardware kernel for acceleration'\n\n2. For the loop processing the mean calculation:\n   - Use 'Don't automatically pipeline this loop'\n   - Use 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - Use 'Run the iterations of this loop one after the other, not in parallel'\n\n3. For the loop processing the standard deviation calculation:\n   - Use 'Automatically pipeline this loop'\n   - Use 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - Use 'Run the iterations of this loop one after the other, not in parallel'\n\n4. For the loop processing the center and reduce of column vectors:\n   - Use 'Automatically pipeline this loop'\n   - Use 'Break this loop into chunks with a factor of 8 for better performance'\n   - Use 'Run the iterations of this loop in parallel with a factor of 6'\n\n5. For the loop calculating the correlation matrix:\n   - Use 'Automatically pipeline this loop'\n   - Use 'Break this loop into chunks with a factor of 30 for better performance'\n   - Use 'Run the iterations of this loop in parallel with a factor of 7'\n   \nThese instructions will guide the generation of the given HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, disable automatic loop pipelining, set the tiling factor to 116, and run the iterations of the outer loop in parallel while treating the inner loops as reduction operations on 'tmp' and 'y' arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the first loop.\n3. Process the loop in its original, full form without breaking it into smaller chunks using a tile factor of 116.\n4. Run the iterations of the second loop one after the other, not in parallel with a parallel factor of 1.\n5. For the inner loop, parallelize the reduction operations for the variable 'tmp' with a parallel factor of 1.\n6. For the second set of reduction operations, parallelize the reduction for the variable 'y' with a parallel factor of 1.\n\nBy following these comprehensive but concise instructions, you can generate the desired HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. `#pragma ACCEL kernel` - Treat the following function as a hardware kernel for acceleration.\n\n2. `#pragma ACCEL PIPELINE auto{}` - Automatically pipeline the loop for acceleration.\n\n3. `#pragma ACCEL TILE FACTOR=auto{8}` - Break the loop into smaller chunks using the specified factor for acceleration.\n\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` - Run the iterations of the loop in parallel with the specified factor for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the loop to run continuously without automatic pipelining and break it into smaller chunks of 8, then run the loop iterations in parallel while managing the reductions, and lastly, treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, for the subsequent loops, turn off automatic loop pipelining, tile the loops with a factor of 4, and run the iterations one after the other, without parallelization. Also, for the innermost loop, apply parallel reduction with a factor of 1 for both the 'tmp' and 'y' arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for HLS acceleration, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. For the outermost loop, prevent automatic pipelining:\n   - Use: \"Don't automatically pipeline this loop\"\n   - Corresponding pragma: #pragma ACCEL PIPELINE auto{off}\n\n2. For the outermost loop, process in its original, full form without breaking it into smaller chunks:\n   - Use: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   - Corresponding pragma: #pragma ACCEL TILE FACTOR=auto{1}\n\n3. For the outermost loop, run the iterations one after the other, not in parallel:\n   - Use: \"Run the iterations of this loop one after the other, not in parallel\"\n   - Corresponding pragma: #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n\nRepeat the same process for the inner loops using specific instructions to ensure proper optimization.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\nFor any loops or functions you want to accelerate in the provided C code, apply the corresponding #pragma values mentioned above to achieve the HLS-specific optimizations.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instruction for the respective #pragma values:\n1. \"Treat the following loop as a fully unrolled piece of hardware\" for #pragma ACCEL PIPELINE auto{off}\n2. \"Split the following loop into smaller chunks, each with a size of 8\" for #pragma ACCEL TILE FACTOR=auto{8}\n3. \"Run the iterations of this loop in parallel with a factor of 1\" for #pragma ACCEL PARALLEL FACTOR=auto{1}\n4. \"Treat the following function as a hardware kernel for acceleration\" for #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of the loop one after the other, not in parallel. Also, treat specific parallel reductions separately for the inner loop to achieve optimal performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code from the following C function while maintaining original loops and handling array dependencies:\n\n#pragma ACCEL kernel\n\nvoid kernel_atax(int m, int n, double A[116][124], double x[124], double y[124], double tmp[116]) {\n  int i;\n  int j;\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n\n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the given C code with HLS, use the following comprehensive but concise natural language instruction:\n\"Process the loop iterations in the original, full form without breaking them into smaller chunks, and run the iterations of these loops in parallel at auto{__PARA__L0}, auto{__PARA__L4}, auto{__PARA__L1}, auto{__PARA__L5}, auto{__PARA__L2}, auto{__PARA__L6}, and auto{__PARA__L7_0} factor, while also automatically pipelining each loop for better performance, and treat the 'kernel_correlation' function as a hardware kernel for acceleration.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire function as a hardware kernel for acceleration. Utilize automatic pipeline, automatic tiling with factor 1, and automatic parallelization with varying factors for different loops to accelerate the execution.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, here are the corresponding concise natural language instructions:\n\n1. `#pragma ACCEL PIPELINE auto{off}`: Don't automatically pipeline this loop.\n2. `#pragma ACCEL TILE FACTOR=auto{1}`: Process this loop in its original, full form without breaking it into smaller chunks.\n3. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`: Run the iterations of this loop one after the other, not in parallel.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`: Run the iterations of this loop one after the other, not in parallel.\n5. `#pragma ACCEL PIPELINE auto{}`: Automatically pipeline this loop.\n6. `#pragma ACCEL TILE FACTOR=auto{1}`: Process this loop in its original, full form without breaking it into smaller chunks.\n7. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`: Run the iterations of this loop one after the other, not in parallel.\n8. `#pragma ACCEL PIPELINE auto{__PIPE__L3}`: Automatically pipeline this loop.\n9. `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`: Process this loop in its original, full form without breaking it into smaller chunks.\n10. `#pragma ACCEL PARALLEL FACTOR=auto{2}`: Run the iterations of this loop in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration by adding the pragma '#pragma ACCEL kernel' before the function definition.\n\n2. For the loop iterating over 'j' from 0 to 79, disable automatic loop pipelining by using the pragma '#pragma ACCEL PIPELINE auto{off}'.\n\n3. Process the loop in its original, full form without breaking it into smaller chunks by adding the pragma '#pragma ACCEL TILE FACTOR=auto{1}'.\n\n4. Run the iterations of this loop one after the other, not in parallel by using the pragma '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'.\n\n5. After the loop calculating the 'mean', turn on automatic loop pipelining by adding the pragma '#pragma ACCEL PIPELINE auto{}'.\n\n6. For the loop iterating over 'j' from 0 to 79, process the loop in its original, full form without breaking it into smaller chunks by adding the pragma '#pragma ACCEL TILE FACTOR=auto{1}'.\n\n7. Run the iterations of this loop one after the other, not in parallel by using the pragma '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'.\n\n8. After the loop calculating the 'stddev', turn off automatic loop pipelining by adding the pragma '#pragma ACCEL PIPELINE auto{off}'.\n\n9. For the loop iterating over 'i' from 0 to 99 and 'j' from 0 to 79, process the loop in its original, full form without breaking it into smaller chunks by adding the pragma '#pragma ACCEL TILE FACTOR=auto{2}'.\n\n10. Run the iterations of this loop in parallel by using the pragma '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'.\n\n11. For the loop iterating over 'i' from 0 to 79, disable automatic loop pipelining by using the pragma '#pragma ACCEL PIPELINE auto{__PIPE__L3}'.\n\n12. Process the loop in its original, full form without breaking it into smaller chunks by adding the pragma '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'.\n\n13. Run the iterations of this loop in parallel by using the pragma '#pragma ACCEL PARALLEL FACTOR=auto{2}', considering the interdependencies among different iterations and data movements.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions for the specified #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80])\n{\n  // function body\n}\n```\n\n2. Don't automatically pipeline this loop.\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n```\n\n5. Treat the following loop as a pipeline with automatic initiation interval.\n```c\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n```\n\n6. Process this loop using a specific auto-generated factor value.\n```c\n#pragma ACCEL TILE FACTOR=auto{2}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n```\n\nThese natural language instructions correspond to the provided HLS codes in C and specify the behavior of each loop in terms of pipelining, tiling, and parallelization.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should:\n\n1. Treat the function `kernel_correlation` as a hardware kernel for acceleration by using `#pragma ACCEL kernel`.\n2. Turn off automatic pipelining for the loops by using `#pragma ACCEL PIPELINE auto{off}`.\n3. Ensure the loops are processed in their original, full form without breaking into smaller chunks by using `#pragma ACCEL TILE FACTOR=1`.\n4. Run the iterations of the loops one after the other, not in parallel, by using `#pragma ACCEL PARALLEL FACTOR=1` for the specific loops indicated in the code.\n\nYou can use these natural language instructions to guide the generation of the HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, follow these concise natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline the loop.\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. Process the loop in its original, full form without breaking it into smaller chunks with a tile factor of 25.\n   ```c\n   #pragma ACCEL TILE FACTOR=25\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel with a parallel factor of 1.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n5. Don't automatically pipeline the loop.\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n6. Process the loop in its original, full form without breaking it into smaller chunks with a tile factor of 1.\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n7. Run the iterations of this loop in parallel with a parallel factor of 1.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n8. Don't automatically pipeline the loop.\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n9. Process the loop in its original, full form without breaking it into smaller chunks with a tile factor of 1.\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n10. Run the iterations of this loop in parallel with a parallel factor of 2.\n    ```c\n    #pragma ACCEL PARALLEL FACTOR=2\n    ```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1.  For the kernel function `kernel_correlation`, allow HLS to analyze and automatically determine whether to pipeline the loops.\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n2.  Use HLS to automatically determine the tiling factor for the loops in `kernel_correlation`, preserving their original, full form without breaking them into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n3.  Enable HLS to automatically parallelize the loops in `kernel_correlation`, specifying the parallelization factor based on the specific computation and hardware capabilities.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n   ```\n\n4.  Utilize HLS to automatically pipeline the subsequent loops after analysis.\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   ```\n\n5.  Allow HLS to automatically determine the tiling factor for the following loops in `kernel_correlation`, preserving their full form without breaking them into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{2}\n   ```\n\n6.  Enable HLS to automatically parallelize the subsequent loops with a specific parallelization factor determined by the analysis.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n   ```\n\n7.  Integrate HLS to automatically pipeline the loops in the next section of the code.\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   ```\n\n8.  Specify HLS to automatically determine the tiling factor for the subsequent loops in `kernel_correlation`, preserving their original form without dividing into smaller chunks.\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n9.  Enable HLS to automatically parallelize the following loops in `kernel_correlation`, with a specific parallelization factor determined through analysis.\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n   ```\n\n10. Integrate HLS to automatically pipeline the subsequent loops in the code.\n    ```c\n    #pragma ACCEL PIPELINE auto{__PIPE__L3}\n    ```\n\n11. Specify HLS to determine the tiling factor for the next loops in `kernel_correlation` based on the hardware capabilities and computations.\n    ```c\n    #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    ```\n\n12. Enable HLS to automatically parallelize the final loops in `kernel_correlation` with a specific parallelization factor determined through analysis.\n    ```c\n    #pragma ACCEL PARALLEL FACTOR=auto{2}\n    ```\n\n**Note:** The `auto` and `auto{}` indicate automatic determination by the HLS tool, and the specific factors or identifiers have been denoted as `auto{__...__}` based on the original code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given C code with the following natural language instruction:\n\n- For the outer loop, turn off automatic pipelining\n- Use a tile factor of 1 for loop optimization\n- Run the iterations of the loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can follow these concise natural language instructions for the specific #pragma values:\n\n1. Use 'Don't automatically pipeline this loop' for '#pragma ACCEL PIPELINE auto{off}'\n2. Use 'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Use 'Run the iterations of this loop one after the other, not in parallel' for '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' for the first loop, and 'auto{__PARA__L1}' for the second loop, 'auto{__PARA__L2}' for the third loop, 'auto{__PARA__L4}' for the nested loop within the first loop, 'auto{__PARA__L5}' for the nested loop within the second loop, 'auto{__PARA__L6}' for the nested loop within the third loop, 'auto{__PARA__L7_0}' for the nested loop within the last loop.\n\nBy adhering to the above instructions, you will generate the given HLS codes in C with the specified #pragma values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Then, turn off automatic pipelining for the loop. Use a tiling factor of 116 for the loop to enable loop unrolling. Finally, parallelize the loops with a parallel factor of 1 and apply reductions for the specified variables.\"\n\nThese instructions correspond to the following #pragma values in the HLS C code:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{116}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with natural language instructions, you can use the following concise but comprehensive instructions:\n\n1. Declare the following function as a hardware kernel for acceleration.\n   \n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loop.\n\n   `#pragma ACCEL PIPELINE off`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n   `#pragma ACCEL PARALLEL FACTOR=1`\n\nThese instructions will correspond to the specific #pragma values used in the HLS C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\nAdditionally, for the specific factors needed for auto-tuning:\n- Use `auto{2}` for `#pragma ACCEL TILE FACTOR=auto`\n- Use `auto{1}` for `#pragma ACCEL PARALLEL FACTOR=auto`\n- Use `auto{1_0}` for `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto`\n- Use `auto{1_1}` for `#pragma ACCEL PARALLEL reduction=y FACTOR=auto`\n\nImplementing these natural language instructions will generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Break the second loop into smaller chunks of size 8 for tiling. Run the iterations of the third loop one after the other, not in parallel. Use reduction for the inner loops with a parallel factor of 1 for both 'tmp' and 'y' arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'atax' function as a hardware kernel for acceleration and apply auto pipelining, tiling with a factor of 2, and parallelizing factor 1 for both 'tmp' and 'y' reduction loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the first loop.\n3. Process the first loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of the first loop one after the other, not in parallel.\n5. Automatically parallelize the second loop with a reduction operation on the variable 'tmp' and a parallel factor of 1.\n6. Automatically parallelize the third loop with a reduction operation on the variable 'y' and a parallel factor of 1.\n\nWith these instructions, you can generate the provided HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use the following natural language instructions and #pragma values:\n\n1. For the outermost loop, don't automatically pipeline this loop:\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n2. For the outermost loop, process this loop in its original, full form without breaking it into smaller chunks:\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. For the first nested loop, run the iterations of this loop one after the other, not in parallel:\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n4. For the second nested loop, run the iterations of this loop one after the other, not in parallel:\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n5. For the third nested loop, run the iterations of this loop one after the other, not in parallel:\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n6. For the fourth nested loop, pipeline this loop with a factor of 3:\n   Use: `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n7. For the fourth nested loop, process it in tiles with a factor of 3:\n   Use: `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n8. For the fourth nested loop, run the iterations of this loop with a parallel factor of 2:\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\nNote: Replace \"auto\" with the specific value where applicable.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given C code, you can use the following natural language instruction as a comprehensive but concise description:\n\n\"Optimize this function for hardware acceleration, turning off automatic pipeline, setting tile factor to 1, and running iterations of the loops sequentially with a factor of 1. Perform parallel reductions for both 'tmp' and 'y' arrays.\"\n\nThis instruction covers the specific #pragma values to use in the C code:\n\n1. '#pragma ACCEL kernel' for hardware acceleration optimization\n2. '#pragma ACCEL PIPELINE auto{off}' to turn off automatic pipeline\n3. '#pragma ACCEL TILE FACTOR=auto{1}' to set the tile factor to 1\n4. '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run iterations of the loops sequentially with a factor of 1\n5. '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}' for parallel reduction on the 'tmp' array\n6. '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}' for parallel reduction on the 'y' array", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in its original, full form without breaking it into smaller chunks.\nEnsure that the iterations of the loops run sequentially, not in parallel.\nTreat the following function as a hardware kernel for acceleration.\nDon't automatically pipeline the loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, for the first loop, don't automatically pipeline it. For the second loop, break it into smaller chunks with a factor of 4. Finally, run the iterations of the second loop one after the other, not in parallel, and apply reductions for the 'tmp' and 'y' variables.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you can use the following concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Don't automatically pipeline this loop.\n\n6. Process this loop as a single chunk without breaking it into smaller chunks.\n\n7. Run the iterations of this loop in parallel.\n\n8. Don't automatically pipeline this loop.\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n\n10. Run the iterations of this loop in parallel.\n\n11. Process this loop in its original, full form without breaking it into smaller chunks.\n\n12. Run the iterations of this loop in parallel.\n\n13. Process this loop with a pipeline factor that suits the design.\n\n14. Process this loop in its original, full form without breaking it into smaller chunks.\n\n15. Run the iterations of this loop in parallel.\n\n16. Process this loop with a pipeline factor that suits the design.\n\n17. Process this loop in its original, full form without breaking it into smaller chunks.\n\n18. Run the iterations of this loop in parallel.\n\n19. Process this loop with a pipeline factor that suits the design.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n1. Treat the function 'kernel_correlation' as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops in the code.\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n4. Run the iterations of the loops one after the other, not in parallel.\n\nFor specific #pragma values:\n- '#pragma ACCEL kernel'\n- '#pragma ACCEL PIPELINE auto{off}'\n- '#pragma ACCEL TILE FACTOR=auto{1}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}'\n- '#pragma ACCEL PIPELINE auto{__PIPE__L3}'\n- '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n- '#pragma ACCEL PIPELINE auto{__PIPE__L7}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, without automatically pipelining the first loop, tiling the second loop with a factor of 116, and running the iterations of the second loop one after the other, not in parallel.\"\n\nThis comprehensive but concise instruction encapsulates the specific #pragma values to use for the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but very concise natural language instruction for each #pragma:\n\n- '''Make the loop not automatically pipelined''': Use '#pragma ACCEL PIPELINE off'\n- '''Process the loop without breaking it into smaller chunks''': Use '#pragma ACCEL TILE FACTOR=1'\n- '''Run the loop iterations one after the other, not in parallel''': Use '#pragma ACCEL PARALLEL FACTOR=1'\n- '''Treat the following function as a hardware kernel for acceleration''': Use '#pragma ACCEL kernel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you need to use the following natural language instructions for the specific #pragma values:\n\n1. For the kernel function:\n   - Use the instruction \"Treat the following function as a hardware kernel for acceleration\".\n\n2. For the pipeline directive:\n   - Use the instruction \"Automatically pipeline this loop\" for pipeline auto{}\n\n3. For the tile directive:\n   - Use the instruction \"Automatically determine the tile factor for this loop\" for tile auto{}\n\n4. For the parallel directive:\n   - Use the instruction \"Automatically determine the parallel factor for this loop\" for parallel auto{__PARA__L0}, auto{__PARA__L1}, and so on.\n\nReplace '__PARA__L0', '__PARA__L1', '__PARA__L2', '__PARA__L4', '__PARA__L5', '__PARA__L6', and '__PARA__L7_0' with the specific factors determined for each loop in the parallel directive. Similarly, replace '__PIPE__L3' and '__PIPE__L7' with the specific factors determined for each loop in the pipeline directive. Also, replace '__TILE__L3' with the specific factor determined for the tile directive.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the specific #pragma values:\n\n1. For the loop that computes the mean vector, do not automatically pipeline this loop:  \n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n2. For the loop that computes the mean vector, process this loop in its original, full form without breaking it into smaller chunks:  \n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n3. For the loop that computes the mean vector, run the iterations of this loop one after the other, not in parallel:  \n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n4. For the loop that computes the standard deviation vector, do not automatically pipeline this loop:  \n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n5. For the loop that computes the standard deviation vector, process this loop in its original, full form without breaking it into smaller chunks:  \n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n6. For the loop that computes the standard deviation vector, run the iterations of this loop one after the other, not in parallel:  \n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n7. For the loop that centers and reduces the column vectors, apply automatic pipeline optimization:  \n   ```c\n   #pragma ACCEL PIPELINE \"auto\"\n   ```\n\n8. For the loop that centers and reduces the column vectors, process this loop using a tile factor of 2:  \n   ```c\n   #pragma ACCEL TILE FACTOR=2\n   ```\n\n9. For the loop that centers and reduces the column vectors, run the iterations of this loop using a parallel factor of 2:  \n   ```c\n   #pragma ACCEL PARALLEL FACTOR=2\n   ```\n\n10. For the loop that calculates the correlation matrix, apply automatic pipeline optimization with the given pipeline label:  \n   ```c\n   #pragma ACCEL PIPELINE \"auto{__PIPE__L3}\"\n   ```\n\n11. For the loop that calculates the correlation matrix, process this loop using a tile factor of 2 with the given tile label:  \n   ```c\n   #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n   ```\n\n12. For the loop that calculates the correlation matrix, run the iterations of this loop using a parallel factor of 2:  \n   ```c\n   #pragma ACCEL PARALLEL FACTOR=2\n   ```\n\nBy applying these natural language instructions to generate the specified #pragma values in the given C code, the HLS codes can be produced accordingly.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Break the loop into smaller chunks with a factor of 2 for tiling the second loop. Run the iterations of the second loop one after the other, not in parallel. In the first nested loop, perform a reduction with a factor of 1 for variable 'tmp'. In the second nested loop, perform a reduction with a factor of 1 for variable 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate hardware kernel function with parallel and pipelined loops\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_atax(int m, int n, double A[116][124], double x[124], double y[124], double tmp[116]) {\n  int i;\n  int j;\n  \n  #pragma ACCEL PIPELINE \"off\"\n\n  #pragma ACCEL TILE FACTOR=2\n\n  #pragma ACCEL PARALLEL FACTOR=1\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n  \n    #pragma ACCEL PARALLEL reduction=tmp FACTOR=1\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n    #pragma ACCEL PARALLEL reduction=y FACTOR=1\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply these specific directives: \n\n1. Disable automatic pipelining for the loop\n2. Keep the loop in its original, full form without breaking it into smaller chunks\n3. Run the iterations of the loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To fully optimize the given C code, you need to perform the following steps, \n- Ensure that the loop is not automatically pipelined.\n- Process the loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of the loop one after the other, not in parallel.\n- Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following natural language instructions for the specific #pragma values:\n\n1. For the first loop involving the calculation of mean values:\n   - Process this loop in its original, full form without breaking it into smaller chunks\n   - Process the iterations sequentially, not in parallel\n\n2. For the second loop involving the calculation of standard deviation and handling near-zero std. dev. values:\n   - Process this loop in its original, full form without breaking it into smaller chunks\n   - Run the iterations in a sequence, not in parallel\n\n3. For the third loop involving centering and reducing the column vectors:\n   - Process this loop in its original, full form without breaking it into smaller chunks\n   - Run the iterations in a sequential manner, not in parallel\n   - Pipeline this loop automatically\n\n4. For the final loop involving the calculation of the correlation matrix:\n   - Automatically pipeline this loop\n   - Break this loop into tiles with an automatic factor\n   - Run the iterations in parallel with a factor of 30\n\nEnsure to mark the whole function as a hardware kernel for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and for each of the following loops: \n- Don't automatically pipeline the loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the corresponding #pragma values:\n\n1. For the loop that calculates the mean of the columns:\n   Use 'Don't automatically pipeline this loop' -> '#pragma ACCEL PIPELINE \"off\"'\n   Use 'Process this loop in its original, full form without breaking it into smaller chunks' -> '#pragma ACCEL TILE FACTOR=1'\n   Use 'Run the iterations of this loop one after the other, not in parallel' -> '#pragma ACCEL PARALLEL FACTOR=1'\n\n2. For the loop that calculates the standard deviation of the columns:\n   Use 'Don't automatically pipeline this loop' -> '#pragma ACCEL PIPELINE \"off\"'\n   Use 'Split this loop into 4 smaller parts for processing in parallel' -> '#pragma ACCEL TILE FACTOR=4'\n   Use 'Run the iterations of this loop one after the other, not in parallel' -> '#pragma ACCEL PARALLEL FACTOR=1'\n\n3. For the loop that centers and reduces the column vectors:\n   Use 'Don't automatically pipeline this loop' -> '#pragma ACCEL PIPELINE \"off\"'\n   Use 'Process this loop in its original, full form without breaking it into smaller chunks' -> '#pragma ACCEL TILE FACTOR=1'\n   Use 'Run the iterations of this loop in chunks of 2 in parallel' -> '#pragma ACCEL PARALLEL FACTOR=2'\n\n4. For the loop that calculates the correlation matrix:\n   Use 'Automatically pipeline this loop with a specific depth' -> '#pragma ACCEL PIPELINE auto{__PIPE__L3}'\n   Use 'Automatically split this loop into smaller parts for parallel processing' -> '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'\n   Use 'Run the iterations of this loop in chunks of 2 in parallel' -> '#pragma ACCEL PARALLEL FACTOR=2'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\nThese natural language instructions correspond to the specific #pragma values in the given C code, allowing for the generation of the HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration. Use automatic pipelining for all applicable loops. For tiling, set the factor to 1 for some loops and let the tool determine the factor for others. In parallel loops, use the tool-defined factor for some, and set the factor to 30 for one specific loop. Lastly, disable automatic pipelining for a designated loop to process it off.\"\n\nBy following this comprehensive but concise natural language instruction, you can generate the provided HLS codes with the correct #pragma values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks and treat the following function as a hardware kernel for acceleration. Additionally, run the iterations of the outer loop one after the other, not in parallel, and automatically pipeline the loops with a pipeline factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline each loop. Automatically break the loops into smaller chunks with a factor of 2 for tiling. Run the iterations of the loops in parallel with factor __PARA__L0. Pipeline each loop automatically. Keep the loops in their original form without tiling. Run the iterations of the loops in parallel with factor __PARA__L1. Automatically pipeline each loop. Keep the loops in their original form without tiling. Run the iterations of the loops in parallel with factor __PARA__L2. Parallelize the loops with a factor of 30 and pipeline automatically with factor __PIPE__L3. Break the loops into smaller chunks with a factor of __TILE__L3 for tiling, and parallelize the loops with a factor of __PARA__L7_0.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and do not automatically pipeline the first loop, tile the loops with a factor of 8, and run the iterations of the loops one after the other, not in parallel. Also, parallelize the inner loops with a reduction clause for the variable 'tmp' and 'y'.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Treat the given function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks, and treat the following function as a hardware kernel for acceleration, and automatically pipeline the loops, and run the iterations of the outer loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process and pipeline the loops, and run iterations in parallel with appropriate factors for reduction variables. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the loop that initializes `y` array, to not automatically pipeline it: `#pragma ACCEL PIPELINE auto{off}`.\n\n2. For the first nested loop that iterates over `i` and needs to be processed in its original form without breaking it into smaller chunks, use: `#pragma ACCEL TILE FACTOR=auto{8}`.\n\n3. For the second nested loop that iterates over both `i` and `j` and needs to run the iterations one after the other, not in parallel, set the parallel factor to 1 with: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n\n4. For the parallel processing inside the nested loop, to perform reduction on `tmp` array with a factor of 1, use: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}`.\n\n5. For the second reduction inside the nested loop on the array `y`, also with a factor of 1, utilize: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}`.\n\nIn summary, follow these natural language instructions to apply the respective #pragma values for the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. For the outer loop, disable automatic pipelining.\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n2. Break the outer loop into smaller chunks using auto-factorization.\n   Use: `#pragma ACCEL TILE FACTOR=auto{25}`\n\n3. For the outer loop, run iterations sequentially (not in parallel).\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n4. Disable automatic pipelining for the next loop.\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n5. Process the next loop in its original, full form without breaking it into smaller chunks.\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n6. Run the next loop iterations sequentially (not in parallel).\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n7. Disable automatic pipelining for the following loop.\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n8. Process the next loop in its original, full form without breaking it into smaller chunks.\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n9. Run the next loop iterations sequentially (not in parallel).\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n10. Enable automatic pipelining and specify the factor for the final loop.\n    Use: `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n11. Break the final loop into smaller chunks using auto-factorization.\n    Use: `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n12. Run the final loop iterations in parallel with a factor of 2.\n    Use: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\nFollowing these instructions using the specific #pragma values, you can generate the given HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, follow these natural language instructions:\n\n\"Declare the following function as a hardware kernel for acceleration.\"\n\nInside the function \"kernel_atax\", use the following specific #pragma values:\n\n- \"Don't automatically pipeline this loop\" => #pragma ACCEL PIPELINE auto{}\n- \"Process this loop in its original, full form without breaking it into smaller chunks\" => #pragma ACCEL TILE FACTOR=auto{8}\n- \"Run the iterations of this loop one after the other, not in parallel\" => #pragma ACCEL PARALLEL FACTOR=auto{1_1}", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration.\nDon't automatically pipeline this loop.\nProcess this loop in its original, full form without breaking it into smaller chunks.\nRun the iterations of this loop one after the other, not in parallel.\nParallelize this inner loop with the reduction operation on variable 'tmp', with a factor of 1.\nParallelize this inner loop with the reduction operation on variable 'y', with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_correlation' function as a hardware kernel for acceleration. Do not automatically pipeline the loops but tile them with a factor of 1. Run the iterations of the loops in parallel with a parallel factor equal to the value of '__PARA__L0' for the first loop, '__PARA__L1' for the second loop, '__PARA__L2' for the third loop, and 30 for the fourth loop.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the following C code for hardware acceleration:\n- Use '#pragma ACCEL kernel' to mark the function as a hardware kernel.\n- Disable automatic loop pipelining with '#pragma ACCEL PIPELINE \"off\"'.\n- Set loop tiling factor to 8 using '#pragma ACCEL TILE FACTOR=8'.\n- Set loop parallelization factor to 1 using '#pragma ACCEL PARALLEL FACTOR=1'.\n- Apply reduction parallelization for 'tmp' and 'y' using '#pragma ACCEL PARALLEL reduction=tmp FACTOR=1' and '#pragma ACCEL PARALLEL reduction=y FACTOR=1' respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS codes as follows:\n\n- `#pragma ACCEL PIPELINE auto{}` : Automatically pipeline the loops in the subsequent code.\n- `#pragma ACCEL TILE FACTOR=auto{4}` : Break the loop into smaller chunks with a factor of 4 to improve data locality.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` : Run the iterations of this loop in parallel.\n- `#pragma ACCEL PIPELINE auto{}` : Automatically pipeline the loops in the subsequent code.\n- `#pragma ACCEL TILE FACTOR=auto{1}` : Process this loop in its original, full form without breaking it into smaller chunks.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` : Run the iterations of this loop in parallel.\n- `#pragma ACCEL PIPELINE auto{}` : Automatically pipeline the loops in the subsequent code.\n- `#pragma ACCEL TILE FACTOR=auto{1}` : Process this loop in its original, full form without breaking it into smaller chunks.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` : Run the iterations of this loop in parallel.\n- `#pragma ACCEL PIPELINE auto{__PIPE__L3}` : Automatically pipeline the loops in the subsequent code with specific pipeline parameters.\n- `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}` : Break the loop into smaller chunks with specific tile factor.\n- `#pragma ACCEL PARALLEL FACTOR=auto{30}` : Run the iterations of this loop in parallel with a factor of 30.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function in its original, full form without breaking it into smaller chunks. Run the iterations of these loops one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the loop automatically without breaking it into smaller chunks and run the iterations of the loop in parallel with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you would use the following natural language instructions for specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\nHope this helps!", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions:\n- Treat the function 'kernel_correlation' as a hardware kernel for acceleration\n- Don't automatically pipeline the loops\n- Process the loops in their original full form without breaking them into smaller chunks\n- Run the iterations of the loops one after the other, not in parallel\n- Use the pipeline pragma to automatically pipeline the loops\n- Use the tile pragma to set the tile factor to 'auto{1}' for each loop\n- Use the parallel pragma to set the parallel factor to 'auto{__PARA__Lx}' for each corresponding loop\n- Use the pipeline pragma to turn off automatic pipelining for a specific loop\n- Use the parallel pragma to set the parallel factor to 'auto{__PARA__L7_0}' for a specific loop", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these natural language instructions:\n\n1. Indicate that the outer loop should not be automatically pipelined.\n   Use: \"Don't automatically pipeline this loop\"\n\n2. Specify that the outer loop should be processed in its original form without breaking it into smaller chunks.\n   Use: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n3. Run the iterations of the outer loop one after the other, not in parallel.\n   Use: \"Run the iterations of this loop one after the other, not in parallel\"\n\n4. Ensure that the function is treated as a hardware kernel for acceleration. \n   Use: \"Treat the following function as a hardware kernel for acceleration\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes, use the following comprehensive but concise natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration\n   - Use: `#pragma ACCEL kernel`\n\n2. Automatically pipeline the loop\n   - Use: `#pragma ACCEL PIPELINE auto{}`\n   \n3. Tune the tiling factor for the loop automatically\n   - Use: `#pragma ACCEL TILE FACTOR=auto{2}`\n\n4. Set the parallel factor for the loop to 1 and perform reductions to calculate `tmp` array\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n   - Use: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}`\n\n5. Set the parallel factor for the loop to 1 and perform reductions to calculate `y` array\n   - Use: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n\n4. Run the iterations of this loop one after the other, not in parallel\n\n5. Don't automatically pipeline this loop\n\n6. Set a tile factor of 8 for tiling this loop\n\n7. Run the iterations of this loop in parallel\n\n8. Don't automatically pipeline this loop\n\n9. Process this loop in its original, full form without breaking it into smaller chunks\n\n10. Run the iterations of this loop in parallel\n\n11. Don't automatically pipeline this loop\n\n12. Set a tile factor of 1 for tiling this loop\n\n13. Run the iterations of this loop in parallel\n\n14. Set a pipeline factor of 30 for pipelining this loop\n\n15. Set a tile factor of auto for tiling this loop\n\n16. Run the iterations of this loop in parallel", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the outer loop, tile the loop with a factor of 116, and run the outer loop iterations in parallel with a factor of 1. Then, run the inner loops in parallel with a reduction for each iteration, where the reduction factor for the 'tmp' array is 1 and for the 'y' array is also 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{116}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Use a tile factor of 1 for tiling the second loop. Run the iterations of the third loop one after the other, not in parallel. In the fourth loop, use a parallel reduction for 'tmp' with a factor of 1_0, and in the fifth loop, use a parallel reduction for 'y' with a factor of 1_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Process this loop in its original, full form without breaking it into smaller chunks' for '#pragma ACCEL TILE FACTOR=1', 'Run the iterations of this loop one after the other, not in parallel' for '#pragma ACCEL PARALLEL FACTOR=1', 'Don't automatically pipeline this loop' for '#pragma ACCEL PIPELINE \"off\"', 'Treat the following function as a hardware kernel for acceleration' for '#pragma ACCEL kernel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you can use the following natural language instruction:\n\n\"Designate the 'kernel_correlation' function as a hardware kernel for acceleration, and apply the following transformations to optimize hardware implementation:\n- Pipeline the loops automatically\n- Do not break the loops into smaller chunks\n- Run loop iterations one after the other, not in parallel\n- Turn off automatic pipelining for a specific loop\n- Keep the tile factor as 1\n- Perform parallelization with a specific factor for individual loops\n- Adjust pipeline, tile, and parallel factors based on context\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, the following comprehensive but concise natural language instructions can be used:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Description: Mark the function as a hardware kernel for acceleration\n   - HLS pragma: #pragma ACCEL kernel\n\n2. Don't automatically pipeline this loop\n   - Description: Prevent automatic pipelining of the loop\n   - HLS pragma: #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Description: Set the tile factor to 1 to process the loop in its original form\n   - HLS pragma: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Description: Set the parallel factor to 1 to run iterations sequentially\n   - HLS pragma: #pragma ACCEL PARALLEL FACTOR=1\n\nThese natural language instructions can be used to convey the corresponding HLS pragma values and effectively generate the provided HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware kernel for accelerating the `kernel_correlation` function. Automatically pipeline the loops. Set the tiling factor to 1 for all loops. Parallelize the outer loop with a factor of auto{__PARA__L0} and the inner loop with a factor of auto{__PARA__L4} for the mean calculation. Parallelize the outer loop with a factor of auto{__PARA__L1} and the inner loop with a factor of auto{__PARA__L5} for the standard deviation calculation. Turn off pipeline for the centering and reduction loop, set the tiling factor to 4, and parallelize the outer loop with a factor of auto{__PARA__L2} and the inner loop with a factor of auto{__PARA__L6}. Finally, set the pipeline factor to auto{__PIPE__L3}, the tiling factor to auto{__TILE__L3}, and parallelize the outer loop with a factor of 30 for the correlation matrix calculation.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks.\n\nDon't automatically pipeline this loop.\n\nRun the iterations of this loop one after the other, not in parallel.\n\nTreat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the following C code with HLS pragmas:\n- Treat the function as a hardware kernel for acceleration\n- Don't automatically pipeline the loop\n- Use a tile factor of 8 to break the loop into smaller chunks\n- Run the iterations of the loop one after the other, not in parallel\n- Use a parallel factor of 1 for both reductions\n\nUse the following natural language instruction:\n\"Mark the function as a hardware kernel, disable automatic pipelining, set the tile factor to 8, run the loop iterations sequentially, and use a parallel factor of 1 for both reductions.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks.\nTreat the iterations of this loop as dependent and not to parallelize them.\nApply parallelism to this loop with a factor of 116 to leverage hardware parallelism.\nThe following function should be treated as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the following loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Process the loops with the original tile factor of 1. Run the outer loop iterations one after the other, not in parallel, and use a parallel factor auto{116} for the inner loops with appropriate reduction factors for variables tmp and y.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the following natural language instruction:\n\"Treat the following function as a hardware kernel for acceleration, and process the loops inside using parallelism and pipelining as follows:\n\n- For the loop calculating 'mean', automatically pipeline it, apply tiling with a factor of 1, and run the iterations in parallel with a factor '__PARA__L0'.\n- For the loop calculating 'stddev', turn off automatic pipelining, tile with a factor of 1, and run the iterations in parallel with a factor '__PARA__L1'.\n- For the loop centering and reducing column vectors, automatically pipeline it, tile with a factor of 4, and run the iterations in parallel with a factor '__PARA__L2'.\n- For the loop calculating the correlation matrix, use a fixed pipeline with '__PIPE__L3', apply tiling with a factor of '__TILE__L3', and run the iterations in parallel with a factor of 30.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To achieve the same behavior as the given C HLS codes, use the following natural language instructions for pragma values:\n\n1. \"Process each iteration of the following loop sequentially without parallelization\" for `#pragma ACCEL PIPELINE auto{}`\n2. \"Process this loop in its original, full form without breaking it into smaller chunks\" for `#pragma ACCEL TILE FACTOR=auto{1}`\n3. \"Run the iterations of this loop in parallel with a factor of __PARA__L0\" for `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n4. \"Stop automatically pipelining this loop\" for `#pragma ACCEL PIPELINE auto{off}`\n5. \"Break down the loop into smaller chunks with a factor of 8\" for `#pragma ACCEL TILE FACTOR=auto{8}`\n6. \"Run the iterations of this loop in parallel with a factor of __PARA__L2\" for `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n7. \"Pipeline this loop with a custom pipeline factor\" for `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n8. \"Break down the loop into smaller chunks with a custom factor\" for `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n9. \"Run the iterations of this loop in parallel with a factor of 30\" for `#pragma ACCEL PARALLEL FACTOR=auto{30}`\n10. \"Stop automatic pipelining for this loop\" for `#pragma ACCEL PIPELINE auto{__PIPE__L7}`\n11. \"Run the iterations of this loop in parallel with a custom factor __PARA__L7_0\" for `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}`\n", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following comprehensive but concise natural language instruction:\n\n\"Implement hardware acceleration for the function kernel_correlation. Automatically pipeline the loops and handle parallelization based on the loop bounds. Use tiling with factors determined automatically based on the loop iteration space. Use special handling for near-zero standard deviation values and calculate the correlation matrix using nested loops with appropriate parallelization and pipelining.\"\n\nThis natural language instruction translates to the set of #pragma values needed in the C code:\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) \n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{80}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{100}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{80}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{100}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j], (double)2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n  }\n\n#pragma ACCEL PIPELINE auto{flatten}\n\n#pragma ACCEL TILE FACTOR=auto{4}\n\n#pragma ACCEL PARALLEL FACTOR=auto{400}\n  for (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n\n#pragma ACCEL PIPELINE auto{80}\n\n#pragma ACCEL TILE FACTOR=auto{80}\n\n#pragma ACCEL PARALLEL FACTOR=auto{80}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n\n#pragma ACCEL PIPELINE auto{80}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{8000}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following optimizations: Don't automatically pipeline this loop, process this loop in its original form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the first loop.\n3. Process the first loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of the first loop one after the other, not in parallel.\n5. Run the iterations of inner loops for the calculation of 'tmp' and 'y' one after the other, not in parallel.\n6. Perform reduction operation for 'tmp' and 'y' in parallel for the inner loops.\n\nThese are the natural language instructions for the corresponding #pragma values in the given HLS codes.\n\nLet me know if you need further assistance!", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Designate this function as a hardware kernel for acceleration, and apply automatic pipelining to this loop, disable automatic tiling, and parallelize the loop with a factor of 116.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you can use the following concise but comprehensive natural language instructions for the specific pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration'\n\n2. 'Automatically pipeline this loop'\n\n3. 'Break this loop into smaller chunks and process each chunk one by one'\n\n4. 'Run the iterations of this loop in parallel'\n\n5. 'Automatically tile this loop with a factor of 4'\n\n6. 'Automatically parallelize this loop with a factor of __PARA__L0'\n\n7. 'Automatically tile this loop with a factor of 1'\n\n8. 'Automatically parallelize this loop with a factor of __PARA__L1'\n\n9. 'Turn off automatic pipelining for this loop'\n\n10. 'Automatically tile this loop with a factor of 1'\n\n11. 'Automatically parallelize this loop with a factor of __PARA__L2'\n\n12. 'Turn off automatic pipelining for this loop'\n\n13. 'Tile this loop with a factor of __TILE__L3'\n\n14. 'Parallelize this loop with a factor of 30'\n\n15. 'Automatically pipelined this loop with auto{__PIPE__L7}'\n\n16. 'Blachetically parallelize this loop with a factor of __PARA__L7_0'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n \n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the first loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Add parallelism and reduction for the inner loop with a factor of 1.\n\n6. Add parallelism and reduction for the outer loop with a factor of 1.\n\nBy following these instructions, you can generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks - #pragma ACCEL TILE FACTOR=1\n\nAutomatically pipeline this loop - #pragma ACCEL PIPELINE auto{}\n\nRun the iterations of this loop one after the other, not in parallel - #pragma ACCEL PARALLEL FACTOR=auto{116}\n\nProcess the following loops in parallel and apply reduction to the 'tmp' array - #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n\nProcess the following loops in parallel and apply reduction to the 'y' array - #pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n\nTreat the following function as a hardware kernel for acceleration - #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, here are the natural language instructions corresponding to the specific #pragma values to use:\n\n1. For the loop where variable 'j' ranges from 0 to 79, calculate the mean of the data in parallel and pipeline the calculations automatically:\n   \n   Natural language instruction: 'Calculate the mean of the data in parallel and pipeline automatically'\n\n   HLS code:\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   #pragma ACCEL TILE FACTOR=auto{1}\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n   ```\n\n2. For the loop where variable 'j' ranges from 0 to 79, calculate the standard deviation of the data in parallel and pipeline the calculations automatically:\n   \n   Natural language instruction: 'Calculate the standard deviation of the data in parallel and pipeline automatically'\n\n   HLS code:\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   #pragma ACCEL TILE FACTOR=auto{1}\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n   ```\n\n3. For the loop where variable 'i' ranges from 0 to 99 and variable 'j' ranges from 0 to 79, center and reduce the column vectors in parallel and pipeline the calculations automatically with a tile factor of 4:\n   \n   Natural language instruction: 'Center and reduce the column vectors in parallel with a tile factor of 4'\n\n   HLS code:\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   #pragma ACCEL TILE FACTOR=auto{4}\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n   ```\n\n4. For the loop where variable 'i' ranges from 0 to 79 and variable 'j' ranges from 'i + 1' to 79, calculate the correlation matrix in parallel and pipeline the calculations automatically with a parallel factor of 30:\n   \n   Natural language instruction: 'Calculate the correlation matrix in parallel with a parallel factor of 30'\n\n   HLS code:\n   ```c\n   #pragma ACCEL PIPELINE auto{__PIPE__L3}\n   #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n   #pragma ACCEL PARALLEL FACTOR=auto{30}\n   ```\n", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Set the loop to not be automatically pipelined' for the code `#pragma ACCEL PIPELINE auto{off}`\n\n'Set the loop to be tiled with a factor of 8' for the code `#pragma ACCEL TILE FACTOR=auto{8}`\n\n'Set the loop to be run in parallel with a factor of 1' for the code `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n'Apply reduction on the loop with the variable tmp with a factor of 1_0' for the code `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}`\n\n'Apply reduction on the loop with the variable y with a factor of 1_1' for the code `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, use the following comprehensive but concise natural language instruction:\n\n\"Treat the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Process each loop in its original, full form without breaking it into smaller chunks. Run the iterations of the loops one after the other, not in parallel. Treat the inner loops as parallel and determine the parallelism factors automatically.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outer loop.\n3. Process the outer loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of the outer loop one after the other, not in parallel.\n5. Run the inner loop in parallel with a reduction operation on the 'tmp' variable.\n6. Run the inner loop in parallel with a reduction operation on the 'y' variable.\n\nThis set of instructions will correspond to the specified #pragma values in the provided C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the loop and set the tile factor to 1. Run the iterations of the loop in parallel, with a factor of 116.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following natural language instructions:\n\n1. \"Don't automatically pipeline this loop\" - for the loop you want to disable automatic pipelining\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n2. \"Process this loop in its original, full form without breaking it into smaller chunks\" - for the loop you want to keep in its original form without tiling\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. \"Run the iterations of this loop one after the other, not in parallel\" - for the loop you want to execute sequentially\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` (replace __PARA__L0 with the specific factor if necessary)\n\n4. \"Treat the following function as a hardware kernel for acceleration\" - for the function you want to designate as a hardware kernel\n   Use: `#pragma ACCEL kernel`\n\nFor other loops, replace the loop conditions with the specific pragma values using similar natural language instructions.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To enable hardware acceleration for the given C codes, perform the following steps:\n\n1. Mark the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n2. For the first loop, use `#pragma ACCEL PIPELINE auto{}` to enable automatic pipeline.\n3. For the second loop, set the tile factor to 8 using `#pragma ACCEL TILE FACTOR=auto{8}`.\n4. Parallelize the third loop by setting the parallel factor to a specific value using `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`.\n5. Pipeline the fourth loop with `#pragma ACCEL PIPELINE auto{}`.\n6. Set the tile factor to 1 for the fifth loop using `#pragma ACCEL TILE FACTOR=auto{1}`.\n7. Parallelize the sixth loop by setting the parallel factor to a specific value using `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`.\n8. For the seventh loop, turn off automatic pipelining using `#pragma ACCEL PIPELINE auto{off}`.\n9. Set the tile factor to 1 for the eighth loop using `#pragma ACCEL TILE FACTOR=auto{1}`.\n10. Parallelize the ninth loop by setting the parallel factor to a specific value using `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`.\n11. Pipeline the tenth loop with a specific value using `#pragma ACCEL PIPELINE auto{__PIPE__L3}`.\n12. Set the tile factor to a specific value for the tenth loop using `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`.\n13. Parallelize the tenth loop with a specific parallel factor using `#pragma ACCEL PARALLEL FACTOR=auto{30}`.\n\nThese steps will enable hardware acceleration for the given C codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following natural language instructions:\n\n1. For the kernel_correlation function, treat it as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loop for calculating mean and standard deviation.\n3. Process the loop in its original form without breaking it into smaller chunks.\n4. Run the iterations of the loop one after the other, not in parallel, for calculating mean and standard deviation.\n5. Don't automatically pipeline the loop for centering and reducing the column vectors.\n6. Process the loop for centering and reducing the column vectors without breaking it into smaller chunks.\n7. Run the iterations of the loop in parallel for centering and reducing the column vectors.\n8. Automatically pipeline the loop for calculating the correlation matrix.\n9. Process the loop for calculating the correlation matrix with a tile factor of 30.\n10. Run the iterations of the loop in parallel for calculating the correlation matrix.\n\nEach of these instructions corresponds to the respective pragma values in the provided code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for HLS acceleration, use the following natural language instructions for the corresponding #pragma values:\n\n1. For the outermost loop, use automatic pipelining: '#pragma ACCEL PIPELINE auto{}'\n2. For memory access optimization, use automatic full tiling: '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Parallelize the outer loop with automatic parallelization: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n4. Disable pipelining for a specific loop: '#pragma ACCEL PIPELINE auto{off}'\n5. Apply automatic full tiling for a specific loop: '#pragma ACCEL TILE FACTOR=auto{1}'\n6. Parallelize the outer loop with automatic parallelization: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'\n7. Disable automatic pipelining for a specific loop: '#pragma ACCEL PIPELINE auto{}'\n8. Apply automatic full tiling for a specific loop: '#pragma ACCEL TILE FACTOR=auto{2}'\n9. Parallelize the outer loop with automatic parallelization: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n10. Apply specific pipelining and tiling factors for a loop: '#pragma ACCEL PIPELINE auto{__PIPE__L3}', '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'\n11. Parallelize the outer loop with a specific parallelization factor: '#pragma ACCEL PARALLEL FACTOR=auto{30}'\n\nThese natural language instructions correspond to the respective HLS #pragma values to optimize the given C code for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n5. Don't automatically pipeline this loop.\n6. Break down the iterations of this loop into smaller chunks to improve parallelism.\n7. Treat the following loop as one big chunk to execute sequentially, not in parallel.\n8. Don't automatically pipeline this loop.\n9. Break down the iterations of this loop into smaller chunks where each chunk includes 4 iterations.\n10. Run the iterations of this loop in parallel, each chunk containing 4 iterations.\n11. Don't automatically pipeline this loop.\n12. Process this loop in its original, full form without breaking it into smaller chunks.\n13. Run the iterations of this loop one after the other, not in parallel.\n14. Break down the iterations of this loop into smaller chunks to improve parallelism, where each chunk includes 2 iterations.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically enable pipeline for the loop. Automatically determine the tile factor for the loop. Execute the iterations of the loop in parallel using a factor determined automatically. Reduce the variables 'tmp' and 'y' in parallel for the inner loops with factors determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following steps to generate the given HLS codes:\n\n1. Treat the 'kernel_atax' function as a hardware kernel for acceleration.\n\n2. Do not automatically pipeline the loop.\n\n3. Process the loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of the loop one after the other, not in parallel.\n\nHere's the comprehensive but very concise natural language instruction to generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the hardware acceleration of the given C code, use the following natural language instructions for the specific #pragma values:\n\n1. Optimize the loop by turning off automatic pipelining\n   - #pragma ACCEL PIPELINE \"off\"\n\n2. Process the loop in its original, full form without breaking it into smaller chunks\n   - #pragma ACCEL TILE FACTOR=1\n\n3. Run the iterations of the loop one after the other, not in parallel\n   - #pragma ACCEL PARALLEL FACTOR=1\n\n4. Treat the function as a hardware kernel for acceleration\n   - #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for C:\n\nTo generate HLS code for the given C program, use the following comprehensive and concise natural language instructions:\n\n1. Treat the function as a hardware kernel for acceleration: \n   - Add the pragma '#pragma ACCEL kernel' before the function declaration.\n\n2. Automatically pipeline the loop:\n   - Add the pragma '#pragma ACCEL PIPELINE auto{}' before the loop that requires automatic pipelining.\n\n3. Process the loop in its original, full form without breaking it into smaller chunks:\n   - Add the pragma '#pragma ACCEL TILE FACTOR=auto{1}' before the loop that should not be tiled.\n\n4. Run the iterations of the loop one after the other, not in parallel:\n   - Add the pragma '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' before the loop that should not be parallelized based on the factor value.\n\n5. Turn off automatic pipelining for the loop:\n   - Add the pragma '#pragma ACCEL PIPELINE auto{off}' to turn off automatic pipelining for the specified loop.\n\n6. Center the looppe parallelization with a specific factor:\n   - Add the pragma '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' before the loop that needs to be parallelized with a specific factor value.\n\n7. Automatically pipeline the loop with a specific label:\n   - Add the pragma '#pragma ACCEL PIPELINE auto{}' followed by the specific label before the loop that requires automatic pipelining with a specific label.\n\n8. Process the loop with a specific tile factor:\n   - Add the pragma '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}' before the loop that should be tiled with a specific tile factor.\n\n9. Run the iterations of the loop in parallel:\n   - Add the pragma '#pragma ACCEL PARALLEL FACTOR=auto{30}' before the loop that should be parallelized with a specific factor value.\n\nEnsure to replace '__PARA__L0', '__PARA__L1', '__PARA__L4', '__PARA__L5', '__PARA__L6', '__PARA__L7_0', '__PIPE__L3', and '__TILE__L3' with the appropriate values based on the specific requirements of the C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, turn off automatic loop pipelining, set the loop tiling factor to 1 for original form, and specify the parallelization factor for the outer loop as 116, and for the inner loops, use parallel reduction for both `tmp` and `y` with factors 116_0 and 116_1 respectively.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following natural language instruction: \n\"Analyze and transform the following function for hardware acceleration.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loop with variable 'j', automatically pipeline it. For the same loop, break it into smaller chunks to process in parallel. For the loop with variable 'j' again, switch off automatic pipelining. For the same loop, process it in its original, full form without breaking into smaller chunks. For a loop with variable 'i', run the iterations one after the other, not in parallel. Finally, for the loop with variable 'i' and 'j', automatically pipeline and process in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following natural language instructions for #pragma values:\n\n1. 'Don't automatically pipeline this loop' >> #pragma ACCEL PIPELINE \"off\"\n2. 'Process this loop in its original, full form without breaking it into smaller chunks' >> #pragma ACCEL TILE FACTOR=1\n3. 'Run the iterations of this loop one after the other, not in parallel' >> #pragma ACCEL PARALLEL FACTOR=1\n4. 'Treat the following function as a hardware kernel for acceleration' >> #pragma ACCEL kernel", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use natural language instructions as follows:\n\n- For the loop containing the calculation of 'mean' and 'stddev', treat loops as hardware kernel for acceleration by adding `#pragma ACCEL kernel`.\n\n- For the loop calculating 'mean', set the pipeline directive to 'off' by adding `#pragma ACCEL PIPELINE off`.\n\n- For the loop calculating 'mean', keep the loop in its original, full form without breaking it into smaller chunks by adding `#pragma ACCEL TILE FACTOR=1`.\n\n- For the loop calculating 'mean', run the iterations of this loop one after the other, not in parallel by adding `#pragma ACCEL PARALLEL FACTOR=1`.\n\n- For the loop calculating 'stddev', set the pipeline directive to its default value by adding `#pragma ACCEL PIPELINE`.\n\n- For the loop calculating 'stddev', divide the loop into smaller chunks with a factor of 4 by adding `#pragma ACCEL TILE FACTOR=4`.\n\n- For the loop calculating 'stddev', run iterations of this loop in parallel using a default factor by adding `#pragma ACCEL PARALLEL`.\n\n- For the loop centering and reducing column vectors, don't automatically pipeline this loop by adding `#pragma ACCEL PIPELINE off`.\n\n- For the loop centering and reducing column vectors, keep the loop in its original, full form without breaking it into smaller chunks by adding `#pragma ACCEL TILE FACTOR=1`.\n\n- For the loop centering and reducing column vectors, run the iterations of this loop one after the other, not in parallel by adding `#pragma ACCEL PARALLEL FACTOR=1`.\n\n- For the loop calculating the correlation matrix, set the pipeline directive to a specific value by adding `#pragma ACCEL PIPELINE`.\n\n- For the loop calculating the correlation matrix, divide the loop into smaller chunks by adding `#pragma ACCEL TILE FACTOR`.\n\n- For the loop calculating the correlation matrix, run iterations of this loop in parallel using a specific factor by adding `#pragma ACCEL PARALLEL FACTOR`.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop with original, full form without breaking into smaller chunks, and treat the following function as a hardware kernel for acceleration. Additionally, don't automatically pipeline the first loop, and run the iterations of the second loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, the following natural language instruction can be used:\n\n\"Mark the following function as a hardware kernel for acceleration, don't automatically pipeline the loops, process each loop in its original, full form without breaking it into smaller chunks, and run the iterations of the loops one after the other, not in parallel.\"\n\nThis instruction captures the process of marking the function as an HLS hardware kernel, optimizing the loops' processing, and specifying the parallelization and pipelining configurations using the appropriate #pragma values in the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To enable hardware acceleration for the following C codes:\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n\n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    // ... code omitted for brevity\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    // ... code omitted for brevity\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    // ... code omitted for brevity\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    // ... code omitted for brevity\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nUse the following pragma values:\n\n1. Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE \"off\"`\n2. Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n3. Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`\n4. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for hardware acceleration, follow these natural language instructions for pragma values:\n1. Declare the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n\n2. For the loop processing mean calculations, use `#pragma ACCEL PIPELINE auto` to automatically pipeline the loop, `#pragma ACCEL TILE FACTOR=25` to tile the loop with a factor of 25, and `#pragma ACCEL PARALLEL FACTOR=__PARA__L0` to process iterations in parallel.\n\n3. For the loop processing standard deviation calculations, use `#pragma ACCEL PIPELINE auto` to automatically pipeline the loop, `#pragma ACCEL TILE FACTOR=1` to keep the loop in its original form, and `#pragma ACCEL PARALLEL FACTOR=__PARA__L1` to process iterations in parallel.\n\n4. For the loop centering and reducing column vectors, use `#pragma ACCEL PIPELINE off` to turn off automatic loop pipelining, `#pragma ACCEL TILE FACTOR=1` to keep the loop in its original form, and `#pragma ACCEL PARALLEL FACTOR=__PARA__L2` to process iterations in parallel.\n\n5. For the loop calculating correlation matrix, use `#pragma ACCEL PIPELINE __PIPE__L3` to pipeline the loop with a specific factor, `#pragma ACCEL TILE FACTOR=__TILE__L3` to tile the loop with a specific factor, and `#pragma ACCEL PARALLEL FACTOR=2` to process iterations in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate this given HLS code in C, you would use the following natural language instructions:\n1. Treat the 'kernel_correlation' function as a hardware kernel for acceleration.\n2. Automatically pipeline the loops.\n3. Use a tiling factor of 1 for the loops.\n4. Process the loops in parallel factor equal to '__PARA__L0', '__PARA__L4', '__PARA__L1', '__PARA__L5', '__PARA__L2', '__PARA__L6', and '__PARA__L7_0' respectively.\n5. Turn off automatic pipelining for the specific loop.\n6. Set the tiling factor to 4 for the specific loop.\n7. Add an additional pipeline to the specific loop with factor '__PIPE__L3'.\n8. Set the tiling factor to '__TILE__L3' for the specific loop.\n9. Process the loop in parallel factor equal to 2 for the specific loop.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and use the following pragma directives:\n\n1. 'Don't automatically pipeline the loop'\n   - #pragma ACCEL PIPELINE \"off\"\n\n2. 'Process the loop in its original, full form without breaking it into smaller chunks'\n   - #pragma ACCEL TILE FACTOR=1\n\n3. 'Run the iterations of this loop one after the other, not in parallel'\n   - #pragma ACCEL PARALLEL FACTOR=1", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_atax' function as a hardware accelerator kernel. Then, turn off automatic loop pipelining. Set the tiling factor to 1 for the following loop. Finally, execute the iterations of the loop in parallel with a factor of 116 and perform reduction operations on 'tmp' and 'y' in parallel with factors of 116_0 and 116_1 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following C code to be executed as hardware kernels for acceleration and use the specific #pragma values as follows:\n\n1. Use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loop.\n2. Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the loop in its original, full form without breaking it into smaller chunks.\n3. Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' to run the iterations of the loop in parallel.\n4. Use '#pragma ACCEL PIPELINE auto{off}' to turn off automatic pipelining for the loop.\n5. Use '#pragma ACCEL TILE FACTOR=auto{2}' to specify a tile factor of 2 for the loop.\n6. Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' to run the iterations of the loop in parallel.\n7. Use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loop.\n8. Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the loop with a tile factor of 1.\n9. Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' to run the iterations of the loop in parallel.\n10. Use '#pragma ACCEL PIPELINE auto{__PIPE__L3}' to automatically pipeline the loop with a specific pipeline label.\n11. Use '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}' to specify a tile factor for the loop with a specific tile label.\n12. Use '#pragma ACCEL PARALLEL FACTOR=auto{30}' to run the iterations of the loop in parallel with a factor of 30.\n\nThis comprehensive but concise natural language instruction covers the specific #pragma values to use for the given C code, enabling its proper execution as hardware kernels for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, and for the first loop, do not automatically pipeline it, then break it into smaller chunks of 8, and finally run the iterations of the second loop one after the other, not in parallel.\"\n\nUse the following #pragma values respectively:\n- #pragma ACCEL kernel\n- #pragma ACCEL PIPELINE auto{off}\n- #pragma ACCEL TILE FACTOR=auto{8}\n- #pragma ACCEL PARALLEL FACTOR=auto{1}", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Direct all pipelining decisions to the compiler, maintain original loop structure with no tiling, and run loop iterations in parallel using the auto modifier.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Assign the following function as a hardware kernel for acceleration. Then, use automatic pipelining for all loops. Further, set the tile factor to be automatically determined for the first and second loop, and to 20 for the third loop. Finally, parallelize the loops with factors '__PARA__L0', '__PARA__L4', '__PARA__L1', '__PARA__L5', '__PARA__L2', '__PARA__L6', '__PARA__L7_0', and 30 for the m * m correlation matrix calculation loop.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these comprehensive but concise natural language instructions:\n- Treat the function 'kernel_correlation' as a hardware kernel for acceleration.\n- Don't automatically pipeline the first loop, leave it to be optimized automatically.\n- Process the first loop in its original, full form, without breaking it into smaller chunks automatically.\n- Run the iterations of the first loop one after the other, not in parallel.\n- Disable automatic pipelining for the second loop.\n- Break the second loop into smaller chunks to process in parallel with a factor of 2.\n- Run the iterations of the second loop in parallel with a factor of 1.\n- Process the third loop by automatically pipelining it.\n- Break the third loop into smaller chunks to process in parallel with a factor of 1.\n- Run the iterations of the third loop in parallel with a factor of 1.\n- Automatically pipeline the fourth loop.\n- Break the fourth loop into smaller chunks to process in parallel with a factor of 1.\n- Run the iterations of the fourth loop in parallel with a factor of 2.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Process the first loop in its original, full form without breaking it into smaller chunks. Run the iterations of the second loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is a concise natural language instruction for the given HLS codes written in C:\n\n\"Accelerate the 'kernel_correlation' function by pipelining all applicable loops, breaking them into smaller chunks for parallel processing, and treating the function as a hardware kernel for acceleration. When processing each loop, automatically determine the pipeline, tile factor, and parallel factor values to optimize hardware acceleration and maximize performance.\"\n\nI hope this helps! Let me know if you need further assistance.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Mark the function as a hardware kernel for acceleration.\n\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline the loop.\n\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process the loop in its original, full form without breaking it into smaller chunks.\n\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you can use the following natural language instruction to describe the process and specific #pragma values to use:\n\n\"Mark the following function as a hardware kernel for acceleration and then apply the following optimizations: pipeline the loops automatically, process each loop in its original, full form without breaking it into smaller chunks, run the loop iterations in parallel with a specified parallel factor, disable automatic pipelining for specific loop blocks, handle specific pipelining and tiling factors, and run certain loops with a different parallel factor.\"\n\nI hope this comprehensive but concise natural language instruction will help to understand and apply the specific #pragma values and process for generating the given HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Execute the loop in parallel with factor 1 for i in range 0 to 116. Auto-pipeline the loop for i in range 0 to 116. Apply tiling with factor 4 on the loop for i in range 0 to 116. Use parallel reduction with factor 1 for both 'tmp' and 'y' in the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the first loop iterating over 'j' and calculating 'mean':\n   - Automatically pipeline this loop: \n     `#pragma ACCEL PIPELINE auto{}` \n   - Break the loop into smaller chunks, factor = 4:\n     `#pragma ACCEL TILE FACTOR=auto{4}` \n   - Run the iterations of this loop in parallel, factor = variable__PARA__L0:\n     `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` \n\n2. For the loop iterating over 'j' and calculating 'stddev':\n   - Automatically pipeline this loop:\n     `#pragma ACCEL PIPELINE auto{}` \n   - Do not break the loop into smaller chunks, factor = 1:\n     `#pragma ACCEL TILE FACTOR=auto{1}` \n   - Run the iterations of this loop in parallel, factor = variable__PARA__L1:\n     `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` \n\n3. For the loop iterating over 'i' and 'j' to center and reduce the column vectors:\n   - Do not automatically pipeline this loop:\n     `#pragma ACCEL PIPELINE auto{off}` \n   - Do not break the loop into smaller chunks, factor = 1:\n     `#pragma ACCEL TILE FACTOR=auto{1}` \n   - Run the iterations of this loop in parallel, factor = variable__PARA__L2:\n     `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` \n\n4. For the loop iterating over 'i' and 'j' to calculate the correlation matrix:\n   - Automatically pipeline this loop, factor = variable__PIPE__L3:\n     `#pragma ACCEL PIPELINE auto{__PIPE__L3}` \n   - Break the loop into smaller chunks, factor = variable__TILE__L3:\n     `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}` \n   - Run the iterations of this loop in parallel, factor = 2:\n     `#pragma ACCEL PARALLEL FACTOR=auto{2}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n1. Use automatic pipeline for the loops.\n2. Apply tiling with a factor of 25.\n3. Use automatic parallelization for the outer loop by specifying 'auto' for the parallel factor.\n4. Turn off automatic pipeline for the loops.\n5. Set the tile factor to 1 for the loops.\n6. Apply automatic parallelization for the outer loop with proper 'auto' value for the parallel factor.\n7. Apply automatic pipeline for the loops.\n8. Use tiling with a factor of 1.\n9. Use automatic parallelization for the outer loop by specifying the proper 'auto' parallel factor.\n10. Apply automatic pipeline for the loops with a specific name.\n11. Set the tile factor to a specific name for tiling.\n12. Apply automatic parallelization with a specific factor for the loops.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the natural language instructions as below:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   Instruction: \"Treat the following function as a hardware kernel for acceleration\"\n\n   Code: \n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Disable automatic pipelining for this loop.\n   Instruction: \"Don't automatically pipeline this loop\"\n\n   Code:\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   Instruction: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n   Code:\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   Instruction: \"Run the iterations of this loop one after the other, not in parallel\"\n\n   Code:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Treat the following function as a hardware kernel for acceleration\n2. Don't automatically pipeline this loop\n3. Process this loop in its original, full form without breaking it into smaller chunks\n4. Run the iterations of this loop one after the other, not in parallel\n5. Pipeline this loop with a set initiation interval\n6. Break this loop into smaller chunks for efficient hardware acceleration\n7. Run iterations of this loop in parallel with a specified factor\n\nPlease let me know if you need any further assistance!", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nThen, for the loop starting from \"for (i = 0; i < 124; i++)\", don't automatically pipeline this loop, process this loop in its original, full form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel.\nFor the loop starting from \"for (i = 0; i < 116; i++)\", automatically pipeline this loop with a factor of 1, and run it in parallel with 116 instances.\nWithin this loop, parallelize the inner loop with a reduction on the 'tmp' array, and parallelize another inner loop with a reduction on the 'y' array, both with a factor of 116.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following natural language instructions for the specific #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration' for `#pragma ACCEL kernel`\n\n2. 'Don't automatically pipeline this loop' for `#pragma ACCEL PIPELINE \"off\"`\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' for `#pragma ACCEL TILE FACTOR=1`\n\n4. 'Run the iterations of this loop one after the other, not in parallel' for `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these concise natural language instructions:\n\n1. Declare the following function as a hardware kernel for acceleration.\n   - Use '#pragma ACCEL kernel' to define the function as a hardware kernel.\n\n2. Process the loop in its original, full form without breaking it into smaller chunks.\n   - Use '#pragma ACCEL TILE FACTOR=1' to process the loop in its original form.\n\n3. Don't automatically pipeline the loop.\n   - Use '#pragma ACCEL PIPELINE \"off\"' to prevent automatic pipelining of the loop.\n\n4. Run the iterations of the loop one after the other, not in parallel.\n   - Use '#pragma ACCEL PARALLEL FACTOR=1' to execute the loop iterations sequentially, not in parallel.\n   - Use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=116_0' and '#pragma ACCEL PARALLEL reduction=y FACTOR=116_1' for specific parallel optimizations.\n\nFollowing these natural language instructions will generate the given HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in its original, full form without breaking it into smaller chunks, and automatically pipeline this loop and run the iterations of this loop in parallel on hardware while treating the function as a hardware kernel for acceleration. Use the following pragma values: `#pragma ACCEL TILE FACTOR=1`, `#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL PARALLEL FACTOR=auto{116}`, `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}`, and `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following comprehensive but concise natural language instructions:\n\n1. For the kernel function `kernel_correlation`, do not automatically pipeline the loops.\n\n2. Process the nested loops in their original, full form without breaking them into smaller chunks.\n\n3. Run the iterations of the outermost loop one after the other, not in parallel.\n\n4. Treat the function `kernel_correlation` as a hardware kernel for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you would first define the function as a hardware kernel for acceleration using '#pragma ACCEL kernel'. Then, use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loops. Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the loops in their original form without breaking them into smaller chunks. For running iterations of a loop one after the other, use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' for the first loop, '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' for the second loop, and so on. Lastly, for specific pipeline and parallel factors, use the corresponding values such as '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{30}', and '#pragma ACCEL PARALLEL FACTOR=auto{2}'.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration using HLS. Pipeline the loops automatically and do not break the loops into smaller chunks. Run the iterations of the loops in parallel, factorized automatically. Set the tile factor to 1 for the loops. For specific loops, turn off automatic pipelining and set the tile factor to 2, running parallel iterations factorized automatically. Finally, set the pipeline factor to a specific value, and adjust the tile factor and parallel factor for the last loop iterations.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C using natural language instructions, you may use the following concise instructions:\n\n1. \"Treat the following function as a hardware kernel for acceleration\" to generate `#pragma ACCEL kernel`.\n2. \"Don't automatically pipeline this loop\" to generate `#pragma ACCEL PIPELINE \"off\"`.\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\" to generate `#pragma ACCEL TILE FACTOR=1`.\n4. \"Run the iterations of this loop one after the other, not in parallel\" to generate `#pragma ACCEL PARALLEL FACTOR=1`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n5. Don't automatically pipeline this loop.\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n7. Run the iterations of this loop one after the other, not in parallel.\n8. Don't automatically pipeline this loop.\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n10. Run the iterations of this loop one after the other, not in parallel.\n\nThese instructions correspond to the respective #pragma values in the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in its original, full form without breaking it into smaller chunks;\nAutomatically pipeline this loop;\nRun the iterations of this loop in parallel, the number of parallelism equals 116;\nRun the inner loops with parallel reduction of variable 'tmp' and 'y', the number of parallelism equals 116.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instructions for the respective #pragma values:\n\n1. For loop with variable 'j' - Automatically pipeline this loop: \n   Use `#pragma ACCEL PIPELINE auto{}`\n\n2. For loop with variable 'j' - Process this loop in its original, full form without breaking it into smaller chunks: \n   Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. For loop with variable 'j' - Run the iterations of this loop one after the other, not in parallel: \n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n4. After the first loop with variable 'j' - Turn off automatic pipelining: \n   Use `#pragma ACCEL PIPELINE auto{off}`\n\n5. After the first loop with variable 'j' - Process this loop in its original, full form without breaking it into smaller chunks: \n   Use `#pragma ACCEL TILE FACTOR=auto{1}`\n\n6. After the first loop with variable 'j' - Run the iterations of this loop one after the other, not in parallel: \n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n7. After the second loop with variable 'i' - Automatically pipeline this loop: \n   Use `#pragma ACCEL PIPELINE auto{}`\n\n8. After the second loop with variable 'i' - Process this loop in its original, full form without breaking it into smaller chunks: \n   Use `#pragma ACCEL TILE FACTOR=auto{8}`\n\n9. After the second loop with variable 'i' - Run the iterations of this loop one after the other, not in parallel: \n   Use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n10. After the third loop with variables 'i' and 'j' - Specify pipeline factor: \n    Use `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n11. After the third loop with variables 'i' and 'j' - Specify tiling factor: \n    Use `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n12. After the third loop with variables 'i' and 'j' - Run the iterations of this loop in parallel with a factor of 2: \n    Use `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\nThis set of natural language instructions corresponds to the specific #pragma values used in the given HLS code written in C, and when followed, will yield the same functionality.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the natural language instruction below:\n\n\"Designate the following function as a hardware kernel for acceleration and then automatically pipeline the loops for parallel processing and optimization.\"\n\nYou can use the following specific #pragma values:\n1. '#pragma ACCEL kernel' to designate the function as a hardware kernel for acceleration.\n2. '#pragma ACCEL PIPELINE auto' to automatically pipeline the loops for optimization.\n3. '#pragma ACCEL TILE FACTOR=auto{1}' to optimize the tiling factor for the loop iterations.\n4. '#pragma ACCEL PARALLEL FACTOR=auto{116}' to automatically parallelize the loop iterations with a factor of 116.\n5. '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}' to automatically parallelize the reduction operation for the variable 'tmp' with a factor of 116_0.\n6. '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}' to automatically parallelize the reduction operation for the variable 'y' with a factor of 116_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, don't automatically pipeline the first loop, process the loop in its original, full form without breaking it into smaller chunks, and run the iterations of the loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, you should describe the process in the following natural language instruction:\n\n\"Designate the function as a hardware kernel for acceleration and handle each loop by pipeline, tile, and parallel processing with specific factor values. For the first loop, disable automatic pipelining, use a tile factor of 1, and parallelize with factor value __PARA__L0. For the second loop, utilize automatic pipelining, a tile factor of 1, and parallelize with factor value __PARA__L1. For the third loop, turn off automatic pipelining, use a tile factor of 1, and parallelize with factor value __PARA__L2. Lastly, pipeline with specified factor values, tile with specific factor values, and parallelize with factor value 2 for the final loop.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations in sequential order, not in parallel, using all available hardware resources, and pipeline the loop to improve throughput.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To the following HLS codes in C, the following natural language instructions should be applied:\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n  // Don't automatically pipeline this loop\n#pragma ACCEL PIPELINE auto{}\n\n  // Process this loop in its original, full form without breaking it into smaller chunks\n#pragma ACCEL TILE FACTOR=auto{1}\n\n  // Run the iterations of this loop one after the other, not in parallel\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n    // Run the iterations of this loop one after the other, not in parallel\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n  // Don't automatically pipeline this loop\n#pragma ACCEL PIPELINE auto{}\n\n  // Process this loop in its original, full form without breaking it into smaller chunks\n#pragma ACCEL TILE FACTOR=auto{2}\n\n  // Run the iterations of this loop one after the other, not in parallel\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n    // Run the iterations of this loop one after the other, not in parallel\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j], (double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    /* The following in an inelegant but usual way to handle\n       near-zero std. dev. values, which below would cause a zero-\n       divide. */\n    stddev[j] = (stddev[j] <= eps? 1.0 : stddev[j]);\n  }\n  /* Center and reduce the column vectors. */\n  \n  // Don't automatically pipeline this loop\n#pragma ACCEL PIPELINE auto{}\n\n  // Process this loop in its original, full form without breaking it into smaller chunks\n#pragma ACCEL TILE FACTOR=auto{1}\n\n  // Run the iterations of this loop one after the other, not in parallel\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n    // Run the iterations of this loop one after the other, not in parallel\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n  /* Calculate the m * m correlation matrix. */\n  \n  // Don't automatically pipeline this loop\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n  // Process this loop in its original, full form without breaking it into smaller chunks\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n  // Run the iterations of this loop in parallel with a factor of 30\n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n    // Don't automatically pipeline this loop\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n\n    // Run the iterations of this loop in parallel\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n      // Run the iterations of this loop in parallel\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the relevant #pragma values: \n\n1. Process this loop sequentially without pipeline optimization: \"#pragma ACCEL PIPELINE auto{off}\"\n\n2. Process this loop in its original, full form without breaking it into smaller chunks: \"#pragma ACCEL TILE FACTOR=1\"\n\n3. Run the iterations of this loop in parallel with a specific parallel factor: \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\" (for the first loop), \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\" (for the second loop), \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\" (for the third loop), and \"#pragma ACCEL PARALLEL FACTOR=auto{2}\" (for the fourth loop)\n\n4. Treat the following function as a hardware kernel for acceleration: \"#pragma ACCEL kernel\"\n\nBy using these natural language instructions, the given C HLS codes will be generated with the specified #pragma values for hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Use '#pragma ACCEL kernel' to mark the following function as a hardware kernel for acceleration.\n\n2. To turn off automatic pipelining for a loop, use '#pragma ACCEL PIPELINE auto{off}'.\n\n3. To process a loop in its original, full form without breaking it into smaller chunks, use '#pragma ACCEL TILE FACTOR=auto{1}'.\n\n4. Run the iterations of a loop one after the other, not in parallel, by using '#pragma ACCEL PARALLEL FACTOR=auto{116}'.\n\n5. For parallel reduction over the 'tmp' variable, use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}' within the loop.\n\n6. Similarly, for parallel reduction over the 'y' variable, use '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}' within the loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is treated as a hardware kernel for acceleration. For the loop that calculates the mean vector, don't automatically pipeline it, process it in its original, full form without breaking it into smaller chunks, and run its iterations one after the other, not in parallel. For the loop that calculates the standard deviation vector, pipeline it automatically, process it as its original, full form, and run its iterations one after the other, not in parallel. For the loop that centers and reduces the column vectors, automatically pipeline it and process it with a tile factor of 8, and run its iterations in parallel. Finally, for the loop that calculates the correlation matrix, pipeline it with a factor of 2 and process it with a tile factor of 3, running its iterations in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Treat the following function as a hardware kernel for acceleration\n2. Automatically pipeline the following loop\n3. Tile the following loop with a factor of 2\n4. Run the iterations of the following loop in parallel with a factor of 16\n\nThese instructions correspond to the following HLS codes:\n- #pragma ACCEL kernel\n- #pragma ACCEL PIPELINE auto{flatten}\n- #pragma ACCEL TILE FACTOR=auto{2}\n- #pragma ACCEL PARALLEL FACTOR=auto{16}", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire 'kernel_correlation' function as a hardware kernel for acceleration and use the following pragmas:\n- Use automatic pipelining for all loops\n- Automatically calculate the tile factor for loops\n- Run iterations of the loops in parallel using an automatically calculated factor", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given code in C with HLS pragmas, use the following concise natural language instruction:\n\n\"Transform the 'kernel_correlation' function into a hardware accelerator kernel. Turn off automatic pipelining for the loops, and process each loop in its original, unbroken form. Run the loop iterations one after the other, not in parallel. Utilize automatic pipelining with flattening for the next loop. Use automatic pipelining for another loop with specific tile factors, and process the iterations in parallel as specified. Finally, apply automatic pipelining, tiling, and parallel processing for the last loop.\"\n\nI hope that helps! Let me know if you need further assistance.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n    \n\"Treat the following function as a hardware kernel for acceleration. Automatically pipeline the loops. Set the tile factor to 1. Run the iterations of the first loop in parallel with a factor of 116, and apply parallel reduction with a factor of 116 to the 'tmp' array. Finally, apply parallel reduction with a factor of 116 to the 'y' array. \"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Describe the process of code: This function 'stencil3d' represents a hardware kernel for acceleration. It performs a 3D stencil computation on the arrays 'orig' and 'sol'.\n\nSpecify #pragma values:\n- \"Don't automatically pipeline this loop\": #pragma ACCEL PIPELINE auto{off}\n- \"Process this loop in its original, full form without breaking it into smaller chunks\": #pragma ACCEL TILE FACTOR=auto{1}\n- \"Run the iterations of this loop one after the other, not in parallel\": #pragma ACCEL PARALLEL FACTOR=auto{4}\n- \"Treat the following function as a hardware kernel for acceleration\": #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following comprehensive but very concise description for the specific #pragma values:\n\n- 'Don't automatically pipeline this loop': \n  Use '#pragma ACCEL PIPELINE \"off\"'\n\n- 'Process this loop in its original, full form without breaking it into smaller chunks':\n  Use '#pragma ACCEL TILE FACTOR=1'\n\n- 'Run the iterations of this loop one after the other, not in parallel':\n  Use '#pragma ACCEL PARALLEL FACTOR=1'\n\n- 'Treat the following function as a hardware kernel for acceleration':\n  Use '#pragma ACCEL kernel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and apply the following pragma values:\n- Disable automatic loop pipelining\n- Apply tiling with a factor of 8\n- Run loop iterations in parallel with a factor of 1\n- Use parallel reduction for the 'tmp' variable and 'y' variable with a factor of 1 for both\n\npragma ACCEL kernel\npragma ACCEL PIPELINE auto{off}\npragma ACCEL TILE FACTOR=auto{8}\npragma ACCEL PARALLEL FACTOR=auto{1}\npragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\npragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: \n   ```\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline this loop:\n   ```\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   ```\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel, using 116 as a factor for parallelization:\n   ```\n   #pragma ACCEL PARALLEL FACTOR=116\n   ```\n\n5. Run the iterations of the inner loop in parallel and perform a reduction on the array \"tmp\":\n   ```\n   #pragma ACCEL PARALLEL reduction=tmp FACTOR=116\n   ```\n\n6. Run the iterations of the inner loop in parallel and perform a reduction on the array \"y\":\n   ```\n   #pragma ACCEL PARALLEL reduction=y FACTOR=116\n   ```\n\nApplying these natural language instructions as #pragma values will generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Treat the iterations of this loop as non-parallel and execute them one after the other, not in parallel.\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n\n7. Treat the following loop as a fully sequential loop and execute the iterations one after the other, not in parallel.\n\n8. Run the iterations of this loop one after the other, not in parallel.\n\n9. Don't automatically pipeline this loop.\n\n10. Process this loop in its original, full form without breaking it into smaller chunks.\n\n11. Run the iterations of this loop one after the other, not in parallel.\n\n12. Treat the iterations of this loop as non-parallel and execute them one after the other, not in parallel.\n\n13. Don't automatically pipeline this loop.\n\n14. Process this loop in its original, full form without breaking it into smaller chunks.\n\n15. Run the iterations of this loop one after the other, not in parallel.\n\n16. Process the iterations of this loop in their original form without breaking them into smaller tiles.\n\n17. Run the iterations of this loop with a parallel factor of 2.\n\nThese concise natural language instructions correspond to the specified #pragma values in the given HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and then for the loop:\n\n- Disable automatic loop pipelining\n- Use a tile factor of 1 for the loop\n- Parallelize the loop with a factor of 32", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop in parallel with a factor of 30.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS code written in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   \n```c\n#pragma ACCEL kernel\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n...\n```\n\n2. Automatically pipeline this loop.\n\n```c\n#pragma ACCEL PIPELINE auto{}\nfor (j = 0; j < 80; j++) {\n...\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\nfor (j = 0; j < 80; j++) {\n...\n```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\nfor (j = 0; j < 80; j++) { \n...\n```\n\n5. Turn off automatic pipelining for this loop.\n\n```c\n#pragma ACCEL PIPELINE auto{off}\nfor (j = 0; j < 80; j++) {\n...\n```\n\n6. Apply a tile factor of 8 for this loop.\n\n```c\n#pragma ACCEL TILE FACTOR=auto{8}\nfor (j = 0; j < 80; j++) {\n...\n```\n\n7. Run the iterations of this loop in parallel, with specific parallelization factor \"__PARA__L1\".\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\nfor (j = 0; j < 80; j++) {\n...\n```\n\n8. Automatically pipeline this loop.\n\n```c\n#pragma ACCEL PIPELINE auto{}\nfor (i = 0; i < 100; i++) {\n...\n```\n\n9. Apply a tile factor of 1 for this loop.\n\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\nfor (i = 0; i < 100; i++) {\n...\n```\n\n10. Run the iterations of this loop in parallel, with specific parallelization factor \"__PARA__L2\".\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\nfor (i = 0; i < 100; i++) {\n...\n```\n\n11. Automatically pipeline this loop with the specific pipeline factor \"__PIPE__L3\".\n\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\nfor (i = 0; i < 80 - 1; i++) {\n...\n```\n\n12. Apply a tile factor with specific factor \"__TILE__L3\" for this loop.\n\n```c\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\nfor (i = 0; i < 80 - 1; i++) {\n...\n```\n\n13. Run the iterations of this loop in parallel, with specific parallelization factor 30.\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{30}\nfor (i = 0; i < 80 - 1; i++) {\n...\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code, use the following natural language instructions for the specific #pragma values:\n\n1. Enable pipelining for this loop automatically: `#pragma ACCEL PIPELINE auto{}`\n2. Break this loop into smaller chunks with an automatic tile factor: `#pragma ACCEL TILE FACTOR=auto{1}`\n3. Run iterations of this loop in parallel with an automatic parallel factor of 30: `#pragma ACCEL PARALLEL FACTOR=auto{30}`\n4. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the outer loop to a factor of 1. Pipeline the inner loop to a factor of 4. Tile both loops to a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then for each loop, don't automatically pipeline it, process it in its original, full form without breaking it into smaller chunks, and run the iterations of each loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks, then automatically pipeline this loop, and run the iterations of this loop in parallel with a factor of 30. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code, use the following comprehensive but concise natural language instruction for the corresponding #pragma values:\n\n1. Ensure this loop is not automatically pipelined:  \n   `Don't automatically pipeline this loop`\n\n2. Retain the original, full form of this loop without breaking it into smaller chunks:  \n   `Process this loop in its original, full form without breaking it into smaller chunks`\n\n3. Adjust the parallelism factor for this loop based on optimization considerations:  \n   `Adjust the parallelism factor for this loop based on optimization considerations`\n\n4. Treat the following function as a hardware kernel for acceleration:  \n   `Treat the following function as a hardware kernel for acceleration`", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, then automatically pipeline all loops, tile the loops with a factor of 25, and run the iterations of the loop in parallel with a factor of __PARA__L0. After that, automatically pipeline all loops again, tile the loops with a factor of 1, and run the iterations of the loop in parallel with a factor of __PARA__L1. Then automatically pipeline all loops with a factor of off, tile the loops with a factor of 1, and run the iterations of the loop in parallel with a factor of __PARA__L2. Lastly, automatically pipeline the loops with a factor of __PIPE__L3, tile the loops with a factor of __TILE__L3, and run the iterations of the loop in parallel with a factor of 30.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in a multi-threaded manner with a factor of 32 for parallelization and pipeline the loop automatically.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Don't automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel. Treat the iterations of this loop one after the other, not in parallel. Don't automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop in a non-parallel manner. Run the iterations of this loop in a non-parallel manner. Don't automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original form without breaking them into smaller chunks, don't automatically pipeline the loops, and run the iterations of the loops in parallel. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Generate hardware kernel and pipeline the outer loop and the inner loop, with auto off, then tile the loops with a factor of 1, and parallelize the innermost loop with a factor of 4.\"", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the first loop, ensure it is not automatically pipelined: \n   Use '#pragma ACCEL PIPELINE auto{off}'\n\n2. Divide the loop into smaller chunks for tiling with a factor of 2:\n   Use '#pragma ACCEL TILE FACTOR=auto{2}'\n\n3. Run the iterations of the loop one after the other, not in parallel, with a parallel factor of 1:\n   Use '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n\n4. For the inner loop, perform reduction for variable 'tmp' with a parallel factor of 1:\n   Use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}'\n\n5. For the inner loop, perform reduction for variable 'y' with a parallel factor of 1:\n   Use '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}'\n\nBy following these instructions, you can generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given loop in C, you should use the following #pragma directives:\n\n1. Use #pragma ACCEL kernel to treat the following function as a hardware kernel for acceleration.\n\n2. Use #pragma ACCEL PIPELINE auto{off} to avoid automatic pipelining of the outer loop.\n\n3. Use #pragma ACCEL TILE FACTOR=1 to process the loop in its original, full form without breaking it into smaller chunks.\n\n4. Use #pragma ACCEL PIPELINE auto{__PIPE__L1} to automatically pipeline the inner loop with a specific factor.\n\n5. Use #pragma ACCEL TILE FACTOR=auto{__TILE__L1} to automatically tile the inner loop with a specific factor.\n\n6. Use #pragma ACCEL PARALLEL FACTOR=4 to run the iterations of the innermost loop in parallel with a specific factor.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire function as a hardware kernel for acceleration. Disable automatic pipelining for the outer loop and set tile factor to 1. Enable automatic pipelining for the inner loop with a custom pipeline factor. Enable automatic parallelization with a custom parallel factor for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. To treat the following function as a hardware kernel for acceleration, use the directive: '#pragma ACCEL kernel'.\n\n2. For the loop that calculates 'y[i]', don't automatically pipeline this loop. Use the directive: '#pragma ACCEL PIPELINE \"off\"'.\n\n3. Process the following loop in its original, full form without breaking it into smaller chunks, by using the directive: '#pragma ACCEL TILE FACTOR=1'.\n\n4. Run the iterations of the following loop one after the other, not in parallel, with the directive: '#pragma ACCEL PARALLEL FACTOR=1'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Accelerate the following stencil3d function as a hardware kernel. Do not automatically pipeline the loops. Use a tile factor of 1 for the outer loop. For the inner loops, automatically pipeline with a pipeline level of L1 and use a tile factor of L1. Parallelize the innermost loop with a parallel factor of 4.'", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n\n1. Declare the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n\n2. Don't automatically pipeline the loop for parameter 'j' inside the function using `#pragma ACCEL PIPELINE \"auto\"`.\n\n3. Break the loop iterations into smaller chunks by a factor of 4 using `#pragma ACCEL TILE FACTOR=4`.\n\n4. Run the iterations of loops one after the other, not in parallel, using `#pragma ACCEL PARALLEL FACTOR=1` for the loop 'j'.\n\n5. Turn off automatic loop pipelining using `#pragma ACCEL PIPELINE \"off\"`.\n\n6. Process the loop in its original, full form without breaking it into smaller chunks using `#pragma ACCEL TILE FACTOR=1`.\n\n7. Run the iterations of loops one after the other, not in parallel, using `#pragma ACCEL PARALLEL FACTOR=1` for the loop 'j'.\n\n8. Turn off automatic loop pipelining using `#pragma ACCEL PIPELINE \"off\"`.\n\n9. Process the loop in its original, full form without breaking it into smaller chunks using `#pragma ACCEL TILE FACTOR=1`.\n\n10. Run the iterations of loops one after the other, not in parallel, using `#pragma ACCEL PARALLEL FACTOR=1` for the loops 'i' and 'j' within the function.\n\n11. Pipelining the loop automatically with a specified label using `#pragma ACCEL PIPELINE \"label\"`.\n\n12. Break the loop iterations into smaller chunks by a factor specified using `#pragma ACCEL TILE FACTOR=factor`.\n\n13. Run the iterations of loops in parallel with a specified factor using `#pragma ACCEL PARALLEL FACTOR=factor`.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. \nDon't automatically pipeline the first loop. \nProcess the first loop in its original, full form without breaking it into smaller chunks. \nRun the iterations of the second loop in parallel with a factor of 16. \nWithin the second loop, perform reduction operations for the variable tmp in parallel with a factor of 16_0. \nWithin the second loop, perform reduction operations for the variable y in parallel with a factor of 16_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code using HLS pragmas, use the following concise natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop in parallel with a factor of 32", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Description: Designate the function as a hardware kernel for acceleration\n   - #pragma value: #pragma ACCEL kernel\n\n2. Don't automatically pipeline this loop\n   - Description: Disable automatic loop pipelining\n   - #pragma value: #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Description: Set the tile factor to 1 for the loop, preserving its original form\n   - #pragma value: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Description: Set the parallel factor to 1, executing loop iterations sequentially\n   - #pragma value: #pragma ACCEL PARALLEL FACTOR=1\n\n5. Treat the following loop as a flattened loop\n   - Description: Flatten the loop structure for efficient processing\n   - #pragma value: #pragma ACCEL PIPELINE \"flatten\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outer loop in parallel with a factor of 116, pipeline the inner loops automatically, and use a tile factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware kernel for acceleration:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire function as a hardware kernel for acceleration. Do not automatically pipeline the loops. Process the loops in their original form without breaking them into smaller chunks. Run the iterations of the loops in parallel with the specified parallel factor.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks, run the iterations of this loop in parallel to a factor of 6, and automatically pipeline this loop. Then treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the function `kernel_correlation` as a hardware kernel for acceleration.\n\n2. Process the loops in their original, full form without breaking them into smaller chunks.\n\n3. Run the iterations of the loops in parallel with a factor of `__PARA__L0` and `__PARA__L4` separately.\n\n4. Don't automatically pipeline the loops.\n\n5. Use a tile factor of 8 for one loop and a tile factor of 1 for three other loops.\n\n6. Pipeline one loop with a specific factor and another loop with a different factor.\n\n7. Use a parallel factor of 2 for one loop and a parallel factor of `__PARA__L7_0` for another loop.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the hardware acceleration of the given C code, use the following natural language instruction: \"Enable the loop to be pipelined automatically, with a tile factor of 2, and run iterations of the loop in parallel with a factor of 5. Treat the function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply automatic pipelining to this loop using `#pragma ACCEL PIPELINE auto{}`. Apply automatic tiling to this loop with a factor of 1 using `#pragma ACCEL TILE FACTOR=auto{1}`. Parallelize this loop with a factor of 16 using `#pragma ACCEL PARALLEL FACTOR=auto{16}`. Use parallel reduction with a factor of 16 for the variable 'tmp' using `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}`, and use parallel reduction with a factor of 16 for the variable 'y' using `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}`. Lastly, treat the following function as a hardware kernel by using `#pragma ACCEL kernel`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the outer loop and inner loop, and don't automatically tile the loops, and run the innermost loop iterations in parallel with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To understand the process of the given codes and the specific #pragma values in natural language, you may use the following comprehensive but concise instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Don't automatically pipeline this loop.\n\n6. Process this loop in chunks of 20.\n\n7. Run the iterations of this loop in parallel with a factor of 1.\n\n8. Flatten the pipeline of this loop.\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n\n10. Run the iterations of this loop in parallel with a factor of 1.\n\n11. Flatten the pipeline of this loop.\n\n12. Process this loop in chunks of 1.\n\n13. Run the iterations of this loop in parallel with a factor of 1.\n\n14. Flatten the pipeline of this loop.\n\n15. Process this loop in chunks of 1.\n\n16. Run the iterations of this loop in parallel with a factor of 1.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the 'ellpack' function in C code:\n- Use automatic pipelining for performance optimization: `#pragma ACCEL PIPELINE auto{}`\n- Apply tiling with a factor of 8 for loop optimization: `#pragma ACCEL TILE FACTOR=8`\n- Increase parallelism with a factor of 6 for improved hardware acceleration: `#pragma ACCEL PARALLEL FACTOR=6`\n- Treat the 'ellpack' function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Use manual pipelining for this loop' for '#pragma ACCEL PIPELINE auto{off}', and 'Divide this loop into smaller chunks of appropriate size' for '#pragma ACCEL TILE FACTOR=auto{1}', and 'Run the iterations of this loop in parallel with a factor of 4' for '#pragma ACCEL PARALLEL FACTOR=auto{4}', and 'Treat the following function as a hardware kernel for acceleration' for '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following nested loops for 3D stencil computation with hardware acceleration. Utilize automatic pipeline optimization and a tile factor of 1 for both loop levels. Parallelize the innermost loop with a factor of 4 to fully exploit hardware parallelism. Mark the enclosing function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks.\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\nRun the iterations of this loop one after the other, not in parallel.\n\n#pragma ACCEL PARALLEL FACTOR=auto{116}", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Pipelining for the loop should be turned off' for '#pragma ACCEL PIPELINE auto{off}', 'Use the original loop size without tiling' for '#pragma ACCEL TILE FACTOR=auto{1}', 'Run the iterations of the loop in parallel with a factor of 116' for '#pragma ACCEL PARALLEL FACTOR=auto{116}', 'Perform parallel reduction operation on the variable tmp and use a factor of 116_0' for '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}', and 'Perform parallel reduction operation on the variable y with a factor of 116_1' for '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Automatically pipeline this loop.\n   - Use: `#pragma ACCEL PIPELINE auto{}`\n\n3. Break this loop into smaller chunks and process them in parallel.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{2}`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` for the outer loop and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}` for the inner loop.\n\n5. Turn off pipeline for this loop.\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n7. Run the iterations of this loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` for the outer loop and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}` for the inner loop\n\n8. Turn off pipeline for this loop.\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n10. Run the iterations of this loop one after the other, not in parallel.\n    - Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` for the outer loop and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}` for the inner loop\n\n11. Set specific pipeline and tiling configurations for this loop.\n    - Use: `#pragma ACCEL PIPELINE auto{__PIPE__L3}` and `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}` respectively.\n\n12. Break this loop into smaller chunks and process them in parallel.\n    - Use: `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the outer loop, `#pragma ACCEL PIPELINE auto{__PIPE__L7}` for the middle loop, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}` for the inner loop.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Convert the following function to a hardware kernel for acceleration, automatically pipeline the outer loop, use an automatic tiling factor of 8, and parallelize the inner loop with a factor of 6'", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. Automatically pipeline this loop. Run the iterations of this loop in parallel with a factor of 16. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but very concise natural language instructions:\n\n1. For the kernel_correlation function, treat it as a hardware kernel for acceleration.\n2. Auto-pipeline the loops unless explicitly turned off.\n3. Apply default tiling factor of 1 to break loops into smaller chunks.\n4. Parallelize the loops by a specified factor, __PARA__L0 for the first loop, __PARA__L1 for the second loop, __PARA__L2 for the third loop and so on.\n5. Specifically, turn off auto-pipelining for the second loop after the normalization process.\n6. Set a tiling factor of 2 for the third loop to map the loop iteration to two different compute units.\n7. Finally, apply specific pipeline and parallelization factors according to the loop iteration and computations as indicated in the original C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the specific #pragma values:\n\n1. For the loop with variable 'j' iterating over 80 and calculating mean:\n   - Process this loop in its original, full form without breaking it into smaller chunks (#pragma ACCEL TILE FACTOR=1)\n   - Don't automatically pipeline this loop (#pragma ACCEL PIPELINE \"off\")\n   - Run the iterations of this loop one after the other, not in parallel (#pragma ACCEL PARALLEL FACTOR=1)\n\n2. For the loop with variable 'j' iterating over 80 and calculating stddev:\n   - Run the iterations of this loop one after the other, not in parallel (#pragma ACCEL PARALLEL FACTOR=1)\n   - Set the loop to be pipelined automatically (#pragma ACCEL PIPELINE auto{})\n   - Process this loop in chunks of 2 iterations at a time (#pragma ACCEL TILE FACTOR=2)\n\n3. For the loop with variables 'i' and 'j' specifically processing data:\n   - Center and reduce the column vectors by treating the loop as a flattened pipeline (#pragma ACCEL PIPELINE auto{flatten})\n   - Process this loop in its original, full form without breaking it into smaller chunks (#pragma ACCEL TILE FACTOR=1)\n   - Run the iterations of this loop one after the other, not in parallel (#pragma ACCEL PARALLEL FACTOR=1)\n\n4. For the loop calculating the correlation matrix:\n   - Set the loop to be automatically pipelined with specific behavior (#pragma ACCEL PIPELINE auto{__PIPE__L3})\n   - Process this loop in chunks in an automatic manner (#pragma ACCEL TILE FACTOR=auto{__TILE__L3})\n   - Run the iterations of this loop one after the other, not in parallel (#pragma ACCEL PARALLEL FACTOR=1)\n\nRemember, also specify the '#pragma ACCEL kernel' to treat the function as a hardware kernel for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, don't automatically pipeline the first loop, process the loop in its original form without breaking it into smaller chunks, and run the iterations of the loop in parallel with a factor of 16. For the inner loops, parallelize with a reduction on the 'tmp' array with a factor of 16 and on the 'y' array with a factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, then disable automatic pipelining for the outer loop and use a tile factor of 1 for tiling. Inside the loop, enable automatic pipelining for level 1 and use a tile factor of level 1. Lastly, run the iterations of the innermost loop in parallel with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize this C code for hardware acceleration, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, apply loop-level pipelining, and tile the loops with factor 1. Parallelize the innermost loop with a factor of 4.\"\n\nThis instruction corresponds to the following HLS pragma values:\n\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{4}", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C codes, use the following comprehensive but concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline the loops.\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process the loops with the original, full form without breaking them into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of the loops one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each iteration of the inner loop sequentially, not in parallel and treat the following function as a hardware kernel for acceleration.\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, use the following natural language instructions for the specific #pragma values:\n\n1. Use the function as a hardware kernel for acceleration.\n2. Automatically pipeline the loop for optimization.\n3. Set the tile factor for the loop to auto.\n4. Set the parallel factor for the loop to auto with a value of 16.\n\nBy following these natural language instructions, you can generate the given HLS codes written in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the specific #pragma values:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.\n\nThese instructions will lead to the respective HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive natural language instruction for the corresponding #pragma values:\n\n1. 'Treat this function as a hardware kernel for acceleration'\n   - Use: `#pragma ACCEL kernel`\n   - Description: Instructs the compiler to treat the following function as a hardware kernel for acceleration.\n\n2. 'Don't automatically pipeline this loop'\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n   - Description: Disables automatic loop pipelining for the subsequent loop.\n\n3. 'Break this loop into smaller chunks with a factor of 8 for tiling'\n   - Use: `#pragma ACCEL TILE FACTOR=auto{8}`\n   - Description: Instructs the compiler to break the loop into smaller chunks with a tiling factor of 8.\n\n4. 'Run the iterations of this loop one after the other, not in parallel with a factor of 1'\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n   - Description: Executes the loop iterations sequentially, not in parallel, with a factor of 1.\n\n5. 'Run this loop in parallel with reduction operation on the variable tmp'\n   - Use: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}`\n   - Description: Executes the loop in parallel with reduction operation on the variable `tmp`.\n\n6. 'Run this loop in parallel with reduction operation on the variable y'\n   - Use: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}`\n   - Description: Executes the loop in parallel with reduction operation on the variable `y`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n    - Natural language instruction: \"Treat this function as a hardware kernel for acceleration.\"\n    - #pragma value: #pragma ACCEL kernel\n\n2. Don't automatically pipeline this loop.\n    - Natural language instruction: \"Disable automatic loop pipelining.\"\n    - #pragma value: #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n    - Natural language instruction: \"Set the loop tile factor to 1 for this specific loop.\"\n    - #pragma value: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel.\n    - Natural language instruction: \"Disable parallel execution for this loop.\"\n    - #pragma value: #pragma ACCEL PARALLEL FACTOR=1, or simply remove the parallel pragma for certain loops.\n\nThese natural language instructions can be used to understand and generate the pragma directives in the given HLS code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following pragmas to optimize the loops:\n1. For the first nested loop, automatically pipeline the loop.\n2. For the second nested loop, process it in its original, full form without breaking it into smaller chunks and run the iterations of this loop one after the other, not in parallel.\n3. For the third nested loop, automatically pipeline the loop and apply a tile factor of 30 to optimize memory access.\n4. For the fourth nested loop, apply a pipeline off pragma and a tile factor of 1 to optimize memory access.\n5. For the fifth nested loop, apply a pipeline pragma and a tile factor of 1 to optimize memory access and run iterations in parallel with a factor of 2.\n6. For the sixth nested loop, apply a pipeline pragma, a tile factor of auto, and run iterations in parallel with a factor of auto to optimize memory access.\n\nThis will help to optimize the hardware acceleration and memory access for the given C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Automatically pipeline the first loop. Automatically tile the second loop with a factor of 4. Run the iterations of the second loop in parallel with a factor of 16. Inside the second loop, perform reduction operations on 'tmp' and 'y' arrays in parallel with factors of 16_0 and 16_1, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Transform this function into a hardware kernel for acceleration, and automatically pipeline the loop with a factor of 8 and parallelize the loop with a factor of 6.'", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To specify hardware acceleration and optimization directives for a given function, use the following natural language instructions:\n\n1. Indicate that the function should be treated as a hardware kernel for acceleration.\n    - Natural language instruction: \"Treat this function as a hardware kernel for acceleration\"\n\n2. Disable automatic loop pipelining for the designated loop.\n    - Natural language instruction: \"Don't automatically pipeline this loop\"\n\n3. Set the tiling factor for loop optimization to 8.\n    - Natural language instruction: \"Divide this loop into smaller chunks with a tiling factor of 8\"\n\n4. Set the parallelization factor for the loop to 6 to execute iterations in parallel.\n    - Natural language instruction: \"Execute iterations of this loop in parallel with a factor of 6\"", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Treat the following function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline this loop.\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code process instruction:\n\nTo generate HLS codes for the given C program, follow these concise natural language instructions:\n1. Mark the function 'kernel_correlation' as a hardware kernel for acceleration using '#pragma ACCEL kernel'.\n2. Turn off automatic pipelining for the loop using '#pragma ACCEL PIPELINE auto{off}'.\n3. Set the tile factor for loop tiling to automatic value 2 using '#pragma ACCEL TILE FACTOR=auto{2}'.\n4. Parallelize the outer loop using a parallel factor calculated automatically for parallelization using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'.\n5. Allow automatic pipelining for the loop using '#pragma ACCEL PIPELINE auto{}'.\n6. Set the tile factor for loop tiling to automatic value 1 using '#pragma ACCEL TILE FACTOR=auto{1}'.\n7. Parallelize the outer loop using a parallel factor calculated automatically for parallelization using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'.\n8. Flatten the loop for pipelining using '#pragma ACCEL PIPELINE auto{flatten}'.\n9. Set the tile factor for loop tiling to automatic value 1 using '#pragma ACCEL TILE FACTOR=auto{1}'.\n10. Parallelize the outer loop using a parallel factor calculated automatically for parallelization using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'.\n11. Apply pipeline optimization with a specific name using '#pragma ACCEL PIPELINE auto{__PIPE__L3}'.\n12. Set the tile factor for loop tiling to a specific value using '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'.\n13. Parallelize the outer loop using a parallel factor calculated automatically for parallelization using '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n\nBy following these instructions, the HLS codes for the given C program can be effectively generated.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Do not automatically pipeline the outer loop and the inner loop. Use a tile factor of 1 for both loops. Run the iterations of the innermost loop in parallel with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration. (`#pragma ACCEL kernel`)\n\n2. Don't automatically pipeline this loop. (`#pragma ACCEL PIPELINE \"off\"`)\n\n3. Process this loop in its original, full form without breaking it into smaller chunks. (`#pragma ACCEL TILE FACTOR=1`)\n\n4. Run the iterations of this loop one after the other, not in parallel. (`#pragma ACCEL PARALLEL FACTOR=1`)", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks and use an auto factor of 1 for tiling.\n4. Auto-pipeline this loop with a factor of __PIPE__L1.\n5. Auto-tile this loop with a factor of __TILE__L1.\n6. Auto-parallelize this loop with a factor of 4.\n\nBy following these natural language instructions, you can generate the given HLS codes in C with the specific #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the first loop. Process the main loop in its original, full form without breaking it into smaller chunks. Run the iterations of the main loop in parallel with a factor of 16. Within the main loop, parallelize the inner loops and perform reduction operations on 'tmp' and 'y' with a factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, for different loops:\n1. Automatically pipeline the loop \n2. Tile the loop by a factor of 8 \n3. Run the iterations of the loop in parallel using a factor of 1\n4. Disable automatic pipelining for this loop \n5. Tile the loop with a factor of 1 \n6. Run the iterations of the loop in parallel using a factor of 1 \n7. Automatically pipeline the loop \n8. Tile the loop with a factor of 1 \n9. Run the iterations of the loop in parallel using a factor of 2", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks, then treat the loop as a hardware kernel for acceleration, automatically pipeline this loop, and finally, run the iterations of this loop in parallel with factor 60.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the corresponding HLS codes in C for the given natural language instruction, use the following comprehensive but very concise instruction:\n\n\"Define the following function as a hardware kernel for acceleration, and process the loop in its original, full form without breaking it into smaller chunks. Do not automatically pipeline the loop and run the iterations one after the other, not in parallel.\"\n\nThis natural language instruction is used to generate the following HLS codes in C:\n\n```c\n#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Process the loops in their original full form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and do not automatically pipeline the loops. Then, process the loops in their original, full form without breaking them into smaller chunks, and run the iterations of the innermost loop in parallel with a factor of 5.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, the following natural language instruction can be used:\n\n\"Define the ellpack function as a hardware kernel for acceleration. Disable automatic pipelining for the loop. Use a tile factor of 2 for loop processing. Enable parallel execution with a factor of 6 for the loop iterations.\"\n\nThis natural language instruction corresponds to the HLS codes given in C.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n1. Process the loop in a pipeline automatically - '#pragma ACCEL PIPELINE auto{}'\n2. Break the loop into smaller chunks with a tile factor determined automatically - '#pragma ACCEL TILE FACTOR=auto{1}'\n3. Run the iterations of this loop in parallel using an automatically determined factor - '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' (for the first loop), '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}' (for the second loop), and so on\n4. Turn off automatic pipelining for the loop - '#pragma ACCEL PIPELINE auto{off}'\n5. Use an automatically determined tile factor for loop optimization - '#pragma ACCEL TILE FACTOR=auto{20}'\n6. Run the loop iterations in parallel with an automatically determined factor - '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' (for the third loop), '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}' (for the fourth loop), and so on\n7. Process the loop in a pipeline automatically - '#pragma ACCEL PIPELINE auto{}'\n8. Use an automatically determined tile factor for loop optimization - '#pragma ACCEL TILE FACTOR=auto{1}'\n9. Run the loop iterations in parallel with an automatically determined factor - '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' (for the fifth loop), '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}' (for the sixth loop), and so on\n10. Process the loop in a pipeline with an automatically determined pipeline factor - '#pragma ACCEL PIPELINE auto{__PIPE__L3}'\n11. Use an automatically determined tile factor for loop optimization - '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'\n12. Run the loop iterations in parallel with an automatically determined factor - '#pragma ACCEL PARALLEL FACTOR=auto{30}' (for the seventh loop), '#pragma ACCEL PIPELINE auto{__PIPE__L7}' (for the eighth loop), '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}' (for the ninth loop), and so on\n\nIn summary, use these natural language instructions to specify the different #pragma values for the given HLS codes in C to achieve the desired loop optimizations and parallelizations.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Designate the function as a hardware kernel for acceleration and apply the following optimizations to establish parallelism and pipelining: For the loops, set the tiling factor to 4, parallelize the loops, and prevent automatic pipelining. Then, for subsequent loops, use a tiling factor of 1, re-parallelize the loops, and prevent automatic pipelining. Finally, apply parallelism with a factor of 2 and explicitly set pipelining for the final loop.\"\n\nBy following this natural language instruction, you can generate the given HLS codes in C using the appropriate #pragma values for kernel designation, loop tiling, loop parallelization, and loop pipelining.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Pipeline the outer loop with automatic pipeline control turned off. Also, tile the outer loop with an automatic tile factor of 1. Within the outer loop, pipeline the middle loop with automatic pipeline control using a level 1 pipeline and tile the middle loop with an automatic tile factor of level 1. Finally, parallelize the innermost loop with an automatic parallel factor of 5.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations in parallel using auto-parallelization, then pipeline the loop for increased throughput, and finally break the loop into smaller chunks for optimization using the specified factor values from the pragma statements.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - `#pragma ACCEL kernel`\n   \n2. Don't automatically pipeline the outer loop\n   - `#pragma ACCEL PIPELINE auto{off}`\n   \n3. Process the outer loop in its original, full form without breaking it into smaller chunks\n   - `#pragma ACCEL TILE FACTOR=auto{1}`\n   \n4. Automatically pipeline the inner loop with a factor\n   - `#pragma ACCEL PIPELINE auto{__PIPE__L1}`\n   \n5. Tile the inner loop with a factor\n   - `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`\n   \n6. Run the iterations of the innermost loop in parallel with a factor\n   - `#pragma ACCEL PARALLEL FACTOR=auto{4}`", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop normally without breaking it into smaller chunks. Run the iterations of this loop in parallel with a factor of 60. Automatically pipeline this loop. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the outer loop using an automatic configuration and a specific factor off, then tile the loop using an automatic configuration and a specific factor of 1. Within the tiled loop, pipeline the inner loop using an automatic configuration and a specific level L1, and tile the inner loop using an automatic configuration and a specific factor L1. Finally, parallelize the innermost loop using an automatic configuration and a factor of 5.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given natural language instruction:\n\n'Process this function as a hardware kernel for acceleration' -> '#pragma ACCEL kernel'\n\n'Don't automatically pipeline this loop' -> '#pragma ACCEL PIPELINE \"off\"'\n\n'Process this loop in its original, full form without breaking it into smaller chunks' -> '#pragma ACCEL TILE FACTOR=1'\n\n'Run the iterations of this loop one after the other, not in parallel' -> '#pragma ACCEL PARALLEL FACTOR=1'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, don't automatically pipeline the outer loop and the inner loops, and use a tile factor of 1 for both inner loops. Run the iterations of the innermost loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. For the kernel function, use 'Treat the following function as a hardware kernel for acceleration' to describe '#pragma ACCEL kernel'.\n\n2. For the loop that calculates the mean values, use 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"', 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1', and 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1'.\n\n3. For the loop that calculates the standard deviation, use 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"', 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1', and 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1'.\n\n4. For the loop that centers and reduces the column vectors, use 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"', 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=2', and 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1'.\n\n5. For the loop that calculates the correlation matrix, use 'Pipeline this loop with specific parameters' to describe '#pragma ACCEL PIPELINE auto{__PIPE__L3}', 'Divide and process this loop into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}', and 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1'.\n\nBy using these instructions, you can generate the given C codes with the appropriate HLS pragmas for hardware acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations sequentially, not in parallel, and treat the following function as a hardware kernel for acceleration. For the specified loop, pipeline automatically, factor the tiling pattern automatically with a factor of 4, and factor the parallel computation automatically with a factor of 16. Use parallel reduction for the inner loops with a factor of 16 as well.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Define the following function as a hardware kernel for acceleration and specify the following optimizations for hardware acceleration: turn off automatic loop pipelining, set the tile factor to 1 optimize the inner loop, and parallelize the iterations with a factor of 5. For the outer loop, pipeline automatically and set the tile factor to optimize it.\"\n\nUsing the above comprehensive but very concise natural language instruction, you can generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Automatically pipeline this loop' to indicate '#pragma ACCEL PIPELINE auto{}'\n\n'Automatically determine the tile factor for this loop' to indicate '#pragma ACCEL TILE FACTOR=auto{1}'\n\n'Automatically determine the parallel factor for this loop' to indicate '#pragma ACCEL PARALLEL FACTOR=auto{60}'\n\n'Treat the following function as a hardware kernel for acceleration' to indicate '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for hardware acceleration, use the following comprehensive but concise natural language instruction for the specific #pragma values:\n\n1. Apply automatic pipelining to the loops: \n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   ```\n\n2. Determine the tile factor automatically for tiling optimization:\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{4}\n   ```\n\n3. Automatically parallelize the loops with a factor of 16:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{16}\n   ```\n\n4. Perform reduction operation automatically for the specified variables within the parallel loops:\n   ```c\n   #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n   #pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n   ```\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in its original, full form without breaking it into smaller chunks and treat the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop; automatically tile the second loop with a factor of 4; and run the iterations of the second loop in parallel in chunks of 16 iterations. Additionally, carry out the reduction operation in the nested loops in parallel with a parallel factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should consider the following natural language instructions for pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the following loops.\n\n3. Process the following loops in their original, full form without breaking them into smaller chunks.\n\n4. Run the iterations of the following loops one after the other, not in parallel.\n\n5. Tweak the loop structure to allow for optimal hardware acceleration.\n\n6. Break the following loops into smaller chunks to improve performance.\n\nOnce these natural language instructions are applied to the code, the HLS implementation can be optimized for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Declare the following function as a hardware kernel for acceleration:\n   - Use the pragma: `#pragma ACCEL kernel`\n\n2. Process the following loop without automatically pipelining it:\n   - Use the pragma: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Break the loop into smaller chunks for optimization, using an auto-generated tile factor:\n   - Use the pragma: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of the innermost loop in parallel, with an auto-generated parallel factor of 5:\n   - Use the pragma: `#pragma ACCEL PARALLEL FACTOR=auto{5}`", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following comprehensive and concise natural language instructions for the specified #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration'\n   - #pragma ACCEL kernel\n\n2. 'Automatically pipeline this loop'\n   - #pragma ACCEL PIPELINE auto{}\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - #pragma ACCEL TILE FACTOR=auto{1}\n\n4. 'Run the iterations of this loop one after the other, not in parallel'\n   - #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n\n5. 'Manually disable automatic pipelining for this loop'\n   - #pragma ACCEL PIPELINE auto{off}\n\n6. 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - #pragma ACCEL TILE FACTOR=auto{1}\n\n7. 'Run the iterations of this loop one after the other, not in parallel'\n   - #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n\n8. 'Automatically pipeline this loop'\n   - #pragma ACCEL PIPELINE auto{}\n\n9. 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - #pragma ACCEL TILE FACTOR=auto{4}\n\n10. 'Run the iterations of this loop one after the other, not in parallel'\n    - #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n\n11. 'Automatically pipeline this loop'\n    - #pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n12. 'Process this loop in its original, full form without breaking it into smaller chunks'\n    - #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n13. 'Run the iterations of this loop in parallel with a factor of 2'\n    - #pragma ACCEL PARALLEL FACTOR=auto{2}\n\n14. 'Manually disable automatic pipelining for this loop'\n    - #pragma ACCEL PIPELINE auto{__PIPE__L7}\n\n15. 'Run the iterations of this loop in parallel with a factor of 1'\n    - #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n\nBy following these instructions and using the specified #pragma values, you can generate the given HLS codes effectively in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n- Use '#pragma ACCEL PIPELINE auto{off}' to process the loop without automatically pipelining it.\n- Use '#pragma ACCEL TILE FACTOR=auto{1}' to keep the loop in its original, full form without breaking it into smaller chunks.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' to run the iterations of the loop in parallel, with the specified parallel factor value.\n- Use '#pragma ACCEL PIPELINE auto{flatten}' to automatically flatten the loop pipeline for better performance.\n- Use '#pragma ACCEL TILE FACTOR=auto{20}' to set the tile factor for the loop to the specified automatic value.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' to run the iterations of the loop in parallel, with the specified parallel factor value.\n- Use '#pragma ACCEL PIPELINE auto{off}' to process the loop without automatically pipelining it.\n- Use '#pragma ACCEL TILE FACTOR=auto{1}' to keep the loop in its original, full form without breaking it into smaller chunks.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' to run the iterations of the loop in parallel, with the specified parallel factor value.\n- Use '#pragma ACCEL PIPELINE auto{__PIPE__L3}' to automatically pipeline the loop with the specified pipeline strategy.\n- Use '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}' to set the tile factor for the loop to the specified automatic value.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run the iterations of the loop in parallel, with the specified parallel factor value.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Opt out of automatic pipelining for this loop. Set the tiling factor to 1 for this loop. Parallelize this loop with a factor of 60.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Turn off automatic pipelining for this loop, apply a tile factor of 1, and parallelize the loop with a factor of 5. Treat the function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To parallelize the operations in the given function and enable hardware acceleration, use the following comprehensive but concise natural language instruction:\n\n\"Process the loop in its original form without automatically pipelining, use a tile factor of 1, and parallelize with a factor of 5. Treat the function as a hardware kernel for acceleration.\"\n\nWould you need further assistance with anything else?", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the instructions below:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline this loop.\n\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n5. Optionally, you can replace `auto{off}`, `auto{1}`, and other instances of `auto{__PARA__L0}`, `auto{__PIPE__L3}`, `auto{__TILE__L3}` with specific numerical values or specific factors.\n\nBy following these instructions, you can generate the given HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but concise natural language instructions for the corresponding #pragma values:\n- Use '#pragma ACCEL PIPELINE auto{off}' to prevent automatic pipelining for the loops.\n- Use '#pragma ACCEL TILE FACTOR=auto{1}' to process the loops in their original, full form without breaking them into smaller chunks.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' to run the iterations of the outer loop one after the other, not in parallel.\n- Use '#pragma ACCEL PIPELINE auto{}' to enable automatic pipelining for the inner loops.\n- Use '#pragma ACCEL TILE FACTOR=auto{2}' to automatically tile the inner loops with a factor of 2.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' to process the inner loops in parallel with a specified factor.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the first loop without auto-vectorization. Tile the loop with a factor of 4. Parallelize the outer loop with a factor of 16, and the inner loops with reduction of variables tmp and y.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, you would need to use the following natural language instructions for the specific #pragma values:\n\n1. 'Treat the following function as a hardware kernel for acceleration'\n   To describe: `#pragma ACCEL kernel`\n\n2. 'Don't automatically pipeline this loop'\n   To describe: `#pragma ACCEL PIPELINE \"off\"`\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks'\n   To describe: `#pragma ACCEL TILE FACTOR=1`\n\n4. 'Run the iterations of this loop one after the other, not in parallel'\n   To describe: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code in C to accelerate the \"stencil3d\" function with hardware kernel pragma, turn off automatic pipeline for the outer loop, set tile factor to 1 for the outer loop,  set pipeline with L1 pragma for the inner loop, and set tile factor to L1 for the inner loop while processing iterations one after the other without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following comprehensive but concise pragma values:\n\n1. Disable automatic pipeline for the loop: `#pragma ACCEL PIPELINE auto{off}`\n2. Process the loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n3. Run the iterations of the loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` (for the first loop), `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` (for the second loop), `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` (for the third loop)\n4. Treat the function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration; automatically pipeline the first loop; tile the second loop with a factor of 1; parallelize the outer loop with a factor of 2 and apply reduction to variable tmp; parallelize the inner loop with a factor of 2 and apply reduction to variable y.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code, use the following natural language instructions for the specific pragma values:\n\n- For the kernel function, use the instruction 'Treat the following function as a hardware kernel for acceleration'\n\n- For the first loop, use 'Don't automatically pipeline this loop'\n\n- For the second loop, use 'Process this loop in its original, full form without breaking it into smaller chunks', and 'Run the iterations of this loop one after the other, not in parallel'\n\n- For the third loop, use 'Don't automatically pipeline this loop'\n\n- For the fourth loop, use 'Process this loop in its original, full form without breaking it into smaller chunks'\n\n- For the fifth loop, use 'Don't automatically pipeline this loop', and 'Run the iterations of this loop one after the other, not in parallel'\n\n- For the sixth loop, use 'Pipeline this loop automatically', 'Divide the loop into 4x4 tiles', and 'Run the iterations of this loop in parallel'\n\n- For the seventh loop, use 'Pipeline this loop automatically', 'Divide the loop into 4x4 tiles', and 'Run the iterations of this loop in parallel'\n\n- For the eighth loop, use 'Pipeline this loop automatically', 'Divide the loop into 1x1 tiles', and 'Run the iterations of this loop in parallel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate this C code with HLS pragmas, use the following concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n2. Don't automatically pipeline this loop\n3. Process this loop in its original, full form without breaking it into smaller chunks\n4. Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Apply pipelining to optimize loop execution, but do not automatically detect pipeline potential' describes #pragma ACCEL PIPELINE auto{off}. \n\n'Partition the loop into smaller chunks for improved parallelism, with an automatically determined factor' describes #pragma ACCEL TILE FACTOR=auto{1}.\n\n'Execute loop iterations in parallel with an automatically determined factor for hardware acceleration' describes #pragma ACCEL PARALLEL FACTOR=auto{5}.\n\n'Treat the following function as a hardware kernel for acceleration' describes #pragma ACCEL kernel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nFor the loop starting from j=0 to j<80, run the iterations one after the other, not in parallel. Then, for the inner loop starting from i=0 to i<100, also run the iterations one after the other, not in parallel. This inner loop calculates the mean values.\n\nFor the loop starting from j=0 to j<80, run the iterations one after the other, not in parallel. Then, for the inner loop starting from i=0 to i<100, also run the iterations one after the other, not in parallel. This inner loop calculates the standard deviation values.\n\nFor the loop starting from i=0 to i<100, and the inner loop starting from j=0 to j<80, run both sets of iterations in parallel.\n\nFor the loop starting from i=0 to i<80 - 1, calculate the correlation matrix. Then, for the inner loop starting from j=i+1 to j<80, calculate the correlations between data points and run these iterations one after the other, not in parallel. Finally, for the innermost loop starting from k=0 to k<100, calculate the correlation values in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Create hardware kernel for acceleration, disable auto-pipelining, set tile factor to 1 and execute loop iterations in parallel with factor 1'", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Optimize the loops for hardware acceleration by turning off automatic pipelining, using a tile factor of 1 for the loop, and running the iterations of the loop in parallel with a factor of 1. Treat the following function as a hardware kernel for acceleration.\"\n\nThese instructions correspond to the following pragma values:\n1. Turn off automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n2. Use a tile factor of 1 for the loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n3. Run the iterations of the loop in parallel with a factor of 1: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\nThese pragma values will optimize the loops and treat the function as a hardware kernel for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then do not automatically pipeline the outer loop and the inner loop. Finally, tile the outer loop with a factor of 1 and the inner loop with a factor of 1. Also, run the iterations of the innermost loop in parallel with a factor of 5.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified C code with the corresponding HLS pragmas, use the following comprehensive but concise natural language instruction:\n\n\"Convert the following function into a hardware kernel for acceleration, pipeline the loops manually, tile the outer loop by a factor of 2, and parallelize the outer loop by a factor of 16. Use parallelization with reduction for both inner loops, reducing the variable 'tmp' with a factor of 16_0 and reducing the variable 'y' with a factor of 16_1.\"\n\nThis instruction covers the use of the following pragmas:\n1. `#pragma ACCEL kernel`\n2. `#pragma ACCEL PIPELINE auto{off}`\n3. `#pragma ACCEL TILE FACTOR=auto{2}`\n4. `#pragma ACCEL PARALLEL FACTOR=auto{16}`\n5. `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}`\n6. `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Optimize this loop for data dependencies and parallelism, then automatically break it into smaller chunks for parallel processing.'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, developers should follow these natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Don't automatically pipeline this loop\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n4. Run the iterations of this loop one after the other, not in parallel\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loop: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks with an auto-determined tile factor: `#pragma ACCEL TILE FACTOR=auto{4}`\n\n4. Run the iterations of this loop one after the other, not in parallel, with an auto-determined parallel factor: `#pragma ACCEL PARALLEL FACTOR=auto{16}`\n\n5. Parallelize this loop with an auto-determined reduction variable and parallel factor: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}`\n\n6. Parallelize this loop with another auto-determined reduction variable and parallel factor: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code describes a stencil3d function and uses certain HLS pragmas for hardware acceleration. Here are concise natural language instructions for the specific #pragma values used in the code:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop\n   - `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel, with a parallel factor of 8\n   - `#pragma ACCEL PARALLEL FACTOR=8`", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction to set the specific #pragma values:\n\n\"Process this function as a hardware kernel for acceleration, and do not automatically pipeline the outer loop. Use a tile factor of 1 for the outer loop, and for the inner loops, automatically pipeline with factor __PIPE__L1, and use a tile factor of __TILE__L1. Run the iterations of the innermost loop one after the other, not in parallel.\"\n\nThis comprehensive but very concise instruction provides clear guidance on how to generate the given HLS codes using specific #pragma values in natural language.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and automatically pipeline the outer loop, tile the loop with a factor of 2, and parallelize the loop with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration. Include automatic pipeline optimization for the outer loop and a tile factor of 80 for tiling the outer loop. Inside the outer loop, automatically pipeline the inner loop and set parallel factor 1 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{80}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction for each #pragma value:\n\n1. 'Treat the following function as a hardware kernel for acceleration' - for '#pragma ACCEL kernel'\n2. 'Don't automatically pipeline this loop' - for '#pragma ACCEL PIPELINE auto{off}'\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' - for '#pragma ACCEL TILE FACTOR=auto{1}'\n4. 'Run the iterations of this loop one after the other, not in parallel' - for '#pragma ACCEL PARALLEL FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the first loop with automatic flattening. Tile the second loop with an automatic factor of 2. Parallelize the second loop with an automatic factor of 2. In the innermost loop, parallelize the reduction for array 'tmp' with an automatic factor of 2_0, and parallelize the reduction for array 'y' with an automatic factor of 2_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and do not automatically pipeline the loops. For the outer loop, use a tile factor of 1 for the original full form without breaking into smaller chunks. For the inner loops, automatically pipeline with a factor of __PIPE__L1 and use a tile factor of __TILE__L1. Additionally, parallelize the innermost loop with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction for the specific #pragma values:\n\n1. Keep the pipeline off for automatic initiation: Use '#pragma ACCEL PIPELINE auto{off}'\n2. Set the tiling factor to 8 automatically: Use '#pragma ACCEL TILE FACTOR=auto{8}'\n3. Set the parallelization factor to 2 automatically: Use '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n4. Apply parallel reduction for variable 'tmp' with a factor of 2, version 0: Use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}'\n5. Apply parallel reduction for variable 'y' with a factor of 2, version 1: Use '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}'\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the corresponding #pragma values:\n\n1. Disable automatic pipelining for the loop: \n   - Natural language instruction: 'Don't automatically pipeline this loop'\n   - Corresponding #pragma value: '#pragma ACCEL PIPELINE \"off\"'\n\n2. Process this loop in its original, full form without breaking it into smaller chunks:\n   - Natural language instruction: 'Process this loop without breaking into smaller chunks'\n   - Corresponding #pragma value: '#pragma ACCEL TILE FACTOR=1'\n\n3. Run the iterations of this loop one after the other, not in parallel:\n   - Natural language instruction: 'Run the iterations one after the other, not in parallel'\n   - Corresponding #pragma value: '#pragma ACCEL PARALLEL FACTOR=1'\n\n4. Treat the following function as a hardware kernel for acceleration:\n   - Natural language instruction: 'Treat the following function as a hardware kernel for acceleration'\n   - Corresponding #pragma value: '#pragma ACCEL kernel'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the outer loop:\n   `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process the outer loop in its original, full form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=1`\n\n4. Automatically pipeline the inner loop with a custom pipelining factor:\n   `#pragma ACCEL PIPELINE auto{__PIPE__L1}`\n\n5. Automatically apply tiling to the inner loop with a custom tiling factor:\n   `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`\n\n6. Run the iterations of the innermost loop one after the other, not in parallel:\n   `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C with specific #pragma values, you can use the comprehensive natural language instruction: \"Treat the loops in this function for hardware acceleration, turning off automatic pipelining, applying a tile factor of 25, running parallel processing with factor __PARA__L0 for the first loop, turning off automatic pipelining again, applying a tile factor of 1, running parallel processing with factor __PARA__L1 for the second loop, turning off automatic pipelining once more, applying a tile factor of 1, running parallel processing with factor __PARA__L2 for the third loop, and finally applying specific pipelining, tile factor, and parallel processing factors for the last loop according to the given HLS codes.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the stencil3d function as a hardware kernel for acceleration, and for the nested loops inside the function, turn off automatic pipelining, apply a tiling factor of 1 for both levels, and parallelize the innermost loop with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the corresponding #pragma values:\n\n1. For loop: 'Don't automatically pipeline this loop'\n   - Use: #pragma ACCEL PIPELINE \"off\"\n\n2. For loop: 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - Use: #pragma ACCEL TILE FACTOR=1\n\n3. For loop: 'Run the iterations of this loop one after the other, not in parallel'\n   - Use: #pragma ACCEL PARALLEL FACTOR=1\n\n4. Function: 'Treat the following function as a hardware kernel for acceleration'\n   - Use: #pragma ACCEL kernel\n\nApply these instructions to the respective loops and function to generate the HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following concise natural language instruction for each specific #pragma value:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   - Use pragma: #pragma ACCEL kernel\n\n2. Don't automatically pipeline this loop:\n   - Use pragma: #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   - Use pragma: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   - Use pragma: #pragma ACCEL PARALLEL FACTOR=1", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks - for '#pragma ACCEL TILE FACTOR=1'\n  \nRun the iterations of this loop one after the other, not in parallel - for '#pragma ACCEL PARALLEL FACTOR=1'\n  \nDon't automatically pipeline this loop - for '#pragma ACCEL PIPELINE \"off\"'\n  \nTreat the following function as a hardware kernel for acceleration - for '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\n5. Flatten the pipeline for the following loop\n   - Use: `#pragma ACCEL PIPELINE \"flatten\"`\n\n6. Set the tile factor automatically for the following loop\n   - Use: `#pragma ACCEL TILE FACTOR=auto{4}`\n\n7. Automatically pipeline the following loop with a specific value\n   - Use: `#pragma ACCEL PIPELINE auto{__PIPE__L3}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Automatically pipeline the first loop, tile the second loop with an automatic factor, and parallelize the third and fourth loops with automatic factors and reduction for the specified variables.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the 'ellpack' function, use the natural language instruction: \n'Pipeline the loop automatically, tile the loop with factor 8, and run the loop iterations in parallel with factor 6.'\n", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, you can use the following comprehensive but concise natural language instructions for pragma values:\n\n1. Don't automatically pipeline this loop\n   - Code: `#pragma ACCEL PIPELINE \"off\"`\n\n2. Process this loop in its original, full form without breaking it into smaller chunks\n   - Code: `#pragma ACCEL TILE FACTOR=1`\n\n3. Run the iterations of this loop one after the other, not in parallel\n   - Code: `#pragma ACCEL PARALLEL FACTOR=1`\n\n4. Treat the following function as a hardware kernel for acceleration\n   - Code: `#pragma ACCEL kernel`\n\nWith these instructions, you can use these pragma values to optimize the HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given codes, you can use the following natural language instructions for the specific #pragma values:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   - Natural language instruction: \"Treat the following function as a hardware kernel for acceleration\"\n   - HLS code: #pragma ACCEL kernel\n\n2. Don't automatically pipeline the loop.\n   - Natural language instruction: \"Don't automatically pipeline this loop\"\n   - HLS code: #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Natural language instruction: \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   - HLS code: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Natural language instruction: \"Run the iterations of this loop one after the other, not in parallel\"\n   - HLS code: #pragma ACCEL PARALLEL FACTOR=1", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, don't automatically pipeline the loops, process each loop in its original, full form without breaking it into smaller chunks, and run the iterations of the loops in parallel as specified by the AUTO indicators in the #pragmas.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is treated as a hardware kernel for acceleration and execute the following instructions:\n- Pipe this loop to support automatic pipeline initiation\n- Divide this loop into its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop in parallel with a factor of 16 in parallelismic regions", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions for the specific #pragma values:\n\n1. \"Don't automatically pipeline this loop\": This corresponds to '#pragma ACCEL PIPELINE auto{off}'.\n\n2. \"Process this loop in its original, full form without breaking it into smaller chunks\": This corresponds to '#pragma ACCEL TILE FACTOR=auto{1}'.\n\n3. \"Run the iterations of this loop one after the other, not in parallel\": This corresponds to '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n\nAnd the comprehensive but very concise natural language instruction to generate the entire HLS code would be: \"Treat the spmv function as a hardware kernel for acceleration and don't automatically pipeline the loop, process the loop in its original form, and run the iterations one after the other, not in parallel\".", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, provide the following concise natural language instruction:\n\n\"Designate the function as a hardware kernel for acceleration, disable automatic loop pipelining, set the loop tiling factor to 1, and run loop iterations sequentially instead of in parallel.\"", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   - \"Treat the following function as a hardware kernel for acceleration\" for '#pragma ACCEL kernel'\n\n2. Don't automatically pipeline this loop.\n   - Use '#pragma ACCEL PIPELINE auto{off}' to turn off automatic loop pipelining.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Set the tile factor to 1 with '#pragma ACCEL TILE FACTOR=auto{1}' to keep the loop in its original form.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Set the parallel factor to 1 with '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run the loop iterations sequentially.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\n#pragma ACCEL TILE FACTOR=1\n\nDon't automatically pipeline this loop\n#pragma ACCEL PIPELINE \"off\"\n  \nRun the iterations of this loop one after the other, not in parallel\n#pragma ACCEL PARALLEL FACTOR=1\n  \nTreat the following function as a hardware kernel for acceleration\n#pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and use automatic pipelining for the loop. Tile the loop with an automatic factor of 1. Parallelize the loop with an automatic factor of 116, and use automatic reduction for the inner loops to handle the reduction variables.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, don't automatically pipeline the first loop, set the tile factor to the original full form, and run the iterations of the main loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{116}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{116_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{116_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following comprehensive but very concise natural language instructions for the specific #pragma values:\n\n1. Don't automatically pipeline this loop\n   - For the loop that calculates the mean and standard deviation: \n     `#pragma ACCEL PIPELINE auto{off}`\n\n2. Process this loop in its original, full form without breaking it into smaller chunks\n   - For the loop that calculates the mean and standard deviation: \n     `#pragma ACCEL TILE FACTOR=auto{8}`\n\n3. Run the iterations of this loop one after the other, not in parallel\n   - For the loop that calculates the mean: \n     `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n   - For the loop that calculates the standard deviation: \n     `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n   - For the loop that centers and reduces the column vectors:\n     `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n4. Treat the following function as a hardware kernel for acceleration\n   - For the entire function: \n     `#pragma ACCEL kernel`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in its original, full form without breaking it into smaller chunks, and do not automatically pipeline this loop. Run the iterations of this loop one after the other, not in parallel. Also, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nMake the loop not automatically pipelined.\n\nProcess this loop in its original, full form without breaking it into smaller chunks.\n\nRun the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Generate hardware function for acceleration including specific directives to pipeline, tile, and parallelize the loops as per auto-recommendations provided for optimal performance.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To achieve the following HLS codes in C, follow these natural language instructions for the corresponding #pragma values:\n\n1. \"Treat the following function as a hardware kernel for acceleration.\"\n   - Use: #pragma ACCEL kernel\n\n2. \"Don't automatically pipeline this loop.\"\n   - Use: #pragma ACCEL PIPELINE \"off\"\n\n3. \"Process this loop in its original, full form without breaking it into smaller chunks.\"\n   - Use: #pragma ACCEL TILE FACTOR=1\n\n4. \"Run the iterations of this loop one after the other, not in parallel.\"\n   - Use: #pragma ACCEL PARALLEL FACTOR=1\n\n5. \"Automatically pipeline this loop.\"\n   - Use: #pragma ACCEL PIPELINE auto{}\n\n6. \"Set the tile factor for this loop automatically.\"\n   - Use: #pragma ACCEL TILE FACTOR=auto{1}\n\n7. \"Set the parallel factor for this loop automatically to 16.\"\n   - Use: #pragma ACCEL PARALLEL FACTOR=auto{16}\n\n8. \"Perform reduction operation on variable 'tmp' in this loop with an automatic parallel factor of 16.\"\n   - Use: #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n\n9. \"Perform reduction operation on variable 'y' in this loop with an automatic parallel factor of 16.\"\n   - Use: #pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n\nApply these instructions to the corresponding #pragma values in your C code to achieve the desired HLS behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. `#pragma ACCEL PIPELINE auto{off}` - Don't automatically pipeline this loop.\n\n2. `#pragma ACCEL TILE FACTOR=auto{1}` - Process this loop in its original, full form without breaking it into smaller chunks.\n\n3. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` - Run the iterations of this loop one after the other, not in parallel.\n\n4. `#pragma ACCEL PIPELINE auto{off}` - Don't automatically pipeline this loop.\n\n5. `#pragma ACCEL TILE FACTOR=auto{20}` - Divide this loop into smaller chunks for processing, each chunk consisting of 20 iterations.\n\n6. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` - Run the iterations of this loop in parallel, using a specified parallelization factor.\n\n7. `#pragma ACCEL PIPELINE auto{}` - Automatically pipeline this loop.\n\n8. `#pragma ACCEL TILE FACTOR=auto{1}` - Process this loop in its original, full form without breaking it into smaller chunks.\n\n9. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` - Run the iterations of this loop in parallel, using a specific parallelization factor.\n\n10. `#pragma ACCEL PIPELINE auto{__PIPE__L3}` - Automatically pipeline this loop, using specific pipeline settings.\n\n11. `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}` - Divide this loop into smaller chunks for processing, using specific tiling factors.\n\n12. `#pragma ACCEL PARALLEL FACTOR=auto{1}` - Run the iterations of this loop in parallel, using a parallelization factor of 1.\n\nPlease note that the natural language instructions provided should be translated to the appropriate C code constructs and where specific values are labeled as 'auto' or 'off', these values should be replaced with the appropriate context-specific parameters.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instruction:\n\"Treat the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops. Process the loops in their original, full form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel. Center and reduce the column vectors. Make sure to calculate the m * m correlation matrix using appropriate pipelining, tiling, and parallelization factors for optimization.\"\n\nThis instruction captures the necessary pragma values and their specific usage to generate the given HLS codes.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions:\n\n1. Treat the function 'kernel_correlation' as a hardware kernel for acceleration.\n2. Turn off automatic pipelining for the loops inside the function.\n3. Generate a single tile for the loops inside the function.\n4. Run the iterations of the loops in sequential order, not in parallel.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. **Process this loop without automatic pipeline optimization**:\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n\n2. **Divide this loop into smaller chunks with a factor of 1**:\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n\n3. **Run the iterations of this loop one after the other, not in parallel**:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{1}\n   ```\n\n4. **Treat the following function as a hardware kernel for acceleration**:\n   ```c\n   #pragma ACCEL kernel\n   ```\n\nThese natural language instructions correspond to the given HLS codes, specifying the necessary #pragma values for loop optimization and specifying the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To transform the given HLS code into natural language instruction, you may use the following comprehensive but concise phrases:\n\n1. Optimize the loop unrolling process for better performance and minimize resource usage\n    - Solution: `#pragma ACCEL PIPELINE auto{off}`\n\n2. Divide the loop into its original size without breaking it into smaller chunks, optimizing for full form execution\n    - Solution: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. Execute the loop iterations sequentially, not in parallel, for data dependency\n    - Solution: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n4. Treat the ensuing function as a specialized hardware kernel for acceleration processing\n    - Solution: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and within this function, disable automatic pipelining, set the tile factor to 1, and run the iterations of the loop sequentially without parallelism.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. In addition, don't automatically pipeline this loop and run the iterations of this loop one after the other, not in parallel. Also, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the first loop without specifying a specific factor. Tile the subsequent loop with an automatic factor of 4. Then, parallelize the outer loop with an automatic factor of 16. Within the outer loop, parallelize the inner loop with a reduction on tmp using an automatic factor of 16_0, and parallelize another inner loop with a reduction on y using an automatic factor of 16_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, you can follow the following instructions:\n\n1. Mark the function as a hardware kernel for acceleration\n```c\n#pragma ACCEL kernel\n```\n\n2. Disable automatic pipeline optimization for the loop\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n3. Split the loop into smaller chunks for parallel processing with a tile factor of 2\n```c\n#pragma ACCEL TILE FACTOR=auto{2}\n```\n\n4. Process loop iterations in parallel with a factor of 2\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n```\n\n5. Do parallel reduction for the 'tmp' variable with a factor of 2_0\n```c\n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n```\n\n6. Perform parallel reduction for the 'y' variable with a factor of 2_1\n```c\n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n```\n\nBy following these concise natural language instructions, you will be able to generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given C code, the comprehensive but very concise natural language instruction would be: \n\n\"Process this loop without automatic pipeline, with a tile factor of 8, and run the loop iterations in parallel with a factor of 6. Treat the following function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop as a hardware kernel for acceleration, automatically pipeline this loop, tile with a factor determined automatically, and run the iterations of this loop in parallel with a factor determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{200}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop without automatically pipelining, process it as a single tile without breaking it into smaller chunks, and run the iterations of the loop one after the other, not in parallel. Additionally, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code to hardware kernel acceleration, automatically pipeline the outer loop with a factor of auto, tile the outer loop with a factor of auto, and parallelize the inner loop with a factor of auto.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the following function as a hardware kernel, automatically pipeline the loops, apply tiling with a factor of 4, and parallelize with a factor of 16 for both inner loops along with reduction for arrays 'tmp' and 'y'.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is one comprehensive but very concise natural language instruction used to generate the given HLS codes written in C:\n\n`Process the code without automatic pipelining, in its original, full form without breaking into smaller chunks; Run the iterations of the loops in parallel with a factor of 1, treating the following function as a hardware kernel for acceleration.`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following concise natural language instructions for each specific #pragma value:\n\n1. 'Don't automatically pipeline this loop': #pragma ACCEL PIPELINE \"off\"\n2. 'Process this loop in its original, full form without breaking it into smaller chunks': #pragma ACCEL TILE FACTOR=1\n3. 'Run the iterations of this loop one after the other, not in parallel': #pragma ACCEL PARALLEL FACTOR=1\n4. 'Treat the following function as a hardware kernel for acceleration': #pragma ACCEL kernel", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for HLS, you should define the following pragma values: \n\n1. `#pragma ACCEL PIPELINE auto{off}` - This pragma instructs the HLS tool to disable automatic pipelining for the loop, ensuring that it is not automatically broken into smaller stages.\n\n2. `#pragma ACCEL TILE FACTOR=auto{8}` - Use this pragma to guide the tool to automatically apply tiling with a factor of 8 to the loop for better optimization.\n\n3. `#pragma ACCEL PARALLEL FACTOR=auto{2}` - This pragma should be applied to parallelize the loop with an automatic factor of 2, allowing iterations to run in parallel for performance improvement.\n\n4. `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}` - Utilize this pragma to specify automatic parallelization with a reduction operation on the variable `tmp`, with a factor of 2.\n\n5. `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}` - This pragma is used to enable automatic parallelization with a reduction operation on the variable `y`, with a factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Optimize the loop peeling\n```c\n#pragma ACCEL PIPELINE off\n```\n\n2. Use an appropriate tiling factor for loop optimization\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n3. Parallelize the loop iterations for acceleration\n```c\n#pragma ACCEL PARALLEL FACTOR=5\n```\n\n4. Treat the following function as a hardware kernel for acceleration\n```c\n#pragma ACCEL kernel\n```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Run the iterations of the outer loop one after the other, not in parallel. Process the outer loop in its original, full form without breaking it into smaller chunks. Don't automatically pipeline the inner loop.\"\n\nThis instruction covers the usage of the specific #pragma values as mentioned in the HLS code:\n\n- For marking the function as a hardware kernel: '#pragma ACCEL kernel'\n- For running the iterations of the outer loop one after the other, not in parallel: '#pragma ACCEL PARALLEL FACTOR=1'\n- For processing the outer loop in its original, full form: '#pragma ACCEL TILE FACTOR=1'\n- For not automatically pipelining the inner loop: '#pragma ACCEL PIPELINE auto{off}'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes by setting the following directives:\n\n1. The outer loop should not be automatically pipelined\n   - Directive: '#pragma ACCEL PIPELINE \"off\"'\n\n2. The loop should be processed in its original, full form without breaking into smaller chunks\n   - Directive: '#pragma ACCEL TILE FACTOR=1'\n\n3. Run the iterations of the loop in parallel, with a parallel factor of 2\n   - Directive: '#pragma ACCEL PARALLEL FACTOR=2'\n\n4. Treat the function as a hardware kernel for acceleration\n   - Directive: '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you would use the following natural language instructions for the specific #pragma values:\n\n- For the loop: \"Don't automatically pipeline this loop.\"\n    Use: `#pragma ACCEL PIPELINE auto{off}`\n\n- For the loop: \"Process this loop in its original, full form without breaking it into smaller chunks.\"\n    Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n- For the loop iterations: \"Run the iterations of this loop one after the other, not in parallel.\"\n    Use: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n- For the function: \"Treat the following function as a hardware kernel for acceleration.\"\n    Use: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\"Treat the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop, process the loop in its original form without breaking it into smaller chunks, and run the iterations of the loop in parallel, with a parallelism factor of 2. Additionally, use parallelism factor of 2 for both reduction operations within the loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that this function is treated as a hardware kernel for acceleration. Do not automatically pipeline the loops within this function. Process the loops in their original, full form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel.\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n    int i;\n    int j;\n    int k;\n    double eps = 0.1;\n\n#pragma ACCEL PIPELINE \"off\"\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=1\n    for (j = 0; j < 80; j++) {\n        mean[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=1\n        for (i = 0; i < 100; i++) {\n            mean[j] += data[i][j];\n        }\n        mean[j] /= float_n;\n    }\n\n#pragma ACCEL PIPELINE \"off\"\n\n#pragma ACCEL TILE FACTOR=4\n\n#pragma ACCEL PARALLEL FACTOR=1\n    for (j = 0; j < 80; j++) {\n        stddev[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=1\n        for (i = 0; i < 100; i++) {\n            stddev[j] += pow(data[i][j] - mean[j], (double)2);\n        }\n        stddev[j] /= float_n;\n        stddev[j] = sqrt(stddev[j]);\n        /* The following in an inelegant but usual way to handle\n           near-zero std. dev. values, which below would cause a zero-\n           divide. */\n        stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n    }\n    /* Center and reduce the column vectors. */\n\n#pragma ACCEL PIPELINE \"off\"\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=1\n    for (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=1\n        for (j = 0; j < 80; j++) {\n            data[i][j] -= mean[j];\n            data[i][j] /= sqrt(float_n) * stddev[j];\n        }\n    }\n    /* Calculate the m * m correlation matrix. */\n\n#pragma ACCEL PIPELINE \"auto\"\n\n#pragma ACCEL TILE FACTOR=auto\n\n#pragma ACCEL PARALLEL FACTOR=1\n    for (i = 0; i < 80 - 1; i++) {\n        corr[i][i] = 1.0;\n\n#pragma ACCEL PIPELINE \"auto\"\n        for (j = i + 1; j < 80; j++) {\n            corr[i][j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=1\n            for (k = 0; k < 100; k++) {\n                corr[i][j] += data[k][i] * data[k][j];\n            }\n            corr[j][i] = corr[i][j];\n        }\n    }\n    corr[80 - 1][80 - 1] = 1.0;\n}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "You want to define 'Treat the following loop as a hardware kernel for acceleration' as '#pragma ACCEL kernel'\nand \n'Treat the iterations of this loop one after the other, not in parallel' as '#pragma ACCEL PARALLEL FACTOR=1'\nand \n'Don't automatically pipeline this loop' as '#pragma ACCEL PIPELINE \"off\"'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform data stencil computations and output results to array 'sol' based on input array 'orig' and constants C0 and C1. Mark the 'stencil3d' function as a hardware kernel for acceleration.\n\n```\n#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n\n#pragma ACCEL PIPELINE off\n\n#pragma ACCEL TILE FACTOR=1\n  for (long i = 1; i < 33; i++) {\n\n#pragma ACCEL PIPELINE __PIPE__L1\n\n#pragma ACCEL TILE FACTOR=__TILE__L1\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=4\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop in parallel with a factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code into HLS, you want to:\n1. Treat the function as a hardware kernel for acceleration\n2. Apply automatic pipelining to the loops\n3. Automatically set tile factor for the loop\n4. Automatically set parallel factor for the loops with reduction variables\n\nUse the following #pragma directives:\n- For function acceleration: `#pragma ACCEL kernel`\n- For loop pipelining: `#pragma ACCEL PIPELINE auto{}`\n- For loop tiling: `#pragma ACCEL TILE FACTOR=auto{8}`\n- For loop parallelization with reduction variables: `#pragma ACCEL PARALLEL FACTOR=auto{2}`, `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}`, `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\n- This would be described using the following natural language instruction: \"Don't automatically tile this loop\"\n\nAutomatically pipeline this loop\n- This would be described using the following natural language instruction: \"Automatically pipeline this loop\"\n\nRun the iterations of this loop one after the other, not in parallel\n- This would be described using the following natural language instruction: \"Run the iterations of this loop sequentially, not in parallel\"\n\nTreat the following function as a hardware kernel for acceleration\n- This would be described using the following natural language instruction: \"Treat the following function as a hardware kernel for acceleration\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Transform the following function into a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n2. Utilize automatic pipelining for optimization purposes: `#pragma ACCEL PIPELINE auto`\n\n3. Employ tiling with an automatically determined factor of 8: `#pragma ACCEL TILE FACTOR=auto{8}`\n\n4. Enable parallel processing with an automatically determined factor of 2: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n5. Perform parallel reduction operations on the variable \"tmp\" with an automatically determined factor of 2_0: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}`\n\n6. Perform parallel reduction operations on the variable \"y\" with an automatically determined factor of 2_1: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you should use the following natural language instructions for #pragma values:\n\n1. **'Don't automatically pipeline this loop'** - This corresponds to `#pragma ACCEL PIPELINE auto{off}`.\n2. **'Process this loop in its original, full form without breaking it into smaller chunks'** - This corresponds to `#pragma ACCEL TILE FACTOR=auto{1}`.\n3. **'Run the iterations of this loop one after the other, not in parallel'** - This corresponds to `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` for the first loop and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` for the second loop.\n4. **'Treat the following function as a hardware kernel for acceleration'** - This corresponds to `#pragma ACCEL kernel` for the kernel function `kernel_correlation`.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Declare the function as a hardware kernel for acceleration using '#pragma ACCEL kernel'.\n2. Turn off automatic pipelining for the loop using '#pragma ACCEL PIPELINE auto{off}'.\n3. Process the loop in its original, full form without breaking it into smaller chunks using '#pragma ACCEL TILE FACTOR=auto{1}'.\n4. Run the iterations of the loop in parallel with a factor of 2 using '#pragma ACCEL PARALLEL FACTOR=auto{2}'.\n5. Apply parallelism with reduction for the 'tmp' variable using '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}'.\n6. Apply parallelism with reduction for the 'y' variable using '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code, use the following natural language instructions:\n\n1. Enable manual pipelining for the loop:\n   - Use `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipelining for the loop.\n\n2. Process the loop in its original, full form without breaking it into smaller chunks:\n   - Set the tile factor using `#pragma ACCEL TILE FACTOR=auto{1}` to specify the tile factor as 1 for the loop.\n\n3. Run the iterations of this loop one after the other, not in parallel:\n   - Set the parallel factor using `#pragma ACCEL PARALLEL FACTOR=auto{1}` to specify the parallel factor as 1 for the loop.\n\nCombine these natural language instructions with the corresponding `#pragma` directives to generate the given HLS C code.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To process the loop in its original form without breaking it into smaller chunks, enable automatic pipelining, and run the loop iterations in parallel, use the following comprehensive concise natural language instruction:\n'Process the loop in its original form without breaking it into smaller chunks, automatically pipeline the loop, and run loop iterations in parallel.'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop. Process the first loop in its original, full form without breaking it into smaller chunks. Run the iterations of the second loop one after the other, not in parallel. Additionally, parallelize the inner loops with a parallel factor of 29.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the #pragma values:\n\n- Set the following function to be treated as a hardware kernel for acceleration.\n- Don\u2019t automatically pipeline the loop.\n- Process this loop by dividing it into smaller chunks of size 4 for optimization.\n- Run the iterations of this loop in parallel, dividing it into 29 chunks for parallel execution.\n- Parallelize the inner loop with a reduction variable 'tmp' for optimised parallel execution.\n- Parallelize the inner loop with a reduction variable 'y' for optimised parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   \n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loop.\n\n   `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks using an automatic factor of 8.\n\n   `#pragma ACCEL TILE FACTOR=auto{8}`\n\n4. Run the iterations of this loop in parallel with an automatic factor of 2.\n\n   `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n5. Run the inner loop iterations in parallel with a reduction on variable 'tmp' and an automatic factor of 2_0.\n\n   `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}`\n\n6. Run the inner loop iterations in parallel with a reduction on variable 'y' and an automatic factor of 2_1.\n\n   `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the following C code for HLS by applying the following directives:\n- 'Enable automatic pipelining for the loop' - #pragma ACCEL PIPELINE auto\n- 'Use automatic tiling with a factor of 8' - #pragma ACCEL TILE FACTOR=auto{8}\n- 'Apply automatic parallelization with a factor of 2' - #pragma ACCEL PARALLEL FACTOR=auto{2}\n- 'Perform reduction operation on tmp using automatic parallelization with a factor of 2_0' - #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n- 'Perform reduction operation on y using automatic parallelization with a factor of 2_1' - #pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n- 'Designate the following function as a hardware kernel for acceleration' - #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Treat the following function as a hardware kernel for acceleration\n\n2. Don't automatically pipeline this loop\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n\n4. Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the function definition:\n   'Treat the following function as a hardware kernel for acceleration'\n\n2. For the loop processing:\n   'Don't automatically pipeline this loop'\n   'Process this loop in its original, full form without breaking it into smaller chunks'\n   'Run the iterations of this loop one after the other, not in parallel'\n\n3. For the loop optimizations:\n   - Pipeline the loop automatically\n   - Set the tile factor for the loop automatically\n   - Parallelize the loop iterations (with specific factors for each loop)\n\nUsing these natural language instructions should help correctly generate the specified HLS codes in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Opt-out of auto-pipelining for this loop: Use '#pragma ACCEL PIPELINE auto{off}' to turn off automatic loop pipelining.\n\n2. Set tile factor to auto{1} to process the loop in its full, original form: Use '#pragma ACCEL TILE FACTOR=auto{1}' to apply a tile factor of 1 to the loop.\n\n3. Run loop iterations sequentially instead of in parallel: Utilize '#pragma ACCEL PARALLEL FACTOR=auto{1}' to execute loop iterations with a parallel factor of 1.\n\n4. Designate the 'spmv' function as a hardware kernel for acceleration: Use '#pragma ACCEL kernel' to treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Keep the loop from breaking into smaller chunks. Run the iterations of the outer loop in parallel with a factor of 29 and the inner loops in parallel with a factor of 29_0 for the tmp array and 29_1 for the y array. Pipeline the loops automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply automatic pipelining to this loop: `#pragma ACCEL PIPELINE auto{}`.\n\nUse automatic tiling with a factor of 2: `#pragma ACCEL TILE FACTOR=auto{2}`.\n\nParallelize this loop with a factor of 32: `#pragma ACCEL PARALLEL FACTOR=auto{32}`.\n\nTreat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loop.\n3. Process this loop with a tile factor of 8.\n4. Run the iterations of this loop in parallel with a factor of 25.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\n1. For the loop, don't automatically pipeline: \n   ```\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n2. Process the loop in its original, full form without breaking it into smaller chunks:\n   ```\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n3. Run the iterations of this loop one after the other, not in parallel:\n   ```\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n4. Treat the following function as a hardware kernel for acceleration:\n   ```\n   #pragma ACCEL kernel\n   ```\n\nApply the above natural language instructions to the corresponding #pragma values in the C code to generate the desired HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following loop in its original, full form without breaking it into smaller chunks, don't automatically pipeline this loop, and run the iterations of this loop in parallel with a factor of 32. Also, treat the following function as a hardware kernel for acceleration using pragma ACCEL kernel.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions to apply the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration and don't automatically pipeline the loops.\n  \n2. Process the loops in their original full form without breaking them into smaller chunks and run the iterations of the loops one after the other, not in parallel.\n\n3. Automatically pipeline the loops and divide them into smaller chunks for processing.\n\n4. Automatically pipeline the loops and process them as a full form without dividing into smaller chunks.\n\n5. Run the iterations of the loops in parallel with a factor of 1.\n\n6. Automatically divide the loops into smaller chunks for parallel processing with a factor of 2 and apply pipeline factor auto.\n\n7. Divide the loops into smaller chunks for parallel processing with specific factors.\n", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow the following comprehensive but concise natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration using:  \n   'Treat the following function as a hardware kernel for acceleration'\n\n2. Disable automatic pipelining for the loop with the comment:  \n   'Don't automatically pipeline this loop'\n\n3. Process the loop in its original, full form without breaking it into smaller chunks by:  \n   'Set the tile factor to 1 for the loop'\n\n4. Run the iterations of the loop one after the other, not in parallel by using:  \n   'Set the parallel factor to 1 for the loop'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes:\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nYou can use the following natural language instructions:\n\n1. Disable automatic pipelining for the loop: \"Don't automatically pipeline this loop\"\n2. Use an automatic tile factor of 25 for the loop: \"Process this loop with an automatic tile factor of 25\"\n3. Run the iterations of this loop in parallel with an automatic parallel factor: \"Process this loop in parallel with an automatic parallel factor\"\n4. Flatten the pipeline for the loop: \"Flatten the pipeline for this loop\"\n5. Use an automatic tile factor of 1 for the loop: \"Process this loop with an automatic tile factor of 1\"\n6. Run the iterations of this loop in parallel with an automatic parallel factor: \"Process this loop in parallel with an automatic parallel factor\"\n7. Disable automatic pipelining for the loop: \"Don't automatically pipeline this loop\"\n8. Use an automatic tile factor of 1 for the loop: \"Process this loop with an automatic tile factor of 1\"\n9. Run the iterations of this loop in parallel with an automatic parallel factor: \"Process this loop in parallel with an automatic parallel factor\"\n10. Use an automatic pipeline directive: \"Use an automatic pipeline directive\"\n11. Use an automatic tile factor for the loop: \"Process this loop with an automatic tile factor\"\n12. Run the iterations of this loop in parallel with a parallel factor of 1: \"Process this loop in parallel with a parallel factor of 1\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use the following natural language instructions:\n\n1. 'Designate the following function as a hardware kernel for acceleration' to describe '#pragma ACCEL kernel'\n\n2. 'Automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE auto{}'\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{1}'\n\n4. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n\n5. 'Automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE auto{}'\n\n6. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{1}'\n\n7. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'\n\n8. 'Automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE auto{}'\n\n9. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{30}'\n\n10. 'Run the iterations of this loop in parallel with a factor of 2' to describe '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n\nRemember to replace '__PARA__L0', '__PARA__L1', and '__PARA__L2' with the appropriate parallel factor values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nThese instructions correspond to the following #pragma values respectively:\n1. #pragma ACCEL kernel\n2. #pragma ACCEL PIPELINE \"off\"\n3. #pragma ACCEL TILE FACTOR=1\n4. #pragma ACCEL PARALLEL FACTOR=1", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS codes written in C, follow these natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n\n2. Do not automatically pipeline the loops.\n\n3. Process the loops with their original, full form without breaking them into smaller chunks.\n\n4. Run the iterations of the loops one after the other, not in parallel.\n\n5. Keep the pipeline auto-managed.\n\n6. Use an auto-managed tiling factor for the loops.\n\n7. Use an auto-managed parallelization factor for the loops.\n\nThese natural language instructions will guide the generation of the specified HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the loop manually by turning off automatic pipelining. Use a factor of 1 for tiling the loop. Run the iterations of the loop in parallel with a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply hardware acceleration to function `kernel_atax` which contains the following loop nests. Set automatic loop pipelining, tiling with a factor of 8, and parallel execution with a factor of 2. Use reduction on the variable `tmp` and `y` with a factor of 2 for both.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code in C:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   Use: `#pragma ACCEL kernel`\n   \n2. Don't automatically pipeline the first loop. \n   Use: `#pragma ACCEL PIPELINE \"off\"`\n   \n3. Process the first loop in its original, full form without breaking it into smaller chunks using an automatic tile factor of 4.\n   Use: `#pragma ACCEL TILE FACTOR=4`\n   \n4. Run the iterations of the second loop one after the other, not in parallel, with an automatic parallel factor of 29 and two reduction operations to variables `tmp` and `y` respectively.\n   Use: `#pragma ACCEL PARALLEL FACTOR=29`, `#pragma ACCEL PARALLEL reduction=tmp FACTOR=29_0`, `#pragma ACCEL PARALLEL reduction=y FACTOR=29_1`\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops.\n3. Process the loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of the loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions to set specific #pragma values:\n\n1. For the kernel function 'kernel_correlation', do not automatically pipeline the loops. Use the instruction: 'Don't automatically pipeline this loop'.\n   Use: #pragma ACCEL PIPELINE \"off\"\n\n2. Process the loops in their original, full form without breaking them into smaller chunks. Use the instruction: 'Process this loop in its original, full form without breaking it into smaller chunks'.\n   Use: #pragma ACCEL TILE FACTOR=1\n\n3. Run the iterations of the loops one after the other, not in parallel. Use the instruction: 'Run the iterations of this loop one after the other, not in parallel'.\n   Use: #pragma ACCEL PARALLEL FACTOR=1\n\n4. Treat the 'kernel_correlation' function as a hardware kernel for acceleration.\n   Use: #pragma ACCEL kernel\n\nFor coding those in C:\n\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE \"off\"\n  \n#pragma ACCEL TILE FACTOR=1\n  \n#pragma ACCEL PARALLEL FACTOR=1\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=1\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE \"off\"\n  \n#pragma ACCEL TILE FACTOR=1\n  \n#pragma ACCEL PARALLEL FACTOR=1\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=1\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n    stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n  }\n  \n#pragma ACCEL PIPELINE \"flatten\"\n  \n#pragma ACCEL TILE FACTOR=30\n  \n#pragma ACCEL PARALLEL FACTOR=1\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=1\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE \"auto{__PIPE__L3}\"\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=1\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE \"auto{__PIPE__L7}\"\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=1\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration. Pipeline the loop, tile the loop with a factor of 1, and run its iterations in parallel with a factor of 80 for the outermost loop, factor of 100 for the inner loop calculating the mean, factor of 80 for the inner loop calculating the standard deviation, factor of 100 for the inner loop centering and reducing the column vectors, and factor of 2 for the correlation matrix calculations. Additionally, disable automatic pipelining for centering and reducing the column vectors.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and apply the following optimizations:\n- Don't automatically pipeline the loop\n- Process the loop with a tiling factor of 4\n- Run the iterations of the loop in parallel with a factor of 29\n- Apply parallel reduction to the 'tmp' array with a factor of 29_0\n- Apply parallel reduction to the 'y' array with a factor of 29_1\"\n\nWhen converting the above natural language instruction to HLS codes in C, use the following #pragma values:\n- #pragma ACCEL kernel\n- #pragma ACCEL PIPELINE \"off\"\n- #pragma ACCEL TILE FACTOR=4\n- #pragma ACCEL PARALLEL FACTOR=29\n- #pragma ACCEL PARALLEL reduction=tmp FACTOR=29_0\n- #pragma ACCEL PARALLEL reduction=y FACTOR=29_1", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. Do not automatically pipeline this loop. Run the iterations of this loop one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. \"Don't automatically pipeline this loop\"\n   - Corresponding HLS code: `#pragma ACCEL PIPELINE auto{off}`\n\n2. \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   - Corresponding HLS code: `#pragma ACCEL TILE FACTOR=auto{4}`\n\n3. \"Run the iterations of this loop one after the other, not in parallel\"\n   - Corresponding HLS code: `#pragma ACCEL PARALLEL FACTOR=auto{29}`\n\n4. \"Specify the reduction to be applied for the 'tmp' variable\"\n   - Corresponding HLS code: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}`\n\n5. \"Specify the reduction to be applied for the 'y' variable\"\n   - Corresponding HLS code: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outer loop in parallel with factor=2, and the inner loops should be parallelized with factor=2. Perform loop pipelining and tiling with a factor of 4. Also, mark the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "## Comprehensive Natural Language Instruction:\n\nProcess the given loop with automatic pipeline optimization, using automatic tiling with a factor of 1, and parallelization with a factor of 32. Treat the enclosing function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given C code using the following instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Description: 'Treat the following function as a hardware kernel for acceleration'\n   - HLS code: '#pragma ACCEL kernel'\n\n2. Don't automatically pipeline this loop\n   - Description: 'Don't automatically pipeline this loop'\n   - HLS code: '#pragma ACCEL PIPELINE \"off\"'\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Description: 'Process this loop in its original, full form without breaking it into smaller chunks'\n   - HLS code: '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Description: 'Run the iterations of this loop one after the other, not in parallel'\n   - HLS code: '#pragma ACCEL PARALLEL FACTOR=1'", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and automatically pipeline the first loop, tile the second loop with factor 1, and parallelize the third loop with a factor of 29. Reduce the temporary variable for the second loop with a factor of 29_0, and reduce variable y for the third loop with a factor of 29_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in natural language, you can use the following comprehensive but concise instructions:\n\n1. Apply hardware acceleration to the following function: '#pragma ACCEL kernel'\n\n2. Do not automatically pipeline this loop: '#pragma ACCEL PIPELINE \"off\"'\n\n3. Execute the loop in its original, unaltered form without breaking it into smaller chunks: '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of this loop sequentially, not in parallel: '#pragma ACCEL PARALLEL FACTOR=1'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\n\nDon't automatically pipeline this loop\n\nRun the iterations of this loop and its parallel regions sequentially, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you might use the following comprehensive but concise natural language instruction:\n\n1. Mark the following function as a hardware kernel for acceleration.\n\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline the loop.\n\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process the loop in its original, full form without breaking it into smaller chunks.\n\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of the loop one after the other, not in parallel.\n\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To pipeline the outer loop automatically and the inner loop with a factor of __PIPE__L1, and tile both loops with a factor of __TILE__L1, use the following pragmas:\n- '#pragma ACCEL PIPELINE auto{}'\n- '#pragma ACCEL TILE FACTOR=auto{8}'\n- '#pragma ACCEL PIPELINE auto{__PIPE__L1}'\n- '#pragma ACCEL TILE FACTOR=auto{__TILE__L1}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n- '#pragma ACCEL kernel' for the function definition", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop with an auto-generated tiling factor of 8.\n4. Run the iterations of this loop in parallel with a factor of 2.\n5. Pipeline this inner loop with an auto-generated level of 1.\n6. Tile this inner loop with an auto-generated tiling factor of level 1.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To unroll the first loop in the stencil3d function, with the stencil3d kernel treated as a hardware kernel for acceleration, and with a pipeline off, auto-tiled with factor 8, pipeline in L1 with auto parallelization factor 2.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop, process it in its original full form without breaking it into smaller chunks, and run the iterations of the second loop in parallel with a factor of 29.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n- Use '#pragma ACCEL kernel' to specify a function as a hardware kernel for acceleration\n- For automatic pipelining of a loop, use '#pragma ACCEL PIPELINE auto{}'\n- Use '#pragma ACCEL TILE FACTOR=auto{4}' to break a loop into smaller chunks with automatic tiling factor of 4\n- To run loop iterations in parallel with a factor of 32, use '#pragma ACCEL PARALLEL FACTOR=auto{32}'", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, apply the following pragmas:\n- Mark the function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Automatically pipeline the loop: `#pragma ACCEL PIPELINE auto{}`\n- Use automatic tiling factor for the loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Use automatic parallel factor for the loop with reduction variables (tmp and y): `#pragma ACCEL PARALLEL FACTOR=auto{29}` and `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}` and `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Pipeline the outer loop automatically and tile the inner loop with a factor of 8. Also, parallelize the innermost loop with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\nUse these #pragma values respectively to achieve the desired HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks and treat the following function as a hardware kernel for acceleration. Pipeline the outer loop automatically, tile the loops with a factor of 8, and parallelize the innermost loop to a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To compile the following C code with HLS pragmas, you can follow these concise natural language instructions:\n\n1. 'Treat the following function as a hardware kernel for acceleration' to describe '#pragma ACCEL kernel'\n\n2. 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"' for the loop iterating over 'j' in the function.\n\n3. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1' for the loop iterating over 'j' in the function.\n\n4. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1' for the loop iterating over 'j' in the function.\n\n5. 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"' for the loop iterating over 'j' in the function.\n\n6. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1' for the loop iterating over 'j' in the function.\n\n7. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1' for the loop iterating over 'j' in the function.\n\n8. 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"' for the loop iterating over 'i' in the function.\n\n9. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1' for the loop iterating over 'i' in the function.\n\n10. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1' for the loop iterating over 'i' in the function.\n\n11. 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"flatten\"' for the loop iterating over 'i' and 'j' in the function.\n\n12. 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1' for the loop iterating over 'i' and 'j' in the function.\n\n13. 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1' for the loop iterating over 'i' and 'j' in the function.\n\n14. Finally, for the last loop in the function, 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"', 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1', and 'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1'.\n\nBy following these instructions, you can achieve the desired HLS coding behavior for the provided C code.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, for the loops inside the function, do not automatically pipeline them, process them in their original, full form without breaking them into smaller chunks, and run the iterations of the loops in parallel with a factor of 29.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks and run the iterations of this loop one after the other, not in parallel, and treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with specific #pragma values, follow these natural language instructions:\n\n1. For the loop, `for (i = 0; i < 124; i++)`, turn off automatic pipelining.\n\n2. For the loops iterating over `i` and `j`, ensure that they are processed in their original, full form without breaking them into smaller chunks.\n\n3. For the loop iterating over `i`, run its iterations one after the other, not in parallel, with a factor of 29.\n\n4. Within the loop iterating over `i`, parallelize the reduction of the variable `tmp` with a factor of 29_0.\n\n5. Again within the loop iterating over `i`, parallelize the reduction of the variable `y` with a factor of 29_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration, pipeline the loop automatically, tile the loop with a factor of 2, and parallelize both inner loops with a factor of 2. Enable reduction for the temporary array and the 'y' array within the inner loops.\"\n\nThese instructions correspond to the following #pragma values:\n- '#pragma ACCEL kernel'\n- '#pragma ACCEL PIPELINE auto{}'\n- '#pragma ACCEL TILE FACTOR=auto{2}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}'\n- '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}'\n- '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes in C by treating the function 'stencil3d' as a hardware kernel for acceleration. Within the function, automatically pipeline the outer loop and tile the loops with a factor of 8. Then, pipeline the inner loop and parallelize the loop iterations with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, and then apply the following optimizations: turn off automatic pipeline generation for the outer loop, set the tile factor for the outer loop to 8, pipeline the inner loop with a specific factor, and tile the inner loop with a specific factor, while running the iterations of the innermost loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instruction and pragma values:\n\n\"Transform the function 'kernel_atax' into a hardware kernel for acceleration, then disable automatic loop pipelining, set the loop tiling factor to 1, and parallelize the loops with a factor of 29.\"\n\nThis comprehensive but concise instruction covers the process of the codes and specifies the pragma values to be used.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To automatically pipeline the outer loop and inner loop, and apply tiling of factor 8 for the outer loop, use the following HLS pragmas in C:\n\n```c\n#pragma ACCEL PIPELINE auto{}\n#pragma ACCEL TILE FACTOR=8\n```\n\nAdditionally, apply automatic pipelining with a different factor for the inner loop using:\n\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n```\n\nFinally, enable automatic parallelization with a factor of 2 for the innermost loop using:\n\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n1. Treat the following function as a hardware kernel for acceleration by using the pragma '#pragma ACCEL kernel'.\n2. Don't automatically pipeline the loop by using the pragma '#pragma ACCEL PIPELINE \"off\"'.\n3. Process the loop in its original, full form without breaking it into smaller chunks by using the pragma '#pragma ACCEL TILE FACTOR=1'.\n4. Run the iterations of the loop one after the other, not in parallel, by using the pragma '#pragma ACCEL PARALLEL FACTOR=1'.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1'\n\n'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1'\n\n'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"'\n\n'Treat the following function as a hardware kernel for acceleration' to describe '#pragma ACCEL kernel'\n", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Turn off automatic pipelining for the outer loop and set the tile factor to 8. For the inner loops, set automatic pipelining with the L1 pragma and automatic tiling with the L1 factor. Additionally, set the parallel factor to 1 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the nested loops in the provided function for hardware acceleration using the following comprehensive but concise natural language instruction:\n-  Treat the outermost loop as a hardware kernel for acceleration.\n-  Automatically pipeline the outer loop with a suitable factor.\n-  Tile the outer and inner loops automatically with a suitable factor.\n-  Parallelize the innermost loop with a suitable factor.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. For the function 'kernel_atax', mark it as a hardware kernel for acceleration: \"Treat this function as a hardware kernel for acceleration\" - This corresponds to the directive '#pragma ACCEL kernel'.\n\n2. For the loop that initializes array 'y': \"Don't automatically pipeline this loop\" - This maps to the directive '#pragma ACCEL PIPELINE \"off\"'.\n\n3. For the loop iterating over 'i' with minimum '1' tile factor: \"Process this loop with its original, full form, without breaking it into smaller chunks\" - This corresponds to the directive '#pragma ACCEL TILE FACTOR=1'.\n\n4. For the nested loops iterating over 'i' and 'j', run the iterations one after the other, not in parallel, with a reduction clause for 'tmp': \"Run the iterations of these loops one after the other, not in parallel. Also, perform a reduction on variable tmp\" - This maps to the directives '#pragma ACCEL PARALLEL FACTOR=1' and '#pragma ACCEL PARALLEL reduction=tmp FACTOR=1'.\n\n5. For the nested loops with a reduction clause for 'y', and a higher parallel factor: \"Run the iterations of these loops in parallel with a reduction on variable y\" - This corresponds to the directive '#pragma ACCEL PARALLEL reduction=y FACTOR=29'.\n\nBy following these natural language instructions, you can generate the given HLS codes in C with the specific #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "- Generate hardware kernel for acceleration for the following function.\n- Turn off automatic loop pipelining.\n- Set auto tile factor of 8 for the loop iteration.\n- Pipeline the outer loop with factor __PIPE__L1.\n- Set tile factor for the outer loop with auto value __TILE__L1.\n- Run inner loop iterations in parallel with factor 2.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration.\"\n\n\"#pragma ACCEL kernel\"\n\n\"Turn off automatic pipelining for this loop.\"\n\n\"#pragma ACCEL PIPELINE auto{off}\"\n\n\"Process this loop in its original unaltered form without breaking it into smaller chunks.\"\n\n\"#pragma ACCEL TILE FACTOR=auto{1}\"\n\n\"Run the iterations of this loop one after the other, not in parallel.\"\n\n\"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given C codes, use the following natural language instructions to generate HLS #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, for the first loop, ensure it's not automatically pipelined. After that, process the loop in its original form without breaking it into smaller chunks, and run the iterations of the second loop in parallel with a factor of 32. Additionally, parallelize the inner loops with reduction operations for both 'tmp' and 'y' using a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you will need to utilize the following pragma values:\n\n1. 'Define the following function as a hardware kernel for acceleration' - This will be represented by the pragma '#pragma ACCEL kernel'\n2. 'Automatically pipeline this loop' - This will be represented by the pragma '#pragma ACCEL PIPELINE auto{}'\n3. 'Automatically determine the tiling factor for this loop' - This will be represented by the pragma '#pragma ACCEL TILE FACTOR=auto{2}'\n4. 'Automatically determine the parallel factor for this loop' - This will be represented by the pragma '#pragma ACCEL PARALLEL FACTOR=auto{29}'\n5. 'Automatically determine the reduction factor for a parallel construct for this loop' - This will be represented by the pragma '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}' for the inner loop and '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}' for the outer loop", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Description: Mark the function as a hardware kernel for acceleration\n   - HLS #pragma: #pragma ACCEL kernel\n\n2. Don't automatically pipeline this loop\n   - Description: Disable automatic loop pipelining\n   - HLS #pragma: #pragma ACCEL PIPELINE \"off\"\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Description: Set the loop tiling factor to 1 to keep the original loop structure\n   - HLS #pragma: #pragma ACCEL TILE FACTOR=1\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Description: Set the parallelization factor to 1 to execute iterations sequentially\n   - HLS #pragma: #pragma ACCEL PARALLEL FACTOR=1\n\nNote: The \"auto\" values in the original codes should be replaced with specific numerical values based on the characteristics of the design and the target hardware platform, such as the pipeline initiation interval, loop tiling factors, and parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes in C for the given function 'stencil3d' with the following pragmas: \n- 'Automatically pipeline the outer loop' \n- 'Use auto value for tiling factor for the outer loop' \n- 'Pipeline the inner loop with L1 pipeline type' \n- 'Use auto value for tiling factor for the inner loop' \n- 'Run the iterations of the inner loop one after the other, not in parallel'", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Automatically pipeline all eligible loops. Divide the outer loop into smaller chunks for parallel processing and use auto values to calculate the tile factor.'", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To provide a comprehensive but concise natural language instruction for the given HLS codes written in C, the following pragma values and processes should be described:\n\n1. For the kernel_correlation function, indicate that it should be treated as a hardware kernel for acceleration. This can be achieved with the pragma:\n   ```c\n   #pragma ACCEL kernel\n   ```\n\n2. Inside the kernel_correlation function, for the loops that calculate the mean, standard deviation, and correlation matrix:\n   - Specify that the loops should be automatically pipelined to maximize throughput without breaking the original loop structure into smaller chunks:\n   ```c\n   #pragma ACCEL PIPELINE auto{}\n   ```\n   - Indicate that the loops should be tiled to facilitate efficient data movements and maximize memory access optimization:\n   ```c\n   #pragma ACCEL TILE FACTOR=auto{1}\n   ```\n   - State that the loops should be parallelized to execute iterations in parallel for improved performance:\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n   #pragma ACCEL PARALLEL FACTOR=auto{2}\n   ```\n3. Additionally, for the specific loops:\n   - For the loop handling column vector centering and reduction, specify that automatic pipelining should be turned off to maintain the original loop structure:\n   ```c\n   #pragma ACCEL PIPELINE auto{off}\n   ```\n   - For the correlation matrix calculation loop, indicate specific pipeline and parallelization settings:\n   ```c\n   #pragma ACCEL PIPELINE auto{__PIPE__L3}\n   #pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n   #pragma ACCEL PARALLEL FACTOR=auto{2}\n   #pragma ACCEL PIPELINE auto{__PIPE__L7}\n   #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n   ```\n\nThis comprehensive but concise natural language instruction provides specific guidance on how to optimize the HLS codes using pragma values for automatic pipelining, tiling, and parallelization, while treating the kernel_correlation function as a hardware kernel for acceleration.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given C function using the following natural language instructions:\n- Consider the following function as a hardware kernel for acceleration \n  ```c\n  #pragma ACCEL kernel\n  ```\n- Don't automatically pipeline the main loop \n  ```c\n  #pragma ACCEL PIPELINE \"off\"\n  ```\n- Process the main loop in its original, full form without breaking it into smaller chunks  \n  ```c\n  #pragma ACCEL TILE FACTOR=1\n  ```\n- Run the iterations of the inner loop one after the other, not in parallel \n  ```c\n  #pragma ACCEL PARALLEL FACTOR=1\n  ```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\n5. Automatically determine the pipeline factor for this loop.\n   - Use: `#pragma ACCEL PIPELINE auto`\n\n6. Automatically determine the tiling factor for this loop.\n   - Use: `#pragma ACCEL TILE FACTOR=auto`\n\n7. Automatically determine the parallelization factor for this loop.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto`\n\n8. Flatten the loop to run its iterations in a single pipeline stage.\n   - Use: `#pragma ACCEL PIPELINE auto{flatten}`", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process an ATA transpose hardware kernel with the following natural language instructions:\n1. Pipeline the loop automatically.\n2. Apply tiling with an automatic factor of 2.\n3. Parallelize the outer loop with an automatic factor of 32.\n4. For the inner parallelized loop, apply a reduction to the variable tmp with an automatic factor of 32 for one instance, and to the variable y with an automatic factor of 32 for another instance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the outer loop automatically. Partition the loop with a tile factor of 4. Parallelize the innermost loop with a parallel factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop without automatic pipelining, then break the loop into smaller chunks with a factor of 1, and finally run the iterations of the loop in parallel with a factor of 32. Additionally, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "## Process a 3D stencil computation by treating the function as a hardware kernel with automatic pipeline and tiling factor, and loop parallelization in certain levels.\n\nIn natural language, the given HLS code can be described as, \"Treat the following function as a hardware kernel for acceleration with automatic pipeline and tiling factor computation. Also, parallelize the loop with specific factors at certain levels.\"", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipelining enabled, automatic tiling with a factor of 200, and automatic parallelization with a factor of 1. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{200}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in a pipelined manner.\nProcess the second loop with tiling factor set to 1 automatically.\nProcess the third loop with parallel factor set to 32 automatically.\nWithin the third loop, carry out reduction across iterations for the variable 'tmp' with a parallel factor of 32 and another reduction for the variable 'y' with a parallel factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To describe the HLS codes written in C in a natural language instruction, you would use the following concise instructions for the specific #pragma values:\n\n1. Treat the function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the following loop.\n3. Process the loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of the loop one after the other, not in parallel.\n5. Automatically pipeline the following loop.\n6. Break the loop into smaller chunks. The factor for tiling is determined automatically.\n7. Process the loop in parallel. The factor for parallelization is determined automatically for different iterations.\n8. Automatically pipeline the following loop with a specific configuration.\n9. Do not pipeline the loop automatically.\n10. Break the loop into smaller chunks with a specific factor determined automatically.\n11. Process the loop in parallel with a specific factor determined automatically for different iterations.\n12. Automatically pipeline the following loop with a specific configuration including loop-specific pipeline factor.\n13. Break the loop into smaller chunks with a specific factor determined automatically, with loop-specific tiling factor.\n14. Process the loop in parallel with a specific factor determined automatically for different iterations.\n\nThese natural language instructions describe the process of the codes and the specific #pragma values to use.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code using the following natural language descriptions:\n\n1. Enable automatic pipelining for the entire function\n   - #pragma ACCEL PIPELINE \"auto{}\"\n\n2. Automatically determine the optimal tiling factor for the outer loop\n   - #pragma ACCEL TILE FACTOR=\"auto{1}\"\n\n3. Enable automatic pipelining with specific level for the outer loop\n   - #pragma ACCEL PIPELINE \"auto{__PIPE__L1}\"\n\n4. Automatically determine the optimal tiling factor for the inner loop\n   - #pragma ACCEL TILE FACTOR=\"auto{__TILE__L1}\"\n\n5. Automatically determine the parallelization factor for the innermost loop\n   - #pragma ACCEL PARALLEL FACTOR=\"auto{2}\"\n\nRemember to replace `\"auto{}` and `auto{__PIPE__L1}` with actual values determined by the tool or user preference.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following comprehensive natural language instruction for the pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline the first loop.\n```c\n#pragma ACCEL PIPELINE off\n```\n\n3. Process the first loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of the second loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use the corresponding #pragma values:\n\n1. \"Don't automatically pipeline this loop\" - Use \"#pragma ACCEL PIPELINE auto{off}\"\n2. \"Process this loop in its original, full form without breaking it into smaller chunks\" - Use \"#pragma ACCEL TILE FACTOR=auto{1}\"\n3. \"Run the iterations of this loop one after the other, not in parallel\" - Use \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\" and \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\" and \"#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\"\n4. \"Treat the following function as a hardware kernel for acceleration\" - Use \"#pragma ACCEL kernel\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the corresponding #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n   - Natural language instruction: Declare this function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n   - Natural language instruction: Disable automatic pipelining for this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n   - Natural language instruction: Keep this loop in its original form without tiling or breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n   - Natural language instruction: Execute the iterations of this loop sequentially, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code written in C:\n\n1. Mark the function as a hardware kernel for acceleration.\n   - Use the pragma: #pragma ACCEL kernel\n\n2. Don't automatically pipeline the first loop.\n   - Use the pragma: #pragma ACCEL PIPELINE \"off\"\n\n3. Process the second loop in its original, full form without breaking it into smaller chunks with a tile factor of 8.\n   - Use the pragma: #pragma ACCEL TILE FACTOR=8\n\n4. Run the iterations of the third nested loop one after the other, not in parallel, with a parallel factor of 2.\n   - Use the pragma: #pragma ACCEL PARALLEL FACTOR=2\n\n5. Inside the third loop, parallelize the reduction of the variable 'tmp' with a parallel factor of 2.\n   - Use the pragma: #pragma ACCEL PARALLEL reduction=tmp FACTOR=2\n\n6. Inside the third loop, parallelize the reduction of the variable 'y' with a parallel factor of 2.\n   - Use the pragma: #pragma ACCEL PARALLEL reduction=y FACTOR=2", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To pipeline all loops automatically and set tile factor and parallel factor to be determined automatically by the tool, use the following pragmas:\n\n- Enable automatic loop pipelining: `#pragma ACCEL PIPELINE auto{}`\n- Enable automatic loop tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Enable automatic loop parallelization: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n- Mark the function as a hardware kernel: `#pragma ACCEL kernel`\n\nThese pragmas will instruct the tool to automatically optimize the pipelining, tiling, and parallelization of the given loops, and treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, you should use the following natural language instruction:\n\n\"Convert the following function into a hardware kernel for acceleration, then automatically pipeline the outer loop, use tiling with a factor of 2 for both dimensions, and run the inner loop iterations in parallel.\"\n\nThis instruction reflects the use of `#pragma ACCEL kernel` to identify the function as a hardware kernel, `#pragma ACCEL PIPELINE auto{}` to automatically pipeline the outer loop, `#pragma ACCEL TILE FACTOR=auto{2}` to apply tiling with a factor of 2 in both dimensions, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` to run the inner loop iterations in parallel with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following natural language instructions for the specific #pragma values:\n\n1. \"Treat the following function as a hardware kernel for acceleration\"\n   - Use: #pragma ACCEL kernel\n\n2. \"Don't automatically pipeline this loop\"\n   - Use: #pragma ACCEL PIPELINE auto{off}\n\n3. \"Process this loop in its original, full form without breaking it into smaller chunks\"\n   - Use: #pragma ACCEL TILE FACTOR=auto{1}\n\n4. \"Run the iterations of this loop one after the other, not in parallel\"\n   - Use: #pragma ACCEL PARALLEL FACTOR=auto{29}\n\n5. \"Run the iterations of this loop in parallel with reduction for 'tmp'\"\n   - Use: #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n\n6. \"Run the iterations of this loop in parallel with reduction for 'y'\"\n   - Use: #pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n\nThese instructions will help generate the provided HLS codes in C with the correct #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\"Treat the following function as a hardware kernel for acceleration. Then, for the first loop, don't automatically pipeline it. For the second loop, process it in its original, full form without breaking it into smaller chunks. Lastly, run the iterations of the third loop in parallel with a factor of 32, and for the innermost loops, perform reduction operations on `tmp` and `y` with a parallel factor of 32 as well.\"\n\nThis instruction corresponds to the following #pragma values:\n- \"#pragma ACCEL kernel\" \n- \"#pragma ACCEL PIPELINE off\"\n- \"#pragma ACCEL TILE FACTOR=1\"\n- \"#pragma ACCEL PARALLEL FACTOR=32\" and \"#pragma ACCEL PARALLEL reduction=tmp FACTOR=32_0\" and \"#pragma ACCEL PARALLEL reduction=y FACTOR=32_1\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate hardware kernel, pipeline with auto scheduling, tile with auto factor, parallel with auto factor 1 at innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire stencil3d function as a hardware kernel for acceleration. Disable automatic pipelining for the outer loop. Set the tile factor to 1 for the outer loop and apply automatic pipelining to the inner loop with L1 pipe pragma. Also, enable automatic tiling and parallelism with a factor of 2 for the middle loop.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, and then apply the following pragma values: \n1. Don't automatically pipeline this loop \n2. Break this loop into smaller chunks with a factor of 2 \n3. Run the iterations of this loop in parallel with a factor of 32 \n4. Run the inner loops in parallel with reducing factor for variable 'tmp' of 32 \n5. Run the inner loops in parallel with reducing factor for variable 'y' of 32", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the first loop:\n   - Process this loop in its original, full form without breaking it into smaller chunks\n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n2. For the second loop:\n   - Don't automatically pipeline this loop\n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n3. For the third loop (outer loop):\n   - Run the iterations of this loop one after the other, not in parallel\n   ```c\n   #pragma ACCEL PARALLEL FACTOR=1\n   ```\n\n4. For the fourth loop (inner loop):\n   - Run the iterations of this loop in parallel with a reduction operation on variable 'tmp'\n   ```c\n   #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n   ```\n\n5. For the fifth loop (inner loop):\n   - Run the iterations of this loop in parallel with a reduction operation on variable 'y'\n   ```c\n   #pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n   ```\n\nIn addition:\n   - Treat the following function as a hardware kernel for acceleration\n   ```c\n   #pragma ACCEL kernel\n   ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in its original, full form without breaking it into smaller chunks. Automatically pipeline the loops. Additionally, run the iterations of the second outer loop sequentially, not in parallel.\nThe natural language instruction is: \n'Process each loop in its original, full form without breaking it into smaller chunks. Automatically pipeline the loops. Additionally, run the iterations of the second outer loop sequentially, not in parallel.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Define the following function as a hardware kernel for acceleration.\n2. Do not automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_correlation' function as a hardware kernel for acceleration. Then, for each nested loop within the function:\n1. Turn off automatic loop pipelining.\n2. Maintain the original loop form without breaking it into smaller chunks.\n3. Run loop iterations one after the other (not in parallel) for each nested loop.\n4. For the last nested loop, apply pipeline and tiling with specific factors to optimize for performance.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, use the following natural language instructions with specific #pragma values:\n\n1. Mark the function as a hardware kernel for acceleration:\n   - Use '#pragma ACCEL kernel' to indicate that the function should be treated as a hardware kernel for acceleration.\n\n2. Disable automatic loop pipelining:\n   - Use '#pragma ACCEL PIPELINE \"off\"' to ensure that loop pipelining is turned off.\n\n3. Process the loop in its original, full form without breaking it into smaller chunks:\n   - Use '#pragma ACCEL TILE FACTOR=1' to specify that the loop should not be broken into smaller chunks and should be processed in its original form.\n\n4. Run the iterations of the loop in parallel with a factor of 32:\n   - Use '#pragma ACCEL PARALLEL FACTOR=32' to indicate that the iterations of the loop should be run in parallel with a factor of 32.\n\n5. Perform reduction operations in parallel for variables 'tmp' and 'y' with a factor of 32:\n   - Use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=32' to perform reduction operations in parallel for the variable 'tmp' with a factor of 32.\n   - Use '#pragma ACCEL PARALLEL reduction=y FACTOR=32' to perform reduction operations in parallel for the variable 'y' with a factor of 32.\n\nFollow these natural language instructions to generate the given HLS code in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C:\n```c\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n, double data[100][80], double corr[80][80], double mean[80], double stddev[80]) {\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{20}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j], (double)2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n\n    stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);\n  }\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n\n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nYou can use the following concise natural language instruction:\n```plaintext\nProcess this loop in its original, full form without breaking it into smaller chunks (#pragma ACCEL TILE FACTOR=1)\nDon't automatically pipeline this loop (#pragma ACCEL PIPELINE auto{off})\nRun the iterations of this loop one after the other, not in parallel (#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0} for the first loop, #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1} for the second loop, #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2} for the third loop, #pragma ACCEL PARALLEL FACTOR=auto{1} for the last loop)\nTreat the following function as a hardware kernel for acceleration (#pragma ACCEL kernel)\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you need to use the following #pragma values:\n\n1. Set the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline these loops.\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of the loops one after the other, not in parallel.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\nRemember to replace \"auto\" and \"__PARA__L0\" with specific values according to your system specifications and requirements.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outer loop in its original, full form without splitting, pipeline the inner loop automatically, and parallelize the outer loop with a factor of 32, also apply parallel reduction for 'tmp' and 'y' arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outer loop in its original, full form without breaking it into smaller chunks and treat the following function as a hardware kernel for acceleration. Automatically pipeline the outer loop and tile it with a factor of 4. For the inner loop, automatically pipeline it with a custom factor of __PIPE__L1 and tile it with a custom factor of __TILE__L1. Run the iterations of the innermost loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking into smaller chunks, and pipeline automatically with a specific level for the outer loop and the inner loop, and parallelize the innermost loop with a specific factor. Also, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these instructions:\n- Mark the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`\n- Disable automatic pipelining for the loop using `#pragma ACCEL PIPELINE auto{off}`\n- Keep the loop in its original, full form without breaking it into smaller chunks using `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the loop iterations one after the other, not in parallel, using `#pragma ACCEL PARALLEL FACTOR=auto{32}`\n- Specify parallel reduction for the inner loops with the appropriate reduction variables and factors using `#pragma ACCEL PARALLEL reduction=<variable> FACTOR=auto{32_<count>}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and apply automatic pipeline, tiling with factor 2, and parallelization with a factor of 32 for both inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"If the tool can automatically determine the pipeline initiation interval, apply automatic pipelining to the loop. For tiling, use an automatic factor. Parallelize the outer loop with a factor of 32 and perform reduction on 'tmp'. Also, parallelize the inner loop with a factor of 32 and perform reduction on 'y'. Lastly, treat the function 'kernel_atax' as a hardware kernel for acceleration.\"\n\nThis comprehensive instruction covers the necessary pragmas to be used for pipelining, tiling, parallelization with reduction, and designating the function as a hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use automatic loop pipeline, automatically determine the optimization factor for loop tiling, automatically determine the optimization factor for loop parallelization with reduction variables, and treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, follow these natural language instructions:\n\n1. The function `kernel_atax` should be treated as a hardware kernel for acceleration.\n2. Do not automatically pipeline the first loop.\n3. Process the loop with a tiling factor of 2.\n4. Run the iterations of the following loop in parallel with a factor of 32.\n5. Use parallelism with a factor of 32 for reduction involving the `tmp` variable.\n6. Use parallelism with a factor of 32 for reduction involving the `y` variable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Set the following function as a hardware kernel for acceleration, and for the loop inside the function, do automatic pipelining and flattening, tile with a factor of 4, and parallelize with a factor of 4 for both reduction arrays `tmp` and `y`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_correlation' function as a hardware kernel for acceleration.\nEnsure that the loop for calculating 'mean' is automatically pipelined and parallelize it with a factor of '__PARA__L0'.\nThe loop for calculating 'stddev' should also be automatically pipelined and parallelized with a factor of '__PARA__L1'.\nCenter and reduce the column vectors in the following loop, and calculate the m * m correlation matrix with auto-pipelining, tiling, and parallelizing with a factor of 2.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply automatic pipeline, 8x8 tiling, and parallelization with a factor of 4 for the outer loop and reduction factors of 4 for both 'tmp' and 'y' arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction in the respective #pragma values:\n\n- 'Don't automatically pipeline this loop'\n  Use: #pragma ACCEL PIPELINE auto{off}\n\n- 'Process this loop in its original, full form without breaking it into smaller chunks'\n  Use: #pragma ACCEL TILE FACTOR=auto{1}\n\n- 'Run the iterations of this loop one after the other, not in parallel'\n  Use: #pragma ACCEL PARALLEL FACTOR=auto{32}\n\n- 'Treat the following function as a hardware kernel for acceleration'\n  Use: #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should follow these natural language instructions:\n\n1. Mark the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the outer loop.\n3. Break the loops into smaller parts, with a tile factor of 8.\n4. Run the iterations of the outer loop in parallel, with a factor of 4.\n5. Run the inner loops in parallel, with reduction for the variable 'tmp' and a factor of 4 for each instance.\n6. Run the inner loops in parallel, with reduction for the variable 'y' and a factor of 4 for each instance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C:\n\n```\n#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n```\n\nThe comprehensive but concise natural language instruction to achieve this is:\n\n\"Automatically pipeline the loops, keep the original loop structure, and process the loops in parallel with the specified factor values. Additionally, treat the 'kernel_correlation' function as a hardware kernel for acceleration.\"", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline the first loop. Tile the second loop with a factor of 8. Parallelize the outer loop with a factor of 4. Reduce the temporary variable inside the first inner loop with a factor of 4_0. Reduce the output variable 'y' inside the second inner loop with a factor of 4_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following comprehensive but concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use `#pragma ACCEL kernel` to designate the function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline the first loop.\n   - Use `#pragma ACCEL PIPELINE \"off\"` to turn off automatic pipelining for the first loop.\n\n3. Process the second loop in its original, full form without breaking it into smaller chunks.\n   - Use `#pragma ACCEL TILE FACTOR=1` to specify a tile factor of 1 for the second loop.\n\n4. Run the iterations of the third loop one after the other, not in parallel, with a parallel factor of 32.\n   - Use `#pragma ACCEL PARALLEL FACTOR=32` to parallelize the iterations of the third loop with a factor of 32.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the first loop. Auto-tile the second loop with a factor of 8. Auto-parallelize the third loop with a factor of 4. Within the third loop, auto-parallelize with reduction for 'tmp' and 'y' variables, each with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in the original, full form without breaking it into smaller chunks.\nAutomatically pipeline the loops.\nRun the iterations of the outer loop one after the other, not in parallel.\nTreat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the following function as a hardware kernel, disable automatic loop pipelining, tile the loop with a factor of 8, and run the loop iterations in parallel with a factor of 4. Perform reduction operations on variable 'tmp' and 'y' with a parallel factor of 4 for both.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive and concise natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To achieve hardware acceleration in the C code, use the following concise natural language instruction:\n\n\"Turn the function into a hardware kernel, then: pipeline the loop automatically, tile the loop with a factor of 1, and parallelize the loop with a factor of 4.\"\n\nYou can use the following #pragma values:\n- `#pragma ACCEL kernel`\n- `#pragma ACCEL PIPELINE auto{}`\n- `#pragma ACCEL TILE FACTOR=auto{1}`\n- `#pragma ACCEL PARALLEL FACTOR=auto{4}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you need to use the following natural language instructions for the specific #pragma values:\n\n1. For the first 'for' loop, use \"Don't automatically pipeline this loop\" to generate '#pragma ACCEL PIPELINE auto{off}'.\n2. For the second 'for' loop, use \"Process this loop in its original, full form without breaking it into smaller chunks\" to generate '#pragma ACCEL TILE FACTOR=auto{1}'.\n3. For the third 'for' loop where 'j' is the iterator, use \"Run the iterations of this loop one after the other, not in parallel\" to generate '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'.\n4. For the fourth 'for' loop where 'j' is the iterator, use \"Don't automatically pipeline this loop\" to generate '#pragma ACCEL PIPELINE auto{off}'.\n5. For the fifth 'for' loop, use \"Process this loop in its original, full form without breaking it into smaller chunks\" to generate '#pragma ACCEL TILE FACTOR=auto{1}'.\n6. For the sixth 'for' loop where 'j' is the iterator, use \"Run the iterations of this loop one after the other, not in parallel\" to generate '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'.\n7. For the seventh 'for' loop, use \"Don't automatically pipeline this loop\" to generate '#pragma ACCEL PIPELINE auto{off}'.\n8. For the eighth 'for' loop, use \"Process this loop in its original, full form without breaking it into smaller chunks\" to generate '#pragma ACCEL TILE FACTOR=auto{30}'.\n9. For the ninth 'for' loop where 'i' is the iterator, use \"Run the iterations of this loop one after the other, not in parallel\" to generate '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'.\n10. For the tenth 'for' loop, use \"Pipeline this loop with automatic initiation interval\" to generate '#pragma ACCEL PIPELINE auto{__PIPE__L3}'.\n11. For the eleventh 'for' loop, use \"Process this loop in its original, full form without breaking it into smaller chunks\" to generate '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}'.\n12. For the twelfth 'for' loop, use \"Run the iterations of this loop one after the other, not in parallel\" to generate '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n\nBy following these natural language instructions, you can generate the given HLS codes in C with the specified #pragma values.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the first loop, don't automatically pipeline it. For the second loop, break it into smaller chunks with a tile factor of 8. Then, run the iterations of the second loop in parallel with a factor of 4. Within the second loop, perform reductions on the 'tmp' and 'y' arrays with a parallel factor of 4 for each.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the function 'kernel_atax' in C, apply these directives: \n1. Use '#pragma ACCEL kernel' to identify the hardware acceleration kernel. \n2. Disable automatic loop pipelining with '#pragma ACCEL PIPELINE auto{off}' to prevent automatic pipeline insertion. \n3. Set the loop tiling factor to 1 with '#pragma ACCEL TILE FACTOR=auto{1}' for full-size loop processing. \n4. Utilize '#pragma ACCEL PARALLEL FACTOR=auto{32}' to enable parallel execution with a factor of 32 for the outer loop. \n5. Apply the '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}' directive for parallel reduction with a factor of 32 for the temporary variable array. \n6. Use '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}' to parallelize the reduction of the 'y' array with a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To efficiently accelerate the `kernel_atax` function written in C, follow these comprehensive natural language instructions for incorporating the appropriate #pragma directives:\n\n\"Automatically pipeline the loops to enable concurrent execution of loop iterations and enhance throughput.\"\n\n\"Tile the loops with a factor of 8 to enable efficient memory access and data reuse within the loop nests.\"\n\n\"Parallelize the loops with a factor of 4 to perform multiple iterations simultaneously and improve hardware resource utilization.\"\n\nRemember to incorporate the generated HLS codes into the C source code to achieve hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For pipelining the loop with automatic settings:\n   Instruction: 'Automatically pipeline this loop'\n   #pragma: #pragma ACCEL PIPELINE auto{flatten}\n\n2. To process the loop in its original, full form without breaking it into smaller chunks:\n   Instruction: 'Automatically set the tile factor to 1 for tiling'\n   #pragma: #pragma ACCEL TILE FACTOR=auto{1}\n\n3. To run the iterations of the loop one after the other, not in parallel:\n   Instruction: 'Automatically set the parallel factor to 4 for parallelization'\n   #pragma: #pragma ACCEL PARALLEL FACTOR=auto{4}\n\n4. To treat the following function as a hardware kernel for acceleration:\n   Instruction: 'Mark the following function as a hardware kernel'\n   #pragma: #pragma ACCEL kernel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process a loop in its original form, automatically pipeline the loop, and then parallelize the loop with a factor of 4, using a tile factor of 8. Also, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following natural language instructions for pragma values:\n\n1. For the initial loop, \"Don't automatically pipeline this loop\"\n\n   Use: `#pragma ACCEL PIPELINE auto{off}`\n\n2. For the loop, \"Process this loop in its original, full form without breaking it into smaller chunks\"\n\n   Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n3. For the outer loop, \"Run the iterations of this loop one after the other, not in parallel\"\n\n   Use: `#pragma ACCEL PARALLEL FACTOR=auto{32}`\n\n4. For the inner loop reduction with 'tmp', \"Run the reduction of this loop in parallel with factor 32\"\n\n   Use: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}`\n\n5. For the inner loop reduction with 'y', \"Run the reduction of this loop in parallel with factor 32\"\n\n   Use: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}`\n\nAdditionally, use the pragma `#pragma ACCEL kernel` to treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Perform the loop iteration in a sequential, non-parallel manner. Then, within the loop, process it without automatically breaking it into smaller chunks. Furthermore, pipeline the loop in its original form, without automatically breaking it into smaller chunks. Run the loop iterations in parallel, with a factor of 4, and ensure that the reduction operation for the variable \"tmp\" is carried out in parallel. Additionally, perform the reduction operation for the variable \"y\" in parallel with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To parallelize the loops and optimize for HLS acceleration, use the following natural language instructions for the given HLS pragmas in the C code:\n\n- Enable automatic pipelining for loops: \"Automatically pipeline this loop for optimization.\"\n- Apply tiling with a factor of 8: \"Use tiling with a factor of 8 to optimize loop processing.\"\n- Parallelize the outer loop with a factor of 4: \"Parallelize the outer loop with a factor of 4 for concurrent processing.\"\n- Perform inner loop reduction for variable \"tmp\" with a factor of 4_0: \"Apply inner loop reduction for variable 'tmp' with a factor of 4_0 for optimization.\"\n- Perform inner loop reduction for variable \"y\" with a factor of 4_1: \"Apply inner loop reduction for variable 'y' with a factor of 4_1 for optimization.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration and do not automatically pipeline the loop. Process the loop in its original, full form without breaking it into smaller chunks with a tile factor of 1. Run the iterations of the loop in parallel with a factor of 4. For the inner loops, parallelize with reduction for both the 'tmp' and 'y' arrays with a factor of 4 for each.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Concurrently process the outer loop with a pipeline, divide the loop into chunks of 8, and execute the inner loops in parallel with a factor of 4 using reductions for both `tmp` and `y`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you can use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, then disable automatic pipelining and set the tile factor and parallel factor to 1 each for the specified loops.\"\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction for the pragma values:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loop.\n   `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of this loop one after the other, not in parallel, with a factor of 4.\n   `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n\n5. Inside the loop, parallelize the reduction operation for the variable 'tmp' with a factor of 4_0.\n   `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}`\n\n6. Inside the loop, parallelize the reduction operation for the variable 'y' with a factor of 4_1.\n   `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, you should use the following natural language instruction:\n\nProcess the entire kernel for hardware acceleration, don't automatically pipeline the loop, use a tile factor of 1 for tiling, and run the iterations of the loops one after the other, not in parallel with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the kernel function 'kernel_adi' by pipelining, tiling, and parallelizing the loops with offloading for optimal hardware acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop in parallel with a factor of 4.\n5. Run the iterations of this loop in parallel with a factor of 4 for a reduction operation on variable 'tmp'.\n6. Run the iterations of this loop in parallel with a factor of 4 for a reduction operation on variable 'y'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Process this loop in its original, full form without breaking it into smaller chunks' - to describe '#pragma ACCEL TILE FACTOR=1'\n\n'Run the iterations of this loop one after the other, not in parallel' - to describe '#pragma ACCEL PARALLEL FACTOR=1'\n\n'Don't automatically pipeline this loop' - to describe '#pragma ACCEL PIPELINE \"off\"'\n\n'Treat the following function as a hardware kernel for acceleration' - to describe '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code using HLS pragmas, follow these concise instructions:\n\n```plaintext\n1. Mark the function as a hardware kernel for acceleration using '#pragma ACCEL kernel'.\n\n2. Disable automatic loop pipelining with '#pragma ACCEL PIPELINE \"off\"'.\n\n3. Use automatic tiling with a factor of 8 using '#pragma ACCEL TILE FACTOR=auto{8}' to break the loop into smaller chunks for improved parallelism.\n\n4. Enable parallelization with a factor of 4 using '#pragma ACCEL PARALLEL FACTOR=auto{4}' to run loop iterations in parallel and improve performance.\n\n5. Use parallel reduction for variable 'tmp' with a factor of 4_0 and for variable 'y' with a factor of 4_1 using '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}' and '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}' respectively for efficient computation.\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nThese instructions correspond to the following #pragma values in C:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Set up the following function as a hardware kernel for acceleration and prioritize resource allocation for the loop iterations by automatically pipelining it, breaking it down into chunks, and running iterations in parallel with specific factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks. Automatically pipeline this loop for improved performance. Run the iterations of this loop in parallel for faster execution. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Specify that the loop can't be automatically pipelined' for '#pragma ACCEL PIPELINE \"off\"'\n'State that the loop should not be divided into smaller parts' for '#pragma ACCEL TILE FACTOR=1'\n'Instruct that the iterations should not run in parallel, but instead sequentially' for '#pragma ACCEL PARALLEL FACTOR=1'\n'Treat the given function as a hardware kernel for acceleration' for '#pragma ACCEL kernel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Convert this function into a hardware kernel for acceleration. Do not automatically pipeline the loops, process each loop in its original form without breaking them into smaller chunks, and run the iterations of the loops one after the other, not in parallel.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, then apply auto-pipelining, auto-tiling with a factor of 8, and auto-parallelism with a factor of 4. Use automatic parallel reduction for the inner loops for both 'tmp' and 'y' with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Convert the function to a hardware kernel for acceleration, disable automatic loop pipelining, set the loop tiling factor to 1, and run loop iterations in parallel with a factor of 4. Also, enable parallel reduction for the 'tmp' array and 'y' array with a factor of 4 for each.\"\n\nYou can use the following #pragma values respectively:\n- \"#pragma ACCEL kernel\"\n- \"#pragma ACCEL PIPELINE auto{off}\"\n- \"#pragma ACCEL TILE FACTOR=auto{1}\"\n- \"#pragma ACCEL PARALLEL FACTOR=auto{4}\"\n- \"#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\"\n- \"#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'atax' function as a hardware kernel for acceleration. Pipeline the loop automatically. Use a tiling factor of 8 for the outer loop. Parallelize the outer loop with a factor of 8. Additionally, parallelize the inner loops with a reduction for the 'tmp' and 'y' arrays using a factor of 8 for both.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the loop is pipelined automatically: `#pragma ACCEL PIPELINE auto`\n\nProcess the loop in its original, full form without breaking it into smaller chunks with automatic tile factor: `#pragma ACCEL TILE FACTOR=auto{1}`\n\nRun the iterations of this loop in parallel with an automatic factor of 4: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n\nRun the inner loop iterations for reduction of `tmp` and `y` variables in parallel, with auto factors 4_0 and 4_1, respectively: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}` and `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the `atax` function written in C, use the following comprehensive but very concise natural language instruction:\n\n\"Define the following function as a hardware kernel for acceleration. For the first loop, do not automatically pipeline it. Break the computation into four smaller chunks for better performance. Run the iterations of the main loop in parallel, also dividing the computation into four smaller chunks for better parallel processing. For the inner loops, apply the reduction operation and distribute the computation across four processing units.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops, use a tile factor of 1, and run the iterations of each loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration and apply the following pragma values: '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{1}', and '#pragma ACCEL PARALLEL FACTOR=auto{1}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Treat the following function as a hardware kernel for acceleration\n2. Don't automatically pipeline this loop \n3. Process this loop in its original, full form without breaking it into smaller chunks\n4. Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform hardware kernel acceleration on the function 'void kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])'. Set the loop pipelines off and the tile factor and parallel factor to 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, don't automatically pipeline and don't use the original, full form of the loops. For the \"Row Sweep\" and \"Column Sweep\" loops, use a tile factor of 1 and run the iterations one after the other, not in parallel. For the innermost loop of each sweep, use a parallel factor of 2 for \"Column Sweep\" and 16 for \"Row Sweep.\" For the subsequent loops, use a parallel factor of 1 for \"Column Sweep\" and 8 for \"Row Sweep.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you would apply the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process the loop in its original, full form without breaking it into smaller chunks\n   - Use: `#pragma ACCEL TILE FACTOR=auto{8}`\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n\n5. For the inner loop, parallelize the reduction of 'tmp' with a factor of 4\n   - Use: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}`\n\n6. For the second inner loop, parallelize the reduction of 'y' with a factor of 4\n   - Use: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code for the given C function with the following natural language instructions:\n\n1. Treat the function 'kernel_adi' as a hardware kernel for acceleration.\n2. Turn off automatic pipelining for the loops.\n3. Use a tile factor of 1 for loop tiling optimization.\n4. Run iterations of the loops one after the other, not in parallel, with a parallel factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code, use the following natural language instructions for the specific #pragma values:\n\n1. '''Treat this function as a hardware kernel for acceleration''' to describe `#pragma ACCEL kernel`.\n2. '''Don't automatically pipeline this loop''' to describe `#pragma ACCEL PIPELINE \"off\"`.\n3. '''Process this loop in its original, full form without breaking it into smaller chunks''' to describe `#pragma ACCEL TILE FACTOR=1`.\n4. '''Run the iterations of this loop one after the other, not in parallel''' to describe `#pragma ACCEL PARALLEL FACTOR=1`.\n\nApply these instructions to optimize the given C code effectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks, and treat the following function as a hardware kernel for acceleration. Also, don't automatically pipeline the loop, but run the iterations of this loop in parallel with a factor of 8, and use a tiling factor of 2 for the loop. Additionally, use a pipeline factor automatically determined by the flattening with parallel reductions for both 'tmp' and 'y' variables.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but very concise natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Don't automatically pipeline these loops: `#pragma ACCEL PIPELINE auto{off}`\n- Process these loops in their original, full form without breaking them into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the iterations of these loops one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n\n2. Do not automatically pipeline the loop.\n\n3. Process the loop using a tiling factor of 2.\n\n4. Run the iterations of the loop in parallel with a parallelization factor of 5.", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire 'kernel_adi' function as a hardware kernel for acceleration. Do not automatically pipeline the loops, process them in their original, full form without breaking them into smaller chunks, and run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1'\n\n'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1'\n\n'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C function defines a hardware kernel that performs an ADI algorithm using nested loops. To optimize the acceleration, various #pragma values are used. \n\nFor the outer loops, the following optimizations are applied:\n- To pipeline the outer loops, use `#pragma ACCEL PIPELINE auto{off}`.\n- To apply loop tiling to the outer loops, use `#pragma ACCEL TILE FACTOR=auto{1}`.\n- To run the iterations of the outer loops one after the other, not in parallel, use `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n\nFor the inner loops, the following optimizations are applied:\n- To pipeline the inner loops, use `#pragma ACCEL PIPELINE auto{off}`.\n- To apply loop tiling to the inner loops, use `#pragma ACCEL TILE FACTOR=auto{1}`.\n- To run the iterations of the inner loops in parallel, use `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the first inner loop and `#pragma ACCEL PARALLEL FACTOR=auto{16}` for the second inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipeline optimization, tile factor of 1, and parallel factor of 8. Use reduction for 'tmp' and 'y' arrays with factor of 8 for each. Additionally, treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration.\n\nAutomatically pipeline the outer loop and apply the default factor.\n\nApply tiling with a factor of 8 to the outer loop.\n\nAutomatically pipeline the middle loop with a custom factor __PIPE__L1.\n\nApply tiling with a custom factor __TILE__L1 to the middle loop.\n\nRun the iterations of the innermost loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the loop with `for (i = 0; i < 124; i++)`, use the directive 'Disable automatic loop pipelining'.\n   - Corresponding pragma: `#pragma ACCEL PIPELINE auto{off}`\n\n2. For the loop with `for (i = 0; i < 116; i++)`, use the directive 'Set loop tiling factor to 8'.\n   - Corresponding pragma: `#pragma ACCEL TILE FACTOR=8`\n\n3. For the same loop, use the directive 'Set loop parallelism factor to 4'.\n   - Corresponding pragma: `#pragma ACCEL PARALLEL FACTOR=4`\n\n4. Inside the loop, for the inner loop with `for (j = 0; j < 124; j++)`, use the directives 'Apply parallel reduction to variable tmp with factor 4_0' and 'Apply parallel reduction to variable y with factor 4_1'.\n   - Corresponding pragma for tmp: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=4_0`\n   - Corresponding pragma for y: `#pragma ACCEL PARALLEL reduction=y FACTOR=4_1`\n\nApply these instructions to generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Generate hardware accelerator for the following function and automatically pipeline the loops with specific factors, then break the loops into smaller chunks and run them in parallel'", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks. Also, treat the following function as a hardware kernel for acceleration, run the iterations of this loop one after the other, not in parallel, and don't automatically pipeline this loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the loop, \"spmv_1\", turn off auto-pipelining for better control:    \n   \"#pragma ACCEL PIPELINE auto{off}\"\n\n2. Process the loop \"spmv_2\" in its original, full form without breaking it into smaller chunks:    \n   \"#pragma ACCEL TILE FACTOR=auto{1}\"\n\n3. Run the iterations of \"spmv_1\" one after the other, not in parallel:    \n   \"#pragma ACCEL PARALLEL FACTOR=auto{1}\"\n\nAdditionally, make sure to treat the \"spmv\" function as a hardware kernel for acceleration:    \n   \"#pragma ACCEL kernel\"", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the function 'kernel_adi' as a hardware kernel for acceleration, then turn off automatic loop pipelining and set the loop tiling and parallelization factors to 1 for all relevant loops. Ensure that proper loop ordering and iteration dependencies are maintained.\"\n\nThis comprehensive but concise natural language instruction summarizes the necessary pragma directives for the given C code, instructing how to deal with loop pipelining, loop tiling, and loop parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\nTo pipeline the first loop automatically.\nTile the following loop with a factor of 8.\nRun the iterations of the next loop in parallel with a factor of 8. \nWithin this loop, run the inner loop's iterations in parallel with a reduction on variable \"tmp\" and a factor of 8_0.\nAlso, run the inner loop's iterations in parallel with a reduction on variable \"y\" and a factor of 8_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code for HLS, follow these steps:\n1. Mark the function as a hardware kernel using `#pragma ACCEL kernel`.\n2. Enable automatic pipeline optimization for loops using `#pragma ACCEL PIPELINE auto{flatten}`.\n3. Use tiling with an automatic tile factor of 8 by adding `#pragma ACCEL TILE FACTOR=auto{8}`.\n4. Parallelize the outer loop with automatic factor 8 using `#pragma ACCEL PARALLEL FACTOR=auto{8}`.\n5. Inside the outer loop, apply parallel reductions with an automatic factor 8 for `tmp` using `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}`, and for `y` using `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops and process each loop without breaking it into smaller chunks. Run the iterations of each loop one after the other, not in parallel, with a parallel factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\n- `#pragma ACCEL TILE FACTOR=1`\n\nDon't automatically pipeline this loop\n- `#pragma ACCEL PIPELINE \"off\"` \n\nRun the iterations of this loop one after the other, not in parallel\n- `#pragma ACCEL PARALLEL FACTOR=1` \n\nTreat the following function as a hardware kernel for acceleration\n- `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the following function as a hardware kernel for acceleration, don't automatically pipeline this loop, process this loop in its original, full form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following functions as hardware kernels for acceleration. In the first loop, automatically pipeline the computation. In the second loop, break it into smaller chunks automatically with a factor of 4 for tiling, and run the iterations of the loop in parallel with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops without automatic pipelining, factor tiling, and parallelization to handle the hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction for #pragma values:\n\n1. Mark the following function as a hardware kernel for acceleration.\n   - Use '#pragma ACCEL kernel' to mark the function as a hardware kernel.\n\n2. Disable automatic pipelining for the following loop.\n   - Use '#pragma ACCEL PIPELINE off' to disable automatic pipelining for the loop.\n\n3. Set the loop tile factor to 1 to retain the loop in its original, full form without breaking it into smaller chunks.\n   - Use '#pragma ACCEL TILE FACTOR=1' to set the loop tile factor to 1.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Use '#pragma ACCEL PARALLEL FACTOR=1' to run the loop iterations one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the outer loop and parallelize the inner loops with a factor of 2. Apply tiling to both the outer and inner loops.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Do not pipeline any part of the code automatically\nUse a tile factor of 1 for the code\nRun the iterations of the loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Add hardware acceleration directives to the given C code as follows:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline this loop\n- Process this loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire function as a hardware kernel for acceleration and apply the following optimizations:\n- Turn off automatic pipelining for the loops\n- Use a tile factor of 1 for loop tiling\n- Run the iterations of the loops one after the other, not in parallel, with a factor of 1 for loop parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply hardware acceleration to the function by using the directive '#pragma ACCEL kernel'. Then, utilize the following directives to optimize the loops:\n- Use '#pragma ACCEL PIPELINE auto{}' to automatically pipeline the loop.\n- Use '#pragma ACCEL TILE FACTOR=auto{8}' to break the loop into smaller chunks for parallel processing.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{8}' to run the loop iterations in parallel with a factor of 8.\n- Inside the loop, use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}' to parallelize the reduction with factor 8 for the 'tmp' variable.\n- Also inside the loop, use '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}' to parallelize the reduction with factor 8 for the 'y' variable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the given C code with the following natural language instruction:\n\nProcess this loop in its original, full form without breaking it into smaller chunks\n```c\n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, the following natural language instructions can be used:\n- Mark the function as a hardware kernel for acceleration\n- Don't automatically pipeline the loops to manage resource consumption\n- Break the loop into smaller chunks with a factor of 2 for tiling\n- Process the loop iterations in parallel with a factor of 8\n- Perform parallel reduction for the temporary variable with a factor of 8\n- Perform parallel reduction for the variable 'y' with a factor of 8", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Compile the code for hardware acceleration by marking the following function as a hardware kernel.\nIn the loop:\n- Turn off automatic pipelining.\n- Keep the loop in its original, full form without dividing it into smaller chunks.\n- Run the iterations of the loop in parallel with a factor of 8 for optimization purposes, both for the 'tmp' reduction and the 'y' reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loops inside, don't automatically pipeline them, and process them in their original, full form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given C code with HLS pragmas, use the following natural language instruction:\n\n\"Declare the following function as a hardware kernel for acceleration, pipeline the loop with automatic settings turned off, tile the loop with an automatic factor of 4, and parallelize the loop with an automatic factor of 2. Inside the loop, parallelize the inner loops with reduction for 'tmp' and 'y' using automatic factor settings in each case.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Treat the function 'kernel_adi' as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops.\n3. Process the loops in their original, full form without breaking them into smaller chunks.\n4. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the equivalent HLS codes written in C, follow these steps: \n\n1. Treat the following function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline this loop.\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of this loop one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```\n\n5. Flatten the loop executions for better performance.\n```c\n#pragma ACCEL PIPELINE \"flatten\"\n```\n\n6. Automatically determine the most optimized pipeline depth for this loop.\n```c\n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n```\n\n7. Automatically determine the most optimized tile factor for this loop.\n```c\n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n```\n\n8. Automatically determine the most optimized parallel factor for this loop.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the first loop and set the tile factor to auto. Parallelize the outer loop with a factor of 8 and use automatic parallelization with reduction for the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in parallel with a factor of 8, and pipeline the loops with a factor of 8_0 for the tmp reduction and a factor of 8_1 for the y reduction. Also treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, perform the following steps using specific #pragma values:\n\n1. Declare the function as a hardware kernel for acceleration.\n2. Turn off automatic pipelining for the entire function.\n3. Use a tile factor of 1 for loop optimization.\n4. Set the parallel factor to 1 for parallelization.\n\nThis will ensure the appropriate acceleration and optimization for the given C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use hardware acceleration and treat the following function as a hardware kernel, and apply automatic pipeline, tiling and parallelization with factor 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Use automatic pipeline and tile factor, and parallelize with factor 2. Also, pipeline with factor __PIPE__L2, tile with factor __TILE__L2, and parallelize reduction with factor __PARA__L3.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_adi' function as a hardware kernel for acceleration.\n\nSet the loop pipelining off and tile factor to 1 automatically for all loops within the function.\n\nRun the iterations of each loop one after the other, not in parallel, automatically with factor 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, and apply the following pragma instructions:\n1. Disable automatic pipelining for all loops within the function.\n2. Set the tile factor to 1 for all loops within the function, allowing them to remain in their original, full form.\n3. Run the iterations of all loops one after the other, not in parallel, by setting the parallel factor to 1 for all loops within the function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't pipeline the outer loop automatically. Use a tile factor of 2 for loop tiling. Run the iterations of the outer loop in parallel with a factor of 32. Within the outer loop, parallelize the inner loops with a reduction factor of 32 for 'tmp' and 'y' calculations, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks, and treat the following function as a hardware kernel for acceleration. Also, don't automatically pipeline the loop but instead allow it to be pipelined manually, and run the iterations of the loop in parallel with a factor of 8 for both the 'tmp' and 'y' reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, turn off automatic pipelining, set the tiling factor to 1, and run the loop iterations one after the other, not in parallel for both the outer and inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the #pragma values:\n\n1. For the first loop, Turn off automatic pipelining.\nExample: 'Disable automatic pipeline for this loop.'\n\n2. For the second loop, Tile the loop with a factor of 8 for better hardware optimization.\nExample: 'Divide this loop into smaller chunks with a factor of 8 for optimization.'\n\n3. For the third loop, Parallelize the loop with a factor of 8 to run iterations in parallel for better acceleration.\nExample: 'Run iterations of this loop in parallel with a factor of 8.'\n\n4. For both innermost loops, apply parallel reduction to the variables 'tmp' and 'y' with different factors for optimization.\nExample: 'Apply reduction operation to variables 'tmp' and 'y' with specific factors for parallel processing.'\n\nBy following these natural language instructions, you can generate the given HLS codes in C with the appropriate #pragma values for hardware acceleration optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive language instructions for specific #pragma values:\n\n1. Tell the compiler not to automatically pipeline the loops: \n   Use \"#pragma ACCEL PIPELINE auto{off}\"\n\n2. Process the loop in its original, full form without breaking it into smaller chunks:\n   Use \"#pragma ACCEL TILE FACTOR=auto{4}\"\n\n3. Run the iterations of this loop one after the other, not in parallel:\n   Use \"#pragma ACCEL PARALLEL FACTOR=auto{8}\"\n\n4. Treat the following function as a hardware kernel for acceleration:\n   Use \"#pragma ACCEL kernel\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware kernel for acceleration.\n\nProcess the first loop in its original, full form without breaking it into smaller chunks. \n\nDon't automatically pipeline the following loop. \n\nRun the iterations of the second loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should:\n\n1. Treat the following function as a hardware kernel for acceleration\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline this loop\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\n4. Run the iterations of this loop one after the other, not in parallel\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```\n\nMake sure to incorporate the above natural language instructions into your C code to generate the desired HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n- Treat the following function as a hardware kernel for acceleration\n- Do not automatically pipeline any loop\n- Process each loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of each loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the loop automatically. Use a tile factor of 1 for the loop. Parallelize the loop with a factor of 8. For the inner loops, parallelize with a reduction on variable 'tmp' and a factor of 8_0, and also parallelize with a reduction on variable 'y' and a factor of 8_1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply pipeline to this loop based on automatic optimization,\nUse tiling with an automatic factor for this loop,\nApply parallelization with an automatic factor of 2 to this loop,\nApply parallelization with an automatic factor of 5 to this loop,\nApply pipeline with an automatic L2 factor to this loop,\nUse tiling with an automatic L2 factor for this loop,\nApply parallelization with an automatic L2 factor to this loop,\nApply parallel reduction with automatic L3 factor to this loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware kernel for acceleration. Pipeline the first loop automatically. Set the tile factor to 1. Parallelize the second loop with a factor of 8 and apply reduction for variable tmp. Also, parallelize the third loop with a factor of 8 and apply reduction for variable y.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks. Also, don't automatically pipeline this loop. For the outer loop, automatically pipeline it with a factor of 1. And for the inner loop, automatically pipeline and tile it with a factor of 1 for parallelization.\n\n```c\n#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply the following directives to the C code:\n\n- Use hardware acceleration for the function: \n  ```c\n  #pragma ACCEL kernel\n  ```\n\n- Turn off automatic pipelining for the loop: \n  ```c\n  #pragma ACCEL PIPELINE auto{off}\n  ```\n\n- Use a tile factor of 1 for the loop: \n  ```c\n  #pragma ACCEL TILE FACTOR=auto{1}\n  ```\n\n- Run the iterations of the loop one after the other, not in parallel: \n  ```c\n  #pragma ACCEL PARALLEL FACTOR=auto{1}\n  ```\n\nEnsure that these directives are placed appropriately in the C code to enable the desired hardware acceleration and loop processing behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these instructions:\n\n1. Wrapping the function with `#pragma ACCEL kernel` to declare the function as a hardware kernel for acceleration.\n\n2. Turning off automatic pipelining for the loop with `#pragma ACCEL PIPELINE auto{off}`.\n\n3. Applying tiling with the factor of 2 for the loop using `#pragma ACCEL TILE FACTOR=auto{2}`.\n\n4. Running the loop iterations in parallel with a factor of 8, with reductions for the 'tmp' and 'y' arrays, using `#pragma ACCEL PARALLEL FACTOR=auto{8}` and `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}` and `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}` inside the loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this hardware kernel function assuming the pragma values of 'PIPELINE' and 'TILE' are to be set to 'off' and '1' respectively, and 'PARALLEL' factor to be '16' for the first loop and '8' for the second loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the corresponding #pragma values:\n\n1. Set the pipeline off for automatic pipelining for the outer loop and inner loops.\n2. Use a tile factor of 1 for the outer loop and inner loops.\n3. Run the iterations of the outer loop and inner loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form, without breaking it into smaller chunks. Treat the following function as a hardware kernel for acceleration. Don't automatically pipeline this loop. Run the iterations of this loop in parallel with factor 2. Inside the nested loop, run the iterations of this loop in parallel with factor 4, and auto-reduce with factor 4. Redistributions of the loop operation are done following the L2 loop protocol.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops of the specified function for hardware acceleration, and apply automatic pipelining, tiling with a factor of 2, and parallelization with auto-factorization for each level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Apply automatic pipeline off, tile factor of 1, and parallel factor of 1 to the loops in the function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Mark the following function to be treated as a hardware kernel for acceleration. Then, don't automatically pipeline the loop. Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of the outer loop in parallel with a factor of 2. Run the iterations of the inner loop in parallel with a factor of 8. Finally, pipeline the inner loop with a specific factor and tile the loops with specific factors.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop two at a time in parallel. For the outer loop, run the iterations in parallel with a factor of five. For the inner loop, run the iterations in parallel with a factor of auto{__PARA__L3} and utilize parallel reduction for the variable C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, don't automatically pipeline this loop, process this loop in its original, full form without breaking it into smaller chunks, and run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration and apply the following optimizations: automatically pipeline the outer loop and use default factor for tiling, parallelize the outer loop with a factor of 2, and parallelize the inner loop with a factor of 8, then pipeline the innermost loop with L2 pipeline pragma and apply tiling and parallelization with factors determined by L2 parameters.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code in C with the following natural language instruction:\n\"Make the given function a hardware kernel for acceleration, and do not automatically pipeline, tile, or parallelize any of the loops within the function.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the kernel_adi function as a hardware kernel for acceleration. Add pragma directives to turn off automatic pipelining and tiling. Also specify to run the iterations of loops one after the other, not in parallel, and ensure to treat the PARALLEL FACTOR as 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and apply the following directives to the loops: \n1. Do not automatically pipeline the loop\n2. Use the original, full form of loop without breaking it into smaller chunks \n3. Run the loop iterations one after the other, not in parallel, with a parallel factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes, you would need to do the following:\n\n1. Use the directive '#pragma ACCEL kernel' to mark the function 'kernel_adi' as a hardware kernel for acceleration.\n\n2. Add '#pragma ACCEL PIPELINE auto{off}' to each of the nested loops to prevent automatic pipeline insertion.\n\n3. Insert '#pragma ACCEL TILE FACTOR=auto{1}' to apply tiling with a factor of 1 to the nested loops for optimization.\n\n4. Include '#pragma ACCEL PARALLEL FACTOR=auto{1}' inside the nested loops to enable parallelization with a factor of 1 for efficient execution in hardware.\n\nBy integrating these instructions, you can generate the desired HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code, use the following comprehensive but concise natural language instruction:\n\"Declare the following function as a hardware kernel for acceleration, disable automatic loop pipelining, set tiling factor to 2, and parallelize the inner loops with a factor of 8 for both the reduction operations on 'tmp' and 'y' arrays.\"\n\nThis would be represented using the following HLS pragma values:\n```c\n#pragma ACCEL kernel\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=2\n#pragma ACCEL PARALLEL FACTOR=8\n#pragma ACCEL PARALLEL reduction=tmp FACTOR=8_0\n#pragma ACCEL PARALLEL reduction=y FACTOR=8_1\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following loop is processed in its original, full form without breaking it into smaller chunks and then run the iterations of this loop one after the other, not in parallel. Finally treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform this function into a hardware kernel for acceleration, and apply automatic pipeline, tiling with factor 8, and parallelization with factor 8 for the outer loop, and reduction with factor 8 for the inner loops by using the following natural language instruction:\n\n\"Convert this function into a hardware kernel for acceleration and apply automatic pipeline, tiling with factor 8, and parallelization with factor 8 for the outer loop. Also, perform reduction with factor 8 for the inner loops.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Then, for the first loop, disable automatic pipelining. For the second loop, maintain its original form without dividing it into smaller sections. Finally, execute the iterations of the third loop sequentially, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for the function `kernel_atax`:\n\n- To automatically pipeline the loop, use `#pragma ACCEL PIPELINE auto{}`\n- To set the tile factor to 1, use `#pragma ACCEL TILE FACTOR=auto{1}`\n- To run the iterations of the loop in parallel with a factor of 8, use `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n- To apply reduction on the `tmp` variable with a factor of 8_0, use `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}`\n- To apply reduction on the `y` variable with a factor of 8_1, use `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}`\n- Lastly, specify that the function should be treated as a hardware kernel for acceleration, use `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Set the following function as a hardware kernel for acceleration and apply the following pragmas:\n- Don't automatically pipeline these loops\n- Process each loop in its original, full form without breaking it into smaller chunks\n- Run the iterations of these loops one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you can use the following comprehensive but very concise natural language instruction:\n\n\"Designate the function as a hardware kernel for acceleration. Ensure that the loop is not automatically pipelined. Utilize tiling with a factor of 2 and parallelize with a factor of 2 for the outer loops. For the innermost loops, auto-pipeline with a specific factor for the L2 pipeline level, then apply tiling with a specific factor and parallelization with a specific factor for the L2 level, and finally, use parallelization with a specific reduction factor for the L3 level.\"\n\nThis comprehensive instruction covers the process of the codes and specifies the pragma values to be used in natural language.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Use the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n   \n2. Don't automatically pipeline the outer loop.\n   - Use: `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process the outer loop in its original, full form without breaking it into smaller chunks.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{2}`\n\n4. Run the iterations of the outer loop in parallel with a factor of 8.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n\n5. Run the inner loop iterations in parallel with a reduction for the array 'tmp' with a factor of 8 for the outer loop computations.\n   - Use: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}`\n\n6. Run the inner loop iterations in parallel with a reduction for the array 'y' with a factor of 8 for the outer loop computations.\n   - Use: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction for the specific #pragma values:\n\nFor the kernel_atax function:\n- Treat the following function as a hardware kernel for acceleration.\n\nFor the loops inside kernel_atax function:\n- Don't automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop in parallel with a factor of 8.\n- Run the iterations of this loop one after the other, not in parallel, with a factor of 1 for reduction of tmp.\n- Run the iterations of this loop one after the other, not in parallel, with a factor of 1 for reduction of y.\n\nUsing these natural language instructions will help in inlining the specific #pragma values to generate the given HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and turn off automatic pipelining and set the tile factor to 1 for all loops while processing iterations sequentially, except for the parallelization factor of 32 for the innermost loop of the row sweep.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. Automatically pipeline this loop. Run the iterations of this loop in parallel, with a factor of 8. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks: ```#pragma ACCEL TILE FACTOR=1```\nRun the iterations of this loop one after the other, not in parallel: ```#pragma ACCEL PARALLEL FACTOR=1```\nDon't automatically pipeline this loop: ```#pragma ACCEL PIPELINE \"off\"```", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "# Compile the following function as a hardware kernel for acceleration, process loop in full form without breaking it into smaller chunks, do not automatically pipeline this loop, and run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the kernel_adi function as a hardware kernel for acceleration. Implement pipeline optimization with auto{off}, tile factor auto{1} for loop t, and parallel factor auto{1} for all loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Don't automatically pipeline the first loop, tile with a factor of 8, and parallelize with a factor of 8. Also, parallelize with reduction variables 'tmp' and 'y' using factors of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the function 'kernel_gemm', treat it as a hardware kernel for acceleration.\n\n2. For the outer loop, do not automatically pipeline it.\n\n3. Process the outer loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of the outer loop in parallel with a factor of 25.\n\n5. For the inner loop, run the iterations one after the other, not in parallel.\n\n6. Pipeline the inner loop automatically with a specific factor for optimization.\n\n7. Break the inner loop into smaller chunks and process those chunks in parallel for optimization.\n\n8. Perform parallel reduction within the innermost loop with a specific factor for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To convert the given code into concise natural language instructions, you could use the following descriptions for the pragma values:\n\n1. Use '#pragma ACCEL kernel' to specify that the following function should be treated as a hardware kernel for acceleration.\n\n2. Implement '#pragma ACCEL PIPELINE auto{off}' for turning off automatic pipelining for the loops.\n\n3. Utilize '#pragma ACCEL TILE FACTOR=auto{1}' to set the tile factor for loop iteration.\n\n4. Apply '#pragma ACCEL PARALLEL FACTOR=auto{1}' for specifying the parallel factor for loop execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the following function as a hardware kernel for acceleration.\n\nProcess the loops in their original, full form without breaking them into smaller chunks. \n\nRun the iterations of these loops one after the other, not in parallel. Set the pipeline to be auto{off}.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate hardware design for the matrix-matrix product calculation with original loop structure, pipelining disabled, and tiling factor set to 1 for efficient data reuse. Additionally, use parallelized implementation with a factor of 25 for the outer loop and factor 1 for inner loop. Apply pipeline and tiling directives for L2 cache optimizations. Employ parallel reduction for the innermost loop with a factor determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code using HLS pragmas, you would need to apply the following natural language instructions:\n\n1. For the loop that calculates y[i], use the pragma 'Don't automatically pipeline this loop.'\n\n2. For the outer loop which iterates over 'i', use the pragma 'Process this loop in its original, full form without breaking it into smaller chunks' with a tile factor of 2.\n\n3. For both inner loops (one computing tmp[i] and the other updating y[j]), use the pragma 'Run the iterations of this loop one after the other, not in parallel' with a parallel factor of 8 for each loop.\n\n4. Lastly, for the entire function, use the pragma 'Treat the following function as a hardware kernel for acceleration.'\n\nBy following these instructions, you can effectively apply the HLS pragmas to the given C code for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Optimize this function for hardware acceleration, divide the loops into smaller tiles, and run the iterations one after the other, not in parallel'", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, follow the natural language instructions:\n\n- Convert the following function into a hardware kernel for acceleration.\n  - Apply pragma: '#pragma ACCEL kernel'\n\n- Turn off automatic pipelining for the following loop.\n  - Apply pragma: '#pragma ACCEL PIPELINE auto{off}'\n\n- Process the loop in its full, original form without breaking it into smaller chunks using a tile factor of 8.\n  - Apply pragma: '#pragma ACCEL TILE FACTOR=8'\n\n- Run the iterations of the loop in parallel with a factor of 4, and reduce the 'tmp' variable using the same factor.\n  - Apply pragma: '#pragma ACCEL PARALLEL FACTOR=4', '#pragma ACCEL PARALLEL reduction=tmp FACTOR=4_0'\n\n- Reduce the 'y' variable in parallel using a factor of 4.\n  - Apply pragma: '#pragma ACCEL PARALLEL reduction=y FACTOR=4_1'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function as a hardware kernel for acceleration and apply the following pragma values to the respective code sections:\n1. Disable automatic pipelining for the outer loop\n2. Use a tile factor of 1 for loop tiling\n3. Parallelize the outer loop with a factor of 25\n4. Pipeline the middle loop with L2 auto pipelining\n5. Apply a tile factor of L2 for loop tiling\n6. Parallelize the middle loop with a factor of L2\n7. For the innermost loop, parallelize with a reduction clause and a factor of L3", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Use `#pragma ACCEL kernel` to specify the function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop\n   - Use `#pragma ACCEL PIPELINE \"off\"` to disable automatic loop pipelining.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Use `#pragma ACCEL TILE FACTOR=1` to process the loop without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Use `#pragma ACCEL PARALLEL FACTOR=1` to run the iterations of the loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you need to apply the following natural language instructions for specific #pragma values:\n\n1. Apply 'Don't automatically pipeline this loop' to the loops using `#pragma ACCEL PIPELINE auto{off}`\n2. Use 'Process this loop in its original, full form without breaking it into smaller chunks' with `#pragma ACCEL TILE FACTOR=auto{1}`\n3. Implement 'Run the iterations of this loop one after the other, not in parallel' using `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n4. Treat the 'kernel_adi' function as a hardware kernel for acceleration by using `#pragma ACCEL kernel`\n\nBy applying these instructions to the given C codes, you will generate the desired HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, automatically pipeline the first loop, set the tiling factor automatically for the second loop, and parallelize the inner loops with a factor of 8, while also applying reductions for both temporary and final variables.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the given function as a hardware kernel for acceleration. Do not automatically pipeline, use a tile factor of 1, and run the iterations of the loops in parallel with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To optimize the given C code using HLS pragmas, use the following natural language instruction:\n\n\"Enable pipeline optimization for loops\" - to describe `#pragma ACCEL PIPELINE auto{off}`\n\"Use tiling with a factor of 1\" - to describe `#pragma ACCEL TILE FACTOR=auto{1}`\n\"Execute loop iterations sequentially, not in parallel\" - to describe `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\"Indicate the function as a hardware kernel for acceleration\" - to describe `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. For the loops, turn off automatic pipelining, set the tile factor to 1, and run the iterations of the loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks and treat the following function as a hardware kernel for acceleration. Additionally, pipeline the loop automatically and apply tiling with a factor of 8, while running the loop iterations in parallel with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the computation and data transfer for the following loops using hardware acceleration and parallelism, and automatically pipeline the loops for efficient processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for pragma values:\n\n1. For the kernel function, specify it as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loops: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process the loops in their original, full form without breaking them into smaller chunks: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of the outermost loop in parallel with a factor of 25: `#pragma ACCEL PARALLEL FACTOR=25`\n\n5. Run the iterations of the innermost loop in parallel with a factor of 2: `#pragma ACCEL PARALLEL FACTOR=2`\n\n6. Automatically pipeline the innermost loop: `#pragma ACCEL PIPELINE auto{__PIPE__L2}`\n\n7. Automatically process the innermost loop with tiling: `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`\n\n8. Automatically run the iterations of the innermost loop in parallel with a reduction for variable C: `#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Pipeline this loop without making any assumptions about the appropriate pipeline configuration\n- For the outer i-loop, use '#pragma ACCEL PIPELINE auto{off}'\n\nProcess this loop in its original, full form without breaking it into smaller chunks\n- For the outer i-loop, use '#pragma ACCEL TILE FACTOR=auto{1}'\n\nRun the iterations of this loop in parallel with a factor that efficiently utilizes the hardware resources\n- For the outer i-loop, use '#pragma ACCEL PARALLEL FACTOR=auto{25}'\n- For the inner j-loop, use '#pragma ACCEL PARALLEL FACTOR=auto{10}'\n\nRun the iterations of this loop one after the other, not in parallel\n- For the inner j-loop, use '#pragma ACCEL PIPELINE auto{__PIPE__L2}'\n- For the inner j-loop, use '#pragma ACCEL TILE FACTOR=auto{__TILE__L2}'\n- For the inner j-loop, use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n- For the inner k-loop, use '#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, automatically pipeline the outer loop, use a tiling factor of 2 for the outer loop, and parallelize the outer loop with a factor of 25. Additionally, automatically parallelize the inner loop with a factor of 2, pipeline the middle loop with a designated factor, use a tiling factor and parallel factor for the middle loop, and automatically parallelize the innermost loop with a designated reduction factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the loop runs in parallel, broken into 25 chunks, and treated as a hardware kernel for acceleration. Also, break the loop into tiles of size 8 and pipeline the inner loops for efficient processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Indicate that the function is a hardware kernel for acceleration: \n   - \"Treat the following function as a hardware kernel for acceleration\"\n\n2. Disable automatic loop pipelining: \n   - \"Don't automatically pipeline this loop\"\n\n3. Specify tile factor for loop transformation: \n   - \"Process this loop with a tile factor of 4\"\n\n4. Set parallel factor for loop parallelization: \n   - \"Run the iterations of this loop in parallel with a factor of 8\"\n\n5. Indicate reduction variables for parallelization:\n   - \"Perform reduction on variable 'tmp' with a parallel factor of 8_0\"\n   - \"Perform reduction on variable 'y' with a parallel factor of 8_1\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C for hardware acceleration:\n1. Mark the function as a hardware kernel for acceleration using '#pragma ACCEL kernel'.\n2. For the loop processing, disable automatic pipelining by adding '#pragma ACCEL PIPELINE \"off\"'.\n3. Use '#pragma ACCEL TILE FACTOR=1' to process the loop in its original, full form without breaking it into smaller chunks.\n4. To run the loop iterations one after the other, and not in parallel, include '#pragma ACCEL PARALLEL FACTOR=1'.\n\nAfter applying these instructions, the C codes will be ready for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_gemm' function as a hardware kernel for acceleration\n\nEnsure that the loop is not automatically pipelined\n\nAdjust the loop tiling factor automatically based on the specific requirements\n\nParallelize the loop with a factor of 25\n\nWithin the nested loops, parallelize with a factor of 2 and apply pipeline transformation automatically\n\nAdditionally, apply tile factor and parallel factor transformations automatically within the nested loops", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given HLS codes written in C, the following concise natural language instructions can be used to describe the #pragma values:\n\n- 'Treat the function as a hardware kernel for acceleration'\n- 'Automatically pipeline the loop'\n- 'Process the loop in its original, full form without breaking it into smaller chunks'\n- 'Run the iterations of this loop in parallel with a parallel factor of 8'\n- 'Run the inner loop in parallel with reduction operation for variable tmp and a parallel factor of 8_0'\n- 'Run the outer loop in parallel with reduction operation for variable y and a parallel factor of 8_1'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and use #pragma ACCEL PIPELINE auto{off}, #pragma ACCEL TILE FACTOR=auto{1}, #pragma ACCEL PARALLEL FACTOR=auto{1} to control loop pipelining, tiling, and parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration: 'Designate the following function as a hardware kernel for acceleration.'\n   \n2. Pipeline the loop automatically: 'Automatically pipeline this loop.'\n   \n3. Tile the loop with a factor to be determined automatically: 'Automatically determine the tiling factor for this loop.'\n   \n4. Parallelize the loop with a factor to be determined automatically: 'Automatically determine the parallelization factor for this loop.'\n   \n5. Parallelize the inner loop with a reduction variable 'tmp' and a factor to be determined automatically: 'Automatically determine the parallelization and reduction factor for the inner loop using the variable 'tmp'.'\n   \n6. Parallelize the inner loop with a reduction variable 'y' and a factor to be determined automatically: 'Automatically determine the parallelization and reduction factor for the inner loop using the variable 'y'.'\n\nThese instructions should help in generating the given HLS codes in C with the appropriate pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for C functions using the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the loop with auto-off and tile it with factor 4. Parallelize the loop with a factor of 8, and apply reduction to the tmp and y variables with a factor of 8 for both.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks\n```c\n#pragma ACCEL TILE FACTOR=1\n```\n\nRun the iterations of this loop one after the other, not in parallel\n```c\n#pragma ACCEL PARALLEL FACTOR=1\n```\n\nDon't automatically pipeline this loop\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\nTreat the following function as a hardware kernel for acceleration\n```c\n#pragma ACCEL kernel\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function as a hardware kernel for acceleration, and apply the following pragmas for loop optimizations:  \n1. Don't automatically pipeline the outer loop  \n2. Process the outer loop with a tile factor of 1  \n3. Run the outer loop iterations in parallel with a factor of 25  \n4. Automatically pipeline the middle loop with an L2 pragma  \n5. Apply a tile factor of L2 to the middle loop  \n6. Run the middle loop iterations in parallel with a factor of L2  \n7. Run a reduction operation on the inner loop iterations in parallel with a factor of L3", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use hardware acceleration for the following function and process the loops in parallel with a factor of 8. Also, pipeline the loops for improved performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Parallelize the outermost loop to run iterations in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=auto{1}', 'Treat the loop as a complete entity without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=auto{1}', and 'Do not automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE auto{off}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction for each specific #pragma value:\n\n1. For '#pragma ACCEL PIPELINE auto{off}', use the instruction 'Don't automatically pipeline this loop'.\n\n2. For '#pragma ACCEL TILE FACTOR=auto{4}', use the instruction 'Break this loop into smaller chunks with a factor of 4'.\n\n3. For '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}', use the instruction 'Run the iterations of this loop in parallel with a factor of __PARA__L0'.\n\n4. For '#pragma ACCEL PIPELINE auto{}', use the instruction 'Automatically pipeline this loop'.\n\n5. For '#pragma ACCEL TILE FACTOR=auto{1}', use the instruction 'Process this loop in its original, full form without breaking it into smaller chunks'.\n\n6. For '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}', use the instruction 'Run the iterations of this loop in parallel with a factor of __PARA__L1'.\n\n7. For '#pragma ACCEL PIPELINE auto{flatten}', use the instruction 'Automatically flatten the loop to improve performance'.\n\n8. For '#pragma ACCEL PIPELINE auto{__PIPE__L3}', use the instruction 'Automatically pipeline this loop with specific pipelining factor __PIPE__L3'.\n\n9. For '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}', use the instruction 'Automatically tile this loop with specific tiling factor __TILE__L3'.\n\n10. For '#pragma ACCEL PARALLEL FACTOR=auto{1}', use the instruction 'Run the iterations of this loop in parallel with a factor of 1'.\n\nThese natural language instructions can be used to generate the given HLS codes written in C.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply automatic pipeline optimization to the loop, turn off automatic pipeline optimization for the loop, set the tiling factor to 8, set the parallelism factor to 25, set the parallelism factor to 2, apply automatic pipeline optimization to the inner loop with L2 specified, set the tiling factor to L2 value, set the parallelism factor to L2 value, apply parallel reduction to C with a factor corresponding to L3 value, and set the parallelism factor to L3 value.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Turn off automatic pipelining for the loops and process the loops with a tile factor of 1. Run the iterations of the loops sequentially, not in parallel. Treat the following function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following natural language instruction:\n\n\"Treat the following function as a hardware kernel for acceleration, do not automatically pipeline this loop, tile the loop with a factor of 8, and parallelize the loops with a factor of 8 with reduction for variable tmp and y.\"\n\nThis instruction covers the pragma values to use in the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks; Run the iterations of this loop one after the other, not in parallel using reduction for both tmp and y; Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{29_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{29_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate hardware design using software function as a kernel.\n\nFor the given C code, use the following directives to generate HLS code: \n\n- Use `#pragma ACCEL kernel` to treat the `kernel_adi` function as a hardware kernel for acceleration.\n\n- Use `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipelining.\n\n- Use `#pragma ACCEL TILE FACTOR=auto{1}` to specify a tile factor of 1 for tiling.\n\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to set the parallel factor to 1 for parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'atax' function as a hardware kernel for acceleration, and use automatic pipelining, tiling with a factor of 1, and parallel processing with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration, pipeline the first loop automatically, use a tile factor of 1 for the second loop, and parallelize the iterations of the second loop with a factor of 8. Reduce variables 'tmp' and 'y' in parallel with a factor of 8 for their respective loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops with automatic pipelining, tiling with a factor of 8, and parallelizing with a factor of 8. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the first loop without automatic optimization. Tile the second loop with a factor of 8. Parallelize the outer loop with a factor of 8 and perform reduction on variable `tmp` and `y` with a factor of 8 in separate parallel regions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and do not automatically pipeline the loops, process the loops and run the iterations in their original form without breaking them into smaller chunks, and execute the iterations one after the other, not in parallel.\"\n\nThis instruction encompasses the necessary pragma values and their respective actions for each pragma in the provided C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - Add the pragma: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop\n   - Add the pragma: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - Add the pragma: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - Add the pragma: `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n   - Use the pragma: `#pragma ACCEL kernel`\n\n2. Automatically pipeline the loop for optimization.\n   - Use the pragma: `#pragma ACCEL PIPELINE auto{}`\n\n3. Break the loop into smaller chunks with a factor of 8 for tiling.\n   - Use the pragma: `#pragma ACCEL TILE FACTOR=auto{8}`\n\n4. Run the iterations of the loop in parallel with a factor of 8.\n   - Use the pragma: `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n\n5. Run the inner loop in parallel with a reduction operation on variable 'tmp' and a factor of 8_0.\n   - Use the pragma: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}`\n\n6. Run the inner loop in parallel with a reduction operation on variable 'y' and a factor of 8_1.\n   - Use the pragma: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you would use the following natural language instructions for the specific #pragma values:\n\n1. `#pragma ACCEL kernel`: Treat the following function as a hardware kernel for acceleration.\n\n2. `#pragma ACCEL PIPELINE auto{}`: Automatically pipeline this loop to enable concurrent execution of loop iterations.\n\n3. `#pragma ACCEL TILE FACTOR=auto{4}`: Break this loop into smaller chunks, each with a factor of 4, to facilitate parallel processing.\n\n4. `#pragma ACCEL PARALLEL FACTOR=auto{25}`: Run the iterations of this loop in parallel, with a parallel factor of 25, for high parallelism.\n\n5. `#pragma ACCEL PIPELINE auto{__PIPE__L2}`: Automatically pipeline this loop with a specific pipeline configuration named \"__PIPE__L2\".\n\n6. `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`: Break this loop into smaller chunks using a specific tile factor configuration named \"__TILE__L2\".\n\n7. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`: Run the iterations of this loop in parallel with a specific parallel factor configuration named \"__PARA__L2\".\n\n8. `#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}`: Perform parallel reduction on this loop with a specific parallel reduction factor configuration named \"__PARA__L3\".\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a GEMM kernel:\n* Populate the function as a hardware kernel for acceleration by calling `#pragma ACCEL kernel`.\n* Turn off automatic pipelining for this loop using `#pragma ACCEL PIPELINE off`.\n* Keep the loop at its original form without tiling by setting `#pragma ACCEL TILE FACTOR=1`.\n* Run the iterations of the outer loop in parallel with a factor of 25 using `#pragma ACCEL PARALLEL FACTOR=25`.\n* Run the iterations of the inner loop in parallel with a factor of 5 using `#pragma ACCEL PARALLEL FACTOR=5`.\n* Apply automatic pipelining with a specific level for the inner loop by using `#pragma ACCEL PIPELINE auto{__PIPE__L2}`.\n* Apply tiling with a specific factor for the inner loop by using `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`.\n* Run the iterations of the inner loop in parallel with a specific factor and reduction for `C` using `#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure this loop is processed in parallel with a factor of 8, and use automatic pipeline optimization. Use a tiling factor of 1 to process the loop in its original form, and automatically parallelize the reduction operations with a factor of 8 for both the 'tmp' and 'y' variables. Lastly, mark the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform matrix multiplication with specified parameters using hardware acceleration:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n  \n#pragma ACCEL PIPELINE off\n  \n#pragma ACCEL TILE FACTOR=1\n  \n#pragma ACCEL PARALLEL FACTOR=25\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=2\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto(__PIPE__L2)\n    \n#pragma ACCEL TILE FACTOR=auto(__TILE__L2)\n    \n#pragma ACCEL PARALLEL FACTOR=auto(__PARA__L2)\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto(__PARA__L3)\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for GEMM kernel using the following instructions:\n\n1. Treat the function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n\n2. Don't automatically pipeline the outer loop.\n```c\n#pragma ACCEL PIPELINE \"off\"\n```\n\n3. Process the outer loop with a tile factor of 2.\n```c\n#pragma ACCEL TILE FACTOR=2\n```\n\n4. Run iterations of the outer loop in parallel with a factor of 25.\n```c\n#pragma ACCEL PARALLEL FACTOR=25\n```\n\n5. Run the inner loop in parallel with a factor of 2.\n```c\n#pragma ACCEL PARALLEL FACTOR=2\n```\n\n6. Pipeline the inner loop automatically using L2 pipeline pragma.\n```c\n#pragma ACCEL PIPELINE \"auto{__PIPE__L2}\"\n```\n\n7. Process the inner loop with a tile factor automatically using L2 tile pragma.\n```c\n#pragma ACCEL TILE FACTOR=\"auto{__TILE__L2}\"\n```\n\n8. Run the inner loop in parallel with a factor determined automatically using L2 parallel pragma.\n```c\n#pragma ACCEL PARALLEL FACTOR=\"auto{__PARA__L2}\"\n```\n\n9. Run the reductions in the innermost loop in parallel with a factor determined automatically using L3 parallel reduction pragma.\n```c\n#pragma ACCEL PARALLEL reduction=C FACTOR=\"auto{__PARA__L3}\"\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the first loop, and use a tile factor of 8 for tiling. Parallelize the iterations of the outer loop with a factor of 8, and also parallelize the inner loops with reduction operations on 'tmp' and 'y' using a factor of 8 for each.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Automatically pipeline the first loop. Break the second loop into chunks with a tile factor of 8. Run the iterations of both the inner loops in parallel with a factor of 8, performing reduction operations on 'tmp' and 'y' respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration: \"#pragma ACCEL kernel\"\n\n2. Don't automatically pipeline the first loop: \"#pragma ACCEL PIPELINE \"off\"\"\n\n3. Process the first loop in its original, full form without breaking it into smaller chunks: \"#pragma ACCEL TILE FACTOR=1\"\n\n4. Run the iterations of the second loop one after the other, not in parallel, for a factor of 1: \"#pragma ACCEL PARALLEL FACTOR=1\"\n\n5. Automatically pipeline the first loop: \"#pragma ACCEL PIPELINE auto{}\"\n\n6. Automatically tile the first loop with a factor determined automatically: \"#pragma ACCEL TILE FACTOR=auto{4}\"\n\n7. Automatically parallelize the first loop with a factor determined automatically: \"#pragma ACCEL PARALLEL FACTOR=auto{8}\"\n\n8. Automatically parallelize the innermost loop with a reduction variable \"tmp\" and a factor determined automatically: \"#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\"\n\n9. Automatically parallelize the innermost loop with a reduction variable \"y\" and a factor determined automatically: \"#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration.\n\nFor the loop iteration:\n- Do not automatically pipeline the loop (#pragma ACCEL PIPELINE \"off\")\n- Split the loop into smaller chunks based on data dependencies with an auto factor of 2 (#pragma ACCEL TILE FACTOR=auto{2})\n- Run the iterations of this loop in parallel with an auto factor of 8 and perform reduction operations on the 'tmp' array and 'y' array for parallelism (#pragma ACCEL PARALLEL FACTOR=auto{8}, #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}, #pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1})", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_adi' function as a hardware kernel for acceleration and do the following:\n1. Don't automatically pipeline the loops.\n2. Process the loops in their original, full form without breaking them into smaller chunks (tile factor=1).\n3. Run the iterations of the loops one after the other, not in parallel (parallel factor=1).", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, follow these natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Do not automatically pipeline this loop.\n- Process this loop by breaking it into smaller chunks of factor 4.\n- Run the iterations of this loop in parallel with parallel factor \"__PARA__L0\".\n- Automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop in parallel with parallel factor \"__PARA__L1\".\n- Automatically pipeline this loop with flatten option.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop in parallel with parallel factor \"__PARA__L2\".\n\nThese instructions correspond to the specific #pragma values used in the given C code.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nFor the loop iterating over variable i, do not automatically pipeline.\n\nFor the loop iterating over variable i, process in its original form without breaking into smaller chunks.\n\nFor the loop iterating over variable i, run the iterations one after the other, not in parallel, and use a factor of 8 for parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, follow these natural language instructions:\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.\n- Set pipeline factor for the loop to a specific value.\n- Set tile factor for the loop to a specific value.\n- Set parallel factor for the loop to a specific value.\n- Set parallel reduction factor for the loop to a specific value.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel with a factor determined automatically; disable automatic loop pipelining; keep the loop in its original form without breaking into smaller chunks and process the second loop in parallel with a factor determined automatically while also using reduction for parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should follow the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n\n2. Don't automatically pipeline this loop.\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. Run the iterations of this loop one after the other, not in parallel.\n\n5. Don't automatically pipeline this loop.\n\n6. Process this loop in its original, full form without breaking it into smaller chunks.\n\n7. Run the iterations of this loop one after the other, not in parallel.\n\n8. Automatically flatten the pipeline for this loop.\n\n9. Process this loop in its original, full form without breaking it into smaller chunks.\n\n10. Run the iterations of this loop in parallel with a parallel factor of __PARA__L2.\n\n11. Automatically pipeline this loop with a pipeline factor of __PIPE__L5.\n\n12. Process this loop in its original, full form without breaking it into smaller chunks.\n\n13. Run the iterations of this loop in parallel with a parallel factor of __PARA__L6.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original form and avoid breaking it into smaller chunks.\nProcess the loop in parallel with a factor of 1, meaning run the iterations of this loop one after the other, not in parallel.\nTreat the following function as a hardware kernel for acceleration.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following directives:\n- Use '#pragma ACCEL kernel' to identify the function as a hardware kernel for acceleration.\n- Use '#pragma ACCEL PIPELINE auto{off}' to disable automatic loop pipelining.\n- Use '#pragma ACCEL TILE FACTOR=auto{8}' to apply tiling with a factor of 8 automatically.\n- Use '#pragma ACCEL PARALLEL FACTOR=auto{8}' to parallelize the loop with a factor of 8 automatically.\n- Use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}' to parallelize the reduction with a factor of 8 for the 'tmp' variable.\n- Use '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}' to parallelize the reduction with a factor of 8 for the 'y' variable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel with automatic factor selection, then pipeline the loop with automatic off setting, and perform tiling with a factor of 1. Process the second loop in parallel with automatic factor selection, and apply reduction for the inner loop in parallel with automatic factor selection. Additionally, treat the function as a hardware kernel for acceleration using '#pragma ACCEL kernel'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline the loops.\n3. Process the loops in their original, full form without breaking them into smaller chunks with tile factor set to 1.\n4. Run the iterations of the loops in parallel, with a parallel factor determined automatically (__PARA__L0).\n5. Automatically pipeline the loops with the flatten option enabled.\n6. Process the loops in their original, full form without breaking them into smaller chunks with tile factor set to 1.\n7. Run the iterations of the loops in parallel, with a parallel factor determined automatically (__PARA__L1).\n8. Turn off automatic loop pipelining.\n9. Process the loops in their original, full form without breaking them into smaller chunks with tile factor set to 1.\n10. Run the iterations of the loops in parallel, with a parallel factor determined automatically (__PARA__L2).\n11. Automatically pipeline the loops with a specific pipeline factor (__PIPE__L5).\n12. Run the iterations of the loops in parallel, with a parallel factor determined automatically (__PARA__L6).\n\nBy following these instructions, you can generate the given HLS codes in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. Then, for all loops within the function, disable automatic pipelining and set the tile factor and parallel factor as 1. For the innermost loops, also disable automatic pipelining and set the tile factor and parallel factor as 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, follow these natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks and set the tile factor to 25.\n4. Run the iterations of this loop one after the other, not in parallel, and set the parallel factor to the value defined as \"__PARA__L0\".\n5. Don't automatically pipeline this loop.\n6. Process this loop in its original, full form without breaking it into smaller chunks and set the tile factor to 1.\n7. Run the iterations of this loop one after the other, not in parallel, and set the parallel factor to the value defined as \"__PARA__L1\".\n8. Automatically flatten pipeline this loop.\n9. Process this loop in its original, full form without breaking it into smaller chunks and set the tile factor to 1.\n10. Run the iterations of this loop one after the other, not in parallel, and set the parallel factor to the value defined as \"__PARA__L2\".\n11. Automatically pipeline this loop and set the flatten attribute.\n12. Process this loop in its original, full form without breaking it into smaller chunks and set the tile factor to 1.\n13. Run the iterations of this loop one after the other, not in parallel, and set the parallel factor to the value defined as \"__PARA__L6\".\n\nBy following these instructions, the given HLS codes written in C can be generated.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes in C for the function 'kernel_adi' with the following natural language instructions:\n\n1. Process this loop in its original, full form without breaking it into smaller chunks. (For both column and row sweep loops)\n   Use: `#pragma ACCEL TILE FACTOR=1`\n\n2. Run the iterations of these loops one after the other, not in parallel. (For both column and row sweep loops)\n   Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\n3. Don't automatically pipeline these loops. (For all loops)\n   Use: `#pragma ACCEL PIPELINE \"off\"`\n\nAdditionally, treat the function 'kernel_adi' as a hardware kernel for acceleration.\nUse: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, the following concise natural language instruction can be used:\n\n\"Designate the following function as a hardware kernel for acceleration, then run the iterations of the first loop in parallel but without breaking it into smaller chunks, and finally, pipeline the loop to enable concurrent processing of iterations.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To achieve accelerated hardware kernel processing in the given C code, follow these concise natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   - Use: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop:\n   - Use: `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   - Use: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   - Use: `#pragma ACCEL PARALLEL FACTOR=1`\n\nThese instructions will ensure proper hardware acceleration for the given C code.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration. For the first loop, don't automatically pipeline and for the second loop, use a tiling factor of 8 and run the iterations in parallel with a factor of 8. Additionally, use parallel reduction for both the 'tmp' and 'y' arrays with a factor of 8 for each.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Treat the following function as a hardware kernel for acceleration\n2. Don't automatically pipeline this loop\n3. Process this loop in its original, full form without breaking it into smaller chunks\n4. Run the iterations of this loop one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Translation: `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop.\n   - Translation: `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - Translation: `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - Translation: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n5. Specific values like `auto`, `__PARA__L0`, `__PARA__L1`, etc., can be determined based on the specific characteristics of the loop and the design requirements.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following natural language tasks:\n\n1. **Disable automatic pipelining for the entire function**: To ensure the loops are not automatically pipelined, use the pragma `#pragma ACCEL PIPELINE \"off\"`.\n\n2. **Set the tiling factor to 1 for the loops**: Use the pragma `#pragma ACCEL TILE FACTOR=1` to instruct the compiler to process the loops in their original, full form without breaking them into smaller chunks.\n\n3. **Set the parallelization factor to 1 for the loops**: Use the pragma `#pragma ACCEL PARALLEL FACTOR=1` to run the iterations of the loops one after the other, not in parallel.\n\nFurthermore, ensure that the entire function is treated as a hardware kernel for acceleration using the pragma `#pragma ACCEL kernel`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes for the C function 'kernel_atax':\n- 'Treat the following function as a hardware kernel for acceleration'\n- 'Automatically pipeline the loop'\n- 'Set the tiling factor to be automatically determined'\n- 'Set the parallelization factor to be automatically determined with a factor of 8 for the inner loops and reduction variables'", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop to run parallel iterations for both \"s\" and \"q\" arrays, also efficiently handle the reductions using parallelism. Additionally, pipeline the loop to optimize the data flow and also perform tiling to enable efficient memory access. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following comprehensive but very concise natural language instructions for the specified #pragma values:\n\n- \"Designate the following function as a hardware kernel for acceleration\" for `#pragma ACCEL kernel`\n- \"Do not automatically pipeline this loop\" for `#pragma ACCEL PIPELINE \"off\"`\n- \"Process this loop in its original, full form without breaking it into smaller chunks\" for `#pragma ACCEL TILE FACTOR=1`\n- \"Run the iterations of this loop one after the other, not in parallel\" for `#pragma ACCEL PARALLEL FACTOR=1`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C:\n\n1. Specify that the following function should be treated as a hardware kernel for acceleration using the pragma: \n```c\n#pragma ACCEL kernel\n```\n\n2. Turn off automatic pipelining for the loop using the pragma: \n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n\n3. Process the loop in its original, full form without breaking it into smaller chunks using the pragma: \n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n\n4. Run the iterations of this loop in parallel with a factor of 8 but automatically determined using the pragma: \n```c\n#pragma ACCEL PARALLEL FACTOR=auto{8}\n```\n\n5. Run the iterations of the inner loop in parallel with a reduction applied to the variable 'tmp', and automatically determined factor using the pragma: \n```c\n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n```\n\n6. Run the iterations of the inner loop in parallel with a reduction applied to the variable 'y', and automatically determined factor using the pragma: \n```c\n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate concise natural language instructions to correspond with the given HLS codes:\n- Treat the following function as a hardware kernel for acceleration\n- Automatically pipeline this loop\n- Tile the loop with a factor of 2\n- Run the iterations of this loop in parallel with a specific factor\n- Automatically pipeline this loop with the flatten option\n- Tile the loop with a factor of 1\n- Run the iterations of this loop in parallel with a specific factor while performing a reduction operation\n- Turn off automatic pipelining for this loop\n- Tile the loop with a factor of 1\n- Run the iterations of this loop in parallel with a specific factor while performing a reduction operation\n- Automatically pipeline this loop with a specific factor\n- Tile the loop with a factor of 1\n- Run the iterations of this loop in parallel with a specific factor while performing a reduction operation", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following loop is fully pipelined and parallelized but not flattened. Additionally, use automatic tiling and parallelization factors to optimize performance. Use the pragma '#pragma ACCEL PIPELINE auto{}', '#pragma ACCEL TILE FACTOR=auto{8}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}', '#pragma ACCEL PIPELINE auto{flatten}', '#pragma ACCEL TILE FACTOR=auto{1}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}', '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{1}', and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for ADI algorithm:\n1. Mark the function as hardware kernel for acceleration\n2. Turn off automatic pipelining for the entire function\n3. Set tile factor to 1 for the entire function\n4. Run iterations one after the other, not in parallel for the entire function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instruction:\n\n\"Consider the following function as a hardware kernel for acceleration. Run the iterations of the first loop in parallel, treat the loop's pipeline automatically, and avoid breaking the loop into smaller chunks. For the second loop, run the iterations sequentially and treat the pipeline automatically, also avoid breaking the loop into smaller chunks.\"\n\nThis instruction could be translated into the following #pragma values:\n\n#pragma ACCEL kernel\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the GEMM function as a hardware kernel for acceleration. Pipeline the outer loop automatically. Tile the loops with a factor of 4. Parallelize the outer loop with a factor of 2. Parallelize the inner loop with a factor of 8 for updating C. Pipeline and tile the middle loop automatically. Parallelize the middle loop with reduction on variable C automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you can use the following natural language instructions:\n\n1. Apply automatic pipeline optimization to the loop\n   - Described by: `#pragma ACCEL PIPELINE auto`\n\n2. Perform loop tiling with a specific factor (e.g., 8) for optimization\n   - Described by: `#pragma ACCEL TILE FACTOR=auto{8}`\n\n3. Parallelize the outer loop with a specific factor (e.g., 2) for optimization\n   - Described by: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n4. Apply automatic pipeline optimization to inner loop with specific label (e.g., __PIPE_L2)\n   - Described by: `#pragma ACCEL PIPELINE auto{__PIPE__L2}`\n\n5. Perform loop tiling with a specific factor (e.g., __TILE_L2) for optimization in the inner loop\n   - Described by: `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`\n\n6. Parallelize the inner loop with reduction and a specific factor (e.g., __PARA_L3) for optimization\n   - Described by: `#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}`\n\nApply these natural language instructions to the respective HLS code segments to generate the desired HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the following function with hardware kernel optimization, automatically pipeline the loops, apply tiling with factor 1, and run parallel iterations with the specified parallel factor values.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nKeep automatically pipelining the loop.\n\nDivide the loop into smaller chunks with auto factor for optimization.\n\nRun the iterations of the loop in parallel with an auto factor for optimization.\n\nParallelize the inner loop with reduction for the variable 'tmp' with an auto factor for optimization.\n\nParallelize the inner loop with reduction for the variable 'y' with an auto factor for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "1. Treat the following function as a hardware kernel for acceleration\n\n2. Don't automatically pipeline this loop\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n\n4. Run the iterations of this loop one after the other, not in parallel", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the iterations of the following loop sequentially, not in parallel; do not automatically pipeline this loop; and keep the loop in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Do not automatically pipeline the loops. Process each loop in its original form without breaking it into smaller chunks. Run the iterations of the loops in parallel, using the appropriate parallel factor for each loop.\"\n\nThis instruction covers the necessary #pragma values for the given HLS codes, including 'kernel', 'PIPELINE off', 'TILE FACTOR=1', and 'PARALLEL FACTOR=__PARA__L0, __PARA__L1, __PARA__L2, __PARA__L3, __PARA__L4, __PARA__L5, and __PARA__L6' respectively.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The natural language instruction for the given HLS codes in C would be:\n\"Designate the following function as a hardware kernel for acceleration, utilize parallelism for the outer loop with automatic factor determination, deactivate automatic loop pipelining, and apply tiling with automatic factor determination, and utilize parallelism for the inner loop with automatic factor and reduction determination.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "'Run the iterations of this loop one after the other, not in parallel' to describe '#pragma ACCEL PARALLEL FACTOR=1', and 'Don't automatically pipeline this loop' to describe '#pragma ACCEL PIPELINE \"off\"', and 'Process this loop in its original, full form without breaking it into smaller chunks' to describe '#pragma ACCEL TILE FACTOR=1', and 'Treat the following function as a hardware kernel for acceleration' to describe '#pragma ACCEL kernel'. The comprehensive but very concise natural language instruction for the given HLS codes would be:\n\n\"Run the iterations of the loop one after the other, not in parallel. Don't automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Treat the following function as a hardware kernel for acceleration.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - Use: `#pragma ACCEL kernel`\n   \n2. Don't automatically pipeline the outermost loop, but let the tool decide for the inner loops.\n   - Use: `#pragma ACCEL PIPELINE auto{}`\n   \n3. Process the outermost loop in its original, full form without breaking it into smaller chunks, and let the tool decide the factor for the inner loops tile.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n   \n4. Don't automatically pipeline the innermost loop, but let the tool decide for the outer loops.\n   - Use: `#pragma ACCEL PIPELINE auto{__PIPE__L1}`\n   \n5. Process the innermost loop in its original, full form without breaking it into smaller chunks, and let the tool decide the factor for the outer loops tile.\n   - Use: `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`\n   \n6. Run the iterations of the innermost loop in parallel with the tool deciding the factor for the outer loops parallelization.\n   - Use: `#pragma ACCEL PARALLEL FACTOR=auto{2}`", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instructions for the specific #pragma values:\n\n1. For the first loop, automatically pipeline it: \n   Use '#pragma ACCEL PIPELINE auto{}'\n\n2. For the second loop, process it in its original, full form without breaking it into smaller chunks: \n   Use '#pragma ACCEL TILE FACTOR=auto{1}'\n\n3. For the third loop, run the iterations one after the other, not in parallel: \n   Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n\n4. For the fourth loop, flatten the pipeline automatically: \n   Use '#pragma ACCEL PIPELINE auto{flatten}'\n\n5. For the fifth loop, automatically tile it with a factor of 4: \n   Use '#pragma ACCEL TILE FACTOR=auto{4}'\n\n6. For the sixth loop, run the iterations in parallel using a specific factor: \n   Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'\n\n7. For the seventh loop, turn off automatic pipelining: \n   Use '#pragma ACCEL PIPELINE auto{off}'\n\n8. For the eighth loop, process it in its original, full form without breaking it into smaller chunks: \n   Use '#pragma ACCEL TILE FACTOR=auto{1}'\n\n9. For the ninth loop, run the iterations in parallel using a specific factor: \n   Use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, please use the following natural language instructions for the specific pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nThese natural language instructions should correspond to the following pragma values in C:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=1\n```", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{20}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the first loop in parallel with automatic parallel factor and treat the following function as a hardware kernel for acceleration. Pipeline the loop and apply automatic tiling factor to the loop. Process the second loop in parallel with automatic parallel factor and use parallel reduction for the inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the iterations of the first loop in parallel, automatically handle the pipeline of the loop, do not split the loop into smaller chunks, then process the iterations of the second loop in parallel with reduction operations, and treat the entire function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration and apply the following optimizations to the loop iterations: automatically pipeline the loops, use auto-factor tiling for a block size of 4, parallelize the loops with a parallel factor of __PARA__L0 for the first loop, use flattening for loop pipelining, apply auto-factor tiling for a block size of 1, parallelize the loops with a parallel factor of __PARA__L1 for the second loop, turn off automatic loop pipelining for the third loop, use auto-factor tiling for a block size of 1, and parallelize the loops with a parallel factor of __PARA__L2 for the third loop.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, ensure the loops are automatically pipelined, employ a tiling factor of 1, parallelize the loops with the given factor, and then turn off automatic pipelining, use a tiling factor of 8, parallelize with the specified factor, flatten the pipeline for the outer loop, and parallelize the final loop with corresponding factors for pipelining and reduction.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in parallel with automatic factor evaluation, but do not pipeline any of them. Additionally, use a tile factor of 1 to maintain the original form of the loops. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks\nProcess this loop in its original, full form without breaking it into smaller chunks\nProcess this loop in its original, full form without breaking it into smaller chunks", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the first loop initializing the array s, use \"Run the iterations of this loop in parallel\" to describe '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n\n2. For the loop updating the arrays s and q, use \"Don't automatically pipeline this loop\" to describe '#pragma ACCEL PIPELINE auto{off}', and \"Process this loop in its original, full form without breaking it into smaller chunks\" to describe '#pragma ACCEL TILE FACTOR=auto{1}', and \"Run the iterations of this loop one after the other, not in parallel\" to describe '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n\nBy following these instructions, you can use the corresponding #pragma values to generate the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code, use the following comprehensive but concise natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration. Pipeline the loops automatically, do not break into smaller chunks, and run iterations in parallel when possible. Set tile factor to 25, factor for the first loop parallelization to auto, reduce mean variable, and turn off automatic pipeline. Set tile factor to 1, parallelization factor to auto, reduce data variable. Flatten pipeline and set tile factor to 1 for the last loop, with appropriate parallelization and reduction of the cov variable.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{25}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outer loop in parallel with auto factor, then process the inner loops in parallel with auto factor and auto reduction factor. Disable automatic pipelining. Keep the loop tiling factor automatic. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nFor the first loop, don't automatically pipeline this loop. Process this loop in its original, full form without breaking it into smaller chunks. Run the iterations of this loop one after the other, not in parallel.\n\nFor the second loop, automatically pipeline but with the off option. Process this loop in smaller chunks with a factor of 2. Run the iterations of this loop in parallel with a factor of 1.\n\nFor the third loop, automatically flatten pipeline. Process this loop with a factor of 1 for tiling. Run the iterations of this loop in parallel with specific pipeline factor.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the loops, but process them in their original, full form without breaking them into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions:\n\n- Treat the function \"kernel_adi\" as a hardware kernel for acceleration. \n- Turn off automatic loop pipelining for the entire function. \n- Set the tile factor to 1 for both column and row loops to process them in their full form without breaking them into smaller chunks. \n- Run the iterations of both column and row loops one after the other, not in parallel, by setting the parallel factor to 1 for both.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS code written in C, use the natural language instruction: \n\n\"Declare the following function as a hardware kernel for acceleration, pipeline the loops automatically, tile factor for loop splitting is 4, and run parallel iterations for the first loop. Disable automatic pipeline for the next loop, use a tile factor of 1, and run parallel iterations for the second loop in the original, full form. Additionally, flatten the pipeline for the third loop and run parallel iterations for it as well.\"\n\nThis instruction corresponds to the following #pragma values: \n```\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{}\n\n#pragma ACCEL TILE FACTOR=4\n\n#pragma ACCEL PARALLEL FACTOR=__PARA__L0\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=__PARA__L1\n\n#pragma ACCEL PIPELINE auto{flatten}\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=__PARA__L2\n```", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To accelerate the encryption process in C, use the following pragma instructions:\n\n1. For the function `aes256_encrypt_ecb`: Treat the following function as a hardware kernel for acceleration with `#pragma ACCEL kernel`.\n\n2. For the loop in the function `aes_expandEncKey_1`: Automatically pipeline this loop with `#pragma ACCEL PIPELINE auto{flatten}`.\n\n3. For the loop in the function `aes_expandEncKey_1`: Process this loop in its original, full form without breaking it into smaller chunks with `#pragma ACCEL TILE FACTOR=auto{1}`.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the ADI algorithm as a hardware kernel for acceleration; prevent automatic pipelining of all loops; set the tile factor to 1 for all loops; run iterations of all loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop in its original, full form without breaking it into smaller chunks. Automatically pipeline this loop. Run the iterations of this loop in parallel with a factor of 8. Treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "A very concise natural language instruction to generate the corresponding HLS codes in C would be as follows:\n- In order to enable hardware acceleration for the function `aes256_encrypt_ecb`, follow these steps:\n 1. Define the function as a hardware kernel using `#pragma ACCEL kernel`\n 2. Automatically pipeline the loop in the function by adding `#pragma ACCEL PIPELINE auto` with the option to flatten the loop (if needed)\n 3. Tile the loop with a factor determined automatically by adding `#pragma ACCEL TILE FACTOR=auto`", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and for the loop iterations do not automatically pipeline and keep the full loop intact without breaking it into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Do not automatically pipeline the loops, and process each loop in its original full form without breaking it into smaller chunks. Run the iterations of the loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "```c\n\"Don't automatically pipeline the loop\"\n#pragma ACCEL PIPELINE \"off\"\n\n\"Process the loop in its original, full form without breaking it into smaller chunks\"\n#pragma ACCEL TILE FACTOR=1\n\n\"Run the iterations of the loop one after the other, not in parallel\"\n#pragma ACCEL PARALLEL FACTOR=1\n\n\"Treat the following function as a hardware kernel for acceleration\"\n#pragma ACCEL kernel\n```", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for C function 'kernel_adi' with the following instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n```c\n#pragma ACCEL kernel\n```\n2. Turn off automatic pipelining for all loops.\n```c\n#pragma ACCEL PIPELINE auto{off}\n```\n3. Use a tile factor of 1 for all loops.\n```c\n#pragma ACCEL TILE FACTOR=auto{1}\n```\n4. Run the iterations of all loops one after the other, not in parallel.\n```c\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive natural language instruction:\n\n\"Mark the function as a hardware kernel for acceleration, turn off automatic loop pipelining, set the tile factor to 1 for loop tiling, and run loop iterations in parallel with a factor of 1.\"\n\nThis natural language instruction corresponds to the following #pragma values:\n```c\n#pragma ACCEL kernel\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following directives before iterating through the loops:\n- 'Don't automatically pipeline this loop'\n- 'Process this loop in its original, full form without breaking it into smaller chunks'\n- 'Run the iterations of this loop one after the other, not in parallel'\n\nThis will apply the necessary pragmas for the given C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C:\n\n1. Start by treating the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n\n2. Then, for the first nested loop:\n   - Disable automatic loop pipelining by using `#pragma ACCEL PIPELINE off`.\n   - Keep the loop in its original form without breaking it into smaller chunks with `#pragma ACCEL TILE FACTOR=1`.\n   - Run the loop iterations sequentially, not in parallel, with `#pragma ACCEL PARALLEL FACTOR=1`.\n\n3. For the second nested loop:\n   - Enable automatic loop pipelining with `#pragma ACCEL PIPELINE auto`.\n   - Divide the loop into smaller chunks with a factor of 8 using `#pragma ACCEL TILE FACTOR=8`.\n   - Run the loop iterations in parallel with a factor of `__PARA__L1` using `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`.\n\n4. For the third nested loop:\n   - Enable automatic loop pipelining with `#pragma ACCEL PIPELINE auto`.\n   - Keep the loop in its original form without breaking it into smaller chunks with `#pragma ACCEL TILE FACTOR=1`.\n   - Run the loop iterations in parallel with a factor of `__PARA__L2` using `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use hardware acceleration for the 'kernel_adi' function and apply the following pragma values: \n- Turn off automatic loop pipelining\n- Set tile factor to 1 for loop tiling\n- Set parallel factor to 1 for loop parallelization", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Turn off automatic pipeline for the loop.\n- Set the tile factor to 1 to process the loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.\n\nThese instructions correspond to the respective #pragma values as follows:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process each loop in its original, full form, without breaking it into smaller chunks. Run the iterations of each loop one after the other, not in parallel. Treat the following function as a hardware kernel for acceleration.\n\n```c\n#pragma ACCEL kernel\n\n...\n\n#pragma ACCEL PIPELINE off\n\n#pragma ACCEL TILE FACTOR=1\n\n#pragma ACCEL PARALLEL FACTOR=1\n\n...\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the kernel_adi function as a hardware accelerator kernel. Disable automatic pipelining for the loops. Use a tile factor of 1 for loop tiling. Run the loops in parallel with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To enable hardware acceleration and offloading for the given C code, use the following natural language instructions for specific pragma values:\n\n1. For the loop in 'ecb2' function, don't automatically pipeline this loop: Use '#pragma ACCEL PIPELINE \"off\"'\n2. For the loop in 'ecb3' function, process this loop in its original, full form without breaking it into smaller chunks: Use '#pragma ACCEL TILE FACTOR=1'", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Mark the following function as a hardware kernel for acceleration, pipeline the loop automatically, process the loop in its full form without breaking it into smaller chunks, and run the iterations of the loop in parallel.\"\n\nThis instruction covers the use of the following #pragma values:\n- #pragma ACCEL kernel\n- #pragma ACCEL PIPELINE auto{}\n- #pragma ACCEL TILE FACTOR=auto{1}\n- #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n- #pragma ACCEL PIPELINE auto{off}\n- #pragma ACCEL TILE FACTOR=auto{2}\n- #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n- #pragma ACCEL PIPELINE auto{off}\n- #pragma ACCEL TILE FACTOR=auto{1}\n- #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n- #pragma ACCEL PIPELINE auto{__PIPE__L5}\n- #pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n\nThese directives cover the required optimizations and designations for accelerating and parallelizing the function `kernel_covariance` according to the given HLS codes in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions for the specific #pragma values:\n\n- For the hardware kernel acceleration: \"Treat the following function as a hardware kernel for acceleration\"\n\n- For pipelining of loops: \"Automatically pipeline the loops using auto{flatten} pragma\"\n\n- For disabling automatic pipelining of loops: \"Turn off automatic pipeline for the loops using auto{off} pragma\"\n\n- For tile factor specification: \"Specify the tile factor for the loops using TILE FACTOR=auto{1}\"\n\n- For parallelization specification: \"Specify the parallel factor for the loops using PARALLEL FACTOR=auto{1}\"\n\nBy following these natural language instructions, you can effectively implement the given HLS codes in C with the necessary #pragma values.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you can use the following comprehensive but concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, then automatically pipeline the loop, use auto for tile factor, and parallelize the loops with a factor of 16 for both inner loops and apply reduction for `tmp` and `y` variables.\"\n\nThis instruction encompasses the necessary steps and pragma values to generate the provided HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Consider the following natural language instructions for the provided HLS codes:\n\n1. \"Execute this loop without automatic pipelining\"\n- This describes `#pragma ACCEL PIPELINE \"off\"`\n\n2. \"Divide this loop into smaller chunks of 4\"\n- This represents `#pragma ACCEL TILE FACTOR=4`\n\n3. \"Execute the iterations of this loop in parallel with a factor of 8\"\n- This corresponds to `#pragma ACCEL PARALLEL FACTOR=8`\n\n4. \"Treat the following function as a hardware kernel for acceleration\"\n- This signifies `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C, use the following comprehensive but concise natural language instruction:\n\n\"Declare the following function as a hardware kernel for acceleration. Then, for each loop, disable automatic pipelining, set the tiling factor to 1, and run the iterations one after the other, not in parallel.\"\n\nThis natural language instruction corresponds to the following #pragma values:\n- '#pragma ACCEL kernel' - Treat the following function as a hardware kernel for acceleration\n- '#pragma ACCEL PIPELINE auto{off}' - Don't automatically pipeline these loops\n- '#pragma ACCEL TILE FACTOR=auto{1}' - Process these loops in their original, full form without breaking them into smaller chunks\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' - Run the iterations of these loops one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS codes for the given C function with the following natural language instruction:\n\n\"Create a hardware kernel for acceleration, do not automatically pipeline the loops, use a tile factor of 1, and run the iterations of the loops in parallel with a factor of 1.\"\n\nThis instruction translates into the following HLS pragma directives:\n```c\n#pragma ACCEL kernel\n#pragma ACCEL PIPELINE auto{off}\n#pragma ACCEL TILE FACTOR=auto{1}\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply the following comprehensive but very concise instruction to the given code:\n\"Parallelize and pipeline the loop iterations automatically at factor 1, treat the function as a hardware kernel for acceleration.\"\n```c\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Use hardware kernel acceleration for the function aes256_encrypt_ecb, disable automatic pipelining for the aes256_encrypt_ecb function, and apply tiling optimization with an automatically determined factor for the loop in ecb3.\"", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes in C for the given natural language instruction, follow these comprehensive but concise instructions:\n\n\"Initialize, expand and copy the encryption key provided and prepare for decryption\"\n\n\"Expand the encryption key using Rijndael key schedule operation\"\n\n\"Encrypt the data using AES-256 algorithm in ECB (Electronic Codebook) mode\"\n\n\"Use pipeline optimization for the encryption loop\"\n\n\"Use tiling optimization with automatic factor for the encryption loop\"\n\n\"Perform substitution, shift row and mix column operations on the data for every round\"\n\n\"Perform the final substitution, shift row and key addition operations for the last round\"\n\n\"Mark the encryption function as a hardware kernel for acceleration\"", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this function as a hardware kernel for acceleration,\nAutomatically pipeline the loop,\nSet the tile factor for loop tiling to automatic,\nSet the parallelization factor for the outer loop to automatic with a factor of 8 and for the inner loops to automatic with reduction factors for both tmp and y.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, and for the loops within it:\n- Disable automatic pipelining for the loops\n- Set the tiling factor to 1 for the loops\n- Set the parallelization factor to 1 for the loops\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instruction:\n- Treat the following function as a hardware kernel for acceleration\n- Don't automatically pipeline these loops\n- Process these loops in their original, full form without breaking them into smaller chunks\n- Run the iterations of these loops one after the other, not in parallel", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and use the following pragma values:\n- '#pragma ACCEL PIPELINE auto{off}'\n- '#pragma ACCEL TILE FACTOR=auto{1}'\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n- Inside the loop for column sweep:\n  - '#pragma ACCEL PIPELINE auto{off}'\n  - '#pragma ACCEL TILE FACTOR=auto{1}'\n  - '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n  - Inside the inner loop for column sweep:\n    - '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n  - After the inner loop for column sweep:\n    - '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n- Inside the loop for row sweep:\n  - '#pragma ACCEL PIPELINE auto{off}'\n  - '#pragma ACCEL TILE FACTOR=auto{1}'\n  - '#pragma ACCEL PARALLEL FACTOR=auto{1}'\n  - Inside the inner loop for row sweep:\n    - '#pragma ACCEL PARALLEL FACTOR=auto{4}'\n  - After the inner loop for row sweep:\n    - '#pragma ACCEL PARALLEL FACTOR=auto{2}'", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nAutomatically pipeline the loops, set tile factor to 1, and run loop iterations in parallel with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. For the function `aes256_encrypt_ecb`:\n   - Use `#pragma ACCEL kernel` to indicate that this function should be treated as a hardware kernel for acceleration.\n\n2. For the loop's inner function `aes_expandEncKey_1`:\n   - Use `#pragma ACCEL PIPELINE auto{flatten}` to enable automatic loop pipelining with flattening.\n\n3. For the loop in the function `aes256_encrypt_ecb` with the comment `ecb2`:\n   - Use `#pragma ACCEL PIPELINE auto{flatten}` to enable automatic loop pipelining with flattening.\n   - Use `#pragma ACCEL TILE FACTOR=auto{8}` to indicate automatic loop tiling with a factor of 8.\n\nThese instructions capture the specific #pragma values to replicate the given HLS codes in C.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction for the specific #pragma values:\n\n\"Mark the following function as a hardware kernel for acceleration\"\n- Use: `#pragma ACCEL kernel`\n\n\"Automatically pipeline this loop\"\n- Use: `#pragma ACCEL PIPELINE auto{}`\n\n\"Break this loop into smaller chunks for processing, with factor 2\"\n- Use: `#pragma ACCEL TILE FACTOR=auto{2}`\n\n\"Run the iterations of this loop one after the other, not in parallel\"\n- Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n\"Turn off automatic loop pipelining\"\n- Use: `#pragma ACCEL PIPELINE auto{off}`\n\n\"Process this loop in its original, full form without breaking it into smaller chunks\"\n- Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n\"Run the iterations of this loop in parallel, with a factor defined by '__PARA__L1'\"\n- Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n\"Turn on loop-flattening for this loop\"\n- Use: `#pragma ACCEL PIPELINE auto{flatten}`\n\n\"Break this loop into smaller chunks for processing, with factor 1\"\n- Use: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n\"Run the iterations of this loop in parallel, with a factor defined by '__PARA__L2'\"\n- Use: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n\"Turn on loop pipelining for this loop, with a specific factor defined by '__PIPE__L5'\"\n- Use: `#pragma ACCEL PIPELINE auto{__PIPE__L5}`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n- 'Treat the following loops as hardware kernels' to describe the use of `#pragma ACCEL kernel`\n- 'Automatically pipeline the loop' to activate the pipeline using `#pragma ACCEL PIPELINE auto{flatten}`\n- 'Turn off automatic pipelining for the loop' to deactivate the pipeline using `#pragma ACCEL PIPELINE auto{off}`\n- 'Apply tiling with an automatic factor of 4' to apply tiling using `#pragma ACCEL TILE FACTOR=auto{4}`", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate software to hardware kernel code in C:\nUse the instruction 'Treat this function as a hardware kernel for acceleration' to add the directive '#pragma ACCEL kernel' to the function 'aes256_encrypt_ecb'.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use hardware acceleration for the function 'kernel_atax' and automatically pipeline the loops. Tile the outer loop with a factor of 1 and parallelize the outer loop with a factor of 8. Also, parallelize inner loops with reduction for 'tmp' and 'y' with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, follow these natural language instructions:\n\n1. Use `#pragma ACCEL PIPELINE auto{off}` to prevent automatic pipelining for the loop in the function `aes256_encrypt_ecb`, to ensure that the loop is not automatically parallelized by the HLS tool.\n\n2. Use `#pragma ACCEL TILE FACTOR=auto{2}` to specify the tiling factor for the loop in the function `aes256_encrypt_ecb`. This will assist in partitioning the loop into smaller chunks, potentially enhancing parallelization.\n\n3. Add `#pragma ACCEL kernel` to declare the function `aes256_encrypt_ecb` as a hardware kernel for hardware acceleration. This indicates that the function should be implemented as a hardware accelerator, rather than being executed on the CPU.\n\nBy following these instructions, the given C code can be effectively translated into HLS code for hardware acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in parallel with automatic factorization and parallel reduction. Do not automatically pipeline the loops and maintain their original full form. Treat the function as a hardware kernel for acceleration.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions for the corresponding #pragma values:\n\n1. For the kernel function `aes256_encrypt_ecb`, use `#pragma ACCEL kernel` to treat the function as a hardware kernel for acceleration.\n\n2. For the `ecb3` loop inside the `aes256_encrypt_ecb` function, use `#pragma ACCEL TILE FACTOR=8` to break the loop into smaller chunks for parallel processing.\n\n3. For the `ecb2` loop inside the `aes256_encrypt_ecb` function, use `#pragma ACCEL PIPELINE auto{flatten}` to automatically pipeline the loop, and ensure it is flattened for efficient parallel processing.\n\n4. For the `ecb2` loop inside the `aes256_encrypt_ecb` function, use `#pragma ACCEL PIPELINE auto{}` to automatically pipeline the loop without flattening it.\n\n5. For the `ecb1` loop inside the `aes256_encrypt_ecb` function, use `#pragma ACCEL PIPELINE auto{}` to automatically pipeline the loop.\n\n6. For the `exp2` loop inside the `aes_expandEncKey_1` function, use `#pragma ACCEL PIPELINE auto{}` to automatically pipeline the loop.\n\n7. For the `exp1` loop inside the `aes_expandEncKey_1` function, use `#pragma ACCEL PIPELINE auto{}` to automatically pipeline the loop.\n\nThese natural language instructions describe the process of the codes and specify the relevant #pragma values to be used for acceleration in the HLS (High-Level Synthesis) environment.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_adi' function as a hardware kernel for acceleration to parallelize the loop iterations and pipeline the computation. Use a tile factor of 1 for loop tiling to keep loop data within the local storage to potentially reduce memory usage and pipeline the data movement.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you may use the following natural language instructions for specific #pragma values:\n\n1. For the function 'aes256_encrypt_ecb', to turn off automatic pipelining for the loop, use `#pragma ACCEL PIPELINE auto{off}`.\n2. For the function 'aes256_encrypt_ecb', to automatically factorize the loop into smaller chunks, use `#pragma ACCEL TILE FACTOR=auto{2}`.\n3. For the function 'aes256_encrypt_ecb', to make the loop potentially parallelable, use `#pragma ACCEL PARALLEL auto{flatten}`.\n4. For the function 'aes256_encrypt_ecb', to treat the function as a hardware kernel for acceleration, use `#pragma ACCEL kernel`.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for specific #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration.\n   - `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline the loop.\n   - `#pragma ACCEL PIPELINE \"off\"`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n   - `#pragma ACCEL TILE FACTOR=1`\n\n4. Run the iterations of this loop one after the other, not in parallel.\n   - `#pragma ACCEL PARALLEL FACTOR=1`", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loop iterations of both nested loops in parallel, with a factor determined automatically for efficiency. Then, do not automatically pipeline the loop. Lastly, process the loop in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in this function in parallel, with automatic factor determination for parallelization and reduction. For the first loop, turn off automatic pipelining. For the second loop, use a tile factor of 1. Finally, designate the entire function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the 'kernel_adi' function by partitioning its computation into smaller tasks and running those tasks in parallel, while also optimizing data pipeline utilization and ensuring that the loop iterations run one after the other, not in parallel.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use the following concise but comprehensive instructions to generate the HLS codes written in C:\n- Mark the function 'kernel_adi' as a hardware kernel for acceleration\n- Disable automatic pipeline optimization for the loop\n- Set the tiling factor to 1 for the loop\n- Set the parallel factor to 1 for the loop\n- Within the loop for 'Column Sweep', disable automatic pipeline optimization\n- Set the tiling factor to 1 for the loop within 'Column Sweep'\n- Set the parallel factor to 1 for the loop within 'Column Sweep'\n- Within the loop for 'Row Sweep', disable automatic pipeline optimization\n- Set the tiling factor to 1 for the loop within 'Row Sweep'\n- Set the parallel factor to 1 for the loop within 'Row Sweep'\n\nThese instructions correspond to the respective HLS codes written in C with the given #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you can utilize the following natural language instruction:\n\n1. Initialize the encryption and decryption keys using the provided key, by copying the key into the context's enckey and deckey arrays in a sequential manner.\n2. Sequentially expand the encryption key by calling the 'aes_expandEncKey_1' function 8 times with an increasing value of 'rcon'.\n3. Copy the encryption key into the key schedule and perform the initial round operation by adding the encryption key to the input buffer using 'aes_addRoundKey_cpy_1' function in a sequential manner.\n4. Sequentially perform the 14 rounds of encryption, each consisting of four steps: substitute bytes using 'aes_subBytes_1', shift rows using 'aes_shiftRows_1', mix columns using 'aes_mixColumns_1', and add round key using 'aes_addRoundKey_1'. For every odd round, use the precomputed last 16 bytes of the key as the next round key instead of expanding it, then perform the add round key step.\n5. Finally, substitute bytes, shift rows, expand the last round key, and add the final round key to complete the encryption process.\n\nUse the following #pragma values:\n- To pipeline the loop in the given for loop, use '#pragma ACCEL PIPELINE auto{flatten}'.\n- To turn off auto-pipelining for the function 'aes_expandEncKey_1', use '#pragma ACCEL PIPELINE auto{off}'.\n- To tile the operations with an automatically calculated factor for the for loop in the encryption process, use '#pragma ACCEL TILE FACTOR=auto{2}'.\n- At the beginning of the 'aes256_encrypt_ecb' function use '#pragma ACCEL kernel' to indicate it should be treated as a hardware kernel for acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this entire function as a hardware kernel for acceleration, turn off automatic pipelining for all loops, use a tile factor of 1 for all loops, and run the iterations of all loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Optimize the loop parallelization and reduction factor automatically. Turn off automatic loop pipelining. Use a tile factor of 1.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes in C, use the following natural language instructions:\n\n1. For the loop in function \"aes256_encrypt_ecb\" that performs encryption, use '#pragma ACCEL PIPELINE auto{off}' to prevent automatic pipelining.\n2. For the loop in function \"aes256_encrypt_ecb\" that requires parallelization, use '#pragma ACCEL TILE FACTOR=auto{4}' to specify the tiling factor for loop parallelization.\n3. For the entire \"aes256_encrypt_ecb\" function, use 'Treat the following function as a hardware kernel for acceleration' to specify this function as a hardware kernel for acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, you should use the following comprehensive but very concise natural language instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n2. Turn off automatic pipeline for the loops in the code.\n3. Set the loop tiling factor to 1.\n4. Make sure the loops are not parallelized.\n\nThese natural language instructions correspond to the specific #pragma values as follows:\n\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the whole 'gemm' function as a hardware kernel for acceleration, and then apply automatic pipelining, tiling with an auto factor, and parallelization with auto factors for different nested loops to optimize for FPGA acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the kernel_adi function as a hardware kernel for acceleration. For the loops inside, turn off automatic pipelining, set the tiling factor to 1, and execute iterations in a non-parallel manner.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow the following natural language instructions:\n\n1. \"#pragma ACCEL PIPELINE auto{off}\" - Don't automatically pipeline this loop.\n2. \"#pragma ACCEL TILE FACTOR=13\" - Process this loop in its original, full form without breaking it into smaller chunks.\n3. \"#pragma ACCEL PARALLEL FACTOR=auto{flatten}\" - Run the iterations of this loop one after the other, not in parallel.\n4. \"#pragma ACCEL kernel\" - Treat the following function as a hardware kernel for acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Run the following function as a hardware kernel for acceleration and apply specific optimizations to loops:\n\nFor the kernel_correlation function:\n- Turn off automatic loop pipelining.\n- Use a tile factor of 1 for loop iteration tiling.\n- Parallelize the outer loop with a factor of __PARA__L0.\n- Turn on automatic loop pipelining.\n- Use a tile factor of 4 for loop iteration tiling.\n- Parallelize the outer loop with a factor of __PARA__L1.\n- Turn off automatic loop pipelining.\n- Use a tile factor of 1 for loop iteration tiling.\n- Parallelize the outer loop with a factor of __PARA__L2.\n- Apply pipeline and tiling optimizations to the final nested loop.\n\nNote: The placeholders like auto{off}, auto{1}, and auto{__PARA__L0} are used to represent unknown values that may need to be determined based on the specific context of the hardware and compilation environment.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, then parallelize the outer loop with a factor determined automatically, pipeline the loops off, and finally tile the loops with a factor determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you need to utilize specific #pragma values for optimization. Here are comprehensive but concise natural language instructions for generating the given HLS codes:\n\n1. For the loop in function 'aes_expandEncKey_1', manually unroll the loop and optimize it for better resource utilization: \n   Use '#pragma ACCEL PIPELINE auto{off}' to turn off automatic loop pipelining for this specific loop.\n\n2. For the loop in function 'aes_expandEncKey_1', avoid breaking the loop into smaller chunks and keep it in its original full form: \n   Apply '#pragma ACCEL TILE FACTOR=1' to prevent the loop from being tiled into smaller components.\n\n3. For the loop in function 'aes_subBytes_1', parallelize it to enhance performance: \n   Implement '#pragma ACCEL PARALLEL FACTOR=auto{flatten}' to process the loop iterations in parallel for better acceleration.\n\n4. In general, to mark the function 'aes256_encrypt_ecb' as a hardware kernel for acceleration:\n   Use '#pragma ACCEL kernel' to specify that the following function should be treated as a hardware kernel for acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following directives: \n- Do not automatically pipeline any loops in this function.\n- Use a tile factor of 1 for loop tiling.\n- Set the parallel factor to 1 for parallelization in loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original, full form without breaking them into smaller chunks; don't automatically pipeline these loops; run the iterations of the innermost loop one after the other, not in parallel; and treat the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, follow these natural language instructions for the #pragma values:\n\n1. For the loop related to 'mean': Enable pipeline auto with \"#pragma ACCEL PIPELINE auto{}\"\n2. For the same loop, use tile factor 1 with \"#pragma ACCEL TILE FACTOR=1\"\n3. Parallelize the loop, setting parallel factor to \"__PARA__L0\" with \"pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\"\n4. For the loop related to 'data': Enable pipeline auto with \"pragma ACCEL PIPELINE auto{}\"\n5. Use tile factor 8 with \"pragma ACCEL TILE FACTOR=8\"\n6. Parallelize the loop, setting parallel factor to \"__PARA__L1\" with \"pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\"\n7. For the loop related to 'cov': Disable pipeline with \"pragma ACCEL PIPELINE auto{off}\"\n8. Set tile factor to 1 with \"pragma ACCEL TILE FACTOR=1\"\n9. Parallelize the loop, setting parallel factor to \"__PARA__L2\" with \"pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\"\n\nAdditionally:\n- Mark the function as a hardware kernel for acceleration with \"pragma ACCEL kernel\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process this loop in its original, full form without breaking it into smaller chunks (#pragma ACCEL TILE FACTOR=1)  \nDon't automatically pipeline this loop (#pragma ACCEL PIPELINE \"off\")  \nRun the iterations of this loop one after the other, not in parallel (#pragma ACCEL PARALLEL FACTOR=1)  \nTreat the following function as a hardware kernel for acceleration (#pragma ACCEL kernel)", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in parallel using auto-parallelization, then pipeline the loops to maximize throughput, and finally use auto-tiling to optimize memory access. Additionally, label the following function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function 'kernel_doitgen' as a hardware kernel for acceleration.\n\nTreat the loops as not automatically pipelined, and keep them in their original, full form without breaking them into smaller chunks.\n\nFor the outer loop, set the tile factor to 1 to process it as a whole.\n\nFor the inner loops, automatically pipeline them, and set the tile factor to a specific value to optimize their performance. Also, run the iterations of the last loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To use the given HLS codes written in C, you should:\n\n1. Prevent automatic pipelining for the loop in function 'aes256_encrypt_ecb' by applying the directive '#pragma ACCEL PIPELINE auto{off}'.\n2. Ensure that the loop in function 'aes256_encrypt_ecb' runs in its full form without breaking into smaller chunks by using the directive '#pragma ACCEL TILE FACTOR=auto{2}'.\n3. Treat the function 'aes256_encrypt_ecb' as a hardware kernel for acceleration using the directive '#pragma ACCEL kernel'.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. For the function `aes_expandEncKey_1`, use the following pragma: \"Automatically pipeline this loop\"\n2. For the function `aes_addRoundKey_cpy_1`, use the following pragma: \"No specific pragma needed\"\n3. For the function `aes_subBytes_1`, use the following pragma: \"No specific pragma needed\"\n4. For the function `aes_shiftRows_1`, use the following pragma: \"No specific pragma needed\"\n5. For the function `rj_xtime_1`, use the following pragma: \"No specific pragma needed\"\n6. For the function `aes_mixColumns_1`, use the following pragma: \"No specific pragma needed\"\n7. For the function `aes_addRoundKey_1`, use the following pragma: \"No specific pragma needed\"\n8. For the function `aes256_encrypt_ecb`, use the following pragma: \"Treat the following function as a hardware kernel for acceleration\"", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions to add specific #pragma values:\n\n1. For the function 'aes_expandEncKey_1':\n   - Description: Parallelize the loop for expanding encryption key\n   - #pragma Value: #pragma ACCEL PIPELINE auto{flatten}\n\n2. For the function 'aes_addRoundKey_cpy_1':\n   - Description: Parallelize the loop for adding round key and copying\n   - #pragma Value: No specific #pragma needed\n\n3. For the function 'aes_subBytes_1':\n   - Description: Vectorize the loop for substituting bytes\n   - #pragma Value: No specific #pragma needed\n\n4. For the function 'aes_shiftRows_1':\n   - Description: Unroll and parallelize the loop for shifting rows\n   - #pragma Value: No specific #pragma needed\n\n5. For the function 'rj_xtime_1':\n   - Description: No specific #pragma needed\n\n6. For the function 'aes_mixColumns_1':\n   - Description: Unroll and parallelize the loop for mixing columns\n   - #pragma Value: No specific #pragma needed\n\n7. For the function 'aes_addRoundKey_1':\n   - Description: Unroll and parallelize the loop for adding round key\n   - #pragma Value: No specific #pragma needed\n\n8. For the function 'aes256_encrypt_ecb':\n   - Description: Treat the function as a hardware kernel for acceleration\n   - #pragma Value: #pragma ACCEL kernel\n\nThese instructions provide the necessary guidance to generate the given HLS codes in C with specific #pragma values for optimization.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function as a hardware kernel for acceleration and apply the following optimizations: \n1. Turn off automatic loop pipelining for the outermost loop. \n2. Do not automatically pipeline the iterations of the outermost loop. \n3. Do not automatically break down the iterations of the outermost loop into smaller chunks. \n4. Automatic parallelization is allowed for the innermost loop with a reduction variable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_bicg' function as a hardware kernel for acceleration. Run the iterations of the first loop one after the other, not in parallel. Don't automatically pipeline the iterations of the second loop. Process the second loop in its original, full form without breaking it into smaller chunks. Run the iterations of the second loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the following function as a hardware kernel for acceleration.\n\nEnable auto pipelining for this loop.\n\nKeep the original loop structure without breaking it into smaller chunks.\n\nRun the iterations of this loop in parallel using a parallelization factor of __PARA__L0.\n\nTurn off automatic pipelining for this loop.\n\nKeep the original loop structure without breaking it into smaller chunks.\n\nRun the iterations of this loop in parallel using a parallelization factor of __PARA__L1.\n\nFlatten the loop (i.e., process it sequentially), and apply the original loop structure without breaking it into smaller chunks.\n\nRun the iterations of this loop in parallel using a parallelization factor of __PARA__L2. Apply pipelining with a factor of __PIPE__L5 and use a reduction operation for parallelization with a factor of __PARA__L6.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please annotate and modify the given C code with the following comprehensive but concise natural language instructions:\n1. Ensure that the loop is not automatically pipelined.\n2. Process this loop in its original, full form without breaking it into smaller chunks.\n3. Run the iterations of this loop one after the other, not in parallel.\n4. Treat the following function as a hardware kernel for acceleration.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the function as a hardware kernel for acceleration and process the loops with manual pipeline and tiling optimization. Also, run the loop iterations in parallel for efficient computation.\"\n\nThis comprehensive but concise instruction encapsulates the process of using the specific #pragma values in natural language for the given HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes written in C, provide the following concise natural language instruction:\n\n\"Mark the function 'kernel_adi' as a hardware kernel for acceleration, pipeline the loops without automatic detection, set the tile factor to 1 for tiling, and achieve parallelism with a factor of 1.\"\n\nThis instruction corresponds to the following #pragma values:\n```c\n#pragma ACCEL kernel\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PARALLEL FACTOR=auto{1}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Pipeline the outermost loop without automatic detection. Tile the outermost loop with a factor of 1. Pipeline the middle loop without automatic detection. Tile the middle loop with a factor of 1. Pipeline the innermost loop without automatic detection. Tile the innermost loop with a factor of 1. Parallelize the innermost loop with automatic detection for reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes in C according to the following natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Run the iterations of the first loop one after the other, not in parallel.\n- Don't automatically pipeline the following loop.\n- Process the second loop in its original, full form without breaking it into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure the following loop nests are treated as hardware kernels for acceleration, use 'Set the loop pipelining off', 'Set the loop tiling factor to 1', and 'Set the loop parallel factor to 1' for both the column sweep and row sweep loops. For the innermost loops within the column and row sweeps, use 'Set the loop parallel factor to 2' and 'Set the loop parallel factor to 1' respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS codes written in C using the natural language instructions provided below:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nThe natural language instruction \"Treat the following function as a hardware kernel for acceleration\" is used to describe '#pragma ACCEL kernel'.\nThe natural language instruction \"Don't automatically pipeline this loop\" is used to describe '#pragma ACCEL PIPELINE auto{off}'.\nThe natural language instruction \"Process this loop in its original, full form without breaking it into smaller chunks\" is used to describe '#pragma ACCEL TILE FACTOR=auto{1}'.\nThe natural language instruction \"Run the iterations of this loop one after the other, not in parallel\" is used to describe '#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function `kernel_doitgen` as a hardware kernel for acceleration. Then, for the loops in the function:\n- Do not automatically pipeline the outermost loop\n- Original form without tiling for the outermost loop\n- Pipeline the outermost loop with a custom factor __PIPE__L1\n- Tile the outermost loop with a custom factor __TILE__L1\n- Pipeline the middle loop with a custom factor __PIPE__L2\n- Tile the middle loop with a custom factor __TILE__L2\n- Parallelize the innermost loop with reduction and a custom factor __PARA__L3", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive but concise natural language instruction to generate the given HLS codes:\n\n\"Transform the following function into a hardware kernel for acceleration, and apply pipeline directives with specific auto-off settings and tile factors for each loop to optimize parallelism and resource utilization.\"\n\nThis natural language instruction encompasses the pragma values used in the given HLS codes, including the specification for kernel acceleration, pipeline settings, and tile factors for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the 'kernel_doitgen' function as a hardware kernel for acceleration, do not automatically pipeline the outermost loop, and use a tile factor of 1 for the loops. Run the iterations of the innermost loop in parallel and treat the reduction of 'sum_tmp' as a parallel factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes written in C, use the following comprehensive but concise natural language instruction: \n\nTreat the following function as a hardware kernel for acceleration. \nKeep the loop from being automatically pipelined. \nDo not break the loop into smaller chunks. \nRun the iterations of the loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, do not automatically pipeline the loops, and process each loop in its original form without breaking it into smaller chunks. Run the iterations of the innermost loop one after the other, not in parallel, and treat each loop as a separate tile.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration.\n\nUse automatic pipeline for this loop.\n\nSplit this loop into smaller chunks with a factor of 1.\n\nRun the iterations of this loop in parallel with a factor of __PARA__L0.\n\nTurn off automatic pipeline for this loop.\n\nSplit this loop into smaller chunks with a factor of 1.\n\nRun the iterations of this loop in parallel with a factor of __PARA__L1.\n\nFlatten the pipeline for this loop.\n\nSplit this loop into smaller chunks with a factor of 4.\n\nRun the iterations of this loop in parallel with a factor of __PARA__L2.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in parallel with automatic factor determination for the outer loop. Pipeline the inner loop and do not automatically tile the loop. Also, treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the outermost loop, and process it in its original, full form without breaking it into smaller chunks. Run the iterations of the inner loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Run the iterations of the first loop one after the other, not in parallel. Don't automatically pipeline the second loop. Process the second loop in its original, full form without breaking it into smaller chunks. Run the iterations of the third loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instructions:\n- Apply manual pipeline to the outer loops.\n- Set tile factor as 1 for the outer loops.\n- Apply manual pipeline and set tile factor for the inner loops.\n- Apply parallelism with reduction for the innermost loop iterations.\n- Treat the function as a hardware kernel for acceleration.\n\nThis set of natural language instructions corresponds to the provided HLS codes with the appropriate #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration\n   - '#pragma ACCEL kernel'\n\n2. Don't automatically pipeline this loop\n   - '#pragma ACCEL PIPELINE \"off\"'\n\n3. Process this loop in its original, full form without breaking it into smaller chunks\n   - '#pragma ACCEL TILE FACTOR=1'\n\n4. Run the iterations of this loop one after the other, not in parallel\n   - '#pragma ACCEL PARALLEL FACTOR=1'\n\nThese instructions will guide the generation of the provided HLS codes in C.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following concise natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, turn off automatic loop pipelining, set the loop tiling factor to 1, pipeline the innermost loop with a specific factor, tile the innermost loop with a specific factor, pipeline the middle loop with a specific factor, tile the middle loop with a specific factor, parallelize the innermost loop with reduction and a specific factor.\"\n\nBy following this instruction, you can generate the provided HLS codes in C with the corresponding #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the loops in their original full form and without breaking them into smaller chunks, treat the function as a hardware kernel for acceleration, and do not automatically pipeline the loops. Use the following pragma values: \n```c\n#pragma ACCEL kernel\n#pragma ACCEL PIPELINE \"off\"\n#pragma ACCEL TILE FACTOR=1\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the hardware acceleration for the following function using the following pragma values:\n\n1. Disable automatic pipeline for the outermost loop:  \n   ```c\n   #pragma ACCEL PIPELINE \"off\"\n   ```\n\n2. Set the tile factor for the outermost loop to 1:  \n   ```c\n   #pragma ACCEL TILE FACTOR=1\n   ```\n\n3. Apply automatic pipeline and tiling for the inner loops:\n   ```c\n   #pragma ACCEL PIPELINE auto{__PIPE__L1}\n   #pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n   #pragma ACCEL PIPELINE auto{__PIPE__L2}\n   #pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n   ```\n\n4. Optimize the parallelization factor for loop level 3, and use reduction for the sum calculation:\n   ```c\n   #pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n   ```\n\nRemember to treat the function as a hardware kernel for acceleration using:  \n```c\n#pragma ACCEL kernel\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instructions for the specific #pragma values:\n\n1. For the kernel function, use the natural language instruction \"Treat the following function as a hardware kernel for acceleration\".\n\n2. To turn off automatic pipelining for the outermost loop, use the natural language instruction \"Don't automatically pipeline this loop\".\n\n3. To set the tile factor to 1 for the outermost loop, use the natural language instruction \"Process this loop in its original, full form without breaking it into smaller chunks\".\n\n4. To automatically pipeline the first nested loop, use the natural language instruction \"Automatically pipeline this loop with a specific factor for the pipeline\".\n\n5. To set the tile factor to a specific value for the first nested loop, use the natural language instruction \"Process this loop with a specific factor for tiling\".\n\n6. To automatically pipeline the second nested loop, use the natural language instruction \"Automatically pipeline this loop with a specific factor for the pipeline\".\n\n7. To set the tile factor to a specific value for the second nested loop, use the natural language instruction \"Process this loop with a specific factor for tiling\".\n\n8. To automatically parallelize the reduction loop with a specific parallelization factor, use the natural language instruction \"Automatically parallelize this loop with a specific factor for parallelization\".\n\nBy following these natural language instructions, you can generate the given HLS codes in C with the appropriate #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration and apply the following directives to the loop nest: Disable automatic loop pipelining, set the tiling factor to 1 for the outermost loop, and use automatic loop factor for pipelining and tiling within the inner loops, and enable parallel execution with a reduction variable for the specified loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should use the following natural language instructions:\n\n- Treat the following function as a hardware kernel for acceleration.\n- Don't automatically pipeline this loop.\n- Process this loop in its original, full form without breaking it into smaller chunks.\n- Run the iterations of this loop one after the other, not in parallel.\n- Use automatic pipelining for Level 1.\n- Use automatic tiling with a factor of 1 for Level 1 tiles.\n- Use automatic pipelining for Level 2.\n- Use automatic tiling with a factor of 1 for Level 2 tiles.\n- Use automatic parallelization with automatic factor for Level 3 parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you should:\n- Use '#pragma ACCEL kernel' to indicate that the following function should be treated as a hardware kernel for acceleration\n- Use '#pragma ACCEL PIPELINE auto{off}' to turn off automatic pipelining for the outermost loop\n- Use '#pragma ACCEL TILE FACTOR=auto{1}' to specify automatic tiling with a factor of 1 for the outermost loop\n- Use '#pragma ACCEL PIPELINE auto{__PIPE__L1}' to enable automatic pipelining with a specific label for the next inner loop\n- Use '#pragma ACCEL TILE FACTOR=auto{__TILE__L1}' to apply automatic tiling with a specific label for the next inner loop\n- Use '#pragma ACCEL PIPELINE auto{__PIPE__L2}' to enable automatic pipelining with a specific label for the innermost loop\n- Use '#pragma ACCEL TILE FACTOR=auto{__TILE__L2}' to apply automatic tiling with a specific label for the innermost loop\n- Use '#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}' to parallelize the loop with reduction for a specific label.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following natural language instructions:\n- To turn off automatic pipelining for the specified loop, use the pragma \"#pragma ACCEL PIPELINE auto{off}\".\n- To specify the tile factor to be 1 for the specified loop, use the pragma \"#pragma ACCEL TILE FACTOR=auto{1}\".\n- To automatically pipeline the loop at level 1, use the pragma \"#pragma ACCEL PIPELINE auto{__PIPE__L1}\".\n- To specify the tile factor automatically at level 1, use the pragma \"#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\".\n- To automatically pipeline the loop at level 2, use the pragma \"#pragma ACCEL PIPELINE auto{__PIPE__L2}\".\n- To specify the tile factor automatically at level 2, use the pragma \"#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\".\n- To parallelize the loop and specify reduction with the specified variable, use the pragma \"#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\".\n- Finally, to specify a function as a hardware kernel for acceleration, use the pragma \"#pragma ACCEL kernel\".", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply manual pipeline directive for the outermost loop and set the tile factor to 1 for all three nested loops within a hardware kernel function. Use parallel directive with reduction for the innermost loop.\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n\n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n\n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Perform the following operations to optimize the given C code using HLS pragmas:\n\n1. **pragma ACCEL kernel** - Treat the 'kernel_bicg' function as a hardware kernel for acceleration.\n\n2. **pragma ACCEL PARALLEL FACTOR=auto{1}** - Run the iterations of the first loop in parallel.\n\n3. **pragma ACCEL PIPELINE auto{off}** - Don't automatically pipeline the subsequent loop.\n\n4. **pragma ACCEL TILE FACTOR=auto{1}** - Process the second loop in its original, full form without breaking it into smaller chunks.\n\n5. **pragma ACCEL PARALLEL FACTOR=auto{1}** - Run the iterations of the third nested loop in parallel and optimize the reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{1}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate hardware kernel for acceleration, turn off automatic loop pipelining, and set tile factor to 1 for the first loop; run iterations of the first loop in parallel; turn on loop pipelining, set tile factor to 1 for the second loop, and run iterations of the second loop in parallel with data reduction; set tile factor to 30 for the third loop and run iterations with loop pipelining and data reduction.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{30}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code written in C, use the following natural language instruction:\n\"Designate the following function as a hardware kernel for acceleration, turn off automatic loop pipelining, and use the original loop form without breaking it into smaller chunks for optimization. Run loop iterations one after the other, not in parallel.\"\n\nThis concise instruction covers the required #pragma values to use in the given HLS code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instructions for the #pragma values:\n\n- Use automatic pipelining for the loop: '#pragma ACCEL PIPELINE auto{}'\n- Break the loop into smaller chunks with a tile factor of 8: '#pragma ACCEL TILE FACTOR=auto{8}'\n- Parallelize the loop iterations with a parallel factor of __PARA__L0: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}'\n- Disable automatic pipelining for the loop: '#pragma ACCEL PIPELINE auto{off}'\n- Use a tile factor of 1 for the loop: '#pragma ACCEL TILE FACTOR=auto{1}'\n- Parallelize the loop iterations with a parallel factor of __PARA__L1: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}'\n- Flatten the loop into a single pipeline: '#pragma ACCEL PIPELINE auto{flatten}'\n- Use a tile factor of 1 for the loop: '#pragma ACCEL TILE FACTOR=auto{1}'\n- Parallelize the loop iterations with a parallel factor of __PARA__L2: '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}'\n- Create a separate pipeline for the loop with label __PIPE__L5: '#pragma ACCEL PIPELINE auto{__PIPE__L5}'\n- Parallelize the reduction operation within the loop with factor __PARA__L6: '#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}'", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code, use the following concise natural language instructions:\n\n1. Identify the function as a hardware kernel for acceleration.\n2. Turn off automatic loop pipelining.\n3. Set the loop tile factor to 1 for the entire loop nest.\n4. Set the parallel factor to 1 for all loop iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction: \n- Apply manual pipeline optimization on the outermost loop and 2 levels of nesting, disable automatic pipelining\n- Apply tiling with a factor of 1 for the outermost loop and 2 levels of nesting, use automatic tiling factor\n- Apply manual pipeline optimization with a custom label for the outermost loop and 2 levels of nesting, use the provided custom labels for pipelining \n- Apply tiling with a custom label for the outermost loop and 2 levels of nesting, use the provided custom labels for tiling \n- Apply parallelization with automatic factor and reduction for the innermost loop, enable automatic parallelization with reduction factor\n- Treat the following function as a hardware kernel for acceleration\n\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the C code with HLS pragmas, use the following concise natural language instruction:\n\"Designate the following function as a hardware kernel for acceleration, turn off automatic pipelining, use a tile factor of 1, and run the loops in parallel with automatic pipeline and tiling factors.\"\n\nThis instruction translates into the following C code with HLS pragmas:\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n\n#pragma ACCEL PIPELINE auto{off}\n\n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n\n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n\n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n\n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n  for (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n  sum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the entire function as a hardware kernel for acceleration and do the following:\n\n1. Turn off automatic loop pipelining.\n2. Use a tile factor of 1 for loop tiling.\n3. Set the parallel factor to 1 for all loops within the function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the C HLS codes provided, you should use the following natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration.\n2. Don't automatically pipeline this loop.\n3. Process this loop in its original, full form without breaking it into smaller chunks.\n4. Run the iterations of this loop one after the other, not in parallel.\n\nThe actual HLS code generated from these instructions are as follows:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE off\n  \n#pragma ACCEL TILE FACTOR=1\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE __PIPE__L1\n    \n#pragma ACCEL TILE FACTOR=__TILE__L1\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE __PIPE__L2\n      \n#pragma ACCEL TILE FACTOR=__TILE__L2\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=__PARA__L3\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes using natural language instructions, you can use the following concise and comprehensive instruction:\n\n\"Utilize hardware acceleration kernel for AES256 encryption in ECB mode, optimize the loop parallelism, and apply loop pipelining.\"\n\nThis instruction encompasses the necessary pragma values to use for the specific operations, including using the `#pragma ACCEL kernel` for hardware acceleration, `#pragma ACCEL PARALLEL` to optimize loop parallelism, and `#pragma ACCEL PIPELINE` to apply loop pipelining automatically.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following comprehensive but concise natural language instruction:\n\n\"Treat the following function as a hardware kernel for acceleration. Turn off automatic pipelining for the loop, set the tiling factor to 1, and run the loop iterations one after the other, not in parallel.\"\n\nYou can simply use the given instructions to generate the HLS codes in C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To prevent automatic pipelining, use `#pragma ACCEL PIPELINE \"off\"`.\n\nTo process a loop in its original, full form without breaking it into smaller chunks, use `#pragma ACCEL TILE FACTOR=1`.\n\nTo run the iterations of a loop one after the other, not in parallel, use `#pragma ACCEL PARALLEL FACTOR=1`.\n\nTo treat a function as a hardware kernel for acceleration, use `#pragma ACCEL kernel`.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Convert the following function into a hardware kernel for acceleration and apply the following optimizations: turn off automatic pipelining, set the tiling factor to 1 for loop iteration and parallelize the loops with a factor of 1.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, the following comprehensive but concise natural language instruction can be used:\n\n\"Identify the following function as a hardware kernel for acceleration, and optimize the loops by disabling automatic pipelining, setting tile factor to 1, and running iterations of the loops in parallel with a factor of 1.\"\n\nThis instruction should be applied to the given C code to generate the specified HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following pragma values for each function in the C code:\n\n1. For the function `aes256_encrypt_ecb`:\n   - Apply auto-pipeline with \"off\" for the loop: `#pragma ACCEL PIPELINE auto{off}`\n   - Apply auto tile factor: `#pragma ACCEL TILE FACTOR=auto{4}`\n\n2. For the function `aes_expandEncKey_1`:\n   - Treat the function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n3. For the function `aes_addRoundKey_cpy_1` and `aes_addRoundKey_1`:\n   - No specific pragma values are required.\n\n4. For the function `aes_subBytes_1`:\n   - No specific pragma values are required.\n\n5. For the function `aes_shiftRows_1`:\n   - No specific pragma values are required.\n\n6. For the function `rj_xtime_1`:\n   - No specific pragma values are required.\n\n7. For the function `aes_mixColumns_1`:\n   - No specific pragma values are required.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS pragmas for the given C code as follows:\n\n- For the function `kernel_adi`, use `#pragma ACCEL kernel` to treat the function as a hardware kernel for acceleration.\n\n- Use `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipelining for the entire function.\n\n- Set the tile factor to 1 using `#pragma ACCEL TILE FACTOR=1` for the entire function.\n\n- Use `#pragma ACCEL PARALLEL FACTOR=1` to run the iterations of all loops one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for the given C codes using the following natural language descriptions:\n- Apply pipeline directive on the loop\n- Tile the loop with factor 1\n- Execute the loop iterations sequentially, not in parallel\n- Treat the function as hardware kernel for acceleration", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, you need to:\n\n1. Generate the hardware kernel for acceleration:\n\n   `#pragma ACCEL kernel`\n\n2. Turn off automatic pipelining for a specific loop:\n\n   `#pragma ACCEL PIPELINE auto{off}`\n\n3. Set the loop tile factor to its original, full form without breaking it into smaller chunks:\n\n   `#pragma ACCEL TILE FACTOR=auto{8}`\n\n4. Process the loop iterations in a potentially parallel manner:\n\n   `#pragma ACCEL PIPELINE auto{flatten}`\n\nThese instructions should be followed to generate the HLS codes in C.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for the provided C functions, use the following natural language HLS instructions:\n\n1. To disable automatic pipelining for the encryption loop:\n   - Use: 'Disable automatic pipelining for the encryption loop'\n   - Corresponding HLS directive: '#pragma ACCEL PIPELINE auto{off}'\n\n2. To specify a tile factor of 1 for optimization of the loop:\n   - Use: 'Optimize the loop by specifying a tile factor of 1'\n   - Corresponding HLS directive: '#pragma ACCEL TILE FACTOR=1'\n\n3. To treat the 'aes256_encrypt_ecb' function as a hardware kernel for acceleration:\n   - Use: 'Treat the function as a hardware kernel for acceleration'\n   - Corresponding HLS directive: '#pragma ACCEL kernel'\n\nAfter incorporating these instructions into the C code, the generated HLS code is now ready for compilation.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Provide acceleration and pipeline directives to the following primitives\n1. `aes256_encrypt_ecb` - Apply `#pragma ACCEL kernel` on the function. \n2. `aes_expandEncKey_1` - Apply `#pragma ACCEL PIPELINE auto{off}` to the function.\n3. `aes_addRoundKey_cpy_1` - No action required; pipeline directives already present.\n4. `aes_subBytes_1` - No action required; pipeline directives already present.\n5. `aes_shiftRows_1` - Apply `#pragma ACCEL TILE FACTOR=4` to the function.\n6. `rj_xtime_1` - No action required; this is a simple function that doesn't require any additional directives.\n7. `aes_mixColumns_1` - Apply `#pragma ACCEL PIPELINE auto{flatten}` to the function.\n8. `aes_addRoundKey_1` - No action required; pipeline directives already present.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration, and for the loops within it, turn off automatic pipelining, use a tile factor of 1, pipeline with a factor of 1 at level 1 and 2, and parallelize the loop iterations with automatic factor optimization at level 3.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, use the following natural language instruction:\n\n\"Designate the following function as a hardware kernel for acceleration, disable automatic pipeline optimization for the outer loop, apply automatic tiling with a factor of 1 for the outer loop, enable automatic pipeline optimization and tiling with specific factors for the inner loops, and finally, enable automatic parallelization with reduction for the innermost loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware kernel for acceleration. Don't automatically pipeline the outermost loop. Process the loop in its original, full form without breaking it into smaller chunks. Run the iterations of each inner loop one after the other, not in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for the given C code using the following natural language instruction:\n\"Turn off automatic pipeline optimization for this loop\"\nUse #pragma ACCEL PIPELINE \"off\"\n\n\"Set the loop tiling factor to 1\"\nUse #pragma ACCEL TILE FACTOR=1\n\n\"Apply automatic pipeline optimization for the L1 loop\"\nUse #pragma ACCEL PIPELINE \"auto{__PIPE__L1}\"\n\n\"Set the tiling factor for the L1 loop with automatic optimization\"\nUse #pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n\n\"Apply automatic pipeline optimization for the L2 loop\"\nUse #pragma ACCEL PIPELINE \"auto{__PIPE__L2}\"\n\n\"Set the tiling factor for the L2 loop with automatic optimization\"\nUse #pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n\n\"Apply parallelization for the L3 loop with automatic reduction\"\nUse #pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\nApply these directives to the corresponding loops in the C code to generate the desired HLS code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, the following natural language instructions can be used for the specific #pragma values:\n\n- Turn off automatic pipelining for the entire kernel: \"Don't automatically pipeline this kernel\"\n- Set the tile factor for automatic tiling to 1: \"Use automatic tiling with a tile factor of 1\"\n- Enable automatic pipelining for the outermost loop with a specific label: \"Automatically pipeline the outer loop with a label __PIPE__L1\"\n- Enable automatic tiling with a specific factor for the outermost loop: \"Use automatic tiling with a tile factor indicated by __TILE__L1 for the outer loop\"\n- Enable automatic pipelining for the middle loop with a specific label: \"Automatically pipeline the middle loop with a label __PIPE__L2\"\n- Enable automatic tiling with a specific factor for the middle loop: \"Use automatic tiling with a tile factor indicated by __TILE__L2 for the middle loop\"\n- Enable automatic parallelization with reduction and a specific label for the inner loop: \"Automatically parallelize the inner loop with reduction of sum_tmp and a label __PARA__L3\"\n\nThese natural language instructions correspond to the specific #pragma values used in the provided HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
