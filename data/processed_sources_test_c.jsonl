{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the described HLS C code, please use the following natural language instruction:\n\n\"Define a function as a hardware acceleration kernel to optimize matrix calculations. Within this function, disable automatic pipelining and do not partition the first two loops, but allow the compiler to choose the amount of parallelism for the innermost loops. Specifically, use the maximum available parallelism for the loop that accumulates `tmp` but limit parallelism to pairs for its outer loop, and completely unroll the innermost loop for updating `D` while maximizing parallelism for its outer loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instructions to generate the described HLS pragmas for the `kernel_2mm` function:\n\n1. Declare the function as a hardware kernel for acceleration.\n2. Prevent the outermost loop (over `i`) from being automatically pipelined, and ensure that it runs with no tiling and no parallelization. Treat it as an uninterrupted, sequential loop.\n3. In the inner loop (over `j`), for the multiplication of matrices A and B, enable complete flattening of the loop by the compiler for potentially enhanced pipelining, but ensure no tiling is applied and execute two iterations concurrently.\n4. For the loop over `k` inside the inner loop, enable parallel computation of the matrix multiplication with reduction on 'tmp', but ensure that while reducing, the operations are not run concurrently but in sequence.\n5. For evaluating the matrix multiplication with matrix C and updating matrix D, revert the outermost loop (over `i`) back to default pipelining behavior determined by the compiler, continue with no tiling and no parallelization.\n6. In the subsequent inner loop (over `j`), disable automatic pipelining, tile this loop in chunks of eight iterations for better data locality, and ensure it runs without any parallel execution.\n7. For the loop over `k` inside the last inner loop, allow the execution of eight concurrent iterations during the reduction process on 'D' to potentially speed up the computation. \n\nTo translate these instructions into #pragma statements while maintaining their respective configurations, use the following guidance:\n\n- `#pragma ACCEL kernel` to mark the function for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` to disable loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to prevent loop tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to disable loop parallelization.\n- `#pragma ACCEL PIPELINE auto{flatten}` to request the compiler to flatten the loop if possible.\n- `#pragma ACCEL PIPELINE auto{}` to allow the compiler to decide on the pipelining strategy.\n- `#pragma ACCEL TILE FACTOR=auto{8}` to tile the loop by a factor of eight.\n- `#pragma ACCEL PARALLEL FACTOR=auto{2}` or `#pragma ACCEL PARALLEL FACTOR=auto{8}` to specify the desired concurrency level (2 or 8) during the execution of a loop.\n- `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{1}` or `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{8}` to enable parallel reduction on 'tmp' or 'D' with specified factors of concurrency. \n\nApply these annotations directly before the loops or scopes they are intended to influence.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function named `kernel_adi` that simulates an Alternating Direction Implicit (ADI) solver for a 2D heat equation, using specifically designated pragmas to manage loop optimizations. For the main time progression loop, disable automatic pipelining, do not tile the loop, and do not parallelize its iterations. Within the Column Sweep and Row Sweep sections, permit aggressive loop flattening for the outer loop `i`, but maintain the inner loop `j` without pipelining or parallelization, except for the last loop in Row Sweep where the parallel factor is increased to 58 and then reduced to 16 for the backward iteration. Use explicit values when setting `#pragma ACCEL TILE` and `#pragma ACCEL PARALLEL` to override defaults in order to ensure each loop operates sequentially (with a factor of 1), except where otherwise specified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "In order to generate the provided HLS (High-Level Synthesis) code, you would use the following succinct natural language instruction:\n\n\"Create an HLS kernel function named 'kernel_adi' for the ADI algorithm that avoids all forms of loop optimization including pipelining, tiling, and parallelization of the outer loops, while applying maximal parallelization available to the innermost 'j' loops except where the backward iterations occur, where a parallel factor of four should apply.\"\n\nSpecifically, this instruction would correspond to applying:\n\n- `#pragma ACCEL kernel` to signify that the function 'kernel_adi' should be treated as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipelining for the designated loops.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to avoid tiling and ensure each loop operates in its original, full form.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to maintain sequential execution of the loop iterations for the outer loops.\n- `#pragma ACCEL PARALLEL FACTOR=auto{58}` to apply maximal parallelization to the innermost 'j' loops during the forward iteration.\n- `#pragma ACCEL PARALLEL FACTOR=auto{4}` to apply a lesser degree of parallelism, specifically a parallel factor of four, to the reversed 'j' loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the high-level synthesis (HLS) compiler to treat the `kernel_adi` function as a hardware kernel suitable for acceleration without pipeline optimization for the outermost loop but with automatic loop pipelining and loop unrolling decisions for the inner loops. For the first set of inner loops, maintain the loop boundaries without tiling and unrolling. For the second set of inner loops, allow an automatic decision-making process for parallelism with a hint that a parallel factor of 2 might be optimal for the second level nested loops. Ensure that this configuration is respected by setting the appropriate #pragma directives accordingly.\n\nSpecific #pragmas:\n1. `#pragma ACCEL kernel` should be used at the beginning of the `kernel_adi` function to indicate it's a candidate for hardware acceleration.\n2. `#pragma ACCEL PIPELINE auto` should be set for the first layer of nested loops (`for (t = 1; t <= 40; t++)`) even though it says \"auto\", it actually disables automatic pipeline optimization as per the example instruction provided. For consistency, similarly tagged pragmas further inside nested loops should apply automatic loop pipelining.\n3. `#pragma ACCEL TILE FACTOR=auto` should disable any tiling optimization for the loops as it auto-selects a tiling factor, which the code indicates should be 1.\n4. `#pragma ACCEL PARALLEL FACTOR=auto` should be applied for all parallelizable inner loops with an automatic decision for the factor of parallelism, which is implied to be 1 for the first set of nested loops and suggested to be 2 for the second set of loops specifically in the `for (i = 1; i < 59; i++)` level.\n5. The 'flatten' directive should be used with the `#pragma ACCEL PIPELINE` directive for additional loop flattening optimizations in the first set of nested loops.\n\nNote: The use of the keyword 'auto' enables the compiler to make automatic decisions for optimizations with respect to the device architecture and performance constraints. However, giving explicit numeric values or 'off' would override the compiler's automatic decision-making process. It seems that the term 'auto{1}' suggests to the reader or the HLS tool that '1' might be the intended optimization parameter while leaving the actual decision to the compiler.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Mark the 'kernel_adi' function to be compiled as a dedicated hardware acceleration kernel. Disable automatic pipelining for all loops, process each loop in its entirety without tiling, and execute iterations of all loops sequentially. For the innermost j-loop within both column and row sweeps, enable parallel execution with all iterations running concurrently except for the last reverse-ordered j-loops of each sweep, which are to be run only four and eight iterations in parallel, respectively. \n\nSpecifically, use the following pragma directives:\n\n- `#pragma ACCEL kernel` to treat 'kernel_adi' as a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE \"off\"` to prevent automatic pipelining of every loop.\n- `#pragma ACCEL TILE FACTOR=1` to avoid tiling and run the loop as a whole.\n- `#pragma ACCEL PARALLEL FACTOR=1` to run loop iterations sequentially, which is used for all loops except specified otherwise.\n- `#pragma ACCEL PARALLEL FACTOR=58` for the innermost j-loop during column and row sweeps, to run all iterations in parallel (total iterations are 58, hence factor 58).\n- `#pragma ACCEL PARALLEL FACTOR=4` for the reverse-ordered j-loop in the column sweep, to run every four iterations in parallel.\n- `#pragma ACCEL PARALLEL FACTOR=8` for the reverse-ordered j-loop in the row sweep, to run every eight iterations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, without automatic pipelining for the outermost loop. Inside the kernel, for the two primary nested loops, utilize pipelining and keep them in their complete form without tiling or altering parallel execution factors. For the third level of nested loops inside the 'Column Sweep' section, execute iterations serially, whereas in the 'Row Sweep' section, employ different degrees of parallelization: no parallelization for the first innermost loop and varying levels of parallelization for the second and third innermost loops (maximum parallelization for the second and some parallelization for the third, following the code respectively).\n\nUse the following pragmas:\n\n- `#pragma ACCEL kernel` to mark the function for acceleration\n- `#pragma ACCEL PIPELINE auto{off}` before the outermost loop to prevent automatic pipelining\n- `#pragma ACCEL TILE FACTOR=auto{1}` within both 'Column Sweep' and 'Row Sweep' to avoid loop tiling\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` at each loop level in the 'Column Sweep' to serialize loop iterations\n- `#pragma ACCEL PIPELINE auto{flatten}` before the first inner loop inside 'Column Sweep' to flatten and pipeline nested loops\n- `#pragma ACCEL PARALLEL FACTOR=auto{} and #pragma ACCEL PARALLEL FACTOR=auto{58}` within the 'Row Sweep' to control the level of parallelism for the inner loops \n- Standardize all loop representations to an increasing order loop instead of a decreasing one.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a kernel function to perform a 2mm computation with the given dimensions. In the first major loop, disable automatic pipelining, keep the loop intact without tiling, and execute iterations serially. In the inner loop of the first major loop, again disable pipelining, keep the loop intact without tiling, but allow for two iterations to be processed in parallel. Employ parallel reduction specifically for the array 'tmp' when accumulating results in the innermost loop, allowing 70 accumulations to potentially happen in parallel. In the second major loop, enable automatic pipelining but flatten the loop to integrate nested loops if possible, tile the intermediate loop with an extent of 80, and run the iterations of this intermediate loop sequentially. For the accumulation in the innermost loop of the second major loop, do not employ parallelism and specifically use parallel reduction for the array 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Identify the function `kernel_2mm` which performs the operation `D := alpha*A*B*C + beta*D` for matrix multiplication and scaling as a hardware acceleration kernel. For the first chunk of the algorithm where you compute the intermediate matrix `tmp[i][j]`, disable loop pipelining to prevent optimization that may consume additional resources and ensure that loops are not tiled or run in parallel. For the second loop nest where you update matrix `D[i][j]`, flatten the outermost loop for pipelining to encourage loop transformation that can lead to parallel execution but restrict tiling and parallelization to a single iteration for the outermost loop and 32 iterations at a time for the innermost loop. Ensure that data dependencies involving `tmp` and `D` are properly managed with parallel reductions and avoid automatic parallelization of the reduction loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for an alternating-direction implicit (ADI) kernel, ensuring the following properties through specific directives:\n\n1. Start by marking the `kernel_adi` function as a hardware kernel targeted for acceleration.\n2. The outermost time loop should not be automatically pipelined; process it sequentially without introducing parallelism.\n3. Force the time loop to remain as a single entity, not tiled into sub-loops.\n4. Ensure all iterations of the time loop run serially without parallel execution of iterations.\n5. Within the time loop, flatten the pipelining of the 'Column Sweep' inner loops, effectively merging nested loops when possible, but don't break loops into smaller chunks or parallelize iterations.\n6. For the 'Row Sweep' section, disable automatic loop pipelining and maintain its full structure; however, allow for a parallel factor of 2 for the first inner loop.\n7. In the 'Column Sweep' and 'Row Sweep' nested loops that reverse iterate, specify parallelization factors that ensure their serial execution, except for the second inner loop in the 'Row Sweep' which should unroll completely with a factor equal to the number of iterations, and for the final reversing nested loop in the 'Row Sweep' which should have a parallel factor of 8. \n\nMake sure to convert all for loops that iterate backwards into a standardized form before applying the parallelization factors.\n\nHere are the pragmas to use for each specific instruction:\n\n1. `#pragma ACCEL kernel` before the function definition.\n2. `#pragma ACCEL PIPELINE auto{off}` inside the outermost time loop.\n3. `#pragma ACCEL TILE FACTOR=auto{1}` inside the outermost time loop.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` inside the outermost time loop.\n5. `#pragma ACCEL PIPELINE auto{flatten}` inside the 'Column Sweep' loop block.\n6. `#pragma ACCEL PARALLEL FACTOR=auto{2}` and `#pragma ACCEL PIPELINE auto{off}` before the 'Row Sweep' loop block.\n7. `#pragma ACCEL PARALLEL FACTOR=auto{58}` for the complete unrolling of the second inner loop in 'Row Sweep'.\n8. `#pragma ACCEL PARALLEL FACTOR=auto{8}` for the final reversing nested loop in 'Row Sweep'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. In the first loop nest, disable automatic loop pipelining, do not tile the loop iterations, and execute the outermost loop without parallelism. Within the inner loop, use a parallelism factor of 2 without automatic pipelining or tiling and a parallelism factor of 4 with reduction on the variable 'tmp' in the innermost loop. In the second loop nest, enable automatic pipelining for the outermost loop, manually flatten nested loops for pipelining, tile the intermediate loop with a factor of 4, set a parallelism factor of 20 for the intermediate loop, and process the innermost loop with a parallelism factor of 1 and reduction on the variable 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a kernel function `kernel_2mm` handling a 2-matrix multiply operation followed by the addition of another matrix multiplication result, with the following directives:\n\n1. Designate the entire function as a hardware acceleration kernel.\n2. Ensure that the top-level loops for both matrix multiplications operate sequentially without pipelining or parallelizing:\n   - Do not use automatic pipelining for the outermost loops iterating over `i`.\n   - Do not tile or partition the top-level `i` loops; process them in their entirety.\n   - Execute iterations of the outermost `i` loops one after another, without any parallelism.\n3. Within the inner loop iterating over `j` for the first matrix multiplication (computing `tmp`), disable pipelining, process full iterations without tiling, and allow for a minimal factor of parallelism, specifically a factor of 2.\n4. Apply a high degree of parallelism with a factor of 10 to the innermost loop iterating over `k` when computing `alpha * A[i][k] * B[k][j]` and add the result to `tmp[i][j]`, while also performing a reduction on `tmp` array for parallel operations.\n5. In the final matrix multiplication and addition to compute D, similarly to steps 2 and 3, keep the outermost loops sequential by disabling pipelining and avoiding loop tiling and partitioning, and set a minimum parallelism factor of 2 for the loop over `j`.\n6. Significantly parallelize the computation within the innermost loop iterating over `k` for updating `D[i][j]`, with a suggested factor of 32, while performing a reduction on the `D` array to handle concurrent updates correctly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the 'kernel_adi' function as a hardware acceleration kernel, disable automatic pipelining for the outermost loop, use the default setting for loop tiling and enforce sequential execution of iterations for all nested loops. Apply automatic loop flattening to the column sweep inner loops but ensure iterations execute sequentially. For the row sweep section, maintain the default loop tiling configuration and parallelize the innermost loop with a fixed factor of 29 for the first inner loop and 8 for the backward iteration loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code in C with specific pragma directives, instruct the code generator using the following natural language explanations:\n\n1. Designate the function `kernel_2mm` as a hardware acceleration kernel.\n2. Suppress automatic pipelining for the top-level loops inside the kernel.\n3. Execute the outermost loops without tiling or parallelization optimizations.\n4. Within the first nested loop computing the matrix multiplication of `alpha * A * B`, use pipelining to flatten nested loops and allow the compiler to choose the tile size while forcing parallelization to a factor of 2 specifically for the innermost loop.\n5. Within the second nested loop updating matrix `D`, again suppress pipelining at the top loop level, use manual tiling with a defined factor of 4 for the outer loop, and manually set a high parallelization factor of 20 specifically for the innermost loop.\n6. Ensure that the intermediate updates to `tmp` and `D` are done in parallel with reduction for respective matrix elements in their innermost loops without additional parallel optimization.\n\nAs per the given code, you would instruct the generator as follows:\n\n- \"Treat `kernel_2mm` as a hardware acceleration kernel.\"\n- \"For both top-level loops (`i` loops) within `kernel_2mm`, turn off automatic pipelining and do not apply tiling or parallelization.\"\n- \"For the first `j` loop responsible for computing `tmp`, enable pipelining with complete loop flattening, keep tiling off, and set a parallel factor of 2 for the inner `k` loop with a parallel reduction on `tmp`.\"\n- \"For the second `j` loop responsible for updating `D`, keep pipelining off but apply a tile factor of 4 to the outer loop, and set an aggressive parallel factor of 20 for the inner `k` loop with a parallel reduction on `D`.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, then for the first major loop pair, prevent pipelining, utilize each loop iteration in its entirety without tiling, and execute iterations sequentially. For the nested loop within, allow automatic pipelining, maintain the loop body's integrity without tiling, and execute two iterations in parallel. For the innermost loop, allow reductions on 'tmp' with maximum parallelism, automatically determined. In the second major loop pair, enable pipelining for the outer loop, refrain from tiling, and run iterations in parallel with a parallelism factor of 32. For the innermost loop, enable reductions on 'D' with a parallelism factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "I will provide you with a series of natural language instructions corresponding to each pragma directive used in the given HLS code for the function `kernel_2mm`.\n\n1. **Treat the following function as a hardware kernel for acceleration**:\n   - Use '#pragma ACCEL kernel' at the function declaration.\n\n2. **Do not automatically manage the pipeline process in the top-level loop, control it explicitly**:\n   - Use '#pragma ACCEL PIPELINE auto{off}' before the top-level loops.\n\n3. **Maintain the original loop bounds without tiling**:\n   - Use '#pragma ACCEL TILE FACTOR=auto{1}' before the loop to prevent tiling.\n\n4. **Execute the iterations of this loop sequentially without any parallel processing**:\n   - Use '#pragma ACCEL PARALLEL FACTOR=auto{1}' before the loop to execute iterations serially.\n\n5. **Allow the compiler to decide on pipelining within the nested loop for optimal performance**:\n   - Use '#pragma ACCEL PIPELINE auto{}' before the nested loops to let the compiler automate pipelining.\n\n6. **For the deepest loop, allow only some level of parallelism when computing the reduction operation on the `tmp` array**:\n   - Use '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}' to indicate a reduction with limited parallelism.\n   \n7. **Increase the parallel factor massively for the innermost loop while updating `D` to maximize throughput**:\n   - Use '#pragma ACCEL PARALLEL FACTOR=auto{32}' to significantly boost parallelism in the loop where `D` is updated.\n\n8. **Allow the compiler to decide how to linearize this nested loop to maximize dataflow efficiency**:\n   - Use '#pragma ACCEL PIPELINE auto{flatten}' to suggest that the compiler should flatten the loop if it improves pipeline efficiency.\n\nBy following these descriptions, you should be able to understand the purpose of each pragma directive in the HLS code provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Prevent pipelining optimization for the top-level loops over the 'i' index, run their iterations sequentially, and process each loop in its entirety. For the nested loop over the 'j' index within the first 'i' loop, enable automatic pipelining, allow loop tiling with a single iteration per tile, and use a parallelism factor of 2. Within the innermost 'k' loop, use parallel reduction on the 'tmp' array with maximum unrolling based on 'nk,' the loop bound. For the second 'i' loop, disable pipelining optimization but flatten nested loops for performance. In the nested loop over 'j', allow for a parallelism factor of 20, and inside the innermost 'k' loop, sequentially process each iteration without parallelism or reduction optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the selected portions of the HLS C code into a comprehensive, yet concise natural language instruction as follows:\n\n1. Mark the `kernel_2mm` function as a hardware acceleration kernel using `#pragma ACCEL kernel`.\n2. Within the first `for` loop, disable automatic pipelining, avoid tiling or parallelizing the loop in chunks. Indicate these settings with `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` respectively.\n3. For the nested `for` loop that calculates the value of `tmp`, continue with no automatic pipelining, set no tiling and allow running two loop iterations in parallel using `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n4. Within this nested loop, use parallel reduction on the variable `tmp`, fully unrolling the loop by setting the factor equivalent to the loop's iteration space with `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}`.\n5. In the second `for` loop, enable automatic pipelining, maintain no tiling, and set to run one loop iteration at a time using `#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` respectively.\n6. When updating the `D` array, auto-flatten the subsequent loop, tile it with a factor of 4, and set to run 20 iterations of the inner loop in parallel with `#pragma ACCEL PIPELINE auto{flatten}`, `#pragma ACCEL TILE FACTOR=auto{4}`, and `#pragma ACCEL PARALLEL FACTOR=auto{20}`.\n7. Finally, use parallel reduction on the `D` array without unrolling the loop, indicated by `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`.\n\nThis comprehensive instruction provides guidance for transforming the `kernel_2mm` function into HLS C code with specific pragma settings used for loop optimizations and data flow control.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_adi` for hardware acceleration. Within this function, for the main time-stepping loop and the first inner loop performing the column sweep, do not employ pipeline optimization, and ensure that the loops execute sequentially without tiling or factoring for parallelism. However, apply loop flattening optimization to inner loop nests where applicable. During the row sweep, disable pipeline optimization for the outer loop, but apply parallelism inside the innermost loop with a parallel factor that varies between iterations, specifically using a factor of 29, then 2, and finally 4 in different loop sections as indicated by the changes in `#pragma ACCEL PARALLEL FACTOR` values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis kernel called `kernel_adi` for accelerating the computation of a 2D heat equation solver that performs alternating direction implicit (ADI) integration, specifying that the main computation loop should not be automatically pipelined, and the internal loops should be processed sequentially without automatic tiling or parallelization, but allowing automatic parallel execution for specific loops handling the tridiagonal system solutions with prescribed parallelism factors:\n\n1. Declare the kernel function, and specify the parameters for time steps, spatial dimensions, and the four two-dimensional arrays for intermediate calculations.\n\n2. Define single-precision constants for the spatial discretization (`DX` and `DY`), the time step (`DT`), and numerical coefficients (`B1` and `B2`). Compute multipliers `mul1` and `mul2` based on these constants.\n\n3. Initialize the loop for time-step iterations, ensuring that it is not automatically pipelined but manually manage the tridiagonal solver loops.\n\n4. Define the column sweep section, \n   - Leave the outer `i` loop without targeted optimization settings.\n   - Apply an explicit parallel factor equivalent to the spatial dimension minus two for the inner `j` loops solving the tridiagonal systems, controlling parallel computation extent.\n   - Reverse the direction of a decrementing `j` loop using helper variables for index calculation, and apply a different explicit parallel factor for this reverse loop.\n\n5. Define the row sweep section similarly to the column sweep, applying an explicit parallel factor of half the spatial dimension minus one for the second inner `j` loop, and a doubled parallel factor for the reverse `j` loop.\n\n6. Close the function scope and complete the kernel definition.\n\nThe pseudocode instruction for HLS tool to generate such a code is as follows:\n\n- Mark the function `kernel_adi` to be compiled as a hardware acceleration kernel.\n- Specify that the outer loop iterating over time steps should not be pipelined.\n- Designate that the inner loops should not be tiled, should be executed sequentially in their natural order, and apply automatic parallelization to the tridiagonal solver loops with specified factors of `58` for the forward elimination and `4` and `8`, respectively, for the backward substitution in the column and row sweeps.\n\nFor the HLS pragmas, use `#pragma ACCEL KERNEL` to designate the kernel, `#pragma ACCEL PIPELINE auto{off}` to control loop pipelining, `#pragma ACCEL TILE FACTOR=auto{1}` to control tiling, and `#pragma ACCEL PARALLEL FACTOR=auto{N}` where `N` is the desired explicit parallelization factor, adjusting its value according to the context of its application within different loops in the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel that performs an Alternating Direction Implicit (ADI) method for a fixed number of time steps on a 2D grid, with specific loop optimizations: \n\n1. The outermost time loop should not be automatically pipelined.\n2. The first inner loop (Column Sweep), iterating over one of the spatial dimensions, should be completely flattened for pipelining, but should not be divided into smaller chunks or parallelized beyond a single instance.\n3. The second inner loop within the Column Sweep, which computes the intermediate values p and q, should process its iterations serially, in the given order.\n4. The loop reversing the direction for updating grid v values should also execute its iterations serially, in reversed order.\n5. For the second inner loop (Row Sweep), iterating over the other spatial dimension, do not pipeline it but permit parallel execution of up to 29 simultaneous iterations.\n6. Within the Row Sweep, the loop updating the intermediate p and q should allow two iterations to execute at the same time.\n7. The loop reversing the direction for updating grid u values should also permit two iterations to be computed in parallel.\n\nUse the following #pragma directives accordingly within the code:\n- '#pragma ACCEL KERNEL' to indicate the start of the kernel function.\n- '#pragma ACCEL PIPELINE auto{off}' to prevent automatic pipelining of loops where specified.\n- '#pragma ACCEL TILE FACTOR=auto{1}' to ensure that the loops are not tiled and that the entire range is executed as one single chunk.\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' for the loops that should not be run in parallel beyond a single instance and '#pragma ACCEL PARALLEL FACTOR=auto{value}' with the specific factor (2 or 29) for the loops that are allowed to run specified number of parallel instances.\n- '#pragma ACCEL PIPELINE auto{flatten}' to fully flatten the loop for pipelining without tiling or additional parallelization beyond one instance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to consider the function as a hardware acceleration kernel, avoid automatic pipelining, and ensure no tiling or parallelism is introduced, except for the first '#pragma ACCEL PARALLEL FACTOR' in the row sweep where you set a factor of 29 iterations and the last '#pragma ACCEL PARALLEL FACTOR' in the row sweep where you set a factor of 58 iterations. \n\nFor the rest of the code, specify these pragmas with values set to disable the HLS optimizations or apply them as they appear in the original loop structure. Specifically, use the following instructions for each respective pragma:\n\n1. \"#pragma ACCEL kernel\" - Treat the following function as a dedicated hardware kernel.\n2. \"#pragma ACCEL PIPELINE auto{off}\" - Do not automatically pipeline any loop.\n3. \"#pragma ACCEL TILE FACTOR=auto{1}\" - Do not tile any loop, preserve it in its original uninterrupted form by setting the tile factor to 1.\n4. \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" - Do not introduce parallel execution in loops, each iteration should execute sequentially by setting the parallel factor to 1.\n5. \"#pragma ACCEL PIPELINE auto{flatten}\" - Preserve the nested loop structure, but consider for pipelining while keeping the nested form.\n6. \"#pragma ACCEL PARALLEL FACTOR=auto{29}\" - Set the parallel factor to 29 iterations for that specific loop point, allowing 29 iterations to be processed in parallel.\n7. \"#pragma ACCEL PARALLEL FACTOR=auto{58}\" - Set the parallel factor to 58 iterations for that specific loop point, allowing 58 iterations to be processed simultaneously. \n\nThese instructions are directed at maintaining a balance between preserving the general structure and flow of the original code while allowing for selective optimization in specific loop points where parallel processing factors are explicitly stated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given HLS code, the following comprehensive and concise natural language instructions correspond to the respective `#pragma` directives:\n\n1. Treat the 'kernel_2mm' function as an independent hardware unit for acceleration in the system.\n   - `#pragma ACCEL kernel`\n\n2. For the outermost 'i' loop of the matrix multiplication involving arrays 'A' and 'B':\n   - Do not attempt to automatically parallelize or pipeline this loop; execute it sequentially, just like in software.\n     - `#pragma ACCEL PIPELINE auto{off}`\n   - Process this 'i' loop in its entirety, without tiling (splitting into sub-loops).\n     - `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Perform the iterations of this 'i' loop serially, not concurrently.\n     - `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n3. Within the 'i' loop, for the 'j' loop:\n   - Avoid auto-pipeline optimization for this nested 'j' loop.\n     - `#pragma ACCEL PIPELINE auto{off}`\n   - Do not split the 'j' loop into sub-loops; keep it as a single loop.\n     - `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Execute two iterations of the 'j' loop concurrently.\n     - `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n4. For the inner 'k' loop (performing the actual matrix multiplication for 'tmp'):\n   - Aggressively parallelize this loop to perform 16 iterations at once, accumulating results in 'tmp'.\n     - `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}`\n\n5. For the outermost 'i' loop of the matrix addition involving arrays 'tmp' and 'C':\n   - Allow automatic optimization techniques, such as pipelining, to be applied to this loop as the compiler sees fit.\n     - `#pragma ACCEL PIPELINE auto{}`\n\n6. Do not tile or parallelize the 'i' loop of the addition operation unnecessarily.\n   - `#pragma ACCEL TILE FACTOR=auto{1}`\n   - `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n7. Within this 'i' loop, for the 'j' loop:\n   - Do not automatically pipeline this 'j' loop.\n     - `#pragma ACCEL PIPELINE auto{off}`\n   - Avoid tiling the 'j' loop.\n     - `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Execute pairs of contiguous 'j' loop iterations concurrently.\n     - `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n8. For the inner 'k' loop (performing the actual matrix addition for 'D'):\n   - Perform a significant amount of parallelization, enabling 32 concurrent iterations with a reduction on 'D'.\n     - `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}`\n\nThese instructions aim to control the parallelization, tiling, and pipelining of specific loops to shape the hardware realization of the algorithm optimizing performance and resource usage according to the requirements of the given code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function named `kernel_2mm` that performs a matrix multiplication and addition, mapping the outer loops for each operation to hardware but without any automatic pipelining optimizations. The inner loops of the first operation should be moderately parallelized, while the inner loops of the second operation should employ heavy parallelization and partial unrolling. Specifically:\n\n1. Declare the function `kernel_2mm` with parameters `int ni, nj, nk, nl, double alpha, double beta,` and four 2D array pointers of types `double` named `tmp, A, B, C,` and `D` respectively, marking it as a hardware acceleration kernel.\n2. For the calculation `D := alpha*A*B*C + beta*D`, suppress automatic pipelining, prevent tiling, and disable parallelization entirely for the outer loop encompassing the entire calculation.\n3. Within the first major loop segment, where the temporary matrix `tmp` is calculated, allow automatic pipelining for the loop iterations over `i`. Keep the tile factor at its minimum and parallelize the second loop over `j` iterating 50 times with a parallel factor of 2.\n4. Inside the innermost loop over `k` which iterates 70 times, enable a higher but limited degree of parallelization with a parallel factor of 4 and apply a reduction to the `tmp` array so that dependencies are properly managed during parallel execution.\n5. In the second major loop segment for updating matrix `D`, use automatic pipelining for the outer loop over `i` and flatten the nested loops where practical. Apply tiling with a factor of 4 in the outer loop and an aggressive parallel factor of 20 for the second loop over `j` iterating 80 times to achieve substantial concurrency.\n6. In the final inner loop over `k` which iterates 50 times, disable any form of parallelism, acknowledging the reduction on the `D` array to ensure accumulator updates are handled correctly in a hardware context.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a function `kernel_2mm` marked for acceleration, with specific pipeline, tiling, and parallelization directives, following these instructions:\n\n- Mark the function `kernel_2mm` to be treated as a hardware kernel for acceleration.\n- Inhibit automatic pipelining of the outer loops for both main operations within the function.\n- Ensure the outer loops iterate in their original, unaltered state, without them being tiled or parallelized beyond the natural loop execution.\n- Allow complete loop flattening only for the immediate inner loops, ensuring that there is no tiling or partitioning inside these loops besides the enforced parallel factors.\n- Set a parallel factor of 2 for the second level loop during the first main operation, and a factor of 4 during the second main operation, while keeping the innermost loops single-threaded with a parallel factor of 1, and include a reduction clause to safely accumulate into the shared `tmp` and `D` arrays respectively.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the target function as hardware acceleration kernel, disable automatic pipelining and keep the entire first loop intact without any tiling or parallel optimizations. Within the first nested loop, enable automatic pipelining, use the original loop structure without tiling, and run two iterations in parallel but reduce the partial sum into the 'tmp' array, with maximum possible parallelism for the innermost loop. In the second main loop, enable pipelining with loop flattening, tile the loop into chunks of four iterations each, and run twenty iterations in parallel, while the innermost loop's updates to 'D' should proceed sequentially without parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel. Inside this function, initialize an array 'y' to zeros in a non-pipelined manner. Execute two nested loops where the first loop computes 'tmp' without tiling or parallelization, and within it, compute 'tmp[i]' with elements of 'A' and 'x' in a serial fashion without parallelism. In the second nested loop, perform a reduction on 'y' with a parallelism factor of 10 while summing the product of 'A' elements and 'tmp[i]'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the top-level function 'kernel_adi' as a hardware acceleration kernel, and optimize its loops with various parallelization strategies, but ensure they retain their original structure and sequential execution. Specifically, disable automatic pipelining for the outermost loop, flatten pipeline transformation for the first nested loop within the column sweep, retain the original loop structure for the rest of the nested loops without breaking them into smaller parallel chunks, except for the row sweep's most nested loop, which should be parallelized with a factor of 29, and one of its internal loops with a factor of 2, while another nested loop should be parallelized with a factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` as a hardware acceleration kernel, and for the first set of nested loops, processing the matrix multiplication of `A` and `B` stored in `tmp`, disable automatic pipelining, tiling, and parallelism for the outer `i` loop, allowing only two iterations of the inner `j` loop to run in parallel. In the accumulation loop over index `k`, enable parallel reduction on `tmp` with automatic factor determination and a hint to consider full unrolling across `k`.\n\nFor the second set of nested loops, processing the operation `D = alpha * A * B * C + beta * D`, again, disable automatic pipelining and tiling, and restrict parallelism for the outer `i` loop, but this time flatten the inner `j` loop's pipeline with unspecified factor, and tile it with an automatic factor hinting towards full tiling across `j`. Inside the innermost loop over `k`, enable parallel reduction on `D` but prevent parallel execution across this loop.\n\nThe specific #pragma directives will be:\n\nTreat `kernel_2mm` as a hardware kernel for acceleration:\n`#pragma ACCEL kernel`\n\nFor the first `i` loop:\n- Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- Disable automatic tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Process loop iterations one after the other: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\nFor the first `j` loop:\n- Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- Disable automatic tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run two iterations in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\nFor the `k` loop inside the first `j` loop:\n- Enable parallel reduction on `tmp` with automatic factor for full unrolling: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{70}`\n\nFor the second `i` loop:\n- Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- Disable automatic tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Process loop iterations one after the other: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\nFor the second `j` loop:\n- Flatten the pipeline with an unspecified factor: `#pragma ACCEL PIPELINE auto{flatten}`\n- Tile with an automatic factor hinting at full tiling: `#pragma ACCEL TILE FACTOR=auto{80}`\n- Process loop iterations one after the other: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\nFor the `k` loop inside the second `j` loop:\n- Enable parallel reduction on `D` but with no parallel execution: `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS C code that defines a kernel function named 'kernel_2mm' for matrix multiplication and addition with the following specifications:\n\n1. The function accepts integer dimensions 'ni', 'nj', 'nk', and 'nl', scalar values 'alpha' and 'beta', and four 2D array arguments 'tmp', 'A', 'B', 'C', and 'D'.\n2. Decorate the entire function as a hardware kernel for acceleration.\n3. In the first major loop block, which computes the intermediate matrix product of 'A' and 'B' scaled by 'alpha':\n   - Prevent automatic pipelining of the outer loop 'i'.\n   - For the outer loop 'i', do not tile and do not apply parallelism factors, ensuring the original loop structure is preserved.\n   - Within the nested loop 'j', enable default pipelining, set tiling to the loop's full range, and apply a parallelism factor of 2.\n   - In the innermost loop 'k' that performs the scaling and multiplication, enable parallelism with a custom reduction on 'tmp' with the highest allowed factor, which corresponds to the complete unrolling of the loop, representing the full extent of the 'k' dimension (implemented by the directive with 'FACTOR=auto{70}').\n4. In the second major loop block, which updates matrix 'D' with the product of 'tmp' and 'C' scaled by 'beta':\n   - Do not pipeline the outer loop 'i', maintain the loop in its full form, and apply a parallelism factor of 1 to preserve the original loop structure.\n   - Within the nested loop 'j', disable pipelining, do not tile the loop, and apply a parallelism factor of 2.\n   - In the innermost loop 'k' that accumulates the products into 'D', enable parallelism with a custom reduction on 'D', using the highest allowed factor to unroll the loop fully, representing the full extent of the 'k' dimension (implemented by the directive with 'FACTOR=auto{50}'). \n\nEnsure that the scope pragmas (scop and endscop) are included to define the boundaries of the code region to be optimized.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function named `kernel_2mm` that performs a 2mm matrix multiplication and addition operation without pipelining the outermost loops for 'i', without partitioning them, and with sequential execution of their iterations. In the inner 'j' loop of the first computation block, disable pipelining, don't partition the loop, but set a parallelism factor of 2. For the 'k' loop inside the first block, enable parallel reduction on 'tmp' with as much parallelism as you have input data dependencies, which is represented by a factor of 70 here. For the second computation block of 'i' and 'j', allow automatic pipelining decision for the 'i' loop, don't partition, and parallelize the 'j' loop with a factor of 2. Lastly, for the 'k' loop inside the second block, enable parallel reduction on 'D' with as much parallelism as possible, indicated by a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the described HLS C code, you should instruct the HLS tool as follows:\n\nDefine a hardware acceleration kernel function for the Alternating Direction Implicit (ADI) method with input parameters for time steps, spatial dimensions, and four 2D matrices. Initialize constants and coefficients inside the function for calculations. Within the time-stepping loop, perform a column sweep followed by a row sweep, where each sweep consists of forward and backward substitution.\n\nFor the column sweep, disable automatic pipelining for the outer loop and prevent loop tiling and parallelization optimizations for both the outer and inner loops, forcing them to maintain their sequential order. Within the column sweep, for the inner loop that performs the forward substitution, enable loop flattening which includes parallelizing loops within the same nesting level and disable tiling. For the backward substitution, keep the loops as is without parallel optimization.\n\nFor the row sweep, leave the pipeline optimization to be automatically determined by the HLS tool, disable tiling, and set parallelization for the outer loops to a factor of 2, increasing the parallel factor to the maximum for the inner loop's forward substitution while setting it back to 2 for the backward substitution.\n\nSpecifically, for each #pragma directive:\n1. Treat the function `kernel_adi` as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n2. Disable automatic pipelining for the outer time-stepping loop: `#pragma ACCEL PIPELINE auto{off}`\n3. Maintain the original loop form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Run the iterations of the loops one after the other, sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n5. For the column forward sweep inner loop, enable loop flattening: `#pragma ACCEL PIPELINE auto{flatten}`\n6. For the row sweep outer loop, allow automatic pipeline optimization: `#pragma ACCEL PIPELINE auto{}`\n7. For the row sweep outer loop, set a parallel factor of 2: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n8. For the row forward sweep inner loop, maximize parallelization: `#pragma ACCEL PARALLEL FACTOR=auto{58}`\n9. For the row backward substitution loop, set a parallel factor of 2: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\nEnsure the placeholders `{}` are filled with the appropriate numeric values or `auto` keywords as shown above within the code to match the corresponding HLS directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the provided HLS (High-Level Synthesis) code, use the following natural language instructions to describe the associated `#pragma` values and their intents:\n\n- Treat the 'kernel_2mm' function as a hardware module intended for acceleration purposes. (`#pragma ACCEL kernel`)\n- For the outermost loops `i` in both main matrix multiplications, do not apply automatic pipelining; instead, execute the loop iterations sequentially without HLS optimizations. (`#pragma ACCEL PIPELINE auto{off}`)\n- Execute both outermost loops `i` in their entire iteration range without forming smaller tiled regions. (`#pragma ACCEL TILE FACTOR=auto{1}`)\n- For the outermost loops `i`, run the iterations serially rather than in parallel. (`#pragma ACCEL PARALLEL FACTOR=auto{1}`)\n- For the second loop level `j` in the first matrix multiplication, prevent automatic optimization through pipelining similarly to the outermost loop. (`#pragma ACCEL PIPELINE auto{off}`)\n- For the second loop level `j` in the first matrix multiplication, also do not tile the iteration space. (`#pragma ACCEL TILE FACTOR=auto{1}`)\n- Allow minimal parallel execution of the second loop `j` by running two iterations simultaneously. (`#pragma ACCEL PARALLEL FACTOR=auto{2}`)\n- Within the innermost loop `k` of the first matrix multiplication, enable the reduction of the 'tmp' matrix with a parallelism factor that processes two iterations concurrently. (`#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}`)\n- Conversely, allow the HLS tool to decide whether to pipeline the outermost loop `i` in the second matrix multiplication for potential optimization of execution time. (`#pragma ACCEL PIPELINE auto{}`)\n- For the second loop level `j` in the second matrix multiplication, only tile the iteration space into blocks where each block spans 8 iterations. (`#pragma ACCEL TILE FACTOR=auto{8}`)\n- Parallelize the second level loop `j` in the second sequence by performing two iterations concurrently. (`#pragma ACCEL PARALLEL FACTOR=auto{2}`)\n- Within the innermost loop `k` of the second matrix multiplication, intensify parallel execution by processing up to 16 iterations at the same time, after reducing the 'D' matrix. (`#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}`)", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware accelerated kernel function 'kernel_adi' implementing an alternating direction implicit (ADI) solver with parameters `tsteps` for time steps, `n` for grid size (60x60), and input/output double arrays `u, v, p, q` representing the grid. Turn off automatic pipelining for the top-level time loop. However, allow flattening and pipelining optimizations within the column sweep stage, but ensure each loop iteration inside remains sequential without parallel execution. Standardize a reversed loop so that it counts upwards using a transformed index. In the row sweep stage, disable pipelining and parallelize the outer loop to maximally utilize resources with all available hardware parallelism, allowing 29 iterations to potentially run concurrently. Keep internal iterations of this stage sequential as well. The calculations involve constants DX, DY, DT, B1, B2 and intermediate variables a, b, c, d, e, f, mul1, mul2 initialized at the beginning of the function.\n\nUse the following specific pragmas:\n\n- Treat the `kernel_adi` function as a hardware kernel for acceleration: `#pragma ACCEL kernel`.\n- Disable automatic pipelining for the top-level time loop: `#pragma ACCEL PIPELINE auto{off}`.\n- Disable loop tiling and loop unrolling optimizations to keep the original loop structure by setting the tile factor to 1: `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Process each loop iteration sequentially by setting the parallel factor to 1: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the column sweep section\u2019s first nested loop, enable pipelining, allowing multiple iterations to be processed concurrently for potential performance optimization: `#pragma ACCEL PIPELINE auto{flatten}`.\n- For the row sweep stage, disable pipelining: `#pragma ACCEL PIPELINE auto{off}` and set a high parallel factor to the outer loop to allow maximum parallel execution with available resources: `#pragma ACCEL PARALLEL FACTOR=auto{29}`.\n\nEach array index operation should be using the original loop bounds and calculating array indices according to the ADI algorithm's requirements.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function that treats the function as a hardware acceleration kernel and manages loop pipelining, tiling, and parallelism as follows:\n\n1. For the outermost loops over variables 'i' and 'j', initiate no automatic pipelining, use the original loop form without tiling (factor 1), and execute iterations sequentially (parallel factor 1).\n2. For the nested loop inside the first 'i' loop, flatten the pipelining, use the original loop form without tiling (factor 1), and allow two iterations to be processed in parallel (parallel factor 2).\n3. Within the inner loop over 'k' performing the multiplication and summation to compute 'tmp', enable reduction on 'tmp' with no specific factor.\n4. For the second set of loops, use default pipelining with no specification, original loop form without tiling (factor 1), and within the 'j' loop, process four iterations in parallel (parallel factor 4).\n5. Lastly, in the innermost loop over 'k' for the final computation of 'D', apply a parallel reduction on 'D' with a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function with the following conditions:\n\n1. Treat the function as a hardware kernel for acceleration.\n\n2. For the first top-level loop iterating over `i` and its nested loop over `j`, completely disable pipelining of these loops. Process these loops without subdividing them into smaller chunks and run the iterations sequentially, not in parallel. However, within the innermost loop over `k`, enable parallel execution with a factor of 10 with a reduction on the `tmp` array.\n\n3. For the second set of nested loops over `i` and `j`, do not pipeline the outer loop iterating over `i` and process it in its full original form without breaking it into smaller pieces while running the iterations sequentially, not in parallel. For the inner loop over `j`, the pipelining setting follows the default behavior. Enable parallel execution within the innermost loop over `k` with a high factor of 50 with a reduction on the `D` array.\n\nSpecific #pragma directives you should use:\n- For the kernel function: `#pragma ACCEL kernel`\n- To turn off automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- To process the loop without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- To run loop iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- To enable parallel execution with specific factors: `#pragma ACCEL PARALLEL FACTOR=auto{10}` for the first loop and `#pragma ACCEL PARALLEL FACTOR=auto{50}` for the second loop.\n- To apply reduction to a specific array: `#pragma ACCEL PARALLEL reduction = array_name FACTOR=auto{factor}` where `array_name` is `tmp` for the first reduction and `D` for the second, and `factor` is `10` and `50` respectively.\n\nIn natural language, instruct the HLS tool with a single comprehensive instruction: \"Create a hardware-accelerated 2mm kernel that disables automatic pipelining for all top-level loops and their direct sub-loops, while maintaining sequential processing and preventing any tiling subdivision. Within the first loop nest, allow for parallel reduction of the `tmp` array with a factor of 10 in the innermost loop. In the second loop nest, keep the default pipelining behavior for the sub-loop and allow for a parallel reduction of the `D` array with a parallel execution factor of 50 in the innermost loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function that performs matrix multiplication and addition with given dimensions and scaling factors, where the first loop set computes `tmp = alpha * A * B`, and the second loop set computes `D = beta * D + tmp * C`. For the first loop set, disable automatic pipelining, retain the original loop structure without tiling, and execute the innermost loop iterations in two parallel threads with a reduction on `tmp`. Allow the inner loop of the first loop set to be pipelined by default. For the second loop set, also disable automatic pipelining, retain the original loop structure without tiling, flatten the pipeline for the outer loop, and parallelize the inner loop with 32 threads with a reduction on `D`, while the innermost loop executes sequentially. Use appropriate #pragma directives to designate the kernel function for acceleration and to define the specified behaviors for each loop set and inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate this function as a dedicated hardware acceleration module, and within it, ensure the following: disable automatic pipelining for the first loop; execute the second loop's iterations sequentially and in a single block without subdividing; perform the third loop's iterations one after another without concurrency, employing a serial reduction for variable 'tmp'; and for the fourth loop, allow pairs of iterations to run concurrently with a parallel reduction for array 'y'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Specify the following directives for high-level synthesis (HLS) code generation: \n\n1. Declare the loop nest starting with 'for (i = 0; i < 40; i++)' as a hardware acceleration kernel using '#pragma ACCEL kernel'.\n2. Disable automatic pipelining for the outermost loop with '#pragma ACCEL PIPELINE auto{off}'.\n3. Prevent tiling and parallel execution optimizations for the outermost loop with '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n4. For the second loop level nesting within 'for (j = 0; j < 50; j++)', enable automatic pipelining without specifying the pipeline initiation interval using '#pragma ACCEL PIPELINE auto{}'.\n5. Similar to the outermost loop, prevent tiling optimizations and allow parallel execution of two iterations at a time for the second loop level using '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{2}'.\n6. For the innermost loop 'for (k = 0; k < 70; ++k)', specify automatic parallelism with a factor of two and target the variable 'tmp' for reduction with '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}'.\n7. Repeat steps 2 and 3 for the next loop nest starting with 'for (i = 0; i < 40; i++)' to disable pipelining and optimizations for tiling and parallel execution.\n8. Within this next loop nest, at the level 'for (j = 0; j < 80; j++)', flatten and pipeline the nested loops automatically with '#pragma ACCEL PIPELINE auto{flatten}'.\n9. Again, prevent tiling optimization and allow a much higher degree of parallelism, handling up to 32 iterations simultaneously for this second loop level with '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{32}'.\n10. Lastly, in the innermost loop 'for (k = 0; k < 50; ++k)', enable parallelism with a factor of one targeting the variable 'D' for reduction with '#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware acceleration kernel, don't use automatic pipelining for its first loop, execute both of its nested loops entirely in their original sequential order without concurrency or tiling, and perform the reductions for 'tmp' and 'y' across their respective loops sequentially one element at a time.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, initially prevent loop pipelining, and then, for the main operations within the function:\n\n1. Disable loop pipelining optimization for the outermost loops iterating over the primary arrays.\n2. Prevent the outermost loops from being broken into smaller chunks.\n3. Ensure the outermost loops run sequentially without parallel processing.\n\nNext, for the innermost loops handling the matrix multiplications and additions:\n\n1. Allow loop flattening for the inner loops to potentially increase the level of parallelism.\n2. Use a fixed loop tiling factor of 1, keeping the inner loops unaltered.\n3. Split the inner loops' iterations into two chunks for the first inner loop and four for the second one, processed in parallel.\n\nFinally, for the very innermost loops that accumulate results into the 'tmp' and 'D' matrices:\n\n1. Enable parallel reduction with a factor of 1 to perform accumulation without parallelism.\n2. Specify that the reduction is focused on the 'tmp' matrix for the first accumulation and on the 'D' matrix for the second one.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code for the kernel function `kernel_2mm` performing matrix-matrix multiplication and addition, use the following natural language instructions:\n\n1. Designate the function `kernel_2mm` for hardware acceleration.\n2. Disable automatic pipelining for the entire function to preserve the original loop order without optimization.\n3. Execute the first-level loops without tiling them and without parallelizing their iterations.\n4. Within the nested loop for computing `tmp`, do not pipeline the loop, do not tile it, but process two iterations of the inner `j` loop in parallel.\n5. Enable parallel reduction for the innermost 'k' loop calculating `tmp` with a factor that allows the maximum number of parallel operations, aiming for 32 concurrent operations.\n6. In the second set of loops for updating matrix `D`, enable automatic pipeline optimization without specific constraints.\n7. Again, do not tile the loops but set the parallel execution of the inner `j` loop for updating `D` to execute ten iterations in parallel.\n8. For the innermost 'k' loop updating `D`, perform the operations sequentially without reduction or parallelization.\n\nSpecific #pragma values to use:\n\n- `#pragma ACCEL kernel` to indicate the function is a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` to prevent automatic pipelining of the loops.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to avoid tiling of the loops, preserving the original full loop form.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{2}` to set the degree of parallelism for the loops, specifying sequential or 2-way parallel execution.\n- `#pragma ACCEL PIPELINE auto{flatten}` to allow for a flattened pipeline, optimizing loop execution with potential pipelining across loop nests.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` to define parallel reduction operations for the loops, the first with a high parallel factor and the second without parallel reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS (High-Level Synthesis) C code, use the following natural language instructions for each respective directive:\n\nFor `#pragma ACCEL kernel`: Treat the entire function 'kernel_2mm' as a hardware acceleration kernel.\n\nFor `#pragma ACCEL PIPELINE auto{off}` applied to the outer loop (first occurrence): Do not apply automatic pipelining optimization to the outermost 'i' loop.\n\nFor `#pragma ACCEL TILE FACTOR=auto{1}` applied after the pipeline pragma: Do not partition the outer 'i' loop; process it as a single, non-divided task.\n\nFor `#pragma ACCEL PARALLEL FACTOR=auto{1}` applied after the tiling pragma: Execute the iterations of the 'i' loop sequentially with no parallelism applied.\n\nFor `#pragma ACCEL PIPELINE auto{off}` applied to the middle loop: Prevent automatic pipelining of the middle 'j' loop within the 'i' loop.\n\nFor `#pragma ACCEL TILE FACTOR=auto{1}` applied after the pipeline pragma in the middle loop: Similar to the outer loop, do not partition the 'j' loop.\n\nFor `#pragma ACCEL PARALLEL FACTOR=auto{2}`: Apply a minimal level of parallelism by allowing two concurrent iterations of the 'j' loop.\n\nFor `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}` applied to the innermost loop: Accelerate the calculation of the 'tmp' matrix with a reduction operation, executing 16 iterations of the 'k' loop in parallel.\n\nFor `#pragma ACCEL PIPELINE auto{}` applied to the second outer loop: Enable automatic pipelining on the second outer 'i' loop, allowing HLS to choose the best setting.\n\nFor `#pragma ACCEL PIPELINE auto{flatten}` applied to the next middle loop: Flatten the loop nest to combine loop levels, potentially enabling more parallelism for this 'j' loop within the second 'i' loop.\n\nFor `#pragma ACCEL PARALLEL FACTOR=auto{20}`: Introduce a higher degree of parallel processing by allowing 20 concurrent iterations of the 'j' loop within the second 'i' loop.\n\nFor `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{1}` applied to the innermost loop in the second major section: Perform the D matrix update sequentially for each 'k' loop iteration within the 'j' loop, leveraging the reduction pattern.\n\nPlease note that the provided C code is meant to represent a specific set of directives that guide the HLS tool in transforming the C code into an efficient hardware implementation, the specific \"#pragma\" directives and their `auto` blocks suggest a desire for the HLS tool to automate decision making for the specified directive unless manually specified (as \"off\" or a numeric value).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a hardware acceleration kernel, automatically optimize the pipeline settings for the outer loop, let the compiler choose the tiling factor for the second loop while hinting that it should consider at least two levels of hierarchy, and suggest automatic parallelization settings for the innermost loops with a strong hint for extensive parallelization while performing reductions on the `tmp` and `y` arrays. Use the following HLS pragmas: \n\n- Treat the `kernel_atax` function as a dedicated hardware kernel with `#pragma ACCEL kernel`.\n- Enable automatic pipeline optimization for the first loop with `#pragma ACCEL PIPELINE auto{}`.\n- Allow the compiler to determine an appropriate tiling factor, with a suggestion to explore at least two levels with `#pragma ACCEL TILE FACTOR=auto{2}`.\n- Suggest automatic parallelism for the second loop with an openness to high fan-out by using `#pragma ACCEL PARALLEL FACTOR=auto{26}`.\n- Request automatic parallelism for the innermost loop with a targeted reduction on the `tmp` array, indicating a preference for a high degree of parallelism with `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}`.\n- Finally, enforce automatic parallelization with reduction on the `y` array in the last innermost loop with a factor that suggests full utilization of the available resources with `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{410}`.\n\nThe given HLS pragmas guide the compiler towards optimizing the loops for parallel hardware execution, but the \"auto\" key indicates that the compiler has the freedom to determine the specific values based on the context and the targeted hardware capabilities.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{26}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{410}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function that takes matrix dimensions and scalar values along with matrices A, B, C, D, as well as a temporary matrix tmp as input parameters. Apply the following directives: \n\n1. Define the function as an accelerator kernel.\n2. In the first loop nest,\n   - Do not allow automatic pipelining of the outermost loop.\n   - Disable tiling optimizations for the loop by using the smallest tile factor.\n   - Enforce sequential execution of the loop iterations.\n   - Prevent automatic parallelization of the second loop but allow explicit parallelization with a factor of 2.\n   - In the innermost loop for calculating the temporary matrix, use parallel reduction with a full unrolling factor for the matrix B's dimension.\n3. In the second loop nest,\n   - Enable automatic pipelining with default settings for the outermost loop.\n   - Suppress tiling optimizations by setting the smallest tile factor.\n   - Enforce sequential execution of the outer loop iterations while again allowing the second loop to have explicit parallelization with a factor of 2.\n   - Flatten the pipeline of the second loop to maximize parallelism.\n   - For the innermost computation of matrix D, use parallel reduction with the iterations to be executed one after another, disallowing concurrent execution.\n\nEnsure the code reflects these directives with specific pragma values where 'auto' is replaced by the designated settings, for instance, using 'off' for the pipeline directive to prevent pipelining.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS C code to perform a 2mm kernel operation with specific pragmas, indicating that the kernel itself is a hardware acceleration target. The outermost loop should not be pipelined and should execute without tiling or parallel optimization. The first nested loop is flattened for pipelining, allowing the compiler to optimize the structure, but processed without tiling and with limited parallelism of two iterations at a time. Inner reductions on `tmp` are performed serially. The second outermost loop is pipelined by the compiler, without tiling or parallel optimization, whereas the nested loop inside it is not pipelined, has tiling by a factor of two, and parallel execution of four iterations at a time. Inner reductions on `D` are performed with a high parallelism factor of thirty-two.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel, do not use automatic pipelining on the outermost loop, ensure the first loop iterates sequentially without tiling or paralleling, the second loop performs a reduction on 'tmp' without parallelization, and the third loop performs a reduction on 'y' with a parallelization factor of 32.\n\n- To mark the function as a hardware acceleration kernel, use \"#pragma ACCEL kernel\".\n- To disable automatic pipelining on the first loop, use \"#pragma ACCEL PIPELINE auto{off}\". \n- To prevent the compiler from tiling the first loop, use \"#pragma ACCEL TILE FACTOR=auto{1}\".\n- To make the first loop run sequentially, use \"#pragma ACCEL PARALLEL FACTOR=auto{1}\".\n- For the second nested loop within the first loop, to ensure there is a reduction on 'tmp' with no parallelization factor, use \"#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\".\n- For the reduction on 'y' within the third nested loop, while enabling parallel execution with a factor of 32, use \"#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\".", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and apply specific optimization and parallelization directives for its nested loops as follows: Do not pipeline the outermost loop, and ensure it runs in its entirety without tiling or parallelizing its iterations. Within the first nested loop, do not pipeline, tile with a factor of 1, but allow for limited parallel execution with a parallel factor of 2. Apply parallel reduction to the innermost loop of the first nest with a high parallel factor of 16 to expedite the computation of the product of matrices `A` and `B`. For the second nest of loops, enable default pipelining, avoid tiling, and limit parallelism with a factor of 1 in the outer loop, while setting it to 4 in the first inner loop. Use a parallel reduction with a factor of 10 within the innermost loop as the matrix `D` is updated with the products of `tmp` and `C`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function `kernel_2mm` as a hardware acceleration kernel. Disable automatic pipelining for the entire function, but do not tile or parallelize the outermost loops. Within the first nested loop, allow loop flattening for efficient pipelining, use a tiling factor of 1 to avoid loop tiling, and set a parallelization factor of 2 to enable modest parallelism at that level. Inside the innermost loop of the first nested loop, enable parallel reduction on `tmp` without loop parallelism. In the second set of nested loops, disable pipelining for the intermediate loop, do not tile, but allow a higher parallelization factor of 4 for the outer loop. Finally, enable a parallel reduction on `D` with maximum parallelization for the innermost loop of the second nested part.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "First, Initialize an AES256 encryption context with a secret key and then copy this key to the decryption context. Use the `aes_expandEncKey_1` function to expand the decryption keys and take note to avoid pipelining of this stage. Encrypt the input buffer using AES256 in the ECB mode. This involves multiple rounds of substitution using sbox, shifting rows, mixing columns, and adding round keys. Apply different optimizations to these stages: Pipelining the stages for efficient use of the hardware resources and configuring the loop tile factor to automatically for the hardware to determine looping structure of the third loop for better resource utilization. Finally, apply the round keys to the buffer again to complete the encryption process. Treat the whole process as a hardware kernel for hardware acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function named 'kernel_adi' for accelerating the alternation direction implicit (ADI) method, which involves a series of computation stages including initializations, column sweep, and row sweep within a double-nested loop construct. For the outer loop, refrain from automatic pipelining. Ensure that each loop is processed in its entirety by using a loop tiling factor of one and do not introduce parallelism between loop iterations. For the inner column sweep loop, automatically apply pipelining optimizations, maintain full iteration form, and process iterations sequentially. However, within the next-level inner loop, enable parallel processing of 58 iterations. When standardizing the reverse loop, parallelize 4 iterations at a time. Similarly, for the inner row sweep loop, again disallow automatic pipelining, maintain full iteration form, and allow two iterations to be processed in parallel, then, within its inner loop, enable 58 iterations to be processed in parallel. When standardizing the corresponding reverse loop in the row sweep, parallelize 4 iterations at a time. Use the following pragmas at their respective positions:\n\n- Treat 'kernel_adi' function as a hardware acceleration kernel using `#pragma ACCEL kernel`.\n- Disable automatic pipelining of the outer loop with `#pragma ACCEL PIPELINE auto{off}`.\n- Process the outer loop without tiling with `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Run outer loop iterations sequentially with `#pragma ACCEL PARALLEL FACTOR=auto{1}`. \n- For the inner column sweep loop, enable automatic pipelining with `#pragma ACCEL PIPELINE auto{}`.\n- Parallelize 58 iterations within the innermost loop of column sweep using `#pragma ACCEL PARALLEL FACTOR=auto{58}` and standardize the reverse loop with parallelism of 4 iterations `#pragma ACCEL PARALLEL FACTOR=auto{4}`.\n- Apply the same strategy for the row sweep loops, but allow parallelism of 2 iterations for the loop with `#pragma ACCEL PARALLEL FACTOR=auto{2}`, then 58 and 4 iterations for inner loops as done in column sweep.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS hardware kernel function named 'kernel_adi' to process 2D arrays 'u', 'v', 'p', and 'q' without any loop pipelining, tiling, or parallelism optimizations by default, except for a specific inner loop where you should enable a modest level of parallelism with a factor of 8. Use the following instructions to achieve this:\n- Mark the 'kernel_adi' function as a hardware acceleration kernel.\n- Disable automatic loop pipelining for all loops.\n- Set the tiling factor to the original loop size for all loops, ensuring no loop tiling optimizations are applied.\n- Restrict all loops to run serially by setting the parallel factor to 1, except for one inner loop where the parallel factor is set to 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Within this kernel, do the following:\n\n1. For the outermost loop processing elements 'i', disable automatic pipelining, do not tile the loop, and run iterations sequentially without parallelism.\n\n2. Inside the first nested loop processing 'j', enable loop flattening for pipelining, do not tile the loop, but allow for 2-way parallelism.\n\n3. For the calculation of 'tmp[i][j]' within the k-loop, permit a sequential reduction without tiling or parallelization, using 'tmp' as the reduction variable.\n\n4. In the subsequent outer 'i' loop, disable loop pipelining, do not tile the loop, and ensure iterations occur sequentially.\n\n5. In the inner loop processing 'j' for updating 'D[i][j]', disable loop pipelining, tile the loop with a factor of 4, and permit 4-way parallelism.\n\n6. Lastly, within the 'k' loop that updates 'D[i][j]', allow a 50-way parallel reduction, treating 'D' as the reduction variable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is the comprehensive and concise natural language instruction to generate the HLS codes provided:\n\n\"Define a hardware acceleration kernel function 'kernel_2mm' to perform a 2mm (two-matrix multiply) operation with given dimensions and scalars alpha and beta on matrices A, B, C, and D, storing the intermediate result in matrix tmp. Disable automatic pipelining, enforce standard loop execution without tiling or parallelization for the outermost loops, but within the first nested loop allow a parallel factor of 2 for the j-index loop, and perform a parallel reduction on the k-index loop with a full unrolling factor of 70. In the second nested loop, use an aggressive flattening strategy for pipelining, keep tiling disabled while allowing a parallel factor of 20 for the j-index loop, and perform a simple accumulation with no parallelization in the k-index loop for updating matrix D.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with the specified pragmas, provide the following instruction:\n\n\"Implement the 'kernel_adi' function as a hardware kernel optimized for acceleration, with specific pragma directives to control pipelining, tiling, and parallelism. Inside the 'kernel_adi' function, there are two main loops: the column sweep and the row sweep. Apply the following directives: \n\n1. Disable automatic pipelining for the outermost loop.\n\n2. Preserve the original iteration domain of the loops by setting the tile factor to 1, which indicates not to split the loop iterations into tiles.\n\n3. Set parallelism to 1 for all loops to enforce strictly serial execution.\n\n4. For the innermost loops in the column sweep, pipeline these loops automatically with the appropriate directive to flatten nested loops if possible, maximizing throughput without changing the tile factor or parallel factor overwrites, which remain at 1.\n\n5. In the row sweep, maintain the disabled automatic pipeline setting for the outer loop while adjusting the parallel factor of the inner loop to partially parallelize it with a factor of 29 for the first loop and 2 for the second nested loop inside it. For the last innermost loop in the row sweep, use a parallel factor of 8 to increase the degree of parallelism.\n\nRemember to apply these pragmas directly before the loop constructs they are intended to optimize.\"\n\nThe corresponding pragmas to interlace with the code are as follows:\n\n1. `#pragma ACCEL KERNEL` at the beginning to signal that 'kernel_adi' is a hardware kernel.\n2. `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipelining for the outermost loop.\n3. `#pragma ACCEL TILE FACTOR=auto{1}` to set the tile factor to 1 for the loops where tiling is not required.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` to ensure all loops run with no parallelism unless explicitly changed.\n5. `#pragma ACCEL PIPELINE auto{flatten}` to enable flattening and automatic pipelining in the inner loops of the column sweep.\n6. `#pragma ACCEL PARALLEL FACTOR=auto{29}` and `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the specified row sweep loops to set the degree of parallelism according to the instruction.\n7. Lastly, `#pragma ACCEL PARALLEL FACTOR=auto{8}` for the final innermost loop of the row sweep to further control the parallel execution degree.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate 'kernel_2mm' as a hardware acceleration kernel without automatic optimization of pipeline, tiling, and parallelization for the outer loops, while the second inner loop has a parallel factor of 2 and the innermost loop has a parallel factor of 70 with a reduction on 'tmp'. In the final outer loop, use automatic flattening for pipeline optimization, a parallel factor of 4 for the second inner loop, and maintain standard parallel processing for the final innermost loop with a parallel reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This section of code defines multiple static utility functions that are components of the AES-256 Cypher algorithm such as `aes_expandEncKey_1`, `aes_addRoundKey_cpy_1`, `aes_subBytes_1`, `aes_shiftRows_1`, `rj_xtime_1`, `aes_mixColumns_1`, `aes_addRoundKey_1`. Lastly, it defines a kernel named `aes256_encrypt_ecb` for the hardware acceleration. This kernel encrypts 32-byte plaintext using a 32-byte key through expanded keys and substitution boxes.\n\nThe encryption function does the following:\n1. Initialize an encryption and decryption key from the input key.\n2. Expand the decryption key to 240 bytes.\n3. Copy the encryption key to the buffer with bitwise exclusive OR operation.\n4. Perform 14 rounds of AES-256, each round consists of byte substitution, row shifting, column mixing.\n5. Pipeline the loop in the encryption kernel function for automatic optimization to ensure non-stop data flow.\n6. Tile the loop in the encryption kernel function into smaller sequential chunks for better parallel execution.\n\nFor the `#pragma ACCEL PIPELINE` and `#pragma ACCEL TILE FACTOR`, 'auto' and 'FACTOR=auto{1}' are used respectively to allow the HLS compiler to decide the best factor value.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided with appropriate pragmas, use the following concise natural language instructions for each pragma and its placement:\n\n- Treat the entire function 'kernel_2mm' as a hardware acceleration kernel with the given arguments.\n- Within the 'kernel_2mm' function, but outside any loop, prevent the automatic pipelining of the top-level loop nests for both 'for (i = 0; i < 40; i++)' loops.\n- Inside each top-level 'for (i = 0; i < 40; i++)' loop, again, ensure that the loop is not auto-pipelined or auto-tiled and executed one iteration after another by using factor 1, preserving its original structure.\n- Within the first nested 'for (j = 0; j < 50; j++)' loop, allow the tool to automatically apply pipelining, but for tiling and parallel execution, specify to fully unroll the loop with a parallel factor of 2 and maintain the loop's original structure by setting the tile factor to 1.\n- For the innermost 'for (k = 0; k < 70; ++k)' loop, apply parallel execution with a parallelization factor of 32 to be used during the reduction operation on the 'tmp' array.\n- For the second 'for (j = 0; j < 80; j++)' loop, disable automatic pipelining and set the loop to be executed sequentially with a factor of 1, but apply tiling with a factor of 8 and parallel execution with a factor of 4.\n- Finally, in the second innermost 'for (k = 0; k < 50; ++k)' loop, apply parallel execution with a reduction on the 'D' array using a parallelization factor of 16.\n\nThese instructions should lead to the generation of HLS C code encrusted with the pragmas as shown above, optimizing the function 'kernel_2mm' for a hardware acceleration context.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel for function `kernel_2mm` which performs a 2mm (double matrix multiplication) calculation without automatically pipelining the outer loops and maintaining the original loop structures. In the first nested loop, pipeline the inner loop automatically but do not tile it, and allow a limited parallel factor of 2 for its most inner loop with explicit reduction on `tmp`. In the second nested loop, do not pipeline the intermediate loop, enable tiling with a factor of 1 for the outermost loop, and set a parallel factor of 4 for the intermediate loop with a reduction on `D` at a factor of 2 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as a hardware acceleration kernel, and apply the following directives for loop optimizations: Disable automatic loop pipelining, keep the original loop structure without tiling, use sequential execution within loops, enable pipelining while flattening nested loops for column sweep, and use a parallel factor of 29 for the row sweep. Specifically set:\n\n1. Use '#pragma ACCEL kernel' to specify 'kernel_adi' as an acceleration kernel.\n2. Apply '#pragma ACCEL PIPELINE off' for the outermost loop to prevent automatic loop pipelining.\n3. Use '#pragma ACCEL TILE FACTOR=1' to maintain the original loop structure without any tiling optimizations.\n4. Set '#pragma ACCEL PARALLEL FACTOR=1' for the loops within the column sweep to ensure sequential execution of iterations.\n5. For flattening nested loops, put '#pragma ACCEL PIPELINE flatten' before the second loop in the column sweep. \n6. Adjust to '#pragma ACCEL PARALLEL FACTOR=29' before the row sweep to specify that 29 iterations of the loop can be processed in parallel.\n7. Within the row sweep, use '#pragma ACCEL PARALLEL FACTOR=2' for the second loop to process every two iterations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel, and for all nested loops within this kernel, ensure that the outer loops are executed sequentially without applying automatic pipelining, not subdivided into smaller sections, and with no parallelization of iterations. However, for the first inner loop, enable parallel execution of iterations with a specific factor of 2; for the reduction operations inside the nested loop, decompose the operations across available resources with respective factors set to the total number of iterations in the loop being reduced. Use the following directives:\n\n1. Use `#pragma ACCEL kernel` to specify that the function `kernel_2mm` is targeted for hardware acceleration.\n\n2. Use `#pragma ACCEL PIPELINE auto{off}` at the beginning of each loop nest to prevent automatic loop pipelining.\n\n3. Use `#pragma ACCEL TILE FACTOR=auto{1}` following the pipeline directive to prevent the loop from being tiled into smaller chunks.\n\n4. Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` after the tile directive for the outer loops to ensure sequential iteration without parallelization.\n\n5. Inside the first nested loop, after disabling pipelining and tiling, override the outer parallel factor using `#pragma ACCEL PARALLEL FACTOR=auto{2}` to run two iterations of this loop in parallel.\n\n6. For the innermost loop where reduction occurs, implement reduction using `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{total_iterations}` where `variable` is the array to be reduced (`tmp` or `D`) and `total_iterations` is the size of the dimension being reduced over (`70` for `tmp`, `50` for `D`).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for matrix multiplication followed by matrix addition, where:\n\n- The function `kernel_2mm` should be accelerated as hardware kernel.\n- The top-level loops iterating over `i` should not be automatically pipelined and should not be broken into smaller chunks or run in parallel (use a single iteration at a time).\n- For the `j` loop in the first major loop section, iterations should not be automatically pipelined, should not be broken into smaller chunks, and run two iterations in parallel.\n- When calculating the intermediate matrix `tmp`, enable parallelization across the `k` dimension with a complete unrolling (considering `k` has a known loop trip count of 70).\n- In the second major loop for updating matrix `D` with the temporary results, the top-level `j` loop should run four iterations in parallel without automatic pipeline settings and should not be broken into tiles.\n- For calculating the final value of matrix `D`, enable parallelization with partial unrolling by a factor of 2 in the inner loop over `k`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following high-level operations into HLS pragmas to define how the 'kernel_2mm' function will be synthesized into FPGA hardware:\n\n1. Identify the 'kernel_2mm' function as a targeted hardware acceleration kernel.\n2. Disable automatic pipelining for the outermost loops within the kernel.\n3. Do not tile (split) the outermost loops.\n4. Execute the iterations of the outermost loops sequentially.\n5. Enable automatic optimization for the pipelining of the second-level loop 'i', but do not split it into tiles and execute every two iterations of the 'j' loop in parallel.\n6. For the innermost 'k' loop updating the 'tmp' array, perform the summation in parallel across all 'k' iterations.\n7. Enable automatic pipelining for the second 'i' loop and flatten any nested loops for more efficient execution, tile the 'j' loop into chunks of 8, and execute four iterations of the 'j' loop in parallel.\n8. For the innermost 'k' loop updating the 'D' array, do not perform any parallel reduction or other optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The provided code is a multi-step process for encryption using the Advanced Encryption Standard (AES). Specific steps include a key expansion step, a substitution step, a shift row step, a column mixing step, and a round key addition step. The majority of these steps are executed inside the \"aes256_encrypt_ecb\" kernel, which is indicated by the '#pragma ACCEL kernel' command. \n\nFor the 'aes_expandEncKey_1' and 'aes_addRoundKey_cpy_1' functions, they are executed linearly, making use of all available cores, which is indicated by '#pragma ACCEL PIPELINE auto'. \n\nIn the encryption step inside the 'aes256_encrypt_ecb' kernel, we find a loop which performs the substitution, shift rows, mix columns and adding round key operations. This loop will be optimized for parallel execution by the HLS compiler, indicated by '#pragma ACCEL PIPELINE auto{flatten}', and distributed automatically across multiple iterations, indicated by '#pragma ACCEL TILE FACTOR=auto{4}'. The flatten qualifier allows nested loops to be considered as a single larger loop, potentially enabling more parallelism. \n\nNotice that the pragma commands are outside the actual loops in question, instructing the compiler that the subsequent block of code is to be treated as specified.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given C codes form the different modules of an AES 256 (Advanced Encryption Standard with 256 bit key) encryption algorithm. The HLS pragmas are used to define specific behaviors of each part of the code when they are run on FPGAs.\n\nFirstly, the keyword \"#pragma ACCEL kernel\" is used at the beginning of the function 'aes256_encrypt_ecb', it indicates that the following function will be used as a hardware kernel for acceleration when being synthesized to FPGA.\n\nSecondly, there are two \"#pragma ACCEL PIPELINE auto{}\" pragmas in two different for-loops inside the 'aes256_encrypt_ecb' function. In general, this indicates that these loops could be pipelined with compiler deciding the initiation interval of the pipeline based on the loop operations.\n\nLastly, the \"#pragma ACCEL TILE FACTOR=auto{2}\" is used to specify that the iterations of the following loop could be executed in chunks/blocks (tiles) to utilize temporal locality. The \"{2}\" is used to hint to the compiler that it should try to split the iteration space into two, if possible according to the operations and data dependencies in the loop, with the actual value determined by the compiler.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an AES-256 encryption function for block cipher mode. \n\nFirstly, define a structure to store key arrays for encryption and decryption along with a substitution box array, and create functions for essential operations within AES like expanding encryption key, adding round key, byte substitution, row shifting, column mixing and xtime.\n\nThen create the main encryption function. \n\nIt initializes keys with user provided keys and expand the decryption key 8 times using the function 'aes_expandEncKey_1'. Later the function loops 14 times (the number of rounds in AES-256) where each round consists series of operations i.e., substituting bytes with the values in the sbox, shifting rows, applying mix column transform to each column, and XORing the result with a round key.\n\nUse parallel pragma directives like 'ACCEL PIPELINE' to optimize the for loops to run in parallel and 'ACCEL TILE' to unroll loop iterations. 'ACCEL kernel' is used to highlight that this function can be accelerated by hardware. \n\nYou need to guide the HLS tool that you want it to pipeline the second loop in the function, and the unrolling factor should be determined by the tool for the first loop inside the function. Furthermore, you would ask the tool to flatten out function calls in the second loop inside the function to potentially expose more parallelism.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the function 'kernel_2mm' to a hardware kernel for acceleration, and configure loop optimizations as follows:\n\n1. For the outermost two loops, disable automatic pipelining, set tiling to process the loops in their original full form, and enforce strictly sequential execution with no parallelization.\n2. In the first nested loop pair, process the innermost loop with a parallel factor of 2, and use the 'reduction' directive with the temporary matrix 'tmp'.\n3. In the second nested loop pair, flatten the inner loop pipeline, tile the loops without dividing them into chunks, and run the innermost loop iterations with a high parallel factor of 32. Write to matrix 'D' with no additional parallel factor.\n4. The first set of nested loops calculates the temporary matrix according to a given formula using 'alpha' and matrices 'A' and 'B'.\n5. The second set of nested loops updates matrix 'D' by first scaling with 'beta', then adding contributions from the temporary matrix 'tmp' and matrix 'C'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a program to encrypt a 256-bit AES block cipher in the ECB mode. The encryption process involves box substitution, row shifting, and column mixing, followed by key addition on the input buffer. Bias the dynamically controlled pragmas to automatically target the most computationally intensive blocks, to improve the kernel's overall latency and throughput.\n\nBegin by copying the content of the key and into the encryption and decryption key, then expand the decryption key 8 times. Follow that by copying the original key along with the block cipher input (buf) and store it in the `enckey` and `key` variables, respectively.\n\nAt the decryption stage, perform block cipher encryption, which includes the following operations: byte substitution (`aes_subBytes_1`), row shifts (`aes_shiftRows_1`), column mixing (`aes_mixColumns_1`), and the addition of the round key (`aes_addRoundKey_1`). These operations are repeated 14 times. Place the decryption process in the pipeline pragma to overlap the processing of multiple iterations of the loop for higher throughput. In addition, enforce that the tool to flatten the loop hierarchy allowing multiple sub-loops inside a flattened loop to iterate simultaneously. Then, decompose the loop to smaller chunks using the auto-tile factor to further optimize the hardware utilization during parallel execution.\n\nFinally, call byte substitution, row shift, key expansion and addition of the round key on the buffer. Mark the entire function as a kernel for hardware acceleration using the `pragma ACCEL KERNEL` directive.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware acceleration kernel and apply the following optimizations and constraints to its loops:\n\n1. For the outermost loops iterating over the 'i' index that loop over sizes 40 and 50, disable automatic pipelining, prevent loop tiling, and execute iterations sequentially, without parallelism.\n2. For the innermost 'j' loop within the first 'i' loop, also disable automatic pipelining, set tiling to a factor of 1, and allow a fixed degree of parallelism with a factor of 2.\n3. For the 'k' loop nested within the 'j' loop, use a parallel reduction on 'tmp' with a high degree of parallelism at a factor of 32. Do not specify any pragmas for pipelining or tiling here.\n4. For the second occurrence of the 'i' loop iterating over size 40 and 80, turn on the default automatic pipelining, prevent loop tiling, and maintain a sequential execution without parallelism.\n5. Inside this second 'i' loop, for the 'j' loop, disable automatic pipelining, set a larger tile size with a factor of 8, and allow a degree of parallelism with a factor of 4.\n6. For the 'k' loop within this 'j' loop, perform a parallel reduction on 'D' with a degree of parallelism at a factor of 16. As with the other 'k' loop, do not apply pipelining or tiling pragmas here.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named `kernel_2mm` that accelerates a matrix multiplication and addition operation with no automatic pipelining of the outermost loop. Inside this loop, use the default settings for pipelining and parallelization, but do not tile the loop. As we compute the product of alpha, A, and B, apply a reduction to the `tmp` array with a parallelization factor allowing up to 32 simultaneous operations. In the final stage of the function, where D is updated, allow pipelining with loop flattening and set the tiling factor for the associated loop to twice its original block size. Parallelize this last loop to allow 10 simultaneous iterations, and within this loop allow full sequential execution of the nested loop by setting its parallelization factor to 1, without applying any reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The task involves computational operations on multi-dimensional arrays resembling a 2mm kernel function commonly found in dense linear algebra computations. The set of pragma directives provided effects various optimization strategies for hardware synthesis from high-level C code, using the High-Level Synthesis (HLS) tool.\n\nHere is a comprehensive, yet concise description to instruct the algorithm to generate the appropriate HLS C code pragmas:\n\n\"Treat the entire function as a hardware kernel. For the two main for-loops calculating the 'tmp' and 'D' arrays, disable automatic pipelining and manual tiling and parallelization optimization directives, preserving the original loop structure (single iteration per loop cycle). In the nested j-loops within both main loops, enable automatic pipelining without specifications, manually set tile size to 1 to not partition the loop, and manually set parallelization to run two iterations concurrently. Within the inner k-loops for 'tmp' array computations, set a reduction pragma for the 'tmp' array with automatic factor for simultaneous summation across iterations and likewise for 'D' array computations with a reduction on 'D' array.\" \n\nThis description can be transcribed into specific #pragma directives as follows:\n\n1. `#pragma ACCEL kernel` - Mark the function as a kernel for hardware acceleration.\n2. `#pragma ACCEL PIPELINE auto{off}` - Do not automatically pipeline the outer loops.\n3. `#pragma ACCEL TILE FACTOR=auto{1}` - Do not partition the outer loops into smaller chunks (tile size of 1 means no tiling).\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` - Do not run iterations of the outer loops in parallel, i.e., execute one after the other.\n5. Inside the nested j-loops:\n   - `#pragma ACCEL PIPELINE auto{}` - Enable automatic pipelining without specifying the exact behavior.\n   - `#pragma ACCEL TILE FACTOR=auto{1}` - Again, do not partition these loops. \n   - `#pragma ACCEL PARALLEL FACTOR=auto{2}` - Execute two iterations of the loop in parallel.\n6. In the k-loop for 'tmp':\n   - `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}` - Apply parallel reduction to the 'tmp' array with an automatically determined factor, intended for simultaneous aggregation across iterations.\n7. In the k-loop for 'D':\n   - `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}` - Apply parallel reduction to the 'D' array with an automatically determined factor, also intended for simultaneous aggregation across iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The C code includes a cluster of functions for AES-256 encryption process, such as `aes_expandEncKey_1`, `aes_addRoundKey_cpy_1`, `aes_subBytes_1`, `aes_shiftRows_1`, `rj_xtime_1`, `aes_mixColumns_1` and `aes_addRoundKey_1`. Then it generates the main function `aes256_encrypt_ecb` which administers the AES-256 encryption in Electronic Codebook mode. Several loops are operating in this function, the first loop initializes the encoding and decoding keys, while the second loop expands the decoding key. The third loop executes several functions including `aes_subBytes_1`, `aes_shiftRows_1`, `aes_mixColumns_1`, `aes_expandEncKey_1`, `aes_addRoundKey_1` in each iteration and then concludes with key expansion and round key addition.\n\nThree `#pragma` directives are effectively employed to optimize the function `aes256_encrypt_ecb`:\n1. `#pragma ACCEL kernel` is utilized to specify this function to be used as a hardware kernel for acceleration.\n2. `#pragma ACCEL PIPELINE auto{}` advises the HLS compiler to automatically pipeline the loop, which is equivalent to running different loop iterations in parallel on hardware, meaning improving throughput.\n3. `#pragma ACCEL TILE FACTOR=auto{8}` is used to instruct HLS to optimally partition the 'for' loop in the block called 'ecb3', this helps to parallelize the loop iterations in large loop structures thereby boosting the performance of the code.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code for the 2mm kernel function, use the following natural language instruction:\n\n\"Define a hardware kernel function called 'kernel_2mm' that takes as parameters the sizes 'ni', 'nj', 'nk', 'nl', two scalar values 'alpha' and 'beta', and four 2D arrays 'tmp', 'A', 'B', 'C', 'D'. Within this function, create two doubly nested for-loops: In the first for-loop pair, avoid applying any automatic pipelining, keep the loop in its full form without any tiling, and execute iterations sequentially. Inside the innermost loop of this pair, enable loop flattening for pipelining, avoid loop tiling, and set the parallelization factor to 2 while performing a reduction on 'tmp' without parallelization. In the second for-loop pair, turn off automatic pipelining for the outer loop, keep it in its full form without any tiling, and run its iterations sequentially. Set the inner loop to automatically determine pipelining strategies, keep it without tiling, and use a parallelization factor of 4 with a reduction on 'D' at a factor of 50. The computation performed is 'D := alpha * A * B * C + beta * D', where 'D' first gets scaled by 'beta', and then the product of 'alpha', 'A', 'B', 'C' is added.\" \n\nHere are the corresponding #pragma values:\n\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Don't automatically pipeline the outer loops: `#pragma ACCEL PIPELINE auto{off}`\n- Process the outer loops in their original, full form without breaking them into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the iterations of the outer loops one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Automatically determine the pipelining strategy of the inner loops in the first doubly nested pair: `#pragma ACCEL PIPELINE auto{flatten}`\n- Set parallelization factor to 2 for the first inner loop: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n- Perform a reduction on 'tmp' without parallelization: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`\n- Automatically determine pipelining strategies for the inner loop in the second doubly nested pair: `#pragma ACCEL PIPELINE auto{}`\n- Set parallelization factor to 4 for the second inner loop: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n- Perform a reduction on 'D' with a large parallelization factor: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}`\n\nThe `auto{}` and `auto{...}` notations indicate where the pragma is to be configured automatically based on the specified conditions within the braces.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for a 2mm computation function with specific loop optimizations. In the first level of nested loops, prevent automatic pipelining, do not tile, and execute iterations sequentially. In the innermost loop, allow the reduction operation on 'tmp' with automatic factor selection, but enforce at least 14 parallel instances. For the second level of nested loops, enable pipelining with automatic flattening, tile the loop without breaking it into smaller chunks, and parallelize the innermost loop with 20 simultaneous iterations. Limit the final reduction operation on 'D' to sequential processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with appropriate pragmas, instruct the generator with the following guidance:\n\nCreate a hardware-accelerated kernel function named `kernel_2mm` for the purpose of calculating the matrix expression `D := alpha*A*B*C + beta*D`, using standard C variable declarations and 2D array parameters for matrices A, B, C, D, and an intermediate matrix tmp. Dimension sizes are defined by scalar inputs ni, nj, nk, nl, but internally fixed at 40, 50, 70, and 80 as per the loop bounds.\n\nWithin the kernel:\n1. Prevent automatic pipelining of the outermost loops iterating over i.\n2. Maintain the original loop structure by setting the tile factor to 1 for the i loops.\n3. Ensure the iterations of the i loops run serially with a parallelism factor of 1.\n\nInside the first 'i' loop:\n4. Prevent automatic pipelining for the loop over j.\n5. Maintain the original j loop structure by setting the tile factor to 1.\n6. Allow a degree of parallel processing with a factor of 2 for the j loop.\n\nFor the innermost loop over k, which accumulates the product `tmp[i][j] += alpha * A[i][k] * B[k][j]`:\n7. Enable parallel reduction on the tmp array with a full unrolling factor matching the k loop bound, which is implicitly set to 70.\n\nFor the second 'i' loop:\n8. Enable automatic pipelining for loops over j where the code updates the matrix D with `D[i][j] *= beta`.\n9. Again, maintain the structure of the j loop with a tile factor of 1 and pursue a modest parallelism with a factor of 2 for loop j.\n\nFor the k loop inside the second 'i' loop, which performs the final matrix accumulation:\n10. Enable parallel reduction on the D array with the implicit unrolling factor for k loop set to 50, which fully unrolls this loop.\n\nComment the pragmas that are turned on as 'auto' without specification to indicate that the tool should decide the best possible setting for pipelining, tiling, and parallelization in those cases where no explicit off or numeric factor is provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with the specified pragmas, use the following comprehensive but concise instruction:\n\nDefine a function as a hardware acceleration kernel, and within this kernel, disable automatic pipelining, tiling, and parallelization for the top-level loop iterations. For nested loops, selectively disable automatic pipelining, specify non-parallel loop execution or set a low parallelism factor. For innermost loops, introduce parallel reduction with varying parallelism factors.\n\nSpecifically, declare the function 'kernel_2mm' as a hardware kernel for acceleration. For the top-level loops, prevent automatic loop pipelining, tiling, or parallelism by setting each to 'off' or a factor of 1. In nested loops, disable automatic loop pipelining and set tiling to 1 or a specified low factor, and vary parallelism between 1 and 4, depending on the depth of the loop. Inside the innermost loops, enable parallel reduction on the 'tmp' and 'D' matrices, and apply a high parallelism factor, such as 32 or 16, to optimize these operations for concurrent execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Write a program that contains functions to perform AES-256 encryption algorithm. The program has an array, 'sbox', a structure 'aes256_context', and six functions namely 'aes_expandEncKey_1', 'aes_addRoundKey_cpy_1', 'aes_subBytes_1', 'aes_shiftRows_1', 'rj_xtime_1', 'aes_mixColumns_1' and 'aes_addRoundKey_1'. Then, define the kernel function 'aes256_encrypt_ecb', which performs the encryption process using these functions. \n\nUnroll the loop in the 'aes_expandEncKey_1' function, the 'aes_shiftRows_1' function and the loop where '_s_i' goes from 1 to 13. Enable automatic pipeline for these loops where feasible. Apply loop tiling with automatic tiling factor for the loop where '_s_i' goes from 1 to 13.   \n\nThe kernel function includes the 'aes_addRoundKey_cpy_1' function call to copy key values into the encryption key and decryption key, 'aes_expandEncKey_1' function call to expand the key for decryption, and a loop which will make 14 iterations. Inside the loop, call the 'aes_subBytes_1', 'aes_shiftRows_1', 'aes_mixColumns_1' functions for intermediate computations in AES256 algorithm, and 'aes_expandEncKey_1' function call to expand the key for the next round depending on the value of '_s_i', and 'aes_addRoundKey_1' function call to add key values to the block. After the loop, make a call to 'aes_subBytes_1', 'aes_shiftRows_1', 'aes_expandEncKey_1' and 'aes_addRoundKey_1' functions to get the final result after round step of AES algorithm.\n\nTake note that ACCEL PRAGMA is used for applying hardware-specific optimizations.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the above code, follow these steps:\n\n1. Define the `sbox` lookup table and an `aes256_context` struct.\n2. Write a `aes_expandEncKey_1` function to expand a given input encryption key, using the `sbox` for substitution. This function contains two loops. For the first loop use the pragma `#pragma ACCEL PIPELINE auto{}` to perform some optimization techniques that apply to pipelining and for the second loop, which is using the pragma `#pragma ACCEL TILE FACTOR=auto{13}` to divide this loop into smaller chunks for parallel computation.\n3. Write more functions including `aes_addRoundKey_cpy_1` to copy and add a round key, `aes_subBytes_1` to substitute bytes from the `sbox`, `aes_shiftRows_1` to shift rows in the state, `rj_xtime_1` to multiply in GF(2^8), `aes_mixColumns_1` to mix columns in the state and `aes_addRoundKey_1` to XOR the state with a round key.\n4. Write the main function `aes256_encrypt_ecb` marked with `#pragma ACCEL kernel` to indicate that this is a kernel function. This function involves copying keys, expanding the decryption key, and 14 rounds of processing which includes calling the previously defined functions in certain order.\n  \nNote that in the `aes256_encrypt_ecb` function, the `#pragma ACCEL TILE FACTOR=auto{13}` directive is used to automatically determine the best tiling factor, optimizing how the iterations of a loop are divided into smaller chunks. The `#pragma ACCEL PIPELINE auto{}` directive is used to automatically determine the best pipeline factor specifying the number of iterations ready to be executed in parallel. The `#pragma ACCEL kernel` directive tells the HLS compiler to treat this function as a hardware kernel ready for acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a function in C called 'aes256_encrypt_ecb' which performs the AES256 algorithm on a block of 16 characters (this is known as AES-256-ECB). The function accepts two parameters: a context 'ctx' which contains the 256-bit key for encryption, and an input string 'buf' of 16 characters. The keys are expanded using the function 'aes_expandEncKey_1' and the cipher is produced using 'aes_addRoundKey_cpy_1', 'aes_subBytes_1', 'aes_shiftRows_1', 'aes_mixColumns_1', and 'aes_addRoundKey_1' in sequential order. This process is repeated 14 times due to the 14 rounds in AES256 algorithm. \n\nUse the '#pragma ACCEL kernel' directive to indicate that this function should be considered as a hardware kernel for acceleration. Then use the '#pragma ACCEL PIPELINE auto{}' directive before the second for loop to allow for automatic optimization of loop scheduling, and the '#pragma ACCEL PIPELINE auto{off}' directive after it to turn off automatic pipeline optimizations for subsequent loops. Finally, use '#pragma ACCEL TILE FACTOR=auto' before the third for loop to specify the tiling factor automatically (optimizing data locality) for this loop.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following matrix multiplication function into a hardware-accelerated kernel while disabling automatic pipelining for all loops. Ensure that the loops process their iterations sequentially and do not break them into smaller chunks, except for the second loop nest that multiplies the 'tmp' matrix with 'C', where you should partially parallelize across the second dimension with a factor of 10 and the first dimension with a factor of 8, and fully parallelize the reduction across 'tmp'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes with the respective #pragma directives in C, please follow these instructions:\n\n1. Mark the function `kernel_adi` as a hardware acceleration kernel.\n2. Ensure that the top-level loop iterating over `t` is not automatically pipelined, preserving its original execution order.\n3. Prevent the automatic tiling and parallelization of the top-level loop so that it is processed in its full original form and each iteration is executed sequentially.\n4. For the nested loop iterating over `i` (Column Sweep), enable loop flattening with pipelining to combine nested loops and improve throughput, while still processing iterations sequentially due to tiling and parallel factors set to 1.\n5. Within the innermost loop iterating over `j`, ensure that each iteration is executed sequentially, both in the forward and reverse order segments.\n6. For the Row Sweep section, disable automatic pipelining of the `i` loop, but set the parallel factor to 29 to partially parallelize the execution of iterations.\n7. Maintain the sequential execution of iterations for the forward segment of the `j` loop in the Row Sweep with a parallel factor of 1.\n8. Fully parallelize the reverse order segment of the `j` loop in the Row Sweep with a parallel factor of 32.\n\nExplicitly, you should use the following pragma values:\n\n- For disabling automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- For processing loops in full form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- For executing loop iterations one after the other: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- For enabling loop flattening with pipelining: `#pragma ACCEL PIPELINE auto{flatten}`\n- For partial parallelization with a specific factor: `#pragma ACCEL PARALLEL FACTOR=auto{specific factor}`, replacing `specific factor` with the required number, i.e., 29 and 32 in this context.\n- Treat the function `kernel_adi` as a hardware kernel: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a C-program that uses High-Level Synthesis (HLS) to perform Advanced Encryption Standard (AES) 256-bit encryption. Initial Encryption Key (enckey) and Decryption Key (deckey) are defined in an aes256_context struct. Before the encryption process, keys are copied from the provided buffer k to enckey and deckey and the decryption keys are expanded. During the encryption, for 14 iterations, input bytes undergo substitution, shifting, and mixing operations. Different keys are added to the current state depending on the iteration. After all iterations, input data bytes go through substitution and shift operations once again. A final key is expanded and added to get the encrypted data. Use \"#pragma ACCEL kernel\" before the main function to treat it as hardware kernel for acceleration. Use \"#pragma ACCEL PIPELINE auto{}\" for automatic pipelining and \"#pragma ACCEL TILE FACTOR=auto{4}\" for loop tiling, which both improve parallelism. The auto pipelining is turned off with \"#pragma ACCEL PIPELINE auto{off}\" for some specific situations in the code.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Enable high-level synthesis for the 'kernel_2mm' function to perform matrix computation without pipelining loops, without tiling the first loop iteration (but tile the second iteration of the inner loop by a factor of 2), and execute the outermost loop iterations sequentially. For the third loop in the first section, apply a parallel reduction on 'tmp' with up to 32 concurrent operations, and for the innermost loop in the second section, apply a parallel reduction on 'D' with up to 50 concurrent operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function that processes matrix-matrix multiplications and accumulations. Declare the function as a hardware acceleration kernel with arguments for matrix dimensions and scalars, as well as two-dimensional arrays for the matrices. Within the function, do not employ automatic pipelining optimization. Handle the loops at full iteration granularity without tiling or parallelizing the iterations. For the reduction operation inside the innermost loops, utilize a degree of parallelism of 32. However, for one of the nested loops involved in the second operation, enable tiling with a factor of 2 and parallelism with a factor of 10 while keeping the pipelining optimization as default.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel for a 2mm algorithm with specific optimization constraints, such as:\n\n1. For the outermost loop, disable automatic pipelining, keep the loop unaltered without tiling, and have the iterations run sequentially.\n2. In the first nested loop, enable flattened pipelining to combine loop levels for potential parallelism, avoid loop tiling, and process two iterations in parallel.\n3. For the innermost loop within the first double-nested block, enable parallel reduction on `tmp` with sequential iteration processing.\n4. In the second outer loop, enable pipelining without specifying a strategy, refrain from tiling the loop, and run iterations sequentially.\n5. Within the next nested loop level, turn off pipelining, do not tile the loop, and execute four iterations concurrently.\n6. For the innermost loop within the second double-nested block, employ a parallel reduction on `D` with a specific unrolling by 50 iterations.\n\nUse corresponding `#pragma` directives:\n\n- `#pragma ACCEL KERNEL` to mark the function for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` and `#pragma ACCEL PIPELINE auto{flatten}` to control pipelining strategies.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to bypass loop tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`, `#pragma ACCEL PARALLEL FACTOR=auto{2}`, and `#pragma ACCEL PARALLEL FACTOR=auto{4}` to set the degree of parallelism within loops.\n- `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{1}` and `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{50}` to specify reduction operations with different parallel factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This is an implementation of the AES-256 encryption algorithm using HLS C. The user-defined functions are used to implement several standard AES operations: calculating a round constant for key expansion (`aes_expandEncKey_1`), adding a round key in the decryption or encryption processes (`aes_addRoundKey_cpy_1`, `aes_addRoundKey_1`), substitution of bytes in the state with corresponding values in a S-box(`aes_subBytes_1`), permutation of the rows of the state (`aes_shiftRows_1`), and combining each column in the state with a fixed polynomial (`aes_mixColumns_1`). The resulting `aes256_encrypt_ecb` function applies these transformations in several encryption rounds. \nAt the kernel level, `#pragma ACCEL KERNEL` is used to treat the `aes256_encrypt_ecb` function as a hardware kernel. Additionally, `#pragma ACCEL PIPELINE` is used to implement pipeline optimization within `ecb2` and `ecb3` execution blocks, and `#pragma ACCEL TILE FACTOR` is used to split iterations within `ecb3` block into smaller computation units, for improved memory usage and acceleration. These pragmas help optimize the loop functions for the hardware.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function that performs a matrix multiplication and addition without any automatic pipelining optimizations for the outer loops, specifying in the first outer loop that the second loop's iterations are to run in pairs but not in parallel, using a full reduction with 70 iterations for the innermost loop. In the second outer loop, allow natural loop flattening, tile the second loop with a chunk size of 8, and execute four iterations of the second loop in parallel, while keeping the innermost loop's iterations sequential. Apply the following #pragma values:\n\n- Treat the function as a hardware kernel: `#pragma ACCEL kernel`.\n- For the first set of nested loops:\n  - Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`.\n  - Keep the loop in its full form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`.\n  - Run the first loop sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n  - For the second loop inside the first set:\n    - Disable pipelining: `#pragma ACCEL PIPELINE auto{off}`.\n    - Keep the loop in its full form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`.\n    - Parallelize every two iterations of the loop: `#pragma ACCEL PARALLEL FACTOR=auto{2}` with a full reduction on the tmp array: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{70}`.\n- For the second set of nested loops:\n  - Enable automatic pipelining without specific control: `#pragma ACCEL PIPELINE auto{}`.\n  - Keep the loop in its full form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`.\n  - Run the first loop sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n  - For the second loop inside the second set:\n    - Allow loop flattening: `#pragma ACCEL PIPELINE auto{flatten}`.\n    - Tile the loop with a chunk size of 8: `#pragma ACCEL TILE FACTOR=auto{8}`.\n    - Execute four iterations of the loop in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{4}` with sequential execution on the innermost loop: `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the 'kernel_2mm' function as a hardware acceleration kernel, then within the function:\n\n1. Prevent the outermost loop 'for (i = 0; i < 40; i++)' from being automatically pipelined or parallelized to maintain its original execution order without any directive optimizations.\n   \n2. Process the second level loop 'for (j = 0; j < 50; j++)' with regular pipelining to potentially improve its execution latency. Within this loop, avoid tiling optimizations and enable a moderate level of parallel execution for 'tmp[i][j] += alpha * A[i][k] * B[k][j];' by forming 16 parallel instances of multiplying and accumulating operations.\n\n3. Process the next outermost loop 'for (i = 0; i < 40; i++)' similarly to the first loop, with pipelining and no tiling optimizations. \n\n4. In the innermost loop 'for (j = 0; j < 80; j++)', again enable pipelining for potential execution latency improvements, avoid tiling, and establish a higher level of parallel execution, forming 10 parallel instances for 'D[i][j] += tmp[i][k] * C[k][j];', while multiplying the 'D' matrix by a constant 'beta' beforehand in this loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code with the following specifics: First, identify the function `kernel_2mm` as a hardware acceleration kernel. Within this kernel, the first and third major loops should run without automatic pipelining, keep iterations as whole, and disallow parallel execution. For the inner first loop, automatic pipelining should be enabled, but the iterations should be whole and not run in parallel; however, for the innermost loop, apply parallel computing with a reduction on `tmp` by a factor of 32. For the second major loop, enable pipelining but tile the loop by a factor of 2, allow a parallel factor of 10, and for the innermost loop, handle parallelism during reduction on `D` by a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the `kernel_2mm` function as a hardware acceleration kernel, and within the function, disable automatic pipelining and maintain the original loop structure without tiling or parallelism in the first matrix multiplication loop. However, within the innermost loop of the first matrix multiplication, allow parallel processing with a complete unroll for reducing the `tmp` array elements. For the second matrix multiplication loop, again disable automatic pipelining and keep the original loop structure without tiling, but allow limited parallel execution with a parallel factor of 10 in the loop that iterates through the `D` array. Within the innermost loop of the second matrix multiplication, limit parallel execution with a parallel factor of 1 for the reduction of `D` array elements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a hardware acceleration kernel, ensuring not to pipeline the first outer loop automatically. In the second loop, ensure each iteration runs in sequence without tiling or parallelism. Within the nested loops, the first inner loop is to be run sequentially with reduction on `tmp`, while the second inner loop should perform a reduction on `y` in parallel with the capability of processing 16 iterations simultaneously.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS C code for a 2mm kernel function with specific optimization pragmas applying various levels of parallelization, pipelining, and tiling to the loops as follows:\n\n1. Treat the entire 2mm function as a hardware kernel for acceleration.\n2. Disable automatic pipelining for the entire kernel.\n3. Preserve the original loop form for the outer `i` loop without tiling and do not run its iterations in parallel.\n4. In the first nested `j` loop, enable pipelining with default settings, don't tile the loop, and run two iterations in parallel.\n5. In the deepest `k` loop within the first `j` loop, apply parallelization with a factor of 14 and allow reductions on the `tmp` array.\n6. In the second `i` loop outer form, enable pipelining while allowing all nested loops to be flattened into a single level when possible. Again, don't tile the loop and run ten iterations in parallel.\n7. Inside the final `k` loop, do not enable any parallelization, only allow reductions on the `D` array.\n\nEnsure each loop pragma is explicitly marked with \"auto\" followed by the value in curly braces, except where the default setting is used.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a function named 'kernel_adi' which will be used as a hardware accelerator kernel. The function takes integers 'tsteps' and 'n' along with four 2D arrays 'u', 'v', 'p', and 'q' as arguments. Define scalars 'DX', 'DY', 'DT', 'B1', 'B2', 'mul1', 'mul2', 'a', 'b', 'c', 'd', 'e', and 'f' as double precision floating-point types. Initialize these scalars within the function, and apply the following directives:\n\n1. Declare the kernel function suitable for hardware acceleration without any optimizations on loop pipelining.\n2. Ensure the main time-stepping loop is not automatically pipelined, and it processes iterations in a sequential manner without dividing them into smaller chunks or processing multiple iterations in parallel.\n3. For the nested 'i' and 'j' loops performing the column sweep, flatten any nested loops into a single loop level, but maintain the original sequence of execution for each loop level, avoiding aggressive loop unrolling or parallel processing.\n4. In the row sweep part, allow the pipelining optimization to be decided by the compiler, tile the loop by a single iteration factor to keep the loop intact, and ultimately permit the compiler to automatically decide parallelization with a suggestion of a maximum factor of '2' for the outer 'i' loop, '58' for the inner 'j' loop within the row calculations, and '8' for the backward loop iterating over 'j'.\n\nHere's the corresponding pragma information for the specific sections:\n\n- For the hardware kernel: `#pragma ACCEL kernel`\n- For disabling pipelining of the main time step loop: `#pragma ACCEL PIPELINE auto{off}`\n- For maintaining the original loop structure: `#pragma ACCEL TILE FACTOR=auto{1}`\n- For processing iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- For flattening loops during column sweep: `#pragma ACCEL PIPELINE auto{flatten}`\n- For deciding pipelining in the row sweep by the compiler: `#pragma ACCEL PIPELINE auto{}`\n- For suggesting parallelization factors where applicable: Use `#pragma ACCEL PARALLEL FACTOR=auto{}` followed by the suggested number.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given C codes firstly declare an array `sbox[]` of unsigned char and a struct `aes256_context` that includes three arrays `key[]`, `enckey[]`, and `deckey[]` each of 32 unsigned char. It also defines several functions: `aes_expandEncKey_1()`, `aes_addRoundKey_cpy_1()`, `aes_subBytes_1()`, `aes_shiftRows_1()`, `rj_xtime_1()`, `aes_mixColumns_1()`, `aes_addRoundKey_1()` and `aes256_encrypt_ecb()`. \n\nFor the function `aes256_encrypt_ecb()`, it is labeled as the hardware kernel for acceleration by `'pragma ACCEL kernel'`. Then, it includes several operations like assigning value to `rcon`, looping through each element in an array with specific conditions and calling several functions defined before using different pragma specifications. These include `aes_expandEncKey_1()` pipelined with a pragma `'ACCEL PIPELINE auto{flatten}'`, and `aes_subBytes_1()`, `aes_shiftRows_1()`, `aes_mixColumns_1()`, `aes_addRoundKey_1()`, `aes_expandEncKey_1()`, `aes_addRoundKey_1()` executed in the loop with the pragma values `'ACCEL PIPELINE auto{}'` and `'ACCEL TILE FACTOR=auto{1}'`. Finally, the function calls `aes_subBytes_1()`, `aes_shiftRows_1()`, `aes_expandEncKey_1()`, `aes_addRoundKey_1()` consecutively.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS code for a double matrix-multiplication and addition kernel, ensuring the following: Mark the function as a hardware acceleration kernel, keep all loops in their full form without automatic pipelining, partitioning, or parallelization optimization, except for the innermost loop of the first double-nested loop where you should allow up to 32 simultaneous operations for the reduction on 'tmp' array, and within the second double-nested loop, allow up to 10 simultaneous iterations for the outer j-loop while keeping the innermost loop running serially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the High-Level Synthesis (HLS) code provided, use the following natural language instructions:\n\n1. Designate the entire function `kernel_2mm` as a hardware kernel to be accelerated.\n2. Within the function:\n   - For the first three-level nested loops (where the `tmp` matrix is computed), instruct HLS to:\n     - Disable automatic pipelining for the outermost and second loops.\n     - Preserve the natural loop boundaries without tiling or parallelization for the outer two loops.\n     - Parallelize the innermost loop with a factor of 70 and enable reduction for the `tmp` matrix.\n   - For the second set of three-level nested loops (where the `D` matrix is updated), instruct HLS to:\n     - Disable automatic pipelining for the outermost and second loops.\n     - Preserve the natural loop boundaries without tiling or parallelization for the outer two loops.\n     - Parallelize the second loop with a factor of 4 and the innermost loop with a factor of 2, enabling reduction for the `D` matrix.\n\nExplicitly, you can communicate the instructions with these phrases:\n- \"Treat the function `kernel_2mm` as a dedicated hardware kernel for acceleration.\"\n- \"Disable pipelining for the top-level and second-level loops, and disallow loop tiling and parallel processing optimizations within these loops.\"\n- \"In the first set of nested loops, apply parallel processing to the innermost loop with a maximal factor and enable data reduction on the `tmp` array.\"\n- \"In the second set of nested loops, use a parallelization factor of 4 for the second-level loop and a factor of 2 for the innermost loop, with data reduction enabled for the `D` array.\"\n\nPragma values based on instructions:\n- `#pragma ACCEL kernel` - Treat the following function as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` - Don't automatically pipeline the specified loops.\n- `#pragma ACCEL TILE FACTOR=auto{1}` - Don't divide the specified loops into smaller chunks (no loop tiling).\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` - Process the loop iterations sequentially for the outer loops.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}` - Enable parallel processing for the innermost loop of the first set with data reduction for `tmp`.\n- `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}` - Enable parallel processing for the innermost loop of the second set with data reduction for `D`.\n- `#pragma ACCEL PARALLEL FACTOR=auto{4}` - Enable parallel processing for the second loop of the second set with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a candidate for hardware acceleration, and within the function, manage the optimization of the loops as follows: \n\n- Deactivate automatic pipelining for the entire 'alpha*A*B' nested loop block but enable it by default for any inner loops unless specified otherwise.\n- Prevent loop tiling optimization (divide into smaller chunks) and parallelizing of the iterations across all loops by default.\n- In the loop calculating 'tmp', allow up to 32 parallel operations specifically for the reduction operation on 'tmp'.\n- When updating matrix 'D', explicitly set the tile factor to 2 for the outer loop, enable 10-fold parallel processing in the 'D *= beta' loop, and allow a maximum of 50 simultaneous operations for the reduction operation on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function `kernel_2mm` as a hardware acceleration kernel. For each loop processing `tmp[i][j]` and `D[i][j]`, do not use automatic pipelining, do not tile the loops, and do not parallelize them fully (essentially run serially). However, apply parallelization with automatic factor deduction for the innermost loops, ensuring that `tmp` and `D` values are accumulated correctly through reduction with their respective parallel factors being automatically determined, and the second occurrence of `#pragma ACCEL TILE FACTOR=auto{2}` should be applied to only one of the loops within the depicted code block, specifically the loop iterating over `j` when processing `D[i][j]`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware accelerator kernel, use automatic loop flattening and tiling optimizations, but ensure each loop iteration runs sequentially. In the first loop nest, disable automatic pipelining but still apply automatic parallelism and tiling optimizations with manual reduction for the 'tmp' array. In the second loop nest, turn off automatic pipelining, set tiling to the default behavior, parallelize the innermost loop with 16 instances, and apply manual reduction for the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following HLS directives to create an optimized FPGA accelerator for a 2-matrix multiplication operation, ensuring specific loop optimizations:\n\n1. Mark the containing function as a hardware acceleration kernel.\n2. In the first set of nested loops, disable automatic pipelining, ensure each loop iteration runs in full without tiling and processes iterations sequentially.\n3. In the innermost loop of the first set, enable reduction on the `tmp` array and automatically determine the parallelization factor, but ensure at least 70 iterations can be processed in parallel.\n4. In the second set of nested loops, allow automatic pipelining, process the outer loop in its full form without tiling, and ensure the outer loop's iterations execute sequentially.\n5. For the inner loop of the second set, enable pipelining without specifying a preference and set a tiling factor that processes at least two iterations at once, as well as automatically determine a parallelization factor to at least 16.\n6. In the innermost loop of the second set, enable reduction on the `D` array with an automatic parallelization factor ensuring at least 10 iterations can be processed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code is a set of functions implementing the AES-256 (Advanced Encryption Standard) encryption algorithm and the main function called `aes256_encrypt_ecb`.\n\nThe `aes256_encrypt_ecb` function takes a pointer to `aes256_context` structure and two arrays of `unsigned char` as arguments - the 256-bit key and a 16-byte block of plaintext to be encrypted. The function first initializes the encryption and decryption keys with the user-provided key `k` in a for-loop (described as `ecb1` label). Then it calculates the decryption key by running `aes_expandEncKey_1` function eight times with the `rcon` value (described as `ecb2` label). \n\nThen the function engages in a 14-round encryption process (described as `ecb3` label) which is typical for AES-256. Each round consists of four steps: `SubBytes`, `ShiftRows`, `MixColumns` and `AddRoundKey` with `aes_expandEncKey_1` function called on every even iteration. \n\nBetween iterations, the HLS pragma for PIPELINE is set to `off`, to ensure that each iteration is completed before starting the next, which prevents parallel execution of loop iterations and the TILE FACTOR is set to `2`, implying that the iterations are split into groups of 2 to be processed together. \n\nFinally, `SubBytes`, `ShiftRows`, `AddRoundKey` steps are performed once more, and the encrypted text remains in the `buf` array.\n\nOverall, the code is optimized for hardware acceleration using HLS pragmas to guide the conversion process from the C code to hardware instructions. For example, the `#pragma ACCEL kernel` directive indicates to tool that the `aes256_encrypt_ecb` should be implemented as a standalone hardware module in the resulting design, while `#pragma ACCEL PIPELINE` earnestly informs the tool to optimize this loop by implementing it in a pipelined manner.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. For all loops, prevent automatic pipelining, ensure that iterations are not tiled into smaller chunks and that the loop iterations are executed sequentially. However, in the first inner loop, allow full parallel execution for computing the `tmp` matrix with a complete reduction applied across all iterations. In the second inner loop, enable automatic pipelining, apply the parallel directive with a factor of 16 for the outer loop and 32 for the reduction computation on `D`, tiling the inner loop with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and within it, execute the main loops without any pipeline optimization, using their original, unaltered iteration order and preventing parallel execution of iterations. Specifically, disable automatic pipelining, use a tiling factor of 1 to avoid loop tiling, and set the parallel factor to 1 to maintain serial iteration within the top two `for` loops. For the innermost loops that accumulate values into `tmp` and `D`, allow loop iterations to be parallelized with a defined parallel factor and apply reduction to handle dependencies on the loop updates, adjusting the parallelization factor based on loop length. Use the default pipelining for the middle set of loops, but for the loop updating `D`, utilize a tile factor of 2 and a parallel factor of 16 to enhance parallel execution, adjusting the innermost loop parallel factor to 10.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS code that encrypts a given 256-bit AES key, the code does the following:\n\n- Define a unsigned char array `sbox[256]` with provided values.\n- For the `aes256_context` structure, it describes a key which consists of the key itself along with the encryption and decryption keys. \n\n- The function `aes_expandEncKey_1` takes an encryption key `k` and `rc`. It first updates the first four bytes of the key using the sbox and rc. It then updates the key for the next 28 bytes. \n\n- The `aes_addRoundKey_cpy_1` function is used to add the round key and copies it to cpk.\n\n- The function `aes_subBytes_1` replaces all bytes in the buffer with corresponding values from sbox.\n\n- The `aes_shiftRows_1` function performs the ShiftRows step of the AES algorithm. It permutes fthe block using a shift pattern.\n\n- The `aes_mixColumns_1` function performs the MixColumns step of the AES algorithm. It takes as arguments a buffer and modifies it in place. In the loop, it applies the MixColumns transformation to each of the columns in the buffer.\n\n- The `rj_xtime_1` function prepares the input for the multiplication in the MixColumns step. \n\n- The function `aes_addRoundKey_1` performs the AddRoundKey step of the AES algorithm. It takes as arguments a buffer and a round key and modifies the buffer in place.\n\n- Finally `aes256_encrypt_ecb` function encrypts the text using AES-256 ECB mode. It performs the key expansion, applies the initial round key, then applies all round functions except AddRoundKey. This process is pipelined using two pragma commands in a nested way.\n\n- There is a flatten pragma to flatten the hierarchy if needed, and a TILE pragma to tile the loop with a factor of 2 to increase the reuse of locality. The loop iterates through the 14 rounds of the AES-256 algorithm performing the operations of SubBytes, ShiftRows, MixColumns and AddRoundKey as previously described.\n\n- This last function is to be treated as a hardware kernel for FPGA acceleration as indicated by the pragma ACCEL kernel command.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Disable automatic pipelining, maintain original loop structure without tiling, execute loop iterations sequentially, treat the function as a kernel for hardware acceleration, allow loop flattening for the innermost loop with dependencies, use sequential execution within the reduction loop, apply automatic pipelining without specifying off for the second loop nest, and execute the second set of nested loops with a parallel factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare a hardware kernel function named 'kernel_2mm' that computes a matrix multiplication and addition operation with specific optimization directives for loops:\n- Disable automatic pipelining for the entire function.\n- For each loop nest:\n  - Maintain the original loop structure without tiling or parallelization, except for the second inner loop (j loop) of the first nest where you should enable full flattening pipelining without automatic loop chunking.\n  - In the second nest, allow automatic pipelining in the i loop and set a factor of 2 for parallelizing the j loop.\n- Ensure the reduction operations on the 'tmp' and 'D' arrays do not automatically use parallelism, and maintain the original order of execution for each reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and for its computational loops, apply the following High-Level Synthesis (HLS) directives using natural language instructions:\n\n1. For the entire function, do not automatically insert pipelining optimizations.\n2. For the outer two loops, proceed without altering the original loop iteration structure.\n3. For the innermost loops, explicitly specify only certain optimizations:\n   - For the first set of inner loops: Do not parallelize the outer loop iterations and allow all the first loop\u2019s iterations to complete fully before the next begins, but flatten nested loops within it into a single loop where possible. For the reduction operation on the `tmp` array, use the normal associative and commutative properties to allow reordering but without parallel loop execution.\n   - For the second set of inner loops: Allow the outer loop to be parallelized by splitting it into 5 concurrent threads, flatten the nested loops when possible, and apply basic reduction properties similar to the first set for the `D` array, again without parallel execution for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The instruction to generate the given HLS codes in C is as follows: \n\n1. Firstly, create an array of unsigned characters, `sbox[256]`. Declare a structure `aes256_context` with three 32 bytes arrays named `key`, `enckey` and `deckey`.\n2. Next, code various functions including `aes_expandEncKey_1`, `aes_addRoundKey_cpy_1`, `aes_subBytes_1`, `aes_shiftRows_1`, `rj_xtime_1` and `aes_mixColumns_1` to perform initialisation and encoding/decoding operations.\n3. Under `aes256_encrypt_ecb`, initialize `rcon` and variable `i`, use a for loop to assign inputs from `k[32]` to `enckey[32]` and `deckey[32]` in the `aes256_context` structure, and use the `aes_expandEncKey_1` function eight times for deckey[]. \n4. Apply the `aes_addRoundKey_cpy_1` function to `buf[16]` and `enckey[32]` to update the `key[32]`. \n5. Now, loop over 14 iterations with a #pragma ACCEL PIPELINE 'off', and a #pragma ACCEL TILE with `FACTOR=auto{13}`, applying `aes_subBytes_1`, `aes_shiftRows_1`, `aes_mixColumns_1`, and `aes_addRoundKey_1` functions inside the loop. \n6. Finally, outside the loop, add `aes_subBytes_1`, `aes_shiftRows_1`, `aes_expandEncKey_1`, and `aes_addRoundKey_1` functions again to finalize the encryption process.\n7. Keep all of this under pragma command `#pragma ACCEL kernel` to enable hardware acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code is for an AES-256 encryption operation in ECB mode on an array of 16 bytes. \n\nIt starts by defining an S-box and AES-256 encryption context, as well as providing function definitions for several AES operations. \n\nThe main function is `aes256_encrypt_ecb` - it takes a pointer to a the AES-256 encryption context and two arrays of unsigned characters (`k` and `buf`). \n\nFirst the program initializes the encryption and decryption keys from the provided key. It then follows the standard AES encryption process: \n\n1. Preparing the round keys with the `aes_expandEncKey_1` function: and\n2. Managing the main loop of the AES encryption process: AddRoundKey, SubBytes, ShiftRows, MixColumns (depending on the round).\n\nSpecifically, \n- `#pragma ACCEL PIPELINE auto{flatten}` is used to enable the automatic pipeline optimization on the loop inside the block 'ecb2' and flatten the nested loops if there are any. \n- `#pragma ACCEL TILE FACTOR=auto{13}` is applied to the loop inside the block 'ecb3', divides loop iterations into smaller pieces (tiles) of size 13 to optimize for parallelism and data locality.\n- `#pragma ACCEL kernel` is used to inform the HLS compiler that the function `aes256_encrypt_ecb` should be synthesized into a hardware accelerator.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The code contains a series of functions for performing encryption using the AES256 algorithm in the electronic codebook (ECB) mode. Here are the respective descriptions for the HLS pragmas:\n\n1. `#pragma ACCEL kernel`: This pragma is applied to the `aes256_encrypt_ecb` function and it indicates that the function should be treated as a hardware accelerator kernel.\n\n2. `#pragma ACCEL PIPELINE auto{flatten}`: This pragma is applied to the loop labelled `ecb2` and it enables loop pipelining, allowing operations in different iterations of the loop to be executed in parallel to improve throughput. The 'flatten' option suggests the tool to flatten and pipeline the nested loops if possible.\n\n3. `#pragma ACCEL PIPELINE auto{}`: This pragma is applied before a loop labelled `ecb3`, in similar fashion, it implies to leverage loop pipelining. However, there's no specific option provided, hinting the tool to make the optimal choice.\n\n4. `#pragma ACCEL TILE FACTOR=auto{4}`: This pragma is applied to the loop labelled `ecb3`. The tile pragma breaks the iterations of the loop into groups or 'tiles' of size determined by the factor for possible parallel execution. The 'auto{4}' suggests the tool to determine the best factor but no more than 4.\n\nNotice that there is an array `sbox` at the beginning which is the substitution table used in the encryption, and a `aes256_context` structure that holds the 256-bit keys. The function `aes256_encrypt_ecb` takes this structure and a data buffer as arguments to perform AES encryption. On input, `ctx` should contain the encryption keys and `buf` contains the plain text, and after execution, `buf` will contain the cipher text.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Utilize the function `kernel_2mm` as a dedicated hardware acceleration kernel, and within this function, follow these instructions:\n\n1. For the first major loop block, execute without applying automatic loop pipelining and prevent the loop from being divided into smaller tiles. Additionally, ensure the loop iterations run sequentially without concurrency.\n2. In the nested inner loop within this block, enable the loop flattening feature of pipelining to optimize the loop nest structure for parallel execution. Keep the loop from being divided into smaller tiles but permit two-way parallel execution of iterations.\n3. Apply a reduction pragma with a parallelism factor of one for the innermost loop to compute matrix multiplication efficiently while accumulating results into the `tmp` matrix.\n4. For the second major loop block, allow the compiler's default settings for pipelining to be used and maintain the full loop form without tiling or concurrency for iterations.\n5. In the nested inner loop of this block, once again enable loop flattening for pipelining, prevent loop tiling, and set a parallel execution factor of five.\n6. Apply another reduction pragma with a parallelism factor of one for the innermost loop to update the `D` matrix with results from the multiplication of the `tmp` matrix and matrix `C`.\n\nTranslate these instructions into respective #pragma directives as described by your given code example.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware acceleration kernel. In the core algorithm, the first 'for' loop iterating over 'i' should not be automatically pipelined, nor should it be automatically parallelized or tiled. Within this loop, enable automatic pipelining for the second 'for' loop iterating over 'j', but do not automatically tile or parallelize it; however, do enable parallelism with a factor of 2 for the innermost 'for' loop iterating over 'k', specifically allowing the sum operation on 'tmp' to be performed in parallel. In the subsequent algorithm section performing the multiplication and addition with the 'D' matrix, disable automatic pipelining, tiling, and parallelism for the loop over 'i', but within this loop, set a parallelism factor of 4 for the nested 'for' loop iterating over 'j' and allow the reduction on 'D' to be parallelized with a factor of 2 in the innermost 'for' loop over 'k'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given HLS code written in C into natural language instructions for generating its respective pragmas:\n\n1. Designate the entire `kernel_adi` function as a hardware acceleration target.\n2. For the outermost loop `t`, don't explicitly enable or enforce any loop pipelining optimizations.\n3. Don't split the `t` loop into smaller parallel loop iterations; treat it as a single unit without additional parallelism.\n4. Inside the first nested loop block labeled \"Column Sweep\":\n   - Flatten the subsequent loops into a single loop level for pipelined execution.\n   - Avoid splitting the immediate nested `i` loop into smaller chunks or parallel iterations.\n   - Ensure that the innermost `j` loop doesn't run parallel iterations; execute each iteration sequentially.\n5. For the second nested loop block labeled \"Row Sweep\":\n   - Do not apply pipelining optimization to the following loop structure.\n   - Keep the tile size unaltered for the immediate nested `i` loop.\n   - Configure a specific parallel execution factor of 58 for the outer `i` loop within the \"Row Sweep\" section.\n   - The innermost `j` loop running in reverse order should also run its iterations sequentially.\n\nThe code comments and pragmas together provide these insights into how the HLS code should be generated and how the loops are to be handled during the synthesis process.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the function `kernel_2mm` into a hardware kernel for FPGA acceleration without automatically pipelining the main computations. Ensure that the loop iterations within the function are executed sequentially without being broken into smaller chunks or parallelized. However, for the innermost loops, permit a flattened pipeline structure in the first occurrence where we compute the `tmp` array, and in the second occurrence, where we multiply the temporary array with `C` and add it to `D`, allow only the 'k' loop to be parallelized with a factor higher than the 'j' loop by a magnitude of 5 times.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware acceleration kernel, perform matrix multiplication and matrix addition in two separate loops with optimization constraints as follows:\n\n1. In both outer loops, prevent automatic pipelining, use each loop in its entirety without tiling, and execute iterations sequentially.\n2. For the first innermost loop, perform a parallel reduction on the 'tmp' matrix with full parallelization across the complete range of index 'k'.\n3. For the second innermost loop, disable pipelining, set the tiling factor to two, and parallelize the loop with a factor of sixteen; then perform a parallel reduction on the 'D' matrix with a factor of eight for the innermost loop iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following HLS C code into natural language instructions that indicate how each loop should be optimized, including specific pragma settings:\n\n1. Identify the function 'kernel_2mm' as a hardware acceleration kernel.\n2. Do not automatically optimize the first pair of nested loops; ensure they execute sequentially without HLS-initiated pipelining, tiling, or parallel optimizations.\n3. Within the innermost loop of the first pair of nested loops, enable parallel execution with a custom optimization setting that allows up to 32 simultaneous operations specifically for the 'tmp' array reduction process.\n4. Automatically pipeline the second pair of nested loops after the first pair, without specifying the mode.\n5. Fully unroll the j-loop within the second pair of nested loops, flatten any nested loop structures, and set a tiling factor of 2 for improved data locality.\n6. For the innermost loop of the second pair, which involves updating 'D', limit parallelism to iterate in a strict sequential order, with no simultaneous operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS code for a 2mm kernel function by doing the following:\n\n1. Designate the function as a hardware kernel for acceleration.\n2. In the first major loop over 'i', fully process this loop without pipelining but still allow the compiler to consider automatic optimizations if any exist.\n3. Within the first major loop over 'i', when processing the inner loop over 'j':\n   - Permit automatic loop pipelining.\n   - Process the inner loop in its full form without tiling, but allowing compiler's automatic optimizations.\n   - For the innermost loop over 'k', enable parallel execution with a factor that the compiler will determine automatically, and optimize reductions on the 'tmp' array with a goal of performing 32 operations in parallel.\n4. For the second major loop over 'i' processing the matrix 'D':\n   - Turn off loop pipelining but allow the compiler to apply any other beneficial optimizations.\n   - Tile the inner loop over 'j' with a factor of 8, with the compiler deciding on any further optimization possibilities.\n   - Allow for parallel execution in the inner loop over 'j' with a factor the compiler will determine, aiming for 4 operations in parallel.\n   - For the innermost loop over 'k', optimize for reductions on matrix 'D' with the compiler aiming for 16 operations in parallel.\n\nSpecific pragmas to be used:\n- Treat the function as a hardware acceleration kernel: `#pragma ACCEL kernel`\n- Do not pipeline the first 'i' loop, but allow for automatic optimizations: `#pragma ACCEL PIPELINE auto{off}`\n- Process the 'j' loop without tiling but with automatic optimizations: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run iterations of the 'k' loop within 'j' in parallel with compiler-determined factor, optimizing for 'tmp' reductions: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}`\n- Do not pipeline the second 'i' loop, but allow for automatic optimizations: `#pragma ACCEL PIPELINE auto{off}`\n- Tile the 'j' loop within the second 'i' loop with a factor of 8: `#pragma ACCEL TILE FACTOR=auto{8}`\n- Run the iterations of the 'j' loop in parallel within the second 'i' loop with a compiler-determined factor: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n- Optimize reductions on 'D' in the innermost 'k' loop in the second 'i' loop with a factor aimed for 16 operations in parallel: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a function 'aes256_encrypt_ecb' to implement AES-256 encryption in ECB mode, with a structure 'aes256_context' and two arrays 'k' and 'buf' as inputs. The structure 'aes256_context' contains three elements, 'key', 'enckey' and 'deckey', which are all 32-byte keys. The 'k' is the 32-byte initial key and 'buf' is the 16-byte buffer for plaintext and cipher text.\n\nThere are two main parts of this function, key expansion and ciphering procedures.\n\nIn key expansion, set 'rcon' to 1 and copy input key 'k' to 'enckey' and 'deckey' in \"aes256_context\". Then call the function 'aes_expandEncKey_1' for 7 times to expand the decryption key \"deckey\".\n\nIn ciphering, firstly, copy 16 bytes from 'enckey' to 'key' and xor them to 16 bytes in 'buf'. Then perform the following operations for 13 times: substitute bytes of cipher text with 'aes_subBytes_1', shift rows of cipher text with 'aes_shiftRows_1', and do the mixcolumn operation with 'aes_mixColumns_1'. Add round key to cipher text by calling 'aes_addRoundKey_1'. If the round number is odd, add keys that in the range [16,31] in 'key', else expand 'key' and add keys that in the range [0,15]. After the loop, do byte substitution, row shifting, key expanding and adding in sequence to get the final cipher text.\n\nAnd use pragmas to guide the synthesis tool for optimizing:\n\n- Utilize both 'auto' FLATTEN and PIPELINE pragmas to unroll the nested for loops and pipeline them for parallel processing to enhance performance;\n- Specify a TILE FACTOR with 'auto' to the loop, indicating to HLS tools to break the loop into smaller chunks of iterations and process them in parallel.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a kernel function implementing an Alternating Direction Implicit (ADI) solver for use in hardware acceleration. The function should be optimized for a single large FPGA rather than a GPU or multicore processor. The solver updates a fixed-size grid for a predetermined number of time steps using two sweeps, column and row, and intermediate arrays p and q for factorizing tridiagonal systems of equations. \n\nIn the function, define pragma directives to disable automatic pipeline optimization, retain the full structure of loops without tiling, and honor the sequential order of operations where necessary. For two nested loops, configure parallelization to process two iterations concurrently for intermediate array computations. For the final reversed iterations in both column and row sweeps, set parallel factors based on the number of loop iterations, ensuring each directive reflects the appropriate loop context.\n\nSome initial calculations outside the loops should use constant scalars for the grid size and timestep parameters. Inside the loops, initialize boundary conditions with constants, and include standard iterative solver computations for the p and q arrays with appropriate arithmetic operations. Reverse the direction of the final loop in each sweep while ensuring the final calculation respects data dependencies.\n\nUse pragmas similar to `#pragma ACCEL PARALLEL FACTOR=auto{<factor>}` to set the parallelization factor, `#pragma ACCEL TILE FACTOR=auto{<factor>}` for the tiling, and `#pragma ACCEL PIPELINE auto{<mode>}` to control pipelining, replacing `<factor>` and `<mode>` accordingly. Specifically, employ a parallel factor of 2 for one of the inner loops and utilize the number of loop iterations minus one as the parallel factor for the final backward-traversing loops in both the column and row sweeps. All other loops should inhibit pipelining, tiling, and parallel processing optimizations using factors of '1' or the 'off' keyword. Ensure the surrounding function is designated for hardware acceleration with the appropriate kernel pragma.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. For the first outermost loop, avoid automatic pipelining, tiling, and parallelization, ensuring it runs in its original, sequential form. Within the nested loop initializing `tmp`, allow the compiler to decide how to flatten the loop for pipelining. For innermost loops, specifically avoid parallel optimization or tiling to ensure reductions over `tmp` occur serially. In the second set of loops updating `D`, permit automatic pipelining in the outer loop while constraining the level of parallel processing in the inner loop to two-way parallelism and allowing a more aggressive 16-way parallel reduction for the innermost loop with appropriate accumulation in `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please compile the following HLS C code for a kernel function `kernel_2mm` that computes the matrix multiplication and addition expression `D := alpha*A*B + beta*D`. For the first loop nest performing `tmp := alpha*A*B`, disable automatic pipelining, and ensure that the outermost loops run sequentially without tiling or parallelization. For intermediate tmp array calculation, flatten the pipeline of the j-loop and disable parallelization and tiling. Also, ensure that the reduction on `tmp` array is performed non-parallelly. For the second loop nest that computes `D := tmp*C + beta*D`, enable default pipelining and run the i-loop without parallelization or tiling, and for the j-loop, utilize a tiling factor and parallelization factor of 2, while keeping the reduction on the `D` array sequentially. Use the appropriate #pragma directives to implement the above specifications, setting the 'auto' values to the required settings for each #pragma statement, and treating the function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This is an AES-256 encryption code in C language in which:\n\n1. 'aes_expandEncKey_1()' function will expand the encryption key.\n2. 'aes_addRoundKey_cpy_1()' function will XOR the round key with the state data and copy 32 bytes into the buffer.\n3. 'aes_subBytes_1()' function will perform byte substitution on the state data.\n4. 'aes_shiftRows_1()' function will perform a row shift on the state data.\n5. 'rj_xtime_1()' function will calculate xtime which is the product of {02} and the argument (modulo another value such that the result fits into a byte).\n6. 'aes_mixColumns_1()' function will mix the 4 bytes of each column in a state together.\n7. 'aes_addRoundKey_1()' function will XOR the round key to the state data.\n\nIn the 'aes256_encrypt_ecb()' function, it is tagged as the 'ACCEL kernel' using '#pragma ACCEL kernel' to be treated as a hardware kernel for acceleration. A loop that calls 'aes_expandEncKey_1()' function is pipelined with the compiler directive '#pragma ACCEL PIPELINE auto{flatten}'. Another loop that calls 'aes_subBytes_1()', 'aes_shiftRows_1()', 'aes_mixColumns_1()', and 'aes_addRoundKey_1()' functions is pipelined and tiled with the compiler directive '#pragma ACCEL PIPELINE auto{flatten}' and '#pragma ACCEL TILE FACTOR=auto{1}', respectively. These loops are set not to be partitioned into multiple loops to ensure the execution order,\nmeaning that they will be processed in their original, full form without breaking them into smaller chunks.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware-accelerated kernel, ensure that the outer loops execute sequentially without automatic pipelining or tiling optimization, but apply parallelism and pipelining with automated parameters to the inner loops as follows:\n\n- For the first pair of nested loops, disable pipelining and tiling optimizations for the outer two loops, but enable automatic parallel reduction for the innermost loop with a full factor on the 'tmp' array calculation.\n  \n- For the second pair of nested loops, enable automatic pipelining for the outer two loops and use tiling for the second loop with a factor of 4, and parallelize the innermost loop with a partial factor for computing the update on array 'D'.\n\nHere's how to articulate the corresponding `#pragma` directives:\n\n1. Treat 'kernel_2mm' as an acceleration hardware kernel.\n2. For the first loop nest:\n    - Turn off automatic pipelining for the 'i' and 'j' loops.\n    - Disable loop tiling with a tile factor of 1 for the 'i' and 'j' loops.\n    - Ensure the 'k' loop runs in parallel, with a full factor, to perform reduction on 'tmp'.\n3. For the second loop nest:\n    - Allow automatic determination of pipeline settings for the 'i' and 'j' loops.\n    - Set the tile factor to 4 for the loop over 'j'.\n    - Set a partial parallel factor for the 'k' loop, and enable automatic parallel reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function for hardware acceleration without enabling loop pipelining, tiling, or parallelism by default, while selectively applying loop optimizations for specific loops and maintaining original loop bounds. Specifically, do not pipeline the outermost loops, keep their iterations sequential, and disable loop flattening; however, flatten the first inner loop and adjust parallelism per loop for optimal resource utilization:\n1. For the first inner loop, do not apply any optimizations.\n2. For the second inner loop, allow parallel execution with no reduction factor applied.\n3. For the third innermost loop, enable parallel execution with a moderate factor.\n4. Finally, for the last inner loop, enable a high degree of parallelization to accelerate reduction and maximize throughput.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with the respective pragmas, you can instruct as follows:\n\n\"Define a hardware acceleration kernel function named `kernel_2mm` that takes matrix dimensions and matrix data as inputs and computes a matrix equation `D := alpha*A*B*C + beta*D`. For the outermost loops, disable automatic pipelining, do not tile them, and run their iterations sequentially without parallelism. When processing the first pair of nested loops that initializes the `tmp` matrix and computes `tmp[i][j] += alpha * A[i][k] * B[k][j]`, flatten the pipeline of the second loop and avoid parallel execution or tiling across all these loops, but perform a parallel reduction on the innermost loop. Similarly, for the second pair of nested loops that update matrix `D`, keep the pipeline flattened for the second loop and avoid tiling, but set the parallel factor to 2 for processing the iterations in pairs and perform a parallel reduction on the innermost loop. Ensure to code these pragma directives explicitly as shown in the code with the specified 'auto{off}', 'auto{1}', and 'auto{flatten}' values.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the 2mm algorithm into an FPGA-accelerated kernel function by disabling automatic pipelining for the outer loops but enabling it for the inner matrix multiplication loops, sequentially execute outer loops, tile the loops without splitting them, and assign parallel execution within reduction loops with automatic factor selection, specifically utilizing all available resources for the first reduction and splitting the second reduction loop into two parallel threads.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C code defines an encryption method using the AES-256 encryption standard in the electronic codebook (ECB) mode. It requires implementing several smaller functions to perform the steps of the AES encryption. \n\n1. `aes_expandEncKey_1`: It is used to expand the encryption key. Use natural language \"Expand the encryption key, shifting it and applying a substitution using predefined s-box (Substitution box)\". Do not automatically pipeline this loop, i.e., `#pragma ACCEL PIPELINE auto{off}`.\n\n2. `aes_addRoundKey_cpy_1`: Copies the round key to an array and applies it to the input buffer by XORing. It can be referred to as \"Apply round key to the input buffer, while copying it to the appropriate memory location\".\n\n3. `aes_subBytes_1`: It applies a non-linear substitution to all bytes of the input buffer using a predefined s-box. Use the term \"Apply non-linear substitution to all bytes of the input data using a pre-defined s-box\".\n\n4. `aes_shiftRows_1`: It cyclically shifts (permutes) the rows of the input buffer. Consider using \"Cyclically shift the rows of the input data for the 'ShiftRows' step of AES\".\n\n5. `rj_xtime_1`: It is used to multiply an input byte by 2 in the finite field used by AES. Specify this as \"Multiply an input byte by 2 in the finite field used for AES\".\n\n6. `aes_mixColumns_1`: It mixes the data inside the columns of the input buffer. Mention this as \"Mix the data within each column for the 'MixColumns' step of AES\". \n\n7. `aes_addRoundKey_1`: It applies the round key to the input buffer by XORing. The description can be \"Apply round key to the input data for the 'AddRoundKey' step of AES\".\n\nFinally, the main method `aes256_encrypt_ecb` takes care of the AES-256 encryption process. The natural language description can be \"Conduct multiple rounds of the AES encryption process: key expansion, byte substitution, row shifting, column mixing, and applying the round key. Iterate this process up to 14 times depending on the conditions. This process is handled under kernel mode, meaning it is offloaded onto the FPGA for acceleration\". The loop that goes through each encryption round uses pipelining with `#pragma ACCEL PIPELINE auto{off}` and has a tiling factor of `1` with `#pragma ACCEL TILE FACTOR=auto{1}`.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_2mm' as a hardware acceleration kernel, within which disable automatic pipelining and prevent loop tiling and parallelism for the outermost loops over the dimensions 40 and 50, without any pragmas for the loop over dimension 70. Do not employ any loop optimizations for the first matrix multiplication operation but allow maximal pragmas for the second matrix multiplication operation with tiling by a factor of 8 and parallel processing by a factor of 2 for the loop over dimension 80 and parallel processing with a reduction on 'D' by a factor of 16 for the loop over dimension 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function that treats the entire function as a hardware acceleration kernel, does not pipeline the outer loops, processes the two outer loops without tiling or parallelization, pipelines the first inner loop without specifying the pipeline factor and the second inner loop with flattening optimization, performs the second level of inner loops with a parallel factor of 70 and 1 respectively for each set of nested loops, and uses reduction on temporary arrays 'tmp' and 'D' with the respective parallelization.\n\nUse the following #pragma directives:\n- To designate the function as a hardware acceleration kernel, use `#pragma ACCEL kernel`.\n- To disable automatic pipelining on the first level of loops, use `#pragma ACCEL PIPELINE auto{off}`.\n- To avoid tiling of the first outer loops, use `#pragma ACCEL TILE FACTOR=auto{1}`.\n- To avoid parallel execution of the first outer loops, use `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the inner loop where tmp is updated, use `#pragma ACCEL PIPELINE auto{}` without specifying the factor for automatic determination, `#pragma ACCEL TILE FACTOR=auto{1}` to prevent tiling, and `#pragma ACCEL PARALLEL FACTOR=auto{2}` to specify a parallel factor of 2.\n- For the innermost loop of tmp, enable parallel reduction with `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{70}` to specify a parallel factor of 70.\n- For the inner loop where D is updated, use `#pragma ACCEL PIPELINE auto{flatten}` to flatten nested loops, `#pragma ACCEL TILE FACTOR=auto{1}` to prevent tiling, and `#pragma ACCEL PARALLEL FACTOR=auto{4}` to specify a parallel factor of 4.\n- For the innermost loop of D, use `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{1}` to specify a parallel factor of 1 for the reduction on D.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function by specifying the following directives:\n\n1. Declare the function as a hardware acceleration kernel.\n2. For the initial nested loop where 'tmp' matrix computation occurs:\n   - Disable automatic pipelining for the outermost loop ('i').\n   - Disable loop tiling for the 'i' loop by setting the factor to the smallest possible unit.\n   - Disable parallel execution of the 'i' loop by setting the parallel factor to sequential execution.\n   - Flatten the pipeline for the second loop ('j') to merge it into a single stage.\n   - Disable loop tiling for the 'j' loop by setting the factor to the smallest possible unit.\n   - Parallelize the 'k' loop but without any parallel factor; perform reduction on the 'tmp' array.\n3. For the second nested loop where the 'D' matrix is updated:\n   - Disable automatic pipelining for the 'i' loop.\n   - Tiling the 'i' loop with a factor of 4.\n   - Set the parallel factor to 2 for the 'i' loop to allow some parallel execution.\n   - Pipeline the 'j' loop with the automatic configuration turned off.\n   - Set an aggressive parallel factor of 50 for the 'k' loop performing reduction on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel and apply the following directives: disable automatic pipelining for both outer loops, preserve the full loop structure without tiling for all loops, execute all loop iterations sequentially without parallelization, but for the nested loops inside each outer loop, enable pipelining with the default behavior, set tiling to the smallest factor without partitioning the loops, and for the first nested inner loop, maintain sequential execution while for the second one, allow a limited degree of parallel execution with a parallel factor that allows pairing of loop iterations. Additionally, manage temporary data dependencies within each inner-most loop through parallel reductions, first with no explicit factor and then with a higher factor for the subsequent loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Mark the entire function `kernel_2mm` as a hardware acceleration kernel. For both its outer loops and their immediate inner loops, disable automatic pipelining, prevent the loops from being tiled or divided into smaller chunks, and set their iterations to execute sequentially. Within the first nested inner loop (computing `tmp`), allow all iterations to run in parallel with a full reduction across the `k` dimension, and within the second nested inner loop (computing `D`), allow two iterations to run in parallel with a full reduction across the `k` dimension.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, use the following comprehensive instruction:\n\nCreate a hardware acceleration kernel for a matrix multiplication and addition algorithm with two main parts, ensuring the first part (computation of `tmp = alpha * A * B`) does not utilize pipelining, tiling, or parallelism pragmas beyond their minimum values. For the second part (computation of `D = beta * D + tmp * C`), disable pipelining pragmas for the outer loops but allow for a parallel reduction with a factor of 4 on the innermost loop, while keeping minimum values for tiling and parallel factors in other loops. Specifically, set '#pragma ACCEL PIPELINE \"off\"' for all outer loops to turn off automatic pipelining, use '#pragma ACCEL TILE FACTOR=1' to keep loops in their full form without partitioning into smaller blocks, set '#pragma ACCEL PARALLEL FACTOR=1' to run iterations sequentially in outer loops, and only in the innermost loop of the second part, use '#pragma ACCEL PARALLEL FACTOR=4' to allow a degree of parallel computation. Also, employ '#pragma ACCEL PIPELINE \"flatten\"' once within the first loop nest to compress nested loops into a single pipeline stage.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function to perform matrix multiplication and addition with the following characteristics:\n- Declare the function as a hardware acceleration kernel.\n- Prevent automatic pipelining optimizations in the outermost loops.\n- Disable loop tiling optimizations by setting the tile factor to 1 in the outermost loops.\n- Ensure that iterations of the outermost loops run sequentially by setting the parallel factor to 1.\n- For the first nested loop, flatten the pipeline to improve performance without subdividing the loop.\n- Inside the innermost loop of the first nested loop, enable parallel reduction on 'tmp' array with the default parallel factor.\n- For the second nested loop, explicitly allow the pipelining optimization.\n- In the second nested loop, set the parallel factor to 2 to allow some degree of parallel execution.\n- Inside the innermost loop of the second nested loop, enable parallel reduction on 'D' array with an aggressive parallel factor of 25.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the provided HLS code annotations into natural language instructions as follows:\n\n1. Designate the enclosed function as one that should be optimized and implemented as a hardware acceleration module: `#pragma ACCEL kernel`.\n\n2. For the outermost loop, ensure it executes sequentially without pipelining: `#pragma ACCEL PIPELINE auto{off}`.\n\n3. Prevent subdivision of the outermost loop into smaller pieces: `#pragma ACCEL TILE FACTOR=auto{1}`.\n\n4. Ensure that all iterations within the outermost loop run in series, not concurrently: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n\n5. Within the nested loop for calculating `tmp`, streamline the loop iterations to execute in a single flow without loop nests: `#pragma ACCEL PIPELINE auto{flatten}`.\n\n6. Do not subdivide this nested loop (for `tmp`) into smaller portions: `#pragma ACCEL TILE FACTOR=auto{1}`.\n\n7. Again, ensure that iterations within this nested loop (for `tmp`) execute sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n\n8. In the innermost loop (for `tmp`), maintain sequential execution of reductions but do not use parallelism: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`.\n\n9. Execute the subsequent nested loop (for `D`) using default pipelining settings: `#pragma ACCEL PIPELINE auto{}`.\n\n10. Prevent subdivision of this subsequent loop (for `D`) into smaller segments: `#pragma ACCEL TILE FACTOR=auto{1}`.\n\n11. For the outer iteration of this loop (for `D`), enable parallelism with a specific factor: `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n\n12. For the corresponding innermost loop when updating `D`, ensure no parallelization in the reduction process: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function `kernel_2mm` as a hardware acceleration kernel while maintaining the original sequential processing order and preventing automatic optimization of the outer loops. Inside the outer loops, perform matrix multiplication without loop pipelining or tiling, but allow for some degree of parallel computation within the inner `k` loop for the first matrix multiplication, optimizing with a custom factor. For the second matrix multiplication, enable pipeline optimization for the middle loop `j`, adjust the tile factor for improved tiling optimization, and selectively enable some parallelism with a specified factor in the inner loop for a balanced combination of parallelism and reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel for a 2mm algorithm with specific loop optimizations using the following directives:\n\n1. Treat the `kernel_2mm` function as a hardware acceleration kernel.\n2. In the first pair of nested loops, completely disable automatic pipelining for the outer loop, maintain the loop in its whole form without tiling, and execute the iterations sequentially.\n3. Within the inner loop of the first pair, enable automatic pipelining, and also maintain the loop in its whole form and execute iterations sequentially. Additionally, enable parallel processing within the innermost loop with a factor that allows seven simultaneous operations to exploit data-level parallelism and use reduction to safely accumulate values into the `tmp` array.\n4. In the second pair of nested loops, disable automatic pipelining for the outer loop, maintain the loop without tiling, and execute iterations sequentially.\n5. Within the inner loop of the second pair, enable pipelining and flatten the loop completely for increased loop parallelism, set a tiling factor that allows the loop to be processed in chunks of eight, and configure the parallel execution of loop iterations with a factor that allows ten parallel operations.\n6. In the innermost loop, enable parallel processing but do not parallelize (use a factor of one) to ensure serial execution while using reduction to accumulate values safely into the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply the following HLS #pragmas to transform the provided C code into hardware description: treat the whole C function 'kernel_adi' as a dedicated hardware acceleration kernel, disable automatic pipelining for the outer 't' loop, enforce the use of the provided loop structure without allowing the tool to tile or break the 'i' and 'j' loops into smaller chunks, and control parallelism manually by setting specific factors for distinct inner loops within 'kernel_adi'. Specifically:\n\n1. Mark the function 'kernel_adi' for hardware acceleration.\n2. For the outermost loop 'for (t = 1; t <= 40; t++)', turn off the automatic pipelining.\n3. In both column and row sweeps stages, force the compiler to use the loops as written without any tiling optimizations.\n4. For each inner loop 'j', during column and row sweeps, maintain the original sequential execution by setting the parallel factor to 1.\n5. Enhance the row sweep inner loop 'j' with a parallel factor of 29 and the subsequent reversed loop 'j' with a parallel factor of 58.\n6. For the row sweep, further enhance a nested loop 'j' with a parallel factor of 2.\n\nTo ensure the directives are consistent with the code structure, apply #pragma ACCEL PIPELINE with \"off\" to disable pipelining where specified, #pragma ACCEL TILE FACTOR with \"1\" to avoid any loop tiling optimizations, and #pragma ACCEL PARALLEL FACTOR with the respective values of \"1\", \"29\", \"2\", and \"58\" to set the desired levels of loop parallelism in the appropriate sections.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a High-Level Synthesis (HLS) code to define a kernel called 'kernel_2mm' that takes as arguments the dimensions 'ni', 'nj', 'nk', 'nl', the scalars 'alpha' and 'beta', and the matrices 'tmp', 'A', 'B', 'C', and 'D'. In the kernel, disable loop pipelining and use the original loop structure by keeping the loops unaltered and running subsequent iterations serially for both the computation 'tmp = alpha * A * B' and the computation 'D = beta * D + tmp * C'. For the first computation of 'tmp', perform parallel accumulation on 'tmp' with maximum available resources. For the second computation of 'D', use a tile size of 8 for the 'j' loop and a parallelism factor of 2; and for the reduction on 'D', use as many resources as possible for parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a hardware kernel function named `kernel_2mm` which performs a matrix multiplication and addition operation with specific optimization constraints:\n\n- Identify the function as a hardware kernel for acceleration.\n- For the outer loops - `i` loops, do not apply automatic pipelining, tile with a factor of 1, and enforce serial execution for loop iterations.\n- Pipeline the first inner `j` loop automatically and similarly tile with a factor of 1 and enforce serial execution for its iterations.\n- Apply parallelism with reduction on `tmp` to the third loop `k` within the first inner `j` loop with full factors.\n- For the second set of `i` and `j` loops iterating over matrix `D`, turn off automatic pipelining for the `i` loop, use the default tiling factor of 1, and serial execution for loop iterations.\n- Flatten the pipelining of the second inner `j` loop and apply a tiling factor of 1.\n- Enforce a high degree of parallelism with a factor of 10 for the `j` loop and maintain serial execution for the innermost `k` loop with parallel reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel matrix-matrix operation with specific optimization directives. Disable automatic pipelining for the two outermost loops while allowing inner loops to be pipelined automatically. For tiling, keep the loops unaltered by setting factor to 1 for the first series of nested loops, and use a tiling factor of 8 for the second i-loop and its nested j-loop. Set parallel execution factor to 1 for the outer loops, but for the innermost loops set a custom factor with a parallel reduction on 'tmp' array with a factor of 14 and on the 'D' array with a factor of 10. Additionally, assign a parallel factor of 2 for the second level j-loop. Designate the whole function as a hardware accelerator kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Assign the following natural language instructions to generate the corresponding HLS C code with appropriate pragma statements:\n\n1. Identify the function 'kernel_2mm' as a target for hardware acceleration. Use '#pragma ACCEL kernel' to indicate this function should be compiled as an accelerator.\n\n2. For all outer loops within the 'kernel_2mm' function, enforce no automated pipelining. Use '#pragma ACCEL PIPELINE auto{off}' to ensure the loops run without any compiler-inserted pipelining optimizations.\n\n3. Ensure each loop is executed in its entirety without tiling. Apply '#pragma ACCEL TILE FACTOR=auto{1}' to preserve the original loop structure without any tiling optimization by the compiler.\n\n4. Execute all loop iterations sequentially rather than in parallel by using '#pragma ACCEL PARALLEL FACTOR=auto{1}' for the outer loops.\n\n5. For the inner loop calculating 'tmp', enable parallel loop execution with a custom parallel factor of 7 under the 'reduction' operation for the 'tmp' array by specifying '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}'.\n\n6. Meanwhile, within the second main loop processing 'D', explicitly allow the compiler to flatten the 'j' loop for further optimization by using '#pragma ACCEL PIPELINE auto{flatten}'.\n\n7. Additionally, allow a specific degree of tiling within this second loop for 'D' with a factor of 8 through '#pragma ACCEL TILE FACTOR=auto{8}'.\n\n8. Permit parallel execution of the 'j' loop in the second main loop with an aggressive parallel factor of 10 by employing '#pragma ACCEL PARALLEL FACTOR=auto{10}'.\n\n9. For the innermost loop updating 'D', disable parallelism by setting the parallel factor back to 1 using '#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}' to enforce sequential execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, ensuring that the first level loop is not pipelined, the loop body is not tiled, and iterations are not executed in parallel. Inside the first level loop, allow the inner loop to be fully flattened for pipelining, but without tiling or parallel execution. For the reduction operation in the deepest loop, execute iterations sequentially. In a subsequent loop block, prevent pipelining for the outer loop, maintain the original loop form without tiling for the inner loop, and process every second iteration in parallel. For the reduction operation in the deepest loop, allow sixteen iterations to be processed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The C code defines a 256-bit AES encryption algorithm. \n\n1. The process begins by creating an array sbox with 256 elements and a struct object 'aes256_context' that holds three unsigned char arrays with 32 elements each.\n2. Followed by a function 'aes_expandEncKey_1()' that takes in two parameters, an unsigned char array 'k' and an unsigned char pointer 'rc'. This function generates expanded encryption key that is required in the AES encryption. Pragma values are not used.\n3. Next, you want a function called 'aes_addRoundKey_cpy_1()' that applies bitwise XOR operation to each byte of the state with the round key (a part of the expanded key). Pragma directives are not used.\n4. Subsequently, construct a function 'aes_subBytes_1()', that is the first step in encryption round that replaces all values in the state from the sbox. No pragma directives are used.\n5. Create a function, 'aes_shiftRows_1()', which cyclically shifts the bytes in the state's rows. This step doesn't require any pragma directives.\n6. Followed by another function 'rj_xtime_1()' that does multiplication by x in GF(2^8). No pragma values are used here.\n7. Generate a function 'aes_mixColumns_1()', this represents matrix multiplication with a given matrix, in Galois Field GF(2^8). No pragma values are applied.\n8. You'd then construct a function 'aes_addRoundKey_1()' that adds (XORs) the round key to the state. Pragma values are not applied.\n9. Next, create the main function 'aes256_encrypt_ecb()' with two parameters, context of aes256 and data to be encrypted. This function involves different stages like key expansion, initial round, rounds which include subBytes, shiftRows, mixColumns, addRoundKey and final round of encryption. Here, we use '#pragma ACCEL kernel' that declares this function and its subfunctions for acceleration. Then, two nested loops are used. First loop 'ecb2' is set with '#pragma ACCEL PIPELINE' with a flatten option that allows sub loops to be pipelined as well. The next loop 'ecb3', for the rounds of the encryption, is set with '#pragma ACCEL PIPELINE' and 'TILE FACTOR=auto{13}', which means that multiple iterations should be run in parallel, and the choice of how many will be determined automatically for optimization at compile time.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the following function as a hardware kernel that calculates 'D' by multiplying 'alpha' with the product of matrices 'A' and 'B' and then with 'C', and then adding the result to 'beta' multiplied by 'D'. Disable automatic pipelining, tiling, and parallel processing for all loops except for the innermost loop in each double nested loop block; for these, apply a parallel reduction with a parallelization factor corresponding to the full dimension of the matrices being reduced (70 for the first innermost loop and 50 for the second one), and enable parallel processing with a factor of 2 for the second outer loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function for hardware acceleration without applying any automatic pipelining to its loops, and in each loop, maintain its full sequence without subdividing it or executing iterations in parallel. However, in the first inner loop, allow loop flattening for pipelining but don't subdivide or parallelize iterations. In the second inner loop, parallelize its iterations with a factor of two, and for the innermost loops, perform a parallel reduction on the designated accumulator arrays without parallelizing the iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following High-Level Synthesis (HLS) directives and settings into one comprehensive but concise natural language instruction:\n\n1. Treat the whole 'kernel_2mm' function as a dedicated hardware acceleration unit.\n2. For each loop within the function, ensure that the loop is executed in its serial, original form without pipelining optimizations, tiling transformations, or parallelization enhancements:\n    - Specifically, disable automatic pipelining.\n    - Prevent the loop from being broken down into smaller chunks (tiling) by maintaining the original loop factor.\n    - Ensure that each iteration of the loop runs sequentially, not concurrently, with any others.\n3. When performing the matrix multiplication and accumulation in the 'tmp' array, allow the innermost loop that computes the 'tmp' values to leverage parallel reduction with a factor that allocates a default number of resources, optimizing accumulation in hardware.\n4. Similarly, for updating the 'D' array, flatten the outer loop to run iterations serially, while allowing the innermost loop to be parallelized with a doubling of allocated resources compared to the default, except for when performing reduction on the 'D' array where no parallel resources are allocated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel and ensure that for certain loops: (1) you don't apply automatic pipelining, (2) you do not tile or divide the loops, and (3) you do not parallelize loop iterations automatically. Specifically, for the innermost loop that calculates the temporary matrix, apply parallelization with a factor that permits up to 14 simultaneous operations, using a reduction on the temporary matrix. Later, in the innermost loop for updating matrix D, set the tiling factor to allow processing 8 rows at a time and run iterations with a degree of parallelism that allows up to 2 simultaneous operations, using a reduction on matrix D with a parallelism factor allowing up to 10 simultaneous operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code written in C, use the following comprehensive, yet concise natural language instruction:\n\nAccelerate the function 'kernel_2mm' by preventing the outer two loops from being automatically pipelined and ensure they are executed sequentially, without tiling or parallel optimizations. Internally, apply parallelism manually when multiplying matrices. Specifically, in the first loop nest, the addition of alpha times the product of matrices A and B should be done in parallel for each element of `tmp` with a parallel factor of 70 for the innermost loop, indicating the available hardware resources. In the second loop nest, allow for parallel execution with a parallel factor of 2 for the outer loop while ensuring the computation of beta times D plus the product of `tmp` and matrix C elements is done in parallel for each element of `D` with a reduction pragma and a parallel factor of 50 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following natural language instruction:\n\nDesignate the `kernel_2mm` function as a hardware acceleration kernel, and for all loops within it, disable automatic pipelining, maintain the original loop bounds without tiling, and execute loop iterations sequentially without parallelization. However, for the nested loop processing elements of array `D`, selectively enable pipelining with the highest level of nested loop flattening and set the parallel factor to 20 for the innermost loop processing elements of `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function with specific optimization pragmas as follows:\n\n1. Designate the function `kernel_2mm` as a hardware acceleration kernel.\n2. For the outer loops `i` over 40 iterations and `j` over 50 iterations, ensure that:\n   - The loops should not be pipelined.\n   - The loops should be executed in a tiled fashion with a factor of 1, preserving their original full structure.\n   - The iterations of the loops should run sequentially, one after the other, without any parallel execution.\n3. For the inner loop `k` inside the first `i-j` loop nest, flatten and pipeline the loop to allow for potential optimization by the compiler, but do not explicitly set a pipelining factor.\n   - Additionally, process the `k` loop such that its iterations should also run sequentially, and any parallel operations reducing the `tmp` array should have a factor of 1, ensuring no simultaneous updates to the same element.\n4. For the second `i-j` loop nest, where `i` iterates over 40 and `j` over 80:\n   - Similarly to the first loop nest, the `i` loop should not be pipelined, and the `j` loop should have iterations running sequentially with a parallel factor of 1. Tiling should still enforce a factor of 1 to maintain the loop's original structure.\n   - However, for the inner loop `k` within the second `i-j` loop nest, set the parallel factor to 25, allowing concurrent execution of 25 iterations where possible.\n\nPlease note that the use of `auto{off}` and `auto{1}` indicates that these are the intended settings for the respective pragmas, and 'auto' denotes that these values should be applied automatically by the high-level synthesis tool following the specified constraints (off or 1).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the described HLS codes, provide the following comprehensive but very concise instruction:\n\nMark the function 'kernel_2mm' as a hardware kernel for acceleration. Disable automatic pipelining for the loops. For the first two nested loops that initialize 'tmp', ensure the iterations are run sequentially and don't tile the loops. Within the innermost loop, enable parallel computation for the reduction of 'tmp' based on the entire range of 'k'. For the last two nested loops that update 'D', disable automatic pipelining for the outer loop and set the tile factor to 4 for the inner loop, while also partitioning its iterations to run 16 in parallel. Finally, within the innermost loop, use parallel reduction of 'D' with 32 parallel instances.\n\nSpecific #pragma values to use respectively:\n\n1. Treat 'kernel_2mm' as a hardware acceleration kernel: `#pragma ACCEL kernel`\n2. Disable automatic pipelining for the outer loops: `#pragma ACCEL PIPELINE auto{off}`\n3. Process the loops in their original form, without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Ensure loop iterations run sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n5. For the innermost loop in the 'tmp' calculation, enable parallel reduction based on full 'k' range: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}`\n6. For the middle loop in the 'D' calculation, enable automatic pipelining: `#pragma ACCEL PIPELINE auto{}`\n7. Set the tile factor to 4 for the middle loop in the 'D' update: `#pragma ACCEL TILE FACTOR=auto{4}`\n8. Run 16 iterations in parallel for the middle loop in the 'D' update: `#pragma ACCEL PARALLEL FACTOR=auto{16}`\n9. For the innermost loop in the 'D' update, enable parallel reduction with 32 parallel instances: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}`\n\nThe described directives control loop pipelining, loop tiling, and loop parallelism to optimize the generated hardware's performance and resource usage.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel without automatically optimizing loops. For the first doubly nested loop, prevent any pipelining optimization, retain the original loop structure, and execute iterations sequentially. For the innermost loop in the first double loop nested structure, allow parallel execution with a reduction operation on 'tmp' using unpartitioned memory. In the second double loop nested structure, enable default pipelining optimization for the outer loop, flatten pipeline optimizations for the inner loop, set the innermost loop to run two iterations in parallel without reduction, and keep the original structure for the innermost loops iterating over 'C'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code that treats the function `kernel_2mm` as a hardware acceleration kernel, which carries out a matrix multiplication followed by a matrix addition of the form `D := alpha*A*B*C + beta*D`. For the first level of loops across indices `i` and `j`, ensure the compiler does not pipeline the iterations and processes the loop without tiling or parallelization. Then, within those loops, allow automatic pipelining of the loop body.\n\nFor the loop where `tmp[i][j]` is computed, permit parallel reduction on `tmp` with a factor allowing up to four simultaneous operations. In the subsequent double-nested loops for updating `D[i][j]`, enable pipelining and tile the second-level loop, which iterates over index `j`, with a tiling factor of 8 to split the iteration space into chunks, and set parallel loop execution with a factor of 2 for moderate parallelism. Finally, allow a parallel reduction on `D` with a factor of 16 to enable multiple concurrent operations during the update of the `D` matrix. \n\nDo not use specific numeric values for pragma parameters, allowing the compiler to automatically determine the optimal settings where `auto` is indicated. Ensure all the pragmas just mentioned are placed directly before the loops or blocks they are intended to modify.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, instruct the HLS tool as follows: \"Designate the entire function as a hardware acceleration kernel, while disabling automatic pipelining, tiling, and parallelization for the outer loops. For the innermost loop that accumulates the 'tmp' array, enable pipelining, but keep the tiling and parallel factors as if they were one, without any loop unrolling. Then, for the nested loops that update array 'D', disable pipelining for the first-level inner loop and set the parallel factor to two, but greatly increase loop unrolling in the innermost loop that performs the reduction with a parallel factor of eight, while still not tiling the loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function named `kernel_2mm` that operates on matrices `A`, `B`, `C`, `D`, and `tmp`, all with fixed dimensions, to calculate `D := alpha*A*B*C + beta*D` without applying automatic pipelining optimizations, tiling, or parallelism to the outer loops. For the inner reduction loop calculating `tmp`, allow for some parallel execution with a factor of two, while for the inner reduction loop updating `D`, process only sequentially. In the final loop, apply loop flattening, increase tile factor to four, and allow a high degree of parallelism with a factor of twenty for the middle loop only.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code written in C, issue the following comprehensive but terse natural language instruction:\n\nImplement a hardware acceleration kernel function, `kernel_adi`, for solving the Alternating Direction Implicit (ADI) method on a two-dimensional grid of size 60x60, with temporal and spatial discretizations defined by specific constants. Make sure to execute the core loop iterations sequentially and preserve the original loop bounds and structures without applying any pipelining, tiling, or splitting for parallel execution, in all loops except the innermost `j` loop of both the column and row sweeps, where parallel factors of 2 and 16 are to be applied in the forward sweeps and 1 and 8 in the backward sweeps, respectively.\n\nSpecifically, indicate the following:\n- The function should be treated as an acceleration kernel.\n- All loops should be processed in sequence without pipeline optimization, which means disabling any automatic loop pipelining.\n- The loops should not be tiled, i.e., they should run in their original, unaltered form with a tile factor of 1.\n- Parallel execution should only be disabled for all loops except the innermost `j` loops within both the column and row sweeps. In the forward sweeps of the column and row (increasing `j`), parallelize the computations with factors of 2 and 16, while in the reverse sweeps (decreasing `j`), apply parallel factors of 1 and 8, respectively.\n\nApply these directives using the appropriate `#pragma` values in your HLS code:\n- `#pragma ACCEL kernel` to mark the function for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipeline optimization.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to retain the original loop structure without tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{x}` where 'x' is the desired factor, to set the specific parallel execution factor for the inner `j` loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare a function as a hardware acceleration kernel. Inside this function, for both main for-loops:\n\n1. Do not pipeline the loops.\n2. Do not tile any loops.\n3. Run loop iterations sequentially: not in parallel or unrolled.\n\nWithin the innermost loop of the first main for-loop block, enable parallel processing of iterations with a reduction operation on 'tmp', using a factor of 10 for the parallelism.\n\nFor the second main for-loop block, restore the default pipelining behavior for the outer loop. Within the innermost loop, enable parallel processing of iterations with a reduction operation on 'D', but do not parallelize or unroll these iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, prevent loop pipelining and parallelization enhancements on an outer loop, allow automatic optimization on inner loops but without pipelining the first inner loop's iterations consecutively or breaking them into chunks, and use parallel reduction with automatic factor on a deepest loop. Flatten the iteration space of the second outer loop while also processing a certain number of loop iterations concurrently, and again apply parallel reduction with automatic factor on the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and within the function:\n\n1. For the first pair of nested loops (i-j-k), do not auto-pipeline the `i` and `j` loops. Maintain the original order and size of the loop iterations without parallelization or tiling.\n2. Configure the innermost `k` loop to perform a reduction on the array `tmp` with a specified parallelization factor, using auto-detection but suggest a factor of 16.\n\nFor the second pair of nested loops (i-j-k):\n\n1. Enable auto-pipelining on the outer `i` loop with no specific guidance on how to unroll this loop, leaving it to automatic tools to decide.\n2. Flatten the pipeline for the `j` loop, automatically controlling the pipeline stages without any constraints.\n3. Parallelize the innermost `k` loop over the array `D` without any loop unrolling or changes in parallelization, maintaining the original loop structure.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool as follows for the `kernel_2mm` function which performs a 2-matrix multiply operation:\n\n- Designate the whole `kernel_2mm` function as a hardware acceleration kernel.\n- For both the outer loops iterating over 'i' and the first inner loop iterating over 'j':\n  - Disable the automatic loop pipelining.\n  - Keep the loops unaltered without tiling.\n  - Execute loop iterations sequentially, not in parallel.\n- However, for the innermost loop iterating over 'k' that calculates `tmp[i][j]`, enable parallel reduction of the `tmp` array with parallelism unspecified (trusting the tool's judgment).\n- In the last innermost loop where `D[i][j]` is updated, set the parallelism factor explicitly to 32 for the reduction of the `D` array. Additionally, run the second inner loop iterating over 'j' with a parallelism factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, disable automatic pipelining, enforce normal loop execution without tiling or parallelism for the first set of loops. For the innermost loop of the first set, enable flattening of loops when possible but without tiling or parallelism. For the second set of loops, maintain default pipelining, execute without tiling, and allow for parallel execution with a factor of two. Within the innermost loop of the second set, enable a high degree of parallelism with a reduction on the temporary array. Here are the specific pragma instructions:\n\n- Treat 'kernel_2mm' as a hardware acceleration kernel.\n- For the first set of loops, disable automatic loop pipelining, prevent loop tiling, and run loop iterations serially.\n- In the first innermost loop, allow loop flattening but prevent both loop tiling and parallel execution.\n- In the first inner innermost loop, enable parallel execution with a reduction on 'tmp' and disallow parallelism and tiling.\n- For the second set of loops, use the default pipelining behavior, prevent loop tiling, and allow a parallel execution factor of two.\n- In the second innermost loop, greatly increase parallelism on 'D', disable tiling, and enforce a specific parallel factor.\n\nHere's how you would specify these in HLS pragmas:\n\n- Use `#pragma ACCEL kernel` to indicate hardware acceleration for 'kernel_2mm'.\n- `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipelining for the loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to execute loops in full form without tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` for sequential execution of loop iterations.\n- In the first inner loop:\n  - `#pragma ACCEL PIPELINE auto{flatten}` to flatten the loops when possible.\n- In the first inner innermost loop:\n  - `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}` to ensure only a reduction operation can leverage parallelism and to prevent parallelism and tiling for other operations.\n- In the second set of loops:\n  - `#pragma ACCEL PIPELINE auto{}` to default to the automatic pipelining behavior.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{2}` to allow parallel execution with a factor of two.\n- In the second innermost loop:\n  - `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{32}` to enable a significant degree of parallelism with a reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and within it, handle all loops without automatic pipelining, with no loop tiling, and with sequential execution for loop iterations. Specifically, maintain the outer loops for both 'i' and 'j' in their complete form and execute them sequentially. For the innermost 'k' loop, perform reductions on 'tmp' and 'D' matrices without concurrency. Exceptionally, parallelize the most inner 'k' loop within the second 'i' loop across four processing elements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function performing matrix multiplication with specific pragmas: Treat the function as a hardware kernel for acceleration. Do not apply any automatic pipelining optimizations on loops. Process all loops without tiling, keeping them intact. Execute all iterations of each loop sequentially, not in parallel. For the flattening of nested loops, allow the innermost loop to be flattened if possible. Finally, use parallel reduction for the accumulator variable within the innermost loops, but maintain iterative execution with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function 'kernel_2mm' for hardware acceleration with inputs for matrix dimensions and scalar values, along with matrices 'tmp', 'A', 'B', 'C', and 'D'. Compute 'D' as the sum of the product of 'alpha', 'A', and 'B', and the product of 'beta' with 'D'. Prevent optimization efforts to pipeline, tile, or parallelize the outermost loop. In the first nested loop, allow automatic pipelining but prevent tiling and parallel optimizations. Use a parallel reduction for 'tmp' in the innermost loop without changing the factor for parallelism. For the second nested loop compute 'D', automatically enabling pipelining with full flattening, restrict tiling, and parallelize the second innermost loop with a factor of 20 using parallel reduction for 'D' without affecting the parallelism factor for its innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware-accelerated kernel, ensuring that the top-level loop nests are flattened where possible but do not autopipeline the inner loops. Specifically, within the matrix computations, ensure the inner loop iterations are not parallelized beyond their inherent sequential dependency. Use automatic tiling with a factor of one for the loop nests, but allow for up to thirty-two parallel executions in the last loop nest for the outer loop, and ensure reductions on the temporary and result matrices, 'tmp' and 'D', with automatic factor selection for the parallel reduction clauses.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function for hardware acceleration without changing the scheduling of its loops, initially compute a matrix-matrix product while restricting optimization techniques for the outer loops but allowing the innermost loop to fully utilize parallelization resources with a factor of 14, then add another matrix product to the result with the outermost loop flattened for performance optimization and setting a high parallelization factor of 20 for the second innermost loop, leaving the innermost loop to run serially without parallel optimization. Use the following specific pragma directives for each optimization purpose:\n\n1. To indicate a function as a hardware kernel for acceleration, use `#pragma ACCEL kernel`.\n2. To prevent automatic pipelining of a loop, use `#pragma ACCEL PIPELINE auto{off}`.\n3. To keep the loop in its original, full form without tiling, use `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. To run the iterations of the loop serially, use `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n5. To enable full loop pipelining without specifying a factor, use `#pragma ACCEL PIPELINE auto{}`.\n6. To induce parallelization with a specific factor, use `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{specific_factor}`, where `variable` is the array being reduced and `specific_factor` is the chosen degree of parallelism (for example, `14` or `20`).\n7. To flatten nested loops and optimize the scheduling, use `#pragma ACCEL PIPELINE auto{flatten}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. In this function, disable automatic pipelining for the entire function, ensure the primary loops operate sequentially without tiling or parallel execution, and process nested loops as follows:\n\n1. For the first two-level nested loops which initialize 'tmp':\n   - Flatten the execution of the inner loop 'j', treat it as a single loop iteration for pipelining.\n   - Ensure the innermost loop 'k' runs serially and does not perform any parallel reduction on the 'tmp' array.\n\n2. For the second set of two-level nested loops which compute 'D':\n   - Allow the inner loop 'j' to execute with a parallel factor of 20, enabling some level of parallel computation.\n   - Ensure the innermost loop 'k' runs serially and does not perform any parallel reduction on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Fully serialize the execution of the first two loops without pipelining, tiling, or parallelizing them. Within the innermost loop of the first double loop, enable parallelization with a reduction on 'tmp' and adjust the parallel factor automatically to about five. For the second double loop, activate automatic pipelining and tile the second loop with a factor of eight while setting the parallel factor of the second innermost loop to two. In the last inner loop of the second double loop, perform a parallel reduction on 'D' with an automatic parallel factor of approximately sixteen.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, disable automatic pipelining for the entire function, execute the main loops without tiling or parallelization, and flatten inner loops for pipelining optimization. In the first level-1 loop, perform parallel accumulation on 'tmp' with no concurrency. In the second level-1 loop, disable pipelining for the nested loop but divide the innermost loop into four parallel tasks and massively parallelize the reduction on 'D' with a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "First, indicate that the function 'kernel_2mm' should be compiled as a hardware kernel for acceleration. Then, for every loop in the function, apply the following directives:\n\n1. Suppress automatic pipelining for the main loops that iterate over 'i' and 'j' elements in the 'tmp' matrix.\n2. Do not tile any of the loops, which means each loop will execute in its original form without partitioning them into smaller sections.\n3. Run all the loop iterations sequentially to ensure none of the iterations are executed in parallel.\n4. For the innermost loop calculating 'tmp[i][j]', enable data reduction on the 'tmp' matrix while ensuring only one operation per cycle is allowed.\n5. For the loop involved in updating the 'D' matrix, pipeline the outer 'j' loop naturally and set the 'k' loop to execute multiple iterations (up to 50) in parallel only for reducing accumulation on the 'D' matrix.\n\nIn short, you instruct to compile the function into hardware while disabling automatic pipelining and keeping the main loops running without concurrency. Additionally, allow one innermost loop to be pipelined and another to execute multiple parallel accumulations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel. Within the kernel, for each loop, do not use automatic pipelining, do not divide loops into smaller chunks, and run loop iterations sequentially. Flatten the loop hierarchy in the 'tmp' initialization loop but keep all other settings aforementioned. Finally, in the 'D' computation loop, process four iterations of the 'j' loop in parallel, and allow parallel processing of twenty-five iterations of the 'k' loop simultaneously with the associated reduction operation applied to 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, where within the kernel context, inhibit automatic pipelining, prevent loop tiling, and disallow parallel loop execution for both the i-j and i-k loop nests. Within the i-j-k nested loops, enable automatic pipelining without specifying the factor, tile the inner loops with a factor of one, and employ parallel processing with a factor of 32 for the reduction of the tmp array. Lastly, in the i-j-k nested loops for updating matrix D, opt for the loop to be flattened during pipelining, retain a tiling factor of one, allow parallel processing with a factor of 32 for the outer loop, but restrict parallel processing for the reduction on D to a factor of one.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as the hardware kernel target for acceleration, and in the nested loops inside this function, manage pipelining, tiling, and parallel processing of loop iterations as follows:\n\n1. For the first loop block calculating the intermediate array 'tmp', disable pipelining, do not tile the loops, and prevent parallel execution of the first and second loop levels to maintain their sequential nature. \n2. Within the innermost loop of this block that calculates 'tmp[i][j]', enable parallel reduction on 'tmp' with a factor that potentially allows 14 simultaneous operations.\n3. For the second loop block updating 'D' based on the 'tmp' and 'C' variables, enable pipeline optimization, maintaining the outer loops in their full original size without tiling and without parallel processing.\n4. Apply pipeline flattening for the first-level loop inside this block to combine the loop levels for potential optimization, and allow parallel processing of the second-level loop with a significant parallel factor, suggesting that up to 20 iterations may be executed concurrently.\n5. Finally, maintain sequential processing for the innermost loop inside this block while performing a reduction on 'D'.\n\nTranslate these instructions into #pragma values as follows:\n\n- Treat 'kernel_2mm' as a hardware kernel: `#pragma ACCEL kernel`\n- Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- Use the default tiling factor (no tiling): `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run loop iterations sequentially (no parallelization): `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Parallelize with specific reduction on 'tmp' or 'D': `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}` for 'tmp' and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` for 'D'.\n- Enable automatic pipelining without specifics: `#pragma ACCEL PIPELINE auto{}`\n- Apply pipeline flattening: `#pragma ACCEL PIPELINE auto{flatten}` \n- Parallelize with a high parallel factor for the loop: `#pragma ACCEL PARALLEL FACTOR=auto{20}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, handle the outermost loop without optimization directives, allowing the tool's default behavior. In the first nested loop, flatten the iterations of the loop to enhance parallelism. In the second nested loop, use a parallel reduction on 'tmp' with the default parallelization. In the third outer loop, use default tool optimizations. In the fourth nested loop, enhance parallelism by specifying a parallelization factor of 4. Finally, in the last nested loop, carry out a parallel reduction on 'D' with an aggressive parallelization factor, equivalent to the loop's trip count.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel function that performs a two-matrix multiplication followed by another matrix multiplication and then accumulates the results. For the outer loops that iterate over elements of the resulting matrices, disable automatic pipelining, enforce loop execution without tiling, and run iterations sequentially. Inside the first nested loop pair, enable flattened pipelining but again keep iterations sequential without tiling or parallel execution. For the innermost loops where the actual multiplications and additions occur, allow the reduction on temporary and result matrices but limit parallel execution to factors of 1 for the first multiplication and 10 for the second multiplication. Additionally, in the second set of nested loops, allow a moderate level of parallelism with a factor of 4 for the outer loop while keeping other configurations as before.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware-accelerated kernel. Within this function, maintain the original structure of all loops without introducing any pipelining, tiling, or parallel optimization techniques, except for the innermost loops in both main loop nests. Specifically, for the first main loop nest, fully unroll the innermost loop for matrix multiplication between 'A' and 'B'. In the second main loop nest, partially parallelize the innermost loop processing the multiplication between 'tmp' and 'C' with a factor of 8, while strictly running the outer loop without pipelining but with a reduced parallel factor of 4 for the loop iterations over 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, proceed without automatic pipelining for top-level loops, execute outer loops in their entirety without tiling or parallel execution, but flatten the second-level loop 'j' in the first part and run its iteration serially. For the innermost reduction loop, perform serial summation without parallelism and tile size manipulation. In the second part, maintain serial execution for the 'i' and 'j' loops, but allow partial parallel execution of the 'j' loop with a factor of 4. For the final reduction loop, enable a parallel factor of 5 with the summation performed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel, and in the following loop nests within this function, apply the following optimizations:\n\n1. Turn off automatic pipelining for the first-level loops (i.e., do not attempt to optimize these loops by overlapping loop iterations).\n2. Do not partition the first-level loops into sub-loops (i.e., execute the loops in full without tiling).\n3. Execute the first-level loop iterations sequentially rather than in parallel (i.e., one iteration at a time, without parallelism).\n4. Within the nested loops, enable automatic pipelining optimization according to the tool's heuristics.\n5. For the second-level loop that initializes the `tmp` array, set the loop tile size and parallel factor to their default values, ensuring the loop maintains its original structure and sequential iteration order.\n6. For the innermost loop that computes the product for the `tmp` array, enable parallel execution of loop iterations with a reduction operation on `tmp`, utilizing seven parallel processing elements.\n7. In the loop nest responsible for updating the `D` array, allow the tool to automatically pipeline the loops, and do not tile them, similarly to the first loop nest.\n8. Allow up to 32 parallel iterations for the second-level loop in the `D` array update while ensuring the innermost loop performs the reduction operation on `D` with four parallel processing elements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware acceleration kernel, disable automatic pipelining, and prevent tiling and parallelizing of the outer loops. Inside the nested loops, use default settings for automatic pipelining and tiling, while parallelizing the reduction operations in the innermost loops with a factor of 4 and 50 respectively for different sections.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function called `kernel_2mm` as a hardware acceleration kernel, operating on matrices of specified dimensions, where you will perform two matrix-matrix multiplications sequentially with scalar multiplication. Firstly, disallow automatic pipelining, tiling, and parallel processing for all loops at the outer level. In the first two nested loops where the `tmp` matrix is calculated, avoid pipelining and tiling but allow a parallel reduction on the innermost loop with a factor that enables 32 computations to occur concurrently, targeting the `tmp` array. In the second matrix multiplication, where matrix `D` is updated, set the pipeline attribute to flatten, tile the second loop normally, and set the parallel processing factor to 32 on the first nested loop. Finally, compute the updates without parallelism in the innermost loop, ensuring one computation happens at a time while applying a reduction to the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function 'kernel_2mm' as a hardware acceleration kernel. For the outer loops 'i' iterating over 'ni' elements in 'tmp' initialization and 'i' iterating over 'ni' elements in the final 'D' matrix calculation, disable automatic pipelining, maintain the loop in its entirety without tiling, and run iterations sequentially. Within the 'j' loop for initializing 'tmp', similarly disable pipelining, avoid tiling, and maintain sequential execution. While accumulating into 'tmp', enable parallel accumulation with a default factor, optimized by the HLS tool. For the final 'D' matrix calculation, enable loop flattening in the 'j' loop pipelining, set a tiling factor of 8, and a parallel execution factor of 10. For the 'k' loop nested within the 'D' calculation, enforce sequential execution while using the original loop bounds.\n\nIn terms of #pragma directives, this leads to:\n\n- Treat 'kernel_2mm' as a hardware kernel: `#pragma ACCEL kernel`\n\n- For outer 'i' and 'j' loops:\n  - Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n  - Maintain original loop, no tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n  - Execute iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n- For accumulation into 'tmp':\n  - Enable parallel accumulator for 'tmp' with optimization: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}`\n\n- For 'D' matrix calculation:\n  - Enable loop flattening in pipelining: `#pragma ACCEL PIPELINE auto{flatten}`\n  - Set a tiling factor of 8: `#pragma ACCEL TILE FACTOR=auto{8}`\n  - Set a parallel execution factor of 10: `#pragma ACCEL PARALLEL FACTOR=auto{10}`\n\n- For the nested 'k' loop inside 'D' matrix calculation:\n  - Enforce sequential execution: `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, ensure that the main loops and sub-loops are executed without any automatic pipelining, tiling, or parallelization; however, do allow for loop flattening in the outer loop where `D[i][j]` is updated, and enable parallel reduction for the inner loops where `tmp[i][j]` and `D[i][j]` are computed, but maintain the parallel factor to its default value for the loop computing `D[i][j]` while allowing aggressive parallelism (up to factor 70) for computing `tmp[i][j]`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a certain function as a dedicated hardware acceleration module, and while implementing the matrix multiplication and addition, disable automatic pipelining for the outermost loops to maintain the original execution order, keep iterations of all the loops from being divided into smaller segments, and disallow concurrent execution of iterations for outside loops. Within the first block of nested loops, allow the loop to be flattened for better pipelining but retain the original loop iteration structure. Inside the innermost first loop, enable parallel reduction on the temporary array without concurrency. For the second block of nested loops, enable pipelining by default for the outer loop, set a loop tiling factor of 2 to divide the iterations into chunks, allow four iterations to run in parallel, and set a high parallelism factor of 32 for the innermost loop with parallel reduction on the D array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. Inside the function, for all loops, turn off automatic pipelining, keep the loops unpartitioned by specifying a tile factor of 1 and avoid parallel processing of iterations by setting the parallel factor to 1. In the first `i`-`j` loop nest, flatten the pipeline for the inner `j` loop. For the reduction operation on `tmp` within the `k` loop, allow it to run without parallelization by also setting its factor to 1. In the second `i`-`j` loop nest, within the `i` loop, explicitly turn off pipelining and set the parallel factor to 4 for the `j` loop. For the reduction on `D` within the `k` loop, set the parallel factor to 2 for moderate parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for an accelerated kernel function named 'kernel_adi' designed for solving the Alternating Direction Implicit (ADI) method with specific loop optimizations. The function should take the number of time steps 'tsteps' and dimension 'n' as inputs, along with four 2D arrays 'u', 'v', 'p', and 'q', representing intermediate states in the ADI computation. Initialize constants for discretization and coefficients within the function and apply the following loop optimizations:\n\n1. Disable automatic pipelining for all loops.\n2. Prevent tiling for all loops by setting the tile factor to 1.\n3. Run all loop iterations sequentially, by setting the parallel factor to 1.\n4. In the nested loop that updates 'p' and 'q' during the column sweep, allow parallelization up to 58 iterations.\n5. In the reverse order loop that updates 'v' during the column sweep, allow parallelization with a factor of 4.\n6. For the analogous nested loop during the row sweep that also updates 'p' and 'q', allow parallelization up to 58 iterations.\n7. In the reverse order loop that updates 'u' during the row sweep, allow parallelization with a factor of 2.\n\nThe loop structure should include an outer time loop that iterates over 'tsteps', within which there are two phases for the column sweep and the row sweep. Each phase should involve processing the elements of the arrays 'u', 'v', 'p', and 'q' using the given calculations and boundary conditions.\n\nTo generate the HLS C code, include the pragma `#pragma ACCEL kernel` at the beginning of the function to indicate that it is an accelerated kernel. For each loop and nested loop that processes the arrays, use the associated pragmas with the corresponding values:\n- `#pragma ACCEL PIPELINE off` to disable automatic pipelining.\n- `#pragma ACCEL TILE FACTOR=1` to prevent tiling of loops.\n- `#pragma ACCEL PARALLEL FACTOR=1` to ensure that loop iterations are executed sequentially.\n- `#pragma ACCEL PARALLEL FACTOR=58` for the specific 'p' and 'q' update loop in the column sweep.\n- `#pragma ACCEL PARALLEL FACTOR=4` for the reverse order loop updating 'v' in the column sweep.\n- `#pragma ACCEL PARALLEL FACTOR=58` for the 'p' and 'q' update loop in the row sweep.\n- `#pragma ACCEL PARALLEL FACTOR=2` for the reverse order loop updating 'u' in the row sweep.\n\nEnsure that the constants used for the computations are calculated inside the function before the start of any looping structures and that the provided calculations align with the ADI method. Implement the reverse order loops with appropriate index calculations that accomplish the necessary updates to the arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` as a hardware acceleration kernel where we perform a two-matrix-multiply (2MM) operation, implemented without any compilation-driven loop pipelining, tiling, or parallelization optimizations for the outermost loops, but with specific optimization overrides inside.\n\n- Leave the outermost loops unoptimized: use \"Do not pipeline, tile, or parallelize the first level of loops\" for `#pragma ACCEL PIPELINE \"off\"`, `#pragma ACCEL TILE FACTOR=1`, and `#pragma ACCEL PARALLEL FACTOR=1`.\n  \n- Flatten the iteration space of the second-level loop for `i` and `j` without any tiling or independent parallel execution: use \"Completely flatten the second-level loop iterations as a single, serialized sequence\" for `#pragma ACCEL PIPELINE \"flatten\"`, along with \"Do not use loop tiling or parallelism at this level\" for `#pragma ACCEL TILE FACTOR=1` and `#pragma ACCEL PARALLEL FACTOR=1`.\n\n- For the reduction loop over `k` for the `tmp` array, prevent parallel execution: use \"Disallow any parallel execution in the calculation of `tmp` intermediate results\" which corresponds to `#pragma ACCEL PARALLEL reduction = tmp FACTOR=1`.\n\n- For the second matrix multiply updating `D`, turn off pipelining and tiling for the second-level `i` and `j` loops with \"Do not pipeline or tile these loop iterations\" which maps to `#pragma ACCEL PIPELINE \"off\"` and `#pragma ACCEL TILE FACTOR=1`.\n\n- However, allow for a slightly higher degree of parallelism in the `j` loop using \"Enable a small degree of parallel execution for the `D` array update with a factor of two\" which translates to `#pragma ACCEL PARALLEL FACTOR=2`.\n\n- Significantly increase parallelism in the innermost `k` loop for updating `D` with \"Aggressively parallelize the innermost loop when updating `D`, up to a factor of five\" for `#pragma ACCEL PARALLEL reduction = D FACTOR=5`.\n\nThe HLS code adheres strictly to these optimization constraints while computing the equation `D = alpha*A*B + beta*D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel that treats the entire function as a hardware acceleration kernel, preserves the original form of the top-level loops without pipelining or tiling them, processes the inner loops without pipelining or tiling, but parallelizes the reduction loops over 'k' with full unrolling for the first reduction loop and a parallel factor of 4 for the second reduction loop. Use the respective #pragma directives with 'off' for pipeline disabling, a factor of '1' to maintain the original loop form without tiling, and specify reduction variables with their appropriate factors for parallelization of the reduction operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. Maintain the original loop structure for all the loops by preventing automatic pipelining, tiling, and parallelization, except in two specific cases: for the nested loop iterating over `k` inside the `tmp` calculation, parallelize with a factor of 4 to allow four iterations to execute simultaneously while performing a reduction on `tmp`. Then, for the nested loop iterating over `k` inside the `D` calculation, parallelize with a maximum factor of 50 to concurrently execute up to fifty iterations while performing a reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function \"kernel_2mm\" to be synthesized into a hardware acceleration kernel with the following constraints: Disallow automatic pipelining of all loops to maintain sequential execution, ensure each loop is processed in its entirety without tiling, and enforce non-parallel execution of loop iterations. However, within the nested loop where \"tmp\" is calculated, permit parallel execution with partial results aggregated using a reduction pragma, optimized with a parallelization factor of 32. Similarly, permit parallel execution with reduction in the final nested loop updating \"D,\" optimized with a parallelization factor of 16, while allowing tiling with a factor of 8 for the middle loop and parallel processing with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following function into a hardware-accelerated kernel and control loop performance as per the given specifications: \n\n- Prevent the outermost loops from being automatically pipelined.\n- Keep the first pair of nested loops in their full, original size without tiling or parallelizing.\n- Allow flattening of the innermost loop in the first pair to optimize data flow, without tiling or parallelizing.\n- Prohibit optimization techniques such as tiling and parallelism for the second pair of nested loops, except increase the parallelization factor to 5 for the middle loop and to 32 for the innermost loop, while allowing the middle loop to be split into chunks of half its size.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as hardware kernel for acceleration, and apply specific pragma directives to its loops as follows: \n\n1. Disable automatic pipelining for all outer loops and the first nested loop, but leave the 'FACTOR' automatically unassigned (interpreted as \"1\") which implies serial execution.\n2. In the multiplication and accumulation of 'tmp', allow parallel execution with full resource utilization by cascading the loop for maximum resource allocation, and enable parallel reduction for 'tmp' with the 'FACTOR' automatically computed for optimal resource balancing.\n3. In the final nested loop for updating 'D', achieve parallel execution with a pre-computed factor for controlled resource usage and enable parallel reduction with an automatically calculated factor for 'D' for optimal efficiency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS kernel function called `kernel_2mm` that calculates the equation `D := alpha*A*B*C + beta*D` with matrix dimensions ni=40, nj=50, nk=70, and nl=80. The matrices are named `tmp, A, B, C,` and `D`. For the first two nested loops over `tmp` array, set the pipeline, tile, and parallel pragmas off, preventing automatic optimizations. Within the innermost loop calculating `tmp`, enable parallel processing with a factor of 2, but only for reductions. In the second pair of nested loops for updating the `D` array, turn off automatic pipeline optimizations at the outer level, flatten the loop structure at the pipeline level of the inner j-loop, and set the parallel processing of the j-loop with a factor of 32 but set it to 1 for the innermost loop carrying out reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` which performs a 2-matrix multiplication followed by an addition to act as a hardware acceleration kernel. Disable automatic pipelining, tiling, and parallelization for the entire function. In the first nested loop calculating `tmp`, disable loop pipelining and tiling while maintaining sequential execution, but allow parallelization with full reduction on `tmp` using all elements of `B`. For the second nested loop updating `D`, enable automatic pipelining, maintain sequential execution with tiling factor of one, and allow parallelization within the innermost loop but limit it to five concurrent iterations. Additionally, disable pipelining and parallelization for the middle loop, but enable loop flattening for it.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function to be synthesized into a hardware block for acceleration. Within the outer loops, disable loop pipelining and enforce fully sequential execution without tiling or parallelization. In the innermost loop of the first set, allow parallel computation on a fixed accumulation axis with maximal allowable unrolling. For the final computational set, enable default loop pipelining on the outer loop, maintain no tiling on the middle loop but parallelize with a moderate fixed factor, and sequentially process the inner accumulation without parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the `kernel_2mm` function as a dedicated hardware acceleration kernel. Apply the following pragmas to the loops within:\n\n1. Disable automatic pipelining for the entire function.\n2. Do not tile any of the loops and perform each loop iteration sequentially without parallelization by setting tiling and parallelization factors to 1.\n3. Within the first nested loop, enable aggressive loop flattening to combine the loop nests where possible.\n4. Retain the original loop order and perform loop iterations sequentially in the second and third nested loops without any pipelining, tiling, or parallelization optimizations.\n5. In the fourth nested loop, slightly parallelize the second level of the nested loop with a factor of 5 but keep the innermost loop without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Identify the function 'kernel_2mm' as a hardware acceleration point. Disable automatic pipelining, use the original loop structure without tiling, and enforce serial execution of iterations for the outer loops 'i' in both primary 'for' constructs. For the computation of the 'tmp' array inside the first double-nested 'for' loop, enable fine-grained parallelism without any data consolidation optimisation, but for the multiplication inside the innermost loop 'k', use a high degree of parallelism with a factor of 32 while reducing 'tmp'. In the second double-nested 'for' loops when updating 'D', disable pipelining for the outer 'j' loop but set the tiling factor to 8 and the parallelism factor to 4, indicating to run several iterations concurrently, albeit fewer than before; then for the innermost 'k' loop updating 'D', enforce a moderate parallel execution with a reduction on 'D' setting the factor to 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given code, you can use the following comprehensive but concise natural language instruction for the HLS pragmas:\n\n\"Treat the entire function as a dedicated hardware acceleration kernel, avoiding automatic loop pipelining, enforcing full loop execution, ensuring no parallelization, and maintaining the original loop structure for the first double-nested loop. For the innermost loop within this, enable reduction on the 'tmp' array ensuring iterations are processed sequentially. Flatten nested loops when beneficial. In the second double-nested loop, again, avoid pipelining and maintain the original loop structure, but allow for parallel processing of the outer loop with a pair of iterations at a time and the same for reduction on 'D' in the innermost loop.\"\n\nThis instruction corresponds to the following #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   `#pragma ACCEL kernel`\n\n2. Do not automatically pipeline this loop:\n   `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n5. Flatten nested loops if beneficial for performance:\n   `#pragma ACCEL PIPELINE auto{flatten}`\n\n6. Specify the 'tmp' array to be subject to a reduction operation and ensure its iterations are handled sequentially:\n   `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}`\n\n7. Specify that the 'D' array is subject to a reduction operation processing a pair of iterations in parallel:\n   `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{2}`\n\nRemember that the 'auto{value}' syntax is used here as a placeholder to indicate whether the attribute is switched on or off, or to suggest its factor value, and it might differ based on specific HLS tool syntax and versions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_2mm' function as a separate hardware kernel for acceleration purposes. Within this function, for all loops, prevent automatic pipelining, ensure that loops are processed in their full form without tiling, and execute loop iterations sequentially, not in parallel. However, when iterating over j in the second set of nested loops, allow the pipelining to flatten loops structure if beneficial and set parallelism for innermost loop with 32 concurrent threads, but keep the parallel reductions for 'tmp' and 'D' matrix accumulations set to process sequentially. Use these specifications:\n\n1. '#pragma ACCEL kernel' to specify that 'kernel_2mm' is the hardware kernel.\n2. '#pragma ACCEL PIPELINE \"off\"' to disable automatic pipelining for each loop.\n3. '#pragma ACCEL TILE FACTOR=1' to prevent loop tiling and process the loops in their original form.\n4. '#pragma ACCEL PARALLEL FACTOR=1' to run loop iterations sequentially.\n5. For the second set of nested loops over 'j', use '#pragma ACCEL PIPELINE \"flatten\"' to allow loops to be flattened by pipelining if beneficial.\n6. Use '#pragma ACCEL PARALLEL FACTOR=32' to express parallel execution with 32 concurrent threads in innermost loop where 'D[i][j]' is calculated.\n7. Use '#pragma ACCEL PARALLEL reduction' with 'tmp' and 'D' on their respective inner loops to demand that the reductions be processed sequentially.\n\nEnsure that these pragmas are placed directly before the loops they modify.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a 2mm kernel function by treating the function as a hardware kernel for acceleration. For the first major loop over variable 'i', disable automatic pipelining, ensure it runs fully without tiling or parallelization. Inside this loop, flatten the nested 'j' loop with automatic pipelining but do not tile or parallelize it. Within this nested loop, for the 'k' loop, perform a parallel reduction on 'tmp' without specifying a parallel factor.\n\nFor the second major loop over variable 'i', use the default setting for pipelining, do not tile, and set the parallel factor to 4. For the inner 'j' loop, multiply 'D' by 'beta' and for the innermost 'k' loop, perform a parallel reduction on 'D' with a parallel factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code with the following directives: Treat the 'kernel_2mm' function as a hardware kernel for acceleration. Within 'kernel_2mm', disable automatic pipelining for the outer loops processing arrays 'tmp' and 'D', process these loops in their full form without tiling or parallelizing. Inside the first loop for array 'tmp', enable automatic pipelining and keep the loop in its original full form without tiling, but for the inner loop that accumulates the product 'alpha * A[i][k] * B[k][j]' into 'tmp[i][j]', enable parallelism with a factor of 32 and specify reduction on 'tmp'. For the second outer loop for array 'D', use a manual pipeline directive to flatten nested loop structures, set tiling with a factor of 8, and enable parallel processing with a factor of 5 for the 'j' loop, but keep the innermost 'k' loop from parallelizing while performing reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare a hardware-accelerated kernel function that computes the 2mm algorithm on matrices, where 'D' is calculated as 'D = alpha * A * B * C + beta * D'. Set the outermost loops to run sequentially without automatic pipelining, while for their inner loops, allow automatic pipelining. For the multiplication of A and B, set the innermost loop to partially run 32 iterations in parallel with a reduction on 'tmp'. For the addition to D, disable pipelining on the second-level loop, increase the tiling factor of the loop iterations over 'i' to 8, and run 4 iterations in parallel. For the innermost loop where 'D' is updated, run 16 iterations in parallel with a reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS (High-Level Synthesis) C code with the appropriate `#pragma` directives, use the following comprehensive but concise instructions:\n\n- Specify the function `kernel_2mm` to be synthesized into a hardware accelerator.\n- Avoid automatically pipelining the outermost loops by disabling the default loop pipelining optimization.\n- Preserve the original looping behavior of all loops by setting the 'tile' and 'parallel' factors to 1, which prevents the compiler from splitting the loops into smaller chunks or running iterations in parallel.\n- Enable default loop pipelining for the inner loops to improve performance without specific constraints.\n- Process the second loop's 'k'-dimension in parallel with a specific factor to exploit reduction parallelism without specifying the actual value, letting the compiler decide.\n- Disable loop pipelining, set the tile factor to 8, and set the parallel factor to 2 for the third loop to perform a moderate level of loop unrolling and run iterations partly in parallel for the 'j' loop.\n- Process the final loop's 'k'-dimension in parallel with reduction on array 'D', letting the compiler choose the magnitude of parallelism without stating the actual factor.\n\nIn natural language, for each `#pragma` you would instruct as follows:\n\n1. Treat the `kernel_2mm` function as a hardware kernel for acceleration: `#pragma ACCEL kernel`.\n\n2. Don't automatically pipeline the first and third 'i' loops: `#pragma ACCEL PIPELINE auto{off}`.\n\n3. Process the first and third 'i' loops in their original, full form: `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n\n4. Opt into default loop pipelining for inner loops 'j' and 'k' without providing specific values: `#pragma ACCEL PIPELINE auto{}`.\n\n5. For the first 'k' loop, specifically use the compiler's discretion to parallelize the reduction operation on array 'tmp': `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}`.\n\n6. For the third 'i' loop, set the tile factor to 8 and parallel factor to 2 to partially unroll and parallelize the 'j' loop: `#pragma ACCEL TILE FACTOR=auto{8}` and `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n\n7. For the final 'k' loop, enable parallel processing with reduction on array 'D' and allow the compiler to select the factor: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare a function `kernel_2mm` as a hardware acceleration kernel that performs matrix multiplication and addition according to the formula `D := alpha*A*B*C + beta*D` on data stored in 2D arrays, with given alpha and beta scalar values. The matrices have fixed dimensions as follows: `tmp` is 40x50, `A` is 40x70, `B` is 70x50, `C` is 50x80, and `D` is 40x80. Within this function:\n\n1. For the first major loop that iterates over the `tmp` matrix, prevent loop pipelining, maintain the original loop structure without tiling or parallel execution by setting `PIPELINE`, `TILE`, and `PARALLEL` pragma factors to 'off', '1', and '1' respectively.\n2. For the nested loop that processes the `tmp` matrix:\n    - Allow complete loop flattening for the pipeline directive.\n    - Avoid loop tiling.\n    - Run iterations sequentially without parallelism.\n    - For the innermost loop that computes the multiplication of `A` and `B` matrices, use parallel execution with no partitioning factor to accumulate the results into the `tmp` array using the reduction clause.\n3. For the second major loop which updates the `D` matrix:\n    - Turn off pipelining for the loop over matrix `D`.\n    - Do not use loop tiling.\n    - Enable partial parallelization with a factor of 4 for the outer loop.\n    - For the innermost loop that multiplies the `tmp` array with the `C` matrix and adds it to the `D` matrix, use parallel execution with a partitioning factor of 16 to accumulate the results using the reduction clause.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the nested loops as hardware kernels without automatic optimization in the 2mm matrix multiplication and addition algorithm, disabling all automatic loop pipelining, use all tiling and parallel processing features without altering their original full loop forms, while flattening nested loops for pipelining where beneficial. Specifically, for the second-level loop parallelization, allow eight parallel executions in the final stage of the algorithm. Use pragma reductions to manage dependencies when computing the temporary matrix values and the final matrix result.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C using a concise natural language instruction:\n\n\"Create a hardware-accelerated kernel function named 'kernel_2mm' that performs a matrix multiplication and addition following the equation D := alpha*A*B*C + beta*D. It should handle matrices with predetermined sizes: A is 40-by-70, B is 70-by-50, C is 50-by-80, and D is 40-by-80. The multiplication results should be stored in a temporary matrix, 'tmp', which is 40-by-50. The outer loops iterating over 'tmp' elements should not be pipelined or parallelized and should execute without tiling. In the innermost loop where 'tmp' is calculated, only parallelize it with a factor of 4 and apply a reduction pragma to 'tmp'. For the next part of the computation where 'D' is updated, the encapsulating loops are to be left with default pipeline settings but implement an 8-way tile on the second inner loop and a parallel factor of 2. In the final inner loop updating 'D', parallelize with a factor of 16 and apply a reduction pragma to 'D'.\" \n\nThe following #pragma commands correspond to this description:\n\n- Treat the function 'kernel_2mm' as a hardware acceleration kernel: `#pragma ACCEL kernel`.\n- For the initial loops without pipelining, tiling, or parallel execution, write:\n  - Disable automated pipelining for the loop: `#pragma ACCEL PIPELINE auto{off}`.\n  - Keep the loop in its full form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`.\n  - Execute iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the innermost loop that calculates 'tmp', parallelize with a factor of 4: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}`.\n- For updating 'D', allow default pipelining, but specify tiling and parallel execution like so:\n  - Let the tool determine pipeline settings: `#pragma ACCEL PIPELINE auto{}`.\n  - Use an 8-way tile on the loop: `#pragma ACCEL TILE FACTOR=auto{8}`.\n  - Parallelize the loop with a factor of 2: `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n  - Parallelize the final computation loop with a factor of 16: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and for the main computational loops within the kernel, apply the following specifications:\n\n1. Disable automatic pipelining for the outermost loop where `i` ranges from 0 to 39.\n2. Preserve the original loop form without tiling for all loops.\n3. Enforce sequential execution of iterations for the intermediate loops.\n4. For the innermost loop that computes the `tmp` matrix, flatten the pipeline directive, enforce parallel execution with a parallel factor of 1, and enable reduction for the `tmp` array.\n5. Automatically determine the pipeline settings for the subsequent loops that update the `D` matrix.\n6. For the loop that iterates over `j` from 0 to 79 with a stride of 1, apply parallelism with a custom factor of 5.\n7. Lastly, for the innermost loop that finalizes the `D` matrix update, invoke parallel reduction on the `D` array with an aggressive parallel factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the given function as a hardware acceleration kernel and for the matrix multiplication and addition operations, perform the following: Prevent the outermost loops from being pipelined and parallel processed, and do not tile them. Within the first nested loop, disable pipelining and prevent tiling and parallel processing, but allow parallel reduction with a factor of 32 for the loop that accumulates the alpha-scaled matrix product into 'tmp'. For the subsequent operation involving matrix 'D', disable pipelining for the outer loop, apply loop flattening for the first nested loop, tile the first nested loop with a factor of 8, enable parallel processing with a factor of 5, and within that, do not parallelize the reduction loop that adds the product of 'tmp' and 'C' to the beta-scaled 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following function into a hardware acceleration kernel and apply the specified pragmas to the relevant loops. For the matrix-multiplication operation between `A`, `B`, and `C` matrices, disable automatic pipelining, prevent loop tiling, and enforce serial loop execution. For the innermost loop of this operation involving `k`, perform a parallel reduction on `tmp` without loop chunking. For the operation of updating `D` with `alpha * A * B * C + beta * D`, disable automatic pipelining for the outer loop but leave inner loop pipelining decisions to the compiler. Tile the second loop with a factor of 2, and split the innermost loop into multiple parallel operations with a factor of 5, while specifying a parallel reduction on `D` with a high degree of parallelism, set to 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function that performs a double matrix-multiplication and scaling, ensuring that:\n\n1. The entire function `kernel_2mm` is designated for hardware acceleration.\n2. For the first main loop iterating over 'i' and 'j', disable loop pipelining, do not tile the loop, and process iterations sequentially.\n3. Within the inner 'j' loop, allow the compiler to decide how to pipeline the loop, avoid loop tiling, and process iterations sequentially; also use parallelism with a reduction on 'tmp' array without specifying the factor.\n4. For the second main loop iterating over 'i' and 'j', again disable loop pipelining for the outer 'i' loop, but tile the inner 'j' loop with a factor of 4 and process its iterations four at a time in parallel.\n5. Inside the innermost loop (for k) in the second main loop block, specify a parallel reduction on 'D' with a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) accelerator kernel function named 'kernel_2mm' that takes sizes 'ni', 'nj', 'nk', 'nl', a scalar value 'alpha', a scalar value 'beta', and double-precision floating-point 2D arrays 'tmp', 'A', 'B', 'C', and 'D' as parameters for performing a two-matrix multiply operation. For the first part of the operation, disable automatic pipelining of the top-level loop, process it without tiling, and execute iterations sequentially. Within the loop, pipeline inner loop iterations automatically, do not use tiling or loop partitioning, perform a vectorized sum of the product of elements from 'A' and 'B' into 'tmp', with 'alpha' as a coefficient, using a parallel factor of 7 for the reduction operation.\n\nFor the second part of the operation, similarly disable automatic pipelining, do not use tiling, and execute iterations sequentially for the outer loop. Within this loop, automatically pipeline updates to 'D', do not use tiling, but set parallel factor to 32 for the processing in the inner loop. Multiply each element of 'D' by 'beta', and then accumulate the product of elements from 'tmp' and 'C', with a reduction parallel factor of 4 for the update to 'D'. Indicate the region of the code where these kernels should be optimized using '#pragma scop' and '#pragma endscop'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function 'kernel_2mm' as a hardware acceleration kernel. Inside this function, treat the outermost loops iterating over 'i' with default pipelining, tiling, and parallization, while the inner loops over 'j' are pipelined and parallelized with a reduction factor applied. Specifically, in the first block of loops, prevent automatic pipelining, tiling, and parallelization on the 'i' loop, while allowing default pipelining and tiling on the 'j' loop with a parallelization factor of 10 enabled explicitly for the 'k' loop with reduction on 'tmp'. In the second block of loops, pipeline the 'i' loop with all nested loops flattened into it, tile with a factor of 1, and set the parallelization factor to 20 for the 'j' loop while not parallelizing the 'k' loop, applying reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel where you treat the function as a dedicated hardware module. For the outer two loops, disable automatic pipelining, prevent tiling and parallelism optimization (running all iterations in series). Within the first nested loop, apply a parallel reduction on the temporary matrix with full iteration space. In the second set of nested loops, allow the inner loop to be executed in parallel for 32 iterations but perform reductions on the output matrix sequentially. Specifically:\n\n1. Define the function as a hardware kernel to be accelerated.\n2. For the first-level loop `i`, turn off automatic pipelining, set the tiling factor to 1 so the entire loop runs in a single tile, and prevent parallel execution by setting the parallel factor to 1.\n3. Repeat step 2 for the second-level loop `j` within the first loop nest.\n4. Apply a parallel reduction pragma for the `k` loop within the first nested `j` loop, ensuring all 70 iterations are considered for the reduction operation.\n5. Again, apply similar pragmas as step 2 to the first-level loop `i` in the second loop nest.\n6. For the second-level loop `j` within the second loop nest, enable loop flattening for pipelining while leaving the tiling factor at 1 and set the parallel factor to 32 to process 32 iterations simultaneously.\n7. For the third-level loop `k` within the second nested `j` loop, ensure that the reduction is performed sequentially by setting the parallel reduction factor to 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with corresponding pragmas, use the instruction:\n\n\"Define a hardware-accelerated kernel function named kernel_2mm that takes matrix dimensions, scalar values, and four 2D arrays as inputs. Within the kernel function, calculate the matrix multiplication and addition D = alpha * A * B * C + beta * D. Keep the outermost loops for both computations from being automatically pipelined, but flatten the innermost loops. Do not tile or parallelize the loops automatically, except in the second computation's j-loop, where you should automatically parallelize across 20 iterations. Ensure all reduction operations inside the loops are done in parallel without any automatic factor.\" \n\nHere are the specific directives:\n\n- `#pragma ACCEL kernel`: Treat the following function as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}`: Don't automatically pipeline this loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}`: Process this loop in its original, full form without breaking it into smaller chunks.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Run the iterations of this loop one after the other, not in parallel.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`: Perform reduction in parallel on the tmp array, without applying an automatic factor.\n- `#pragma ACCEL PIPELINE auto{flatten}`: Flatten the nested loops for this computation within the pipeline.\n- `#pragma ACCEL PARALLEL FACTOR=auto{20}`: Parallelize the iterations of this loop with a factor of 20, allowing 20 iterations to be processed in parallel.\n- `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`: Perform reduction in parallel on the D array without applying an automatic factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following high-level synthesis (HLS) directives and their implementation into a single natural language instruction:\n\n- Treat 'kernel_2mm' as a hardware kernel to be accelerated.\n- For the outer loops (first and second instances) over indices 'i' and 'j':\n  - Do not automatically pipeline these loops.\n  - Execute the loops without tiling (no break down into smaller chunks).\n  - Perform the loop iterations in a sequential order without parallelization.\n\n- For the loop calculating 'tmp[i][j]':\n  - Automatically determine if pipelining should be applied.\n  - Use tiling as specified\u2014here, no tiling is to be applied, actually.\n  - Proceed without parallelizing this loop.\n  - In the innermost loop over 'k', add up values to 'tmp[i][j]' with the possibility of parallel execution but without specifying the parallelization factor (leaving it to automatic discovery), while employing a reduction to handle possible race conditions.\n\n- For the loop updating 'D[i][j]':\n  - Do not automatically pipeline the outermost loop.\n  - For the 'j' loop:\n    - Allow pipelining and flatten any nested loops inside to optimize performance.\n    - Do not tile the loop.\n    - Parallelize the 'j' loop iterations with a specified factor of 5.\n  - For the innermost loop over 'k', perform the summation to update 'D[i][j]' in sequential order without employing parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the subsequent function as a hardware acceleration kernel, execute the entire primary loop without pipelining, tiling, or parallelizing, and within this loop, perform the same for the secondary loop. Within the innermost loop where 'tmp' is updated, parallelize across a full iteration range with a cumulative effect on 'tmp'. Then, sequentially pipeline the next main loop iteration without applying tiling. Nevertheless, for the following secondary loop, allow limited parallelism by a factor of 4 for the outer iterations, and within its inner loop, apply parallel reduction with a grouping factor of 2 for updating 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes you provided, the following comprehensive but very concise natural language instructions are used:\n\n1. Treat the 'kernel_2mm' function as a dedicated hardware function for acceleration purposes.\n2. For the outer loop of the 'alpha*A*B' matrix multiplication, prevent pipelining, keep the loop in its entirety, and ensure sequential execution of its iterations.\n3. Inside the first nested loop, flatten the pipeline stages allowing increased chances of parallelism within, while also maintaining the loop's full extent and sequential iteration execution.\n4. In the innermost loop of the first matrix multiplication, enable a reduction operation on the 'tmp' array, mandating that the loop executes sequentially.\n5. For the outer loop of the 'alpha*A*B*C + beta*D' computation, allow the compiler to infer the optimal pipelining strategy, but again, prohibit loop tiling and enforce sequential execution of iterations.\n6. In the second nested loop, apply the same flattening to potential pipeline stages, keep the loop in one piece, but allow the processing of four iterations in parallel for improved throughput.\n7. Finally, in the innermost loop of the second part of the computation, establish a reduction on the 'D' array but maintain a sequential processing policy for its iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function with the following characteristics:\n\n1. The function should be identified as a hardware acceleration kernel.\n2. The entire function should not be pipelined, preserving the full loop structure.\n3. Loops within the function should be kept in their original form, with iterations executed sequentially.\n4. Implement a matrix-multiplication and addition sequence `D := alpha*A*B*C + beta*D`, with the initial multiplication `A*B` stored in a temporary matrix.\n5. In the first loop nest, use a fixed parallelization factor for the innermost loop with a reduction on the temporary matrix.\n6. In the second loop nest, pipeline the outer loop without modifying the loop structure, and for the innermost loop, apply a specific parallelization factor with a reduction on the `D` matrix.\n\nSpecifically, you should use:\n\n- `#pragma ACCEL kernel` to mark the function as an acceleration kernel.\n- `#pragma ACCEL PIPELINE \"off\"` to prevent pipelining of the entire function.\n- `#pragma ACCEL TILE FACTOR=1` to execute loop iterations in their original order without tiling.\n- `#pragma ACCEL PARALLEL FACTOR=1` to run loop iterations sequentially.\n- For the first matrix multiplication, enable loop pipelining, set the tiling factor to 1, and use a parallelization factor of 16 for the reduction on the `tmp` matrix.\n- For the addition of the multiplied matrix to the scaled `D` matrix, pipeline the outer loop and flatten its iterations, keep the tiling factor to 1, and use a parallelization factor of 20 for parallel execution of the innermost loop without reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, use the following natural language instruction:\n\n\"Treat the function 'kernel_2mm' as a hardware kernel for acceleration but prevent any automatic pipelining and tiling optimizations by the compiler for the outer loops; maintain the original loop structure and serialization. Within the nested loop where 'tmp' array is being calculated, facilitate the compiler to apply pipelining and consider serial execution for the innermost loop without any parallel reduction for 'tmp'. For the subsequent nested loops updating 'D', enable pipelining for the intermediate loop but explicitly parallelize the innermost loop to run 50 iterations simultaneously alongside specifying a parallel reduction for 'D'.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function to be optimized for hardware acceleration. Within the function, maintain the full sequence of iterations without automatic pipelining for the outer loop 'i' in the multiplication of 'A' and 'B', keeping the loop in its complete form and executing iterations sequentially. For inner loops 'j' and 'k' in the same section, allow compiler-determined pipelining but specify no tiling or parallelization improvements. When computing the 'tmp' product, enable parallel accumulation with a factor determined by the compiler, up to 16 elements at a time.\n\nNext, for updating matrix 'D', automate pipelining with flattened hierarchy for the second 'i' loop, only allowing the concurrent execution of eight 'j' loops. Within this stage, execute the 'k' loop iterations one after the other, with no concurrent or compound operation optimizations when adding the 'tmp' and 'C' product to 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function 'kernel_2mm' as a hardware acceleration kernel. Within it, disable automatic pipelining, maintain original loop size, and prevent parallel loop execution. For the first nested loop set, utilize the natural loop form, only allow serial execution and enforce this for innermost loops with reductions. For the second nested loop set, allow up to eight parallel executions in the middle loop while retaining the original loop size; increase the innermost loop's parallelism factor for reduction on 'D' to 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel and, within it, suppress auto-pipelining, disable loop tiling, and perform loop iterations serially for the loops managing the alpha * A * B matrix multiplication, while for the update to matrix D, enable auto-pipelining, set the tiling factor to 2, and parallelize with a factor of 8, and for the innermost loop using the 'reduction' on D, allow up to 50 parallel instances.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to optimize the provided C function `kernel_2mm`, which performs a matrix-matrix multiplication followed by an addition, as a hardware kernel without enabling automatic pipelining, tiling, or parallelization for the top-level loops. Additionally, specify the automatic application of parallel reduction for specific loop iterations using different parallel factors while disabling tiling and pipelining in some inner loops, and enabling them with specified factors in others.\n\nHere are the specific instructions for each segment of the code:\n\n1. Treat the function `kernel_2mm` as a hardware kernel to be accelerated.\n\n2. For the first two nested loops where `i` and `j` are the loop variables, do not use automatic pipelining, tiling, or parallelism, thus preserving the original flow and structure of the loops.\n\n3. In the third nested loop, the one where `k` iterates over `70`, automatically parallelize the reduction operation on the `tmp` array with the unspecified factor set by the HLS tool.\n\n4. For the final two nested loops where `i` and `j` are the loop variables, avoid automatic pipelining while setting the tiling factor to `4` for the `j` loop and automatically determining the parallel factor (with an unspecified value) for the same loop.\n\n5. In the innermost loop, where `k` iterates over `50`, apply an automatic parallel reduction on the `D` array with the unspecified factor set by the HLS tool.\n\nTo summarize, here are the pragma directives you requested to describe in natural language, matched with concise instructions:\n\n- `#pragma ACCEL kernel`: Accelerate the function as a hardware kernel.\n- `#pragma ACCEL PIPELINE auto{off}`: Disable automatic loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{1}`: Disable automatic loop tiling (maintain original loop structure).\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Disable automatic loop parallelization (execute iterations sequentially).\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}`: Parallelize the reduction on `tmp` with an automatic factor.\n- `#pragma ACCEL PIPELINE auto{off}`: (Repeat) Disable automatic loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{4}`: Set the tiling factor for the loop to `4`.\n- `#pragma ACCEL PARALLEL FACTOR=auto{16}`: Automatically determine the parallel factor for the loop.\n- `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}`: Parallelize the reduction on `D` with an automatic factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function that treats loops with their natural execution order, using the most granular form of iteration without parallelization or pipelining, except for specific loops where parallelism is applied at different factors. For the outermost loops, disable automatic pipelining. For the first inner dimension's pipeline, completely flatten the loop to merge nested loops. Apply a parallelism factor of 1 with reduction for the accumulation across the 'k' dimension when calculating the 'tmp' matrix. When updating matrix 'D', use the default pipeline settings for the 'i' dimension, but enable a parallelism factor of 2 for the 'j' dimension and a factor of 5 with reduction for the 'k' dimension.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Inside the function, disable pipelining for the outer loops but leave the directive's exact implementation up to the compiler. Keep all outer loops intact by specifying the tile size as one, effectively preventing loop unrolling or tiling. Enforce serial execution of the outer loop iterations.\n\nWithin the 'tmp' calculation loop, the innermost 'k' loop should perform partial parallelism with a hint to the compiler to consider a factor, but ultimately let the compiler decide the degree of parallelism (although I suggest a factor of 4).\n\nIn the 'D' matrix update loop, allow the compiler to decide whether to pipeline the loop or not without a specified factor. Set the tile size to one for the 'j' loop, ensuring no loop tiling or unrolling is applied, and advise the compiler to employ advanced parallelism for the inner loop with an arbitrary suggested parallel factor of 8.\n\nFor both innermost loops that involve reductions (updates to 'tmp' and 'D'), use the reduction clause with 'tmp' and 'D', respectively, to enable parallel reduction operations. Specifically hint to the compiler with suggested parallel factors of 4 for 'tmp' updates and 50 for 'D' updates, but ultimately let the compiler control the final level of parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given 2D matrix-multiply-accumulate (2mm) algorithm into a high-level synthesis (HLS) kernel, ensuring no automatic optimization techniques such as pipelining and loop tiling are applied to the first set of nested loops, but allow automatic pipelining in the second set of nested loops. In the reduction loops inside these nested loops, parallelize the iterations with a factor of 4 for the first reduction (alpha * A * B) and a factor of 50 for the second reduction (tmp * C), respectively. Treat both reductions as combinational logic circuits where intermediate sums are stored in parallel. Additionally, specify that the outer loops for 'i' and 'j' should run sequentially without any concurrency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a function named 'kernel_2mm' designed for hardware acceleration by marking it with a kernel pragma. This function will compute a 2mm kernel, which consists of two matrix multiplication steps with the addition of a scalar multiplied matrix and a scalar multiplied update to a result matrix.\n\nIn the first loop nest, avoid using any form of loop pipelining, tiling, or parallelization enhancements for the outer loop 'i' by setting all pragma values regarding pipeline, tile, and parallel to turn off automation or use the default behavior 'auto{off}', 'auto{1}', and 'auto{1}' respectively. For nested loops 'j' and 'k', flatten the pipeline pragma to optimize all nested loops as a single pipeline, but retain the rest of the settings as specified for the outer loop.\n\nWithin the second loop nest, still, refrain from pipelining for the outer loop 'i', setting pragma values accordingly to the default or turned off, while allowing the inner loop 'j' to have an automatically optimized pipeline. Also, for the nested loop 'j', set the parallel factor pragma to 'auto{2}' to hint at potential parallelization.\n\nFor the deepest loop 'k' in both loop nests, allow for the automatic parallel reduction with a pragma specifying the corresponding reduction variable 'tmp' or 'D' and setting the parallel factor to 'auto{1}' and 'auto{4}' respectively to encourage data aggregation and fine-tune potential concurrency. These pragmas will try to optimize the loop execution without fully parallel execution as the factor's value is low for the first 'k' loop or optimized with a higher parallelization factor for the second 'k' loop.\n\nTo summarize, the instructions for the pragmas are as follows:\n\n1. Treat the entire function as a hardware acceleration kernel.\n2. In the first loop nest:\n    a. Do not pipeline the outer loop 'i'.\n    b. Do not tile the outer loop 'i'.\n    c. Do not parallelize the outer loop 'i'.\n    d. Flatten pipelining for the inner loop 'j', but do not tile or parallelize it.\n    e. Set automatic parallel reduction for the deepest loop 'k' with a factor of 1.\n3. In the second loop nest:\n    a. Do not pipeline the outer loop 'i'.\n    b. Do not tile the outer loop 'i'.\n    c. Do not parallelize the outer loop 'i'.\n    d. Allow automatic pipelining for the inner loop 'j' but only tile it with the default factor.\n    e. Set automatic parallel reduction for the deepest loop 'k' with a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with specific #pragma directives, you should instruct the code generator as follows:\n\nIdentify the 'kernel_2mm' function as a hardware acceleration kernel. Disable automatic pipelining for the entire function. Leave both outer loops unoptimized regarding tiling and parallelism by processing them in a straightforward sequential manner. For the innermost loop within the first double-nested loop, flatten the loop to enhance pipelining without further parallelism or tiling optimizations. Retain serial execution for accumulation by disabling parallel reductions for the temporary array. In contrast, enable pipelining without specifying whether to flatten within the second double-nested loop. Implement tiling with a factor of 2 and parallelize the innermost loop with a factor of 8. Finally, process the reduction for the 'D' array with a high degree of parallelism, utilizing a parallel factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive but very concise instruction to generate the HLS code you provided:\n\n\"Create a hardware accelerated kernel function named `kernel_2mm` that calculates the matrix expression `D := alpha * A * B * C + beta * D` with input matrices A, B, C, and D of respective sizes 40x70, 70x50, 50x80, and 40x80. Initialize all elements of the temporary matrix `tmp` with size 40x50 to zero before the first computation. For the first nested loop, prevent automatic pipelining, tiling, and parallel processing, but do flatten the pipeline in the innermost loop and allow reduction for temporary accumulator without parallelism. For the second nested loop structure, disable pipelining and tiling in the outer loops, but set parallelism to factor 4 for the innermost loop and permit reduction while paralleling the update to the D matrix with the same parallelism factor.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS C code to implement a 2mm kernel function with specific optimization directives for hardware acceleration. The function should take matrix dimensions `ni`, `nj`, `nk`, `nl`, scalar values `alpha` and `beta`, and double-precision floating-point matrices `tmp`, `A`, `B`, `C`, and `D`. The core computation is `D := alpha*A*B*C + beta*D`. \n\nFor the outermost loops iterating over the `i` index of `D` and `tmp`, disable automatic pipelining, set both tile and parallelization factors to 1 to prevent loop unrolling or split execution. Inside these loops, enable pipelining without specifying a factor, but ensure the loop retains its full iteration range by setting the tile factor to 1 with no parallel factor specified for the innermost loop.\n\nIn the first nested loop that computes the `tmp` matrix, pipeline the loop with a default factor and set parallel processing for the reduction operation on `tmp` with a factor of 1.\n\nIn the second nested loop for updating the `D` matrix:\n\n1. Enable pipelining without specifying a factor.\n2. Set the inner loop tile factor to 8.\n3. Use a parallel factor of 2 for the loop iterating over the `j` index.\n4. Apply a reduction operation on `D` with a parallel factor of 16 in the deepest loop.\n\nThe `kernel_2mm` function should be treated as a hardware kernel for acceleration purposes. Ensure to include appropriate pragmas to reflect these specifications.\n\nRemember to use `#pragma ACCEL kernel` at the beginning of the function to indicate hardware acceleration, and wrap the core computation inside `#pragma scop` and `#pragma endscop` directives to define the region of interest for the HLS tool's optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, use the following comprehensive but concise natural language instruction:\n\n\"Create a hardware acceleration kernel for a 2mm computation algorithm that performs matrix multiplication and addition without pipelining or parallelizing the outermost loops, but allows for parallel reduction on the innermost loops with a specific parallel factor for different loop levels.\"\n\nSpecifically:\n\n1. Declare the matrix multiplication function as a kernel for hardware acceleration.\n2. For the first set of nested loops calculating `tmp`, disable automatic pipelining for each loop, and ensure that the outer loops run sequentially without tiling or parallelism. Permit a parallel factor of 2 for the innermost `j` loop and set up reduction for `tmp` within the `k` loop with a default factor.\n3. For the second set of nested loops updating matrix `D`, disable automatic pipelining for the `i` loop, allow a tile factor of 8 for the `j` loop, and establish a parallel factor of 1 for the corresponding loops. Implement a parallel reduction for `D` in the innermost `k` loop with a factor of 50.\n\nTranslate these directives using the respective #pragma values as follows:\n\n- `#pragma ACCEL kernel` for the function declaration.\n- `#pragma ACCEL PIPELINE auto{off}` for disabling automatic pipelining on each affected loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to indicate no tiling on the outer loops.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to ensure the outermost loops run without parallelization and `#pragma ACCEL PARALLEL FACTOR=auto{2}` for a parallel factor of 2 on the inner `j` loop in the first set, while `#pragma ACCEL TILE FACTOR=auto{8}` allows a tile factor of 8 on the `j` loop of the second set.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}` to specify parallel reduction for `tmp` and `D` with factors according to their respective inner loops.\n\nThis instruction will ensure the generation of HLS codes that keep the matrix computation loops in a form that runs the iterations in a controlled manner with specific optimizations on inner nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and then manage the execution of its nested loops as follows: \n\n1. Turn off automatic pipelining for the outermost loop, ensuring it executes sequentially without pipelining.\n2. Disable loop tiling for the outermost loop, maintain natural loop bounds.\n3. Inhibit parallelism for the outermost loop, making iterations execute consecutively.\n\nFor the middle loop:\n\n4. Prevent automatic pipelining, ensuring the loop runs in a straightforward, non-pipelined fashion.\n5. Avoid partitioning the loop into multiple tiled loops, preserving the loop as it is.\n6. Forbid concurrent execution of the middle loop iterations.\n\nFor the innermost loop within the first double loop:\n\n7. Activate parallel reduction across iterations on the temporary array 'tmp' with a full unrolling based on the loop's bound.\n\nFor the second double loop:\n\n8. Allow automatic pipelining without specifying an off-state, potentially enabling it if the compiler finds it suitable.\n9. Do not tile the outer loop of this segment, keeping its iteration space intact.\n10. Enforce consecutive execution for the outer part of the second double loop by setting parallel factor to 1.\n\nFor the middle loop of the second double loop:\n\n11. Disable the pipelining of iterations to execute in the original loop order.\n12. Opt out of tiling this loop as well, ensuring original iteration space is preserved.\n13. Set a specific parallel factor that allows some concurrent loop execution, but with controlled parallelism at a factor of 5.\n\nFor the innermost loop of the second double loop:\n\n14. Use parallel reduction for the 'D' array while processing this loop to allow simultaneous accumulation of computed values. The loop is unrolled based on the given bound for efficient reduction.\n\nThe specified #pragma statements should be used respectively for each of the points above with the 'auto{}' syntax reflecting compiler discretion while the 'auto{off}' and 'auto{1}' suggesting specific pragma parameters as guided by the instructions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function `kernel_2mm` as a hardware acceleration kernel, but disable automatic pipelining, tiling, and parallel processing for the outer loops while setting the reduction for the innermost loops, specifying the reduction variable and full parallelism for the `k` loops.\n\nFor the initial matrix multiplication, set the reduction variable to `tmp` and use a parallel factor of the maximum matrix dimension being reduced, in this case, 70. For the second matrix multiplication that updates matrix `D`, set the reduction variable to `D` and likewise use a parallel factor corresponding to the dimension being reduced, which is 50. Also, manually adjust the parallel factor for the second `j` loop to 5.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Mark the following function as a hardware acceleration kernel, disable automatic pipelining for the first-level loop, process all loops in their complete iterations without tiling, execute the first two loop nests sequentially with single iteration at a time, use default pipelining for the nested loops within the first loop nest while marking one of them as a parallel loop with a parallel factor of 2, allowing reductions on the temporary array; in the second loop nest, flatten the pipeline, execute the innermost loop with maximal parallelism with a parallel factor of 32, and specify the outermost loop of this block to run in series with a parallel factor of 1, using default settings otherwise. Here is a concise natural language instruction that could generate the given HLS codes:\n\nDefine `kernel_2mm` as a hardware acceleration kernel, turn off pipelining for the top-level loop, set tiling to non-fragmented, full-form processing for all loops, and run all loop iterations sequentially except for those within the first and second nested loop levels where you use default pipelining and a parallelism factor of 2 for the inner loops, with a special emphasis on paralyzing reductions to the `tmp` array. In the second major loop section, default pipelining settings apply with the loop structure flattened, while the innermost loop runs with a parallelism factor of 32, and reductions on array `D` are specified without additional parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, provide the following natural language instructions for each respective pragma and part of the code:\n\n1. \"Identify the entire function 'kernel_2mm' as a component to be accelerated in hardware.\"\n   - Maps to: `#pragma ACCEL kernel`\n\n2. \"Within the function, treat both outer loops (for 'i' and the second for 'i') as non-pipelined to execute in a simple sequential manner.\"\n   - Maps to (the first occurrence of): `#pragma ACCEL PIPELINE auto{off}`\n   - Maps to (the second occurrence of): `#pragma ACCEL PIPELINE auto{off}`\n\n3. \"Ensure the compact structure of the loops is preserved without applying tiling optimization.\"\n   - Maps to (all occurrences of): `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. \"Do not parallelize the loops; maintain the single-thread flow for each loop without data partitioning.\"\n   - Maps to (all occurrences of): `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n5. \"For the first inner loop (for 'j'), completely unroll or flatten nested loops if any, to aid in potential compiler optimizations.\"\n   - Maps to (the first occurrence of): `#pragma ACCEL PIPELINE auto{flatten}`\n   - Maps to (the second occurrence of): `#pragma ACCEL PIPELINE auto{flatten}`\n\n6. \"While computing the 'tmp' matrix, make sure the accumulation of 'tmp[i][j]' is done sequentially to avoid data hazards.\"\n   - Maps to: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`\n\n7. \"In the second inner loop (for 'j'), allow the compiler to explore parallelizing the loop at a factor of 8, aiming to exploit concurrency and increase throughput.\"\n   - Maps to: `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n\n8. \"In the final accumulation step for 'D[i][j]', handle the reduction in a sequential manner to maintain correct ordering and result.\"\n   - Maps to: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`\n\nThis set of instructions should guide the code generator in creating the HLS code with the specified pragma directives for the 2mm kernel operation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS C code for the kernel_2mm function, use these instructions:\n\n1. Declare the function as a hardware acceleration kernel.\n2. For the first set of nested loops, enforce no automatic pipelining for the outer loop.\n3. For the inner loop computing `tmp[i][j]`, set the tile factor to the original loop bound and parallelize the reduction operation over `tmp` with a factor of 4.\n4. For the second set of nested loops updating `D[i][j]`, disable automatic pipelining for the outer loop as before.\n5. Within the inner loop, set the tile factor to the original loop bound and parallelize the `D` update with a reduction operation, selecting a parallel factor of 50.\n\nHere's how to describe the specific #pragma directives:\n\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{off}`\n- Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Automatically pipeline this loop, with the specific configuration handled by the tool: `#pragma ACCEL PIPELINE auto{}`\n- Parallelize this loop with a reduction on `tmp`, specifying a factor of 4: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}`\n- Parallelize this loop with a reduction on `D`, specifying a higher factor of 50: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}`\n\nEnsure that you use these instructions within the appropriate scope of the loops as shown in the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2-matrix multiplication kernel function, `kernel_2mm`, that performs the computation `D = alpha*A*B*C + beta*D` for given matrix dimensions. The function arguments are the sizes `ni`, `nj`, `nk`, `nl`, scalar values `alpha`, `beta`, and matrices `A`, `B`, `C`, `D`, `tmp`. You should designate the function as a hardware kernel for acceleration and manage loop optimizations as follows:\n\n1. For the outermost loops, disable automatic pipelining and ensure that they are not tiled or parallelized beyond a single iteration at a time.\n2. For the first inner loop calculating `tmp`, enable automatic pipelining without specification but keep tiling and parallelism factors at their original, full loops without breaking them into smaller chunks or parallel execution.\n3. Within this loop, enable parallelism with a reduction on `tmp` but set to run one iteration at a time.\n4. For the second outermost loop updating matrix `D`, turn off pipelining and ensure single-iteration tiling and parallel processing for the top loop, and for the inner loop, use a flattened pipelining approach with a parallel factor of 32, while maintaining single-iteration parallel execution for the innermost loop with reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Within this function, do not pipeline the outermost loops with indices 'i' and 'j', ensuring they execute sequentially in full without optimization. For the innermost loops, disable any tiling optimizations, but allow the loop with the reduction on 'tmp' to compute elements in parallel with a full partitioning of the reduction variable, aiming for 70 parallel computations. Regarding the final loop nests, flatten the pipeline for the middle loop with index 'j' while allowing parallel execution with a limited factor of 5 for the middle loop, with the innermost loop completely sequential. Use 'off' for no pipelining or tiling, '1' for no partitioning or running iterations in series, 'flatten' for collapsing nested loops, and '70'/'5' for specific parallel factors for respective loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware acceleration kernel. Inside the function, for all loops:\n\n1. Disable automatic pipelining.\n2. Preserve the full loop form using a tiling factor of one.\n3. Execute iterations serially by setting the parallel factor to one.\n\nFor the innermost loop of the first double-nested loop:\n\n1. Flatten the loop during pipelining.\n2. Set the tiling factor to one.\n3. Execute iterations serially and enable reduction on the 'tmp' array, specifying a parallel factor of one.\n\nFor the innermost loop of the second double-nested loop:\n\n1. Set the tiling factor to two.\n2. Set the parallel factor to eight for the middle loop and thirty-two for the innermost loop, enabling reduction on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function called `kernel_adi` that is optimized for hardware acceleration; treat this function as a hardware kernel. Within this function, initialize six constant variables `DX`, `DY`, `DT`, `B1`, `B2`, `mul1`, and `mul2` based on provided formulas, and compute six variables `a`, `b`, `c`, `d`, `e`, and `f` from these constants.\n\nDisable automatic pipelining, tiling, and parallel processing for the outermost loop that iterates over `tsteps`, the time steps, and also disable them for the first nested loops for columns and rows that iterate over spatial dimensions, ensuring full iterations are executed sequentially.\n\nFor the innermost loops that update `p` and `q` in the column sweep, enable parallel processing with a factor of 32 while keeping the loop body as is and sequentially iterating to ensure dependencies are respected.\n\nFor the backward iterations of `v` and `u` in the column and row sweeps, execute iterations one by one without parallelism, retaining the inverted loop index calculation by reversing the original decrementing loop and using a temporary index variable.\n\nFor the innermost loops within the row sweep that update `p` and `q`, enable parallel processing with a factor of 8.\n\nSpecific #pragma values to use:\n\n1. Treat the `kernel_adi` function as a hardware kernel: `#pragma ACCEL kernel`\n2. Disable automatic pipelining, tiling, and parallel processing for the outer loops: `#pragma ACCEL PIPELINE off`, `#pragma ACCEL TILE FACTOR=1`, `#pragma ACCEL PARALLEL FACTOR=1`\n3. Enable parallel processing with a factor of 32 for column update loops: `#pragma ACCEL PARALLEL FACTOR=32`\n4. Execute backward iterations of `v` sequentially: `#pragma ACCEL PARALLEL FACTOR=1`\n5. Enable parallel processing with a factor of 8 for row update loops: `#pragma ACCEL PARALLEL FACTOR=8`", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following C function as a hardware accelerator kernel, and implement its loops with varying degrees of optimization directives. Specifically, the first `i-j` nested loop should execute without automatic pipelining or tiling, and with the innermost `k` loop running partial parallel operations while accumulating results for `tmp`. The second `i-j` nested loop should be pipelined and parallelized with specific factor values for tiling, pipelining, and parallel execution that are set or avoided altogether to maintain the original loop structure for certain loops while optimizing others. \n\nFor the first set of nested loops: \n- Disable automatic pipelining for the outermost `i` loop.\n- Preserve the original loop structure without tiling for both `i` and `j` loops.\n- Enforce sequential execution of `i` and `j` loop iterations.\n- Enable partial parallel processing for the innermost `k` loop with a factor that maximizes performance but limits resource usage, using a parallel factor and a reduction clause on `tmp`.\n\nFor the second set of nested loops: \n- Pipeline the outermost `i` loop without specifying whether to enable or disable it automatically, allowing it to be determined by the compiler.\n- Set the tiling factor for the `i` loop to divide it into chunks of a size that unrolls the loop partially.\n- Define a parallel factor to create multiple processing elements for the `j` loop, processing multiple iterations concurrently while ensuring it does not fully parallelize the loop.\n- Parallelize the innermost `k` loop with a suitable factor to increase processing through partial unrolling and a reduction clause on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` as a hardware acceleration kernel. Disable automatic pipelining for the entire function, while inside each nested loop pair allow the inner loop to be analyzed for automatic pipelining optimization. Do not allow the loop to be tiled or parallelized beyond running each iteration sequentially except for the innermost loops where a customized parallel reduction is applied: for the first innermost loop over `k`, set the parallel factor to `2` explicitly, and for the second one leave it at `1` but still specify a reduction on `D`. Finally, in the second outer loop pair over `i` and `j`, automatically flatten nested loops into a single loop with a parallelization factor of `32` for the outer loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes with specific pragmas, please use the following natural language instruction:\n\nTransform the given function 'kernel_2mm' into a hardware-accelerated kernel that operates on two-dimensional arrays A, B, C, D, and tmp, performing matrix multiplication and addition according to the formula `D = alpha * A * B * C + beta * D`. For all outer loops, disable automatic pipelining, avoid tiling, and execute loop iterations sequentially. For the first multiplication loop, permit limited parallel execution with a factor of 4 and enable a reduction clause for the temporary array. For the second addition loop, default to pipelining optimizations, allow sequential execution with tiling disabled, and parallelize the innermost loop with a very high factor for large-scale parallel accumulation into the D array employing a reduction clause.\n\nSpecifically include these directives as comments:\n\n- Treat the function 'kernel_2mm' as a standalone hardware acceleration kernel.\n- For the first level of loops, prevent pipelining, tile factor reduction to 1, and set parallelism factor to 1 to maintain original loop order.\n- In the inner loop for calculating the tmp matrix, disable pipelining, enforce no tiling (factor 1), and restrict parallelism to 1 to keep original order. However, for the innermost k loop, enable a parallelism factor of 4 and specify a reduction on the tmp array.\n- For the subsequent loop updating the D matrix, allow pipelining to happen according to compiler optimization, set tile factor to 1, and for the j loop parallelism factor to 4. In the innermost loop for final matrix adjustment, apply a substantial parallelism factor of 50 with a reduction on the D array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the instruction: \"Implement a kernel function named 'kernel_2mm' for matrix computation without pipelining, tiling, or parallel processing optimizations at the top loops, but use pipelining optimization for the nested loops. For the first level of nested loops, keep the computations serialized, while for the second level of nested loops, introduce a moderate level of parallelism with a factor of five. Ensure that data dependencies in the reduction of matrices 'tmp' and 'D' are respected by not parallelizing these operations.\"\n\nFor the specified `#pragma` values:\n\n- Treat the `kernel_2mm` function as a hardware kernel for acceleration, using `#pragma ACCEL kernel`.\n- Indicate that the outermost loops should not be automatically pipelined by setting `#pragma ACCEL PIPELINE auto{off}`.\n- Specify that the outer loops should run in their full form without tiling using `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Ensure that the outer loops' iterations run sequentially by setting `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Within the first level of nested loops, allow the innermost loop to be completely unrolled or flattened with `#pragma ACCEL PIPELINE auto{flatten}` while keeping it serialized and not parallelizing the associated reductions with `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`.\n- Apply the same pipeline flattening to the second level of nested loops with `#pragma ACCEL PIPELINE auto{flatten}` and introduce a parallel factor of five for the iterations of the inner loop using `#pragma ACCEL PARALLEL FACTOR=auto{5}`, while keeping the reduction for matrix 'D' serialized with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following algorithmic operation into C code using HLS pragmas to specify the hardware acceleration directives:\n\nDefine a kernel function called `kernel_2mm` that performs a matrix multiplication and addition. The function takes in six scalar arguments\u2014`ni`, `nj`, `nk`, `nl`, `alpha`, and `beta`\u2014and six two-dimensional arrays\u2014`tmp`, `A`, `B`, `C`, and `D`\u2014with varying dimensions as parameters.\n\nMark this function as a hardware acceleration kernel. Then, conduct the first operation, `D := alpha*A*B*C + beta*D`, by executing two nested loops:\n\n1. For the first loop iteration over the `tmp` array's rows, turn off automatic pipelining, set the tile and parallel factors each to their original, full form. Inside this loop, for each row, flatten the inner loop's pipelining when iterating over the `tmp` array's columns. Again, set the tile and parallel factors to their full form. For the innermost loop over `k`, perform a parallel reduction on `tmp`, ensuring that iterations run one after the other, not in parallel.\n\n2. For the second set of nested loops modifying the `D` array, carry forward the same directives from the first loop: no automatic pipelining, tile factor of one, and turn off parallelism, using the original, full iteration form for the outermost loop. Within the inner loop, where `D` is modified, stop automatic pipelining, set the tile factor to one, but allow two iterations to run in parallel.\n\nIn each innermost loop, update the respective array elements: `tmp[i][j]` accumulates the product of `alpha`, `A[i][k]`, and `B[k][j]`, while `D[i][j]` accumulates the product of `tmp[i][k]` and `C[k][j]` along with the scaled value `beta * D[i][j]`. Make sure that the reduction operation in both innermost loops is set to run in non-parallel mode.\n\nThe pragmas to be used should include `#pragma ACCEL kernel` to define the kernel, `#pragma ACCEL PIPELINE auto{off}` to prevent automatic pipelining, `#pragma ACCEL TILE FACTOR=auto{1}` to specify no tiling, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` or `#pragma ACCEL PARALLEL FACTOR=auto{2}` to control the parallel execution factor. For the loop containing parallel reductions, indicate the variable being reduced and set the factor accordingly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function `kernel_adi` that operates over a 60x60 grid for a certain number of time steps. This kernel performs an Alternating Direction Implicit solver with initialization constants and a column and row sweep computation.\n\n- Annotate the function with a pragma indicating it's a hardware kernel.\n- Inside the function, declare variables and assign initialization constants outside any loop. Include DX, DY, DT for grid spacing and timestep, B1, B2 for equation constants, and a through f for coefficients derived from these constants.\n- For the main time-stepping loop (up to 40 iterations), annotate with pipelining, tiling, and parallel pragmas configured to avoid automatic optimization, ensuring the loop executes sequentially without parallelism or tiling \u2013 essentially leaving the loop unoptimized.\n- For both the column and row sweep calculations, create nested loops iterating over the grid rows and columns.\n  - The innermost column loop must be parallelized with a factor of 2, and the subsequent reverse loop should be sequential.\n  - For the row sweep section, the innermost loop should be parallelized with a factor of 29, and the reverse loop with a factor of 32.\n- Both column and row sweep outer loops should also have pragmas to turn off automatic pipelining and tiling, and to execute sequentially.\n- Ensure the loops operate on the grid's interior by restricting the index range, i.e., from 1 to 58 for zero-based indexing, avoiding the boundary.\n- Initialize the boundary conditions at the grid edges to 1.0.\n- Account for reverse loops with a pragma parallelized using the specified factors, translating decrementing for-loops into equivalent incrementing forms.\n\nHere are the specific pragma values to use:\n- Treat the function as a hardware kernel: `#pragma ACCEL kernel`\n- Do not pipeline loops automatically: `#pragma ACCEL PIPELINE auto{off}`\n- Do not break loops into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Do not run iterations of loops in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Run selected loop iterations in parallel with a factor of 2 for the column reverse loop: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n- Run selected loop iterations in parallel with a factor of 29 for the row sweep: `#pragma ACCEL PARALLEL FACTOR=auto{29}`\n- Run selected loop iterations in parallel with a factor of 32 for the row reverse loop: `#pragma ACCEL PARALLEL FACTOR=auto{32}`\n\nThe instructions provided ensure the generated HLS C code closely reflects the structure expected by the given task.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, disable automatic pipelining globally, and enforce non-parallel execution of the outermost two loops with their original structure while enabling loop pipelining within the inner two loops. For the middle loop, allow a degree of parallel processing of two iterations concurrently. Additionally, specify a degree of parallelism of 32 for the second outer loop's inner parallel loop while maintaining reduction clauses for accumulation in the target arrays. Use the default settings for loop tiling.\n\nHere's a breakdown of the specific #pragma directives to use for each instruction:\n\n- '#pragma ACCEL kernel' to indicate that the function is a hardware acceleration kernel.\n- '#pragma ACCEL PIPELINE auto{off}' to disable pipelining globally for the function.\n- '#pragma ACCEL TILE FACTOR=auto{1}' for the loops where no loop tiling is to be applied, maintaining the original loop structure.\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' for the loops where no parallelism is allowed, thereby enforcing sequential execution.\n- '#pragma ACCEL PIPELINE auto{}' to enable pipelining within nested loops where there are no explicit settings.\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}' to enable a limited degree of parallel processing, two iterations at a time, within a specific loop.\n- '#pragma ACCEL PARALLEL reduction = array_name FACTOR=auto{70}' or '#pragma ACCEL PARALLEL reduction = array_name FACTOR=auto{10}' to apply parallelism with a reduction clause specific to the respective array (`tmp` or `D`), with the respective factor (70 or 10).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, automatically optimize loop pipelining and tiling, but do not pipeline or tile the outermost loops, keep the innermost calculation of `tmp` sequential, and perform the calculation of `D` by splitting it into 8 parallel tasks without pipelining.\n\nMore specifically: \n- Use the 'kernel' pragma to classify the `kernel_2mm` function as a hardware kernel suitable for acceleration.\n- Set loop pipelining to automatic optimization, but explicitly turn it off (`off`) for the outer loops that iterate over `i`.\n- Keep the tiling factor as the default (do not tile these loops) by setting it to 'auto' with a specified default value of `1`.\n- Set the parallel factor to 'auto' with a specified default value of `1` for all loops except for the outer loop that iterates over `j` when calculating `D`, where you want to set the parallel factor to `8` to process eight iterations concurrently.\n- For both innermost loops, use a 'parallel' pragma with a reduction clause on `tmp` and `D` to enable efficient parallel reduction, with the factor set to 'auto' with a specified value of `1` to maintain sequential processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For the loops within the kernel:\n1. Keep the outermost loops unaltered and prevent them from being automatically pipelined.\n2. For the compute-intensive nested loops, the first level should be pipelined by default.\n3. For the innermost loops that are performing computation on 'tmp' and 'D', allow parallelism at a specific factor, 5 for 'tmp' and 50 for 'D', to optimize the accumulation.\n\nHere are the respective instructions to generate the HLS codes:\n\n- \"Treat the following function as a hardware kernel for acceleration\" for `#pragma ACCEL kernel`.\n- \"Don't automatically pipeline these outer loops\" for `#pragma ACCEL PIPELINE auto{off}` at the start of the first and third outer loop levels.\n- \"Keep these loops in their full form without dividing them\" for `#pragma ACCEL TILE FACTOR=auto{1}` used in the outer and second level loops.\n- \"Run the iterations of these loops one after the other, not in parallel\" for `#pragma ACCEL PARALLEL FACTOR=auto{1}` used in the outer and second level loops.\n- \"Automatically pipeline this loop at its default behavior\" for `#pragma ACCEL PIPELINE auto{}` used in the second level loops.\n- \"Allow parallel execution of this loop with a factor of 5 for computing 'tmp'\" for `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}` in the innermost loop computing 'tmp'.\n- \"Allow parallel execution of this loop with a factor of 50 for computing 'D'\" for `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}` in the innermost loop computing 'D'. \n\nYou can use these natural language instructions to generate the provided HLS codes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes for kernel_2mm in C, here's a natural language instruction encapsulating the directives and the process:\n\n\"Define a hardware acceleration kernel function for a 2mm computation that initially disables automatic pipelining, tiling, and parallel processing for the entire function. Inside the nested loops for matrix multiplication, turn off automatic pipelining, set tiling and parallel factors to their minimum values, and enable parallel reduction on the innermost loop with a full reduction factor to combine the products. For the second major loop section where the matrix D is updated, enable automatic pipelining, apply a tile factor of 2 and a moderate parallel factor of 16 for the outer loop, and set a parallel reduction with a factor of 10 for the innermost loop to accumulate results into matrix D without pipelining.\"\n\nHere are the associated #pragma values for each directive:\n\n1. Treat the function `kernel_2mm` as a hardware kernel: `#pragma ACCEL kernel`\n2. Avoid automatic pipelining for the loops: `#pragma ACCEL PIPELINE auto{off}`\n3. Process the loops without tiling (full loops): `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Execute loops serially (no parallel chunks): `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n5. For the innermost loop in the first major section, use all available compute units for parallel reduction of array `tmp`: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{70}`\n6. For the outer loop in the second major section, turn pipelining back on (automatic), double the size of each tile processed, and allow the loop to be divided into 16 parallel chunks: `#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL TILE FACTOR=auto{2}`, `#pragma ACCEL PARALLEL FACTOR=auto{16}`\n7. For the innermost loop of the second section, enable parallel reduction with a factor of 10 to cooperatively sum into array `D`: `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{10}`\n\nNote that `auto{}` often allows the HLS tool to infer the appropriate setting, but the `{off}` or `{value}` explicitly instructs the HLS tool to disable or set a specific factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS code for a 2mm computation kernel without automatic pipelining or tiling for loops, treat the function as a hardware acceleration kernel, and enable parallel processing with specific factors where appropriate. The first two nested loops must have their outer iterations serialized and any potential optimization automatically turned off. In the innermost loop of the initial double-nested loop, allow the accumulation in 'tmp' to be processed in parallel with a factor of 16. The second double-nested loop's outer iterations are pipelined with no automatic optimization decisions. Within this second loop block, the innermost 'D' update should loop serially, while its outer loop should process eight iterations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive but very concise natural language instruction based on the provided HLS C code and corresponding pragmas:\n\n\"Define a block for hardware acceleration and perform a two-matrix multiply-add computation on inputs A, B, C, and D with scaling factors alpha and beta. For the first loop nest that computes the product of matrices A and B, disable pipeline optimization, retain the original loop structure without tiling or parallel execution enhancements, but allow for a reduction in the innermost loop with full parallelism across the loop's range. In the second loop nest, where the product of the temporary matrix and matrix C is accumulated into D with scaling, enable pipeline with loop flattening, set a tiling factor of 4 for the middle loop, parallelize the innermost loop with a parallelism factor of 20, and force the innermost loop to execute sequentially.\"\n\nSpecific #pragma values to use are as follows:\n\n- Treat the function `kernel_2mm` as a hardware kernel: `#pragma ACCEL kernel`\n- For the first loop nest:\n  - Disable pipeline optimization: `#pragma ACCEL PIPELINE auto{off}`\n  - Retain original loop structure without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n  - Execute iterations sequentially without parallel enhancements: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n  - For the innermost loop of the first nest, enable full parallelism for reduction: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}`\n- For the second loop nest:\n  - Enable pipeline optimization with loop flattening: `#pragma ACCEL PIPELINE auto{flatten}`\n  - Set the tiling factor to 4: `#pragma ACCEL TILE FACTOR=auto{4}`\n  - Parallelize the outer loop with a parallelism factor of 20: `#pragma ACCEL PARALLEL FACTOR=auto{20}`\n  - Force the innermost loop to execute without parallelism: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware kernel function `kernel_2mm` that performs a matrix multiplication and accumulation, where the matrix multiplication `D` is set as alpha times `A` times `B` times `C`, incremented by beta times `D`. The directives should:\n\n1. Treat the complete `kernel_2mm` function as a hardware acceleration kernel.\n2. For the outermost loop, skip automatic pipelining.\n3. Ensure the outermost loop maintains its full iteration space without tiling.\n4. Ensure the execution of the outermost loop's iterations is not parallelized.\n5. For the first nested loop inside the outermost loop (`i` loop), enable automatic pipelining.\n6. Prevent tiling and parallelization within the second nested loop (`j` loop) and within the processing of the `tmp` array computation.\n7. In the computation of the `tmp` array, perform a reduction operation with a custom parallelization factor of 4 to partially parallelize the summation (the `k` loop).\n8. Enable automatic pipelining and prevent tiling and parallelization within the computation of matrix `D` update after the accumulation.\n9. Within this computation block, use a parallelization factor of 8 in the outer loop (`j` loop) and 50 for the reduction operation within the inner loop (`k` loop) while accumulating the `tmp` array and `C` matrix product into `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code, use the following natural language instructions:\n\nDesignate the function \"kernel_2mm\" as a dedicated hardware acceleration kernel. Prevent automatic pipelining of all loops within the kernel. For the first level loop indexed by 'i', iterate without parallel execution and without tiling the loop; this means no splitting of the iteration space nor concurrent execution of its iterations.\n\nWithin the nested 'j' loop in the first section, also disable automatic pipelining. Set a parallel execution factor of 2, allowing two iterations to run simultaneously but without tiling. While computing the 'tmp' array within the 'k' loop, enable parallel reduction on 'tmp' with two iterations running together.\n\nIn the second set of nested loops for updating 'D', first, disable automatic pipelining at the 'i' loop but then flatten the loop nest at the 'j' loop to allow potential merging or reorganization of loop levels. Apply tiling with a factor of 4 to partition the 'j' loop, and within this loop, run iterations with a parallel factor of 20.\n\nFinally, during the reduction of 'D' in the 'k' loop, disable parallelism by setting the parallel factor to 1. \n\nTo reiterate the #pragma values: \n- '{off}' is to be used with PIPELINE to turn off auto pipelining.\n- '{1}' is to be used with TILE FACTOR for single iteration execution without tiling.\n- '{auto}' paired with different numbers to set specific parallel factors and tile sizes.\n- 'reduction' is to be used with PARALLEL to specify the reduction operation on an array.\n- '{flatten}' applied to PIPELINE to potentially enable loop nest reorganization.\n\nPlease note that the 'auto' syntax used in conjunction with pragma values is a placeholder indicating that there might be an automated suggestion by the HLS tool, and the actual pragma might require a specific value or might be set to determine the optimal value automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, you need to provide the following comprehensive but concise natural language instruction:\n\n\"Optimize the `kernel_2mm` function, which computes the matrix-matrix operation `D = alpha * A * B * C + beta * D`, for hardware acceleration. The matrices A, B, C, D, and tmp are two-dimensional arrays with specified dimensions, and alpha and beta are scalar values. Declare `kernel_2mm` as a hardware kernel. For the first two nested loops that compute `tmp = alpha * A * B`, disable automatic pipelining, enforce the entire loop structure without tiling, and do not parallelize beyond single iterations. Enable pipelining and parallelism within the innermost loop with a parallel reduction on `tmp`.\n\nFor the second set of nested loops that update `D` with `D = beta * D + tmp * C`, also disable automatic pipelining and enforce non-tiled execution for the outer loops but set up the middle loop to be pipelined and the innermost loop for parallel reduction on `D` with expanded parallelism.\n\nBe specific about the following #pragma values for the loops:\n1. For the top-level loops, use `#pragma ACCEL PIPELINE \"off\"`, `#pragma ACCEL TILE FACTOR=1`, and `#pragma ACCEL PARALLEL FACTOR=1` implying no pipelining, no tiling, and no parallelism.\n2. For the inner loop calculating `tmp`, auto-enable pipelining and set `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`.\n3. For the middle loop updating `D`, again, auto-enable pipelining, but this time use `#pragma ACCEL TILE FACTOR=auto{2}` to double loop iterations without pipelining intra-iterations, and `#pragma ACCEL PARALLEL FACTOR=auto{8}` allowing some parallelism.\n4. Finally, for the innermost loop updating `D`, set the parallel reduction with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}`, permitting a high degree of parallelism.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration candidate, for the first loop duo, disable automatic pipelining and perform iterations sequentially without tiling or parallelism, but allow the innermost loop to run four iterations in parallel with reduction on the 'tmp' array. For the second loop duo, again disable automatic pipelining, perform iterations sequentially without tiling, and permit four parallel iterations in the middle loop; within, allow up to fifty iterations to execute in parallel with reduction on the 'D' array. Here's how you should set the #pragma values:\n\n- Treat 'kernel_2mm' as a hardware kernel: `#pragma ACCEL kernel`\n- For the first set of nested loops:\n  - Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n  - Prevent loop tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n  - Disable parallel execution: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n  - In the innermost loop (over 'k'):\n    - Perform four operations in parallel with a reduction on 'tmp': `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}`\n- For the second set of nested loops:\n  - Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n  - Prevent loop tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n  - For the middle loop (over 'j'):\n    - Allow four operations to execute in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n  - In the innermost loop (over 'k'):\n    - Allow up to fifty operations in parallel with a reduction on 'D': `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided High-Level Synthesis (HLS) C code with corresponding pragmas, you can use the following instruction:\n\n\"Treat the 'kernel_2mm' function as a hardware acceleration kernel, and for the two main for-loops within this function: first, disable automatic pipelining and tiling, and execute all iterations sequentially; then, in the inner-most loop, perform a reduction on the 'tmp' or 'D' arrays without any partitioning. In the second for-loop, allow the outer loop iterations for 'j' to be partially parallelized with a factor of 4, and fully parallelize the inner-most loop's 'k' iterations with aggressive unrolling up to 50 iterations at once for the reduction on 'D', while keeping the loop for 'i' and the first loop for 'j' without parallelism and pipelining.\"\n\nHere's a direct mapping of the instructions to the respective #pragma directives in the code:\n\n1. `#pragma ACCEL kernel` - Treat the following function 'kernel_2mm' as a hardware kernel for acceleration.\n2. All instances of `#pragma ACCEL PIPELINE auto{off}` - Don't automatically pipeline the respective loop.\n3. All instances of `#pragma ACCEL TILE FACTOR=auto{1}` - Process the respective loop in its original, full form without breaking it into smaller chunks (`FACTOR=1` means no tiling).\n4. The `#pragma ACCEL PARALLEL FACTOR=auto{1}` after the first outer for-loop - Run the iterations of the subsequent loop one after the other, not in parallel.\n5. `#pragma ACCEL PARALLEL FACTOR=auto{4}` for the second main for-loop's 'j' iterator - Enable the outer loop to partially run in parallel with a factor of 4.\n6. `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}` - Parallelize the reduction operation on the 'D' array with aggressive unrolling for 50 iterations in parallel.\n7. `#pragma ACCEL PIPELINE auto{flatten}` - Flatten nested loops within into a single loop level for pipelining purposes, avoiding this in the given code by setting pipeline off.\n8. `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` - Perform the reduction operation on the 'tmp' array without partitioning, running fully in sequence.\n\nNote: In the above code, any `auto{}` syntax followed by a specific directive (like `off`, `1`, `4`, `50`, `flatten`) implies that the specified directive is manually set, overriding the compiler's automatic optimization choices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a 2mm kernel function that:\n\n1. Treats the function as a hardware kernel for optimization.\n2. Executes the first level `i` loop (of the initial matrix multiplication) without any pipelining, tiling, or parallelizing optimizations, running iterations in their original sequence.\n3. Executes the second level `j` loop (inside the first matrix multiplication) just like the first, with no pipelining, tiling, or parallelizing.\n4. Parallelizes the innermost `k` loop (of the first matrix multiplication) but forces the reduction variable `tmp` to remain sequential, with a default parallel factor.\n5. Applies default pipelining optimizations to the first level `i` loop (of the second matrix multiplication) but does not tile or parallelize it.\n6. Pipelines and tiles the second level `j` loop (inside the second matrix multiplication) with aggressive tiling by a factor of 4 and intense parallelism by a factor of 20, and also flattens the nested loops to possibly further improve parallelism.\n7. For the innermost `k` loop (of the second matrix multiplication), disables parallelism by using the minimal factor while allowing for sequential reduction on variable `D`.\n\nAdd pragmas as follows:\n\n- Treat the `kernel_2mm` function as a hardware acceleration kernel using `#pragma ACCEL kernel`.\n- For loops that shouldn't be pipelined, specify `#pragma ACCEL PIPELINE auto{off}`.\n- For loops that should not be tiled, use `#pragma ACCEL TILE FACTOR=auto{1}`.\n- For loops that should not be parallelized, apply `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- To parallelize the reduction on `tmp`, use `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}`.\n- For the second matrix multiplication, pipeline the `i` loop by default with `#pragma ACCEL PIPELINE auto{}`.\n- To aggressively pipeline and tile the `j` loop in the second multiplication, use `#pragma ACCEL PIPELINE auto{flatten}` and `#pragma ACCEL TILE FACTOR=auto{4}` with an intense parallelism `#pragma ACCEL PARALLEL FACTOR=auto{20}`.\n- To disable parallelism in the innermost `k` loop of the second multiplication, use `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an algorithm using High-Level Synthesis (HLS) C code for accelerating the Alternating Direction Implicit (ADI) method with the following specifics:\n\n1. Declare the function `kernel_adi` as a dedicated hardware kernel to accelerate the ADI computations, which consists of both column sweep and row sweep operations over a 2D grid.\n\n2. Disable automatic pipelining optimization for the top-level loop that iterates through the time steps. This ensures that the loop runs with its original latency without pipelining the iterations.\n\n3. Implement the top-level loop iterations and the inner loops for column and row sweeps as single blocks without tiling, ensuring that the loops do not get divided into smaller tiled loops.\n\n4. Execute all parallelizable loops with their iterations running sequentially one after the other, effectively disabling automatic parallelization.\n\n5. In the column sweep section, flatten any loop nest within the pipelined region, that is, treat nested loops as one large loop to optimize loop control overhead.\n\n6. For the backward substitution loop within the row sweep, explicitly set the parallelization factor to 29 to indicate that 29 iterations of the loop can be processed in parallel.\n\n7. Also, for the backward substitution loop within the column sweep, explicitly set the parallelization factor to 4, to review hardware resources against potential gains in parallel execution.\n\nAssign pragma directives to the respective loops as follows:\n\n- Apply `#pragma ACCEL kernel` to the `kernel_adi` function.\n- Apply `#pragma ACCEL PIPELINE \"off\"` for the top-level loop to disable piping.\n- Apply `#pragma ACCEL TILE FACTOR=1` to maintain the original loop layout.\n- Apply `#pragma ACCEL PARALLEL FACTOR=1` to execute loops sequentially.\n- Apply `#pragma ACCEL PIPELINE FLATTEN` within the column sweep to flatten the nested loops.\n- Apply `#pragma ACCEL PARALLEL FACTOR=29` for parallelism in the row sweep's backward substitution loop.\n- Apply `#pragma ACCEL PARALLEL FACTOR=4` for parallelism in the column sweep's backward substitution loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a function `kernel_2mm` that serves as a hardware kernel to accelerate the computation of D = alpha*A*B*C + beta*D. \n\n1. Do not apply any automatic pipelining for the initial for-loops that iterate over `i` and `j`, ensuring that the outer loop computations are fully completed before the next iteration begins.\n2. Avoid tiling or breaking the outer loops into smaller chunks by setting the tiling factor to its minimum of 1.\n3. Ensure the loops run sequentially by setting the parallel factor to 1, so the loop iterations are not parallelized.\n4. In the nested inner loop over `k` that computes `tmp`, apply pipelining with a flattened iteration space to improve loop execution speed without loop nest partitioning.\n5. Use parallel reduction on `tmp` with a parallel factor of 1 to handle the alpha*A*B multiplication and summation in a way that avoids race conditions.\n6. For the second set of loops that iterate over `i` and `j`, and calculate the final result `D`, enable automatic pipelining for the inner loop iterations but specify a tiling factor of 2 to balance the computational load by partially breaking the loop into chunks.\n7. Set the parallel factor to 8 to allow for eight-fold parallel execution in the loop that computes the scaling of `D` by `beta`.\n8. Finally, apply a high degree of parallelization with a parallel factor of 50 for the accumulation loop that involves `D` and `tmp` with a C, signaling that 50 iterations of the loop can be performed in parallel, provided the hardware resources are available.\n\nThe pragma directives reflect these instructions, with \"auto{off}\", \"auto{1}\", \"auto{flatten}\", \"auto{2}\", and \"auto{8}\" indicating specific behaviors regarding pipelining, tiling, and parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C with specific pragma instructions, use the following natural language instruction: \n\n\"Define a hardware kernel function called 'kernel_2mm' that computes a matrix multiplication and addition. The matrix `D` should be updated as the result of `alpha` times the matrix multiplication of `A`, `B`, and `C`, added to `beta` times `D`. Within the outermost two nested loops, completely disable automatic pipelining and do not perform any loop tiling or parallelism optimizations. In the third nested loop, which computes the matrix multiplication into `tmp`, enable parallelism with a factor of 1 and specify a reduction on `tmp`, while keeping the loops in their original full form. For the final nested loops, where `D` is updated, enable a minimal loop tiling factor of 2 and a more aggressive parallelism factor of 5 on the second outer loop, and apply a high parallelism factor of 32 with reduction on `D` for the innermost loop.\" \n\nNow, match this instruction with the pragma values:\n\n- `#pragma ACCEL kernel` indicates that the function `kernel_2mm` is a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` specifies that loop pipelining should be turned off.\n- `#pragma ACCEL TILE FACTOR=auto{1}` sets the loop tiling factor to 1, effectively specifying no tiling should be performed.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` sets the loop parallelism factor to 1, indicating no parallel processing should be performed within that loop.\n- `#pragma ACCEL PIPELINE auto{flatten}` is used to flatten the nested loops and effectively disable pipelining in the given context.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` specifies that parallelization should be applied with a factor of 1, with a reduction operation applied to the `tmp` array.\n- The absence of `auto{}` after `#pragma ACCEL PIPELINE` in the second outer loop section might imply the default pipelining is to be applied.\n- `#pragma ACCEL TILE FACTOR=auto{2}` in the second outer loop section sets the loop tiling factor to 2.\n- `#pragma ACCEL PARALLEL FACTOR=auto{5}` in the second outer loop section sets the loop parallelism factor to 5 in this specific loop.\n- `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}` in the innermost loop of the second section sets a high parallelism factor of 32 with a reduction operation on the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
