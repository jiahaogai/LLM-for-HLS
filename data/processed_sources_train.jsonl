{"instruction": "Generate HLS code with the following instructions:", "input": "Designate this function for hardware acceleration ensuring all loops will consider using automated pipeline optimizations, automated tiling for splitting iterations, and automatic parallelization factor detection where appropriate. Apply a parallel reduction strategy on the innermost loops that accumulate results into `tmp` and `D` arrays, using automated factors for the parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS-optimized hardware kernel for accelerating a 2mm computation, where all loops are to be automatically optimized for pipelining, tiling, and parallel execution. Apply automatic parallel reduction for intermediate accumulations in arrays 'tmp' and 'D'. Use dynamic automatic pragma factors for all optimization directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given matrix multiplication and addition algorithm into an FPGA-accelerated kernel function with automated optimization directives guiding the HLS tool to determine the best pipeline, tile, and parallel processing strategies for the loops within the function, while handling the reduction of partial results for specific intermediate arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Identify the function intended for hardware acceleration and automatically optimize the loops for parallel execution, while allowing the compiler to decide the exact degree of pipelining, tiling, and parallel execution for each loop, and enable the reduction for accumulations on the `tmp` and `D` arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel to accelerate the 2mm algorithm. Automate pipelining, tiling, and parallelization for all loops and enable parallel reduction for the innermost loops where temporary arrays are updated. Use automatic optimization parameters determined at compile-time for all pragmas.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create HLS C code for a matrix multiplication and addition kernel function named 'kernel_2mm' that performs `D = alpha*A*B + beta*D`, targeting hardware acceleration. Set up appropriate pragmas to automate pipelining, tiling, and parallel processing, with all loop optimization factors automatically determined. Also, include parallel reduction on temporary arrays 'tmp' and 'D' within nested loop blocks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and within it, automatically optimize the loops for pipelining, tiling, and parallel execution, letting the compiler decide the optimal factors. Specifically, implement reduction to accumulate values in the 'tmp' and 'D' multi-dimensional arrays while performing matrix multiplication and addition.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration target. Automatically optimize the nested loops within the function for pipelining, tiling, and parallel processing, allowing the compiler to determine the appropriate factors. Utilize parallel reduction within the innermost loops to accumulate results efficiently for the matrices 'tmp' and 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Identify and optimize the function 'kernel_2mm' as a hardware acceleration kernel, allowing the compiler to automatically determine the most efficient pipelining, tiling, and parallelization strategies for each loop nest while aggregating intermediate results with appropriate reduction operations, enhancing overall performance and resource usage in the hardware design.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function to accelerate on hardware. Enable automatic optimization for loop pipelining, loop tiling, and parallel loop execution without specifying values, allowing the tool to determine the optimal factors. For the innermost loop that accumulates values to the `tmp` array, enable parallelism with automatic factor determination and specify the array `tmp` for reduction to handle concurrent updates correctly. Use the same parallelism strategy for the innermost loop that updates the `D` array, with reduction specified for `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated kernel for a 2mm matrix multiplication and addition operation where the loop nests are optimized for parallel execution and pipelining, with compiler-determined factors of tiling, pipelining, and parallelism, including the reduction pattern for temporary accumulation within the inner loops. Specifically, ensure that the outermost loops for the initial multiplication and final addition are each pipeline-optimized, tiled, and have parallel factors determined by the compiler, along with the inner loops that perform the actual computation. The reductions within the innermost loops should be parallelized with the accumulation variable specified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, allowing the HLS tool to apply optimization. Automatically optimize the loops for pipelining, loop tiling, and parallel processing, adjusting the parameters for these optimizations dynamically based on the loop's characteristics. In the matrix multiplication and addition segments, reduce the partial sums 'tmp' and 'D' in parallel for improved computational efficiency. Use the directive placeholders __PIPE__, __TILE__, and __PARA__ to indicate where the HLS tool should insert specific values for pipelining depth, tiling factor, and parallelism factor, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function called 'kernel_2mm' as a hardware acceleration kernel to compute the mathematical operation D := alpha * A * B + beta * D, where D, A, B, and tmp are 2-dimensional double-precision floating-point matrices with given dimensions, and alpha and beta are scalar multipliers.\n\nWithin this function, apply automatic optimization techniques for pipelining, tiling, and parallelizing the three nested loop constructs responsible for the matrix operations. Ensure that intermediate accumulations of matrix elements in temp and D during the matrix multiplications are correctly handled in parallel using reduction clauses. Automatically determine the optimization factors for pipeline stages, tile sizes, and parallel execution widths to achieve optimization without manual tuning.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration candidate, and for each of the nested loops within, enable automatic optimization for pipelining, tiling, and parallel execution. Additionally, incorporate parallel reduction for temporary arrays within the innermost loops to facilitate efficient accumulation operations. Use pragma directives with automatic parameters determined at compilation time to adjust the optimization factors for pipelining, tiling, and parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel; automatically optimize loop pipelining, tiling, and parallel processing for all loops, and allow for automatic reduction parallelization for specific sum operations within the given matrix multiplication and addition algorithm.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the 'kernel_2mm' function for hardware acceleration by automatically configuring the pipeline, tile, and parallelization pragmas at each nest level to enhance performance while maintaining the computation of `D := alpha*A*B*C + beta*D`. Use pragma commands to flag the entire function for hardware acceleration, automatically tune loop pipelining, tiling, and parallel factors, and enable parallel reduction for interim results in matrix multiplications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a target for hardware acceleration. Implement all loops with automatic pipelining for parallel execution, with the system determining the pipelining, tiling, and parallelism factors based on hardware constraints and optimization opportunities. For the innermost loops, apply a parallel reduction on the 'tmp' and 'D' arrays respectively to accumulate values efficiently. Use automatic optimization metrics for all directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 14\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function 'kernel_2mm' as a hardware accelerator kernel. Automate the pipelining, tiling, and parallelization strategies for the top-level loops that iterate over the first and second dimensions, respectively, for matrices involved in the 2mm computation. Additionally, enable automatic parallel reduction for the deepest loops that perform the multiplication and accumulation for 'tmp' and 'D' matrices. Each loop should have its pipelining, tiling, and parallelization pragmas automatically determined, marked with placeholders '__PIPE__', '__TILE__', and '__PARA__' corresponding to each nested loop level. Adjust the parallel reduction for the 'tmp' and 'D' arrays to utilize the optimization features effectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and optimize all nested loops within the `kernel_2mm` function using automatic pipeline, tiling, and parallelization pragmas where applicable. Additionally, employ automatic parallel reduction on specific accumulation operations within the innermost loops to enhance performance. Use the appropriate pragma directives to indicate these optimizations, ensuring that the compiler decides the optimal factors for pipelining, tiling, and parallelization, as well as to specify reduction variables where reduction operations take place.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, then dynamically optimize the pipelining, tiling, and parallelism of the nested loops for matrix operations involved in the calculation of the equation `D := alpha * A * B * C + beta * D`. The outer loops iterating over the dimensions of 'ni' and 'nj' should be automatically optimized for execution, while the inner loops iterating over 'nk' and subsequently over 'nj' (second time) should employ automatic parallel reduction for the accumulation in 'tmp' and 'D' arrays, with pragmas set to adjust based on compiler analysis.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel and optimize the nested loops for performance using pipelining, tiling, and parallelization while allowing the compiler to automatically determine the best factors for each. Furthermore, enable parallel reduction for array elements `tmp` and `D` within their respective innermost loops to optimize the computations of matrix multiplications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following hardware accelerator code with automatic optimization directives: Enable the function 'kernel_2mm' as a hardware kernel. For each of the nested loops, apply the following optimizations automatically: use pipelining to potentially improve the loop execution time, tile the loops to partition the iterations into smaller chunks, and set up parallel execution of loop iterations with an automatic factor. Additionally, facilitate the reduction operation on arrays 'tmp' and 'D' within their innermost loops to enable concurrent accumulation. Use automatic factors for all parallel, pipeline, and tile optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the `kernel_2mm` function by treating it as a hardware kernel, where within this function, the first nested loop calculates the matrix multiplication of `A` and `B` and adds the result to `tmp` after scaling by `alpha`. The second nested loop scales the matrix `D` by `beta` and then adds the product of `tmp` and `C`. Utilize automatic optimization directives for loop pipelining, tiling, and parallelization with factors determined by the compiler for all loops. Apply automatic data reduction for the parallel directives in the innermost loops for accumulating values in `tmp` and `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware-accelerated kernel. Allow for automatic optimization of the two nested loop blocks responsible for matrix computations through directive hints. In the first loop block, automate the pipelining, tiling, and parallelization of the outer `i` loop with dynamic factors, while the inner parallel reduction on `tmp` within the `k` loop should also be automatically parallelized with a dynamic factor. Similarly, in the second loop block, automate the pipelining, tiling, and parallelization of the outer `i` loop. Moreover, the inner parallel reduction on `D` within the `k` loop should have automatic parallelization enabled with a dynamic factor. The specific pragma values are abstracted and indicated by 'auto' followed by placeholders like `__PIPE__L0`, `__TILE__L0`, `__PARA__L0`, and so on, for each loop's optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function for 2D matrix multiplication and addition, enabling automatic optimization with directives to improve parallelism, pipelining, and tiling for each nested loop.\n\n1. Begin by designating the function `kernel_2mm` as a hardware acceleration kernel.\n2. In the first pair of nested loops, automate pipelining, tiling, and parallelization for the outer loop (`i` loop) to optimize performance dynamically.\n3. Within the innermost loop (`k` loop) that multiplies and accumulates arrays `A` and `B` into `tmp`, enable parallel reduction on `tmp` with a dynamic parallelization factor, allowing concurrent execution and accumulation.\n4. For the second pair of nested loops, automate pipelining, tiling, and parallelization for the outer loop (`i` loop) as previously done.\n5. In the innermost loop (`k` loop) that multiplies and adds array `C` with the already scaled `D` matrix, enable parallel reduction on `D` with a dynamic parallelization factor to merge results efficiently.\n\nEach `#pragma ACCEL` directive should be set to 'auto' for the compiler to inherently decide the best optimization values, expressed as `{__PIPE__Lx}`, `{__TILE__Lx}`, and `{__PARA__Lx}` where `x` denotes the loop level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following function into a hardware-accelerated kernel, allowing the compiler to automatically decide how to best optimize the pipelining, tiling, and parallel execution of loops. Additionally, allow parallel reduction on accumulations within nested loops to promote faster computations. Use pragma directives to achieve these optimizations:\n\n1. Declare the function as a hardware acceleration kernel.\n2. Enable automatic optimization of pipeline, tiling, and parallel factor for each loop level; let the compiler infer the most efficient settings.\n3. For accumulation loops, enable parallel reduction to merge partial results concurrently, while letting the compiler choose the optimal parallel factor.\n\nNote: The placeholders `{__PIPE__L0}`, `{__TILE__L0}`, `{__PARA__L0}`, etc., should be replaced with compiler-inferred values for pipeline depth, tile factor, and parallel factor, respectively, at each loop level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a dedicated hardware acceleration kernel and automatically adjust the loop pipelining, tiling, and parallelism pragmas for maximum performance while performing matrix operations according to the specific computation: `D = alpha * A * B * C + beta * D`. Use pragma statements that:\n\n1. Apply automatic loop pipelining where HLS decides the pipeline initiation interval.\n2. Select automatic loop tiling factor for the best memory access pattern and usage.\n3. Choose automatic parallel execution factor for loop iterations to maximize parallelism.\n4. Use parallel pragma with reduction to ensure proper accumulation of intermediate values in the `tmp` and `D` matrices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the given matrix multiplication and accumulation function kernel_2mm for hardware acceleration using C pragmas. Specifically, declare the function as a hardware kernel, enable automatic pipelining, tiling, and parallelization for all loops with their respective factors set to be determined automatically by the compiler, and use parallel reduction for the innermost loops to accumulate the results efficiently with the factors also set to automatic.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Allow the HLS tool to automatically determine the optimal pipelining, tiling, and parallelizing strategies for each nested loop in the matrix multiplication and addition operations, and specify reductions where accumulation occurs.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware-accelerated kernel designed for matrix multiplication and addition following these optimization directives: For each nested loop layer, automatically adjust the pipelining, tiling, and parallel processing optimizations. Achieve parallel accumulation for intermediate results within the innermost loops. Utilize automatic optimization factors for each directive, adjusting them for all loops individually to balance the resource usage and performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 16\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, and within it, apply optimized loop transformations automatically for three nested loops. For the first pair of nested loops, dynamically optimize data pipelining, tiling, and parallel processing for multiplying matrices A and B, with an iterating reduction on the temporary matrix. For the second pair of nested loops, apply the same dynamic optimizations to the element-wise multiplication of the temporary matrix and matrix C, accumulating the results in matrix D, with a prior scaling by beta for each element of matrix D. Use automated pragma factor values for all loop optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the specified function as a hardware acceleration kernel, enable automatic optimization for pipeline, tiling, and parallel factors for all loops, and apply parallel reduction for the innermost loops where temporary matrices are being accumulated:\n\n- Tag the function 'kernel_2mm' as a hardware acceleration kernel.\n- Automate pipeline optimization for all primary loops involved in the matrix calculations.\n- Automate tile factor optimization for all primary and secondary loops to govern loop unrolling and subsystem partitioning.\n- Automate parallel factor optimization for all primary and secondary loops to enhance parallelism opportunities.\n- For the innermost loops, where the 'tmp' and 'D' matrices are being updated, enable parallel reduction to optimize the associative accumulation stages, delegating the optimization order and factor determination to the compiler's heuristics.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel and apply optimization strategies to its loops. Specifically, automatically determine the pipelining, tiling, and parallel processing settings for each loop to enhance parallelism and reduce latency. Additionally, perform parallel reduction on temporary accumulations to optimize associative operations within inner loops.\n\n- Treat 'kernel_2mm' as a hardware acceleration kernel.\n- For the first loop nest over 'i' and 'j', automatically optimize pipeline, tile, and parallel processing. Specifically for the innermost 'k' loop, enable parallel reduction on 'tmp' to optimize the multiplication and accumulation.\n- For the second loop nest over 'i' and 'j', automatically optimize pipeline, tile, and parallel processing. Specifically for the innermost 'k' loop, enable parallel reduction on 'D' to optimize the multiply-accumulate operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, with automated optimization suggestions for loop pipelining, loop tiling, and parallelization. For each loop level, request the HLS tool to automatically determine the best pipeline settings, tile sizes, and parallelization factors. Additionally, for loops that aggregate values into an array, enable parallel reduction with automatic factor determination for the arrays `tmp` and `D` to optimize their associated operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Designate the function 'kernel_2mm' as a hardware acceleration target. Within this function, allow the first-level 'i' loops and the second-level 'j' loops to be automatically pipelined and parallelized, as well as tiled for performance optimization. In addition, automatically apply parallel reduction to the innermost 'k' loops for the variables 'tmp' and 'D' to improve accumulation operations, with the exact parallelization factors to be determined automatically by the tool.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Enable hardware acceleration for the function 'kernel_2mm' that computes the algebraic equation 'D := alpha * A * B * C + beta * D'. Automatically optimize pipelining, tiling, and parallel processing for the loops, with an emphasis on parallel reduction for the innermost loops that accumulate into 'tmp' and 'D' matrices. Apply the corresponding pragmas for automatic optimization to each loop nest, and ensure no pragma directive is set to disable optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given C code into high-level synthesis (HLS) directives for FPGA acceleration:\n\nDefine the function `kernel_2mm` as a hardware kernel, using the directive `#pragma ACCEL kernel` to accelerate the computation of the 2mm algorithm on FPGA. Apply automatic pipelining, tiling, and parallelism determination to the outer and inner loops to enhance loop execution through FPGA optimization.\n\nFor both the calculation of matrix `tmp` and the update of matrix `D`, employ the `#pragma ACCEL PIPELINE auto`, `#pragma ACCEL TILE FACTOR=auto`, and `#pragma ACCEL PARALLEL FACTOR=auto` pragmas to allow the HLS tool to automatically optimize the pipelining depth, tiling factor, and parallelism factor, respectively, for each loop nest.\n\nWithin the innermost loops, enable parallel reduction on the `tmp` and `D` arrays with the directive `#pragma ACCEL PARALLEL reduction=[array name] FACTOR=auto`, instructing the HLS tool to automatically parallelize the reduction operations on these arrays while deriving the optimal factor for parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware kernel for acceleration to perform the matrix-matrix multiplication and addition operation. Implement a tiled, pipelined, and parallelized approach for the loops within the kernel to optimize for parallel execution, where the compiler automatically determines the optimal factor for tiling, pipeline initiation interval, and parallel execution without specifying them manually.\n\nFor the loop computing `tmp[i][j]`, enable pipeline optimization, automatic tiling for data locality, and parallelization with automatic factor determination, and use a reduction pragma for accumulating the product of `alpha`, `A[i][k]`, and `B[k][j]` on `tmp[i][j]`. For the loop updating `D[i][j]`, apply the same pipelining, tiling, and parallelization strategies, and include a parallel reduction on `D[i][j]` when accumulating the product of `tmp[i][k]` and `C[k][j]`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. Allow the compiler to automatically decide the optimization strategies for pipelining, tiling, and parallelizing the loops. Within each loop nest, enable automatic pipelining, tiling, and parallel execution, while also specifying reduction for the appropriate accumulator variables `tmp` and `D`. Use automatic factors for all these optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for optimizing a 2mm algorithm involving matrix multiplication and addition with automatic pipelining, tiling, and parallelization. The algorithm initializes a temporary matrix 'tmp' with the product of matrices 'A' and 'B,' scaled by 'alpha.' This is followed by the scaling of matrix 'D' by 'beta' and then adding the product of 'tmp' and 'C'. Each loop should be automatically optimized: the outer loops 'i' for initializing 'tmp' and scaling 'D' have their optimization levels determined at runtime; the inner loops 'j' also use automatic optimization for pipelining, tiling, and parallelizing. Moreover, for the innermost loops 'k', apply parallel reduction on 'tmp' and 'D' to accumulate values efficiently with optimization levels determined at runtime. Use the respective pragma directives before each loop, with factors set to 'auto' to allow the HLS tool to determine the optimal values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: flatten\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the specified 2D matrix-matrix multiplication kernel function for hardware acceleration by applying automatic pipelining and parallelization to all the for-loops for efficient execution, employ automatic tiling to divide loops into more manageable chunks, and use automatic factor determination for the parallel execution while ensuring the reductions are correctly handled. Use pragma directives to designate it as an accelerated hardware kernel and auto-tune the pipelining, tiling, and parallelism parameters.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, instructing the HLS compiler to consider all suitable optimization strategies, such as pipelining, tiling, and parallel processing, with the specific optimization factors for each loop level determined automatically by the HLS tool.\n\nHere's your natural language instruction:\n\n\"Define the function `kernel_2mm` handling matrix calculations as a hardware acceleration kernel. Allow the HLS compiler to automatically infer optimal pipelining, tiling, and parallel execution factors for each nested loop level to improve performance, and apply parallel reduction to the appropriate intermediate results during computation.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an accelerated hardware kernel function named 'kernel_2mm' that processes matrix multiplication and addition for 2D arrays 'A', 'B', 'C', 'D', and 'tmp'. The function should have outer loops for indices 'i' and 'j' that are fully parallelized, pipelined, and tiled automatically. Inner loops over index 'k' should be parallelized with automatic reduction on 'tmp' and 'D'. The function calculates 'tmp' as the product of matrices 'A' and 'B' scaled by 'alpha', and then 'D' as the scaled original 'D' by 'beta' plus the product of 'tmp' and 'C'. All automations should be enabled with appropriate pragmas for pipeline, tiling, and parallelization, and the reduction variable should be set for the innermost 'k' loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 14\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function called 'kernel_2mm' as a hardware acceleration kernel. The function computes the matrix expression `D := alpha*A*B*C + beta*D` with matrix dimensions ni, nj, nk, nl and scalars alpha, beta. For the matrix multiplication and addition operations, enable automatic pipelining, automatic loop tiling, and automatic parallelism factor determination at each loop level. Additionally, for reductions within inner loops, enable automatic parallel factor assignment with the appropriate reduction variable indicated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 14\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a C function named `kernel_2mm` designed for HLS that performs a matrix multiplication and addition according to the formula `D := alpha * A * B * C + beta * D`, where `alpha` and `beta` are scalar values, and `A`, `B`, `C`, `D`, and `tmp` are matrices with predetermined sizes. Consider `ni`, `nj`, `nk`, and `nl` as the dimensions of these matrices, though the actual values are fixed in the code. Designate this function for hardware acceleration. Optimize the function with automatic parallelism, pipelining, and tiling for all nested loops, and enable automatic setting of the appropriate factors. Include reduction pragmas to enable parallel accumulation in specific loops, again with automatic factor determination. The pragma values such as `__PIPE__L0`, `__TILE__L0`, and `__PARA__L0` placeholders should be automatically generated for each respective loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the provided function `kernel_2mm` for hardware acceleration using automated directives: mark the function for hardware acceleration, enable automatic pipelining, tiling, and parallelization for efficient execution, and apply parallel reduction to accumulate matrix multiplication results. Use dynamic pragma values determined during compilation for optimal loop transformation factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 5\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate code for a 2mm kernel function which treats matrix multiplications and additions as a combinational hardware block with automatic optimizations enabled for pipelining, tiling, and parallelization, including parallel reduction on intermediate accumulations. Specifically:\n\n1. Declare the function 'kernel_2mm' as a hardware kernel for acceleration.\n2. Automatically optimize each loop for pipelining, letting the compiler decide the optimal pipeline settings.\n3. Automatically configure loop tiling, allowing the compiler to determine the tile sizes.\n4. Automatically set parallel factors for loop parallelization, where the compiler decides the level of parallelism.\n5. For accumulation loops, apply a parallel reduction to the temporary accumulation variable 'tmp' in the first double-nested loop and to 'D' in the second double-nested loop, with the compiler automatically selecting the parallel reduction factors.\n\nThe pragma directives should be placed as follows:\n- Before each of the four for-loops, insert pragmas to enable automatic pipeline, tile, and parallel optimizations.\n- Before the innermost accumulation loops, insert a pragma to enable the parallel reduction with automatic factor determination.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and within this kernel, optimize the two-level nested loops for hardware parallelization and pipelining as follows:\n\n1. For the first pair of nested loops where 'tmp' matrix is calculated, automatically configure pipelining, tiling, and parallel execution strategies for both outer 'i' loop and inner 'j' loop.\n\n2. In the innermost 'k' loop of the first nested loops, apply parallel computation to the 'tmp' matrix accumulation with an automatic parallel factor, enabling reduction to handle dependencies efficiently.\n\n3. For the second pair of nested loops where the 'D' matrix is updated, also automatically configure pipelining, tiling, and parallel execution strategies for both outer 'i' loop and inner 'j' loop.\n\n4. In the innermost 'k' loop of the second nested loops, apply parallel computation to the 'D' matrix update with an automatic parallel factor, enabling reduction to handle dependencies.\n\nUse the corresponding '#pragma ACCEL' directives with an 'auto' keyword to let the compiler determine the appropriate values for pipeline stages, tiling factors, and parallel factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Implement the 2mm kernel function with double buffering and hardware acceleration, utilizing automatic optimization directives to improve parallelism, tiling, and pipelining of loop executions, and enabling reduction to accumulate results in specific operations. Use the provided matrix dimensions and coefficients for calculating the 2mm algorithm, where 'D' matrix gets updated with the result of 'alpha' times the matrix multiplication of 'A', 'B', and 'C', then adding 'beta' times the original 'D'. Apply the directives appropriately to each loop to achieve this optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as the target for hardware acceleration, ensuring efficient execution by automatically optimizing pipeline, data tiling, and parallel processing at each nested loop level, and specifically, employ reduction to accumulate results for the matrix operations. Utilize automatic factor determination for pipeline, tile, and parallel pragmas at different loop nests, and allow the compiler to decide the optimal strategy for each pragma directive, with special attention to reducing the `tmp` and `D` arrays where multiple additions to a specific element occur.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 5\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function to be optimized for hardware acceleration and apply automatic loop optimizations using specific pragmas. For all loops at their respective levels: enable automatic pipelining to potentially execute multiple iterations of a loop simultaneously; allow the compiler to automatically determine the loop tiling factor to break loops into smaller, more manageable chunks, if beneficial; and set the parallelization factor to automatically decide how many loop iterations to run in parallel. Also, apply automatic parallel reduction to enable combined parallel execution for loops with associative and commutative operations, specifically in loops where 'tmp' and 'D' accumulations occur. Ensure that directive placeholders are included for compiler flexibility in optimization decisions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function with automatic optimization for pipeline, tiling, and parallel factor pragmas. Apply loop pipelining, tiling, and parallelization with compiler-determined factors to both main nested loop structures and a parallel reduction for intermediate calculations involving matrix multiplications. The outer loops iterate over a 40 by 50 matrix in the first case and a 40 by 80 matrix in the second case, with internal reductions over 70 and 50 element vectors respectively. Additionally, apply a kernel pragma to the whole function for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel, allowing for automatic optimization of loops within it. Leave pipeline optimization for the compiler to decide automatically for the entire function and each nested loop, ensure that loop tiling is also managed automatically by the compiler for optimal chunk processing, and adjust parallel execution of all loops with decisions on parallelism factor left to the compiler's heuristics. For the reductions inside the innermost loops, marking the reduction variable indicates dependency, allowing the compiler to handle parallel reduction of arrays `tmp` and `D` with automatic determination of the best parallelism factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following matrix multiplication and addition algorithm into a high-level synthesis (HLS) hardware kernel, applying automated pipelining, tiling, and parallelism optimizations for all loops, with HLS tool flexibility to determine the specific factors. Enable parallel reductions for intermediate results accumulation. The algorithm performs D = alpha * A * B * C + beta * D, iterating over matrix dimensions with double-precision elements. Specify the implementation for the given matrix sizes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, automatically optimize loop execution by pipelining, tiling, and parallelization where appropriate, and apply reduction to accumulate results within nested loops:\n\n1. Annotate the 'kernel_2mm' function as a target for hardware acceleration.\n2. For the first pair of nested loops calculating the matrix product of 'alpha * A * B' and storing it in 'tmp':\n   - Enable automatic pipelining at the first and second loop levels.\n   - Allow the compiler to automatically determine the tiling factors at both loop levels.\n   - Enable automatic parallelization at all loop levels, with the third loop performing a reduction on 'tmp'.\n3. For the second pair of nested loops updating matrix 'D' with 'beta * D + tmp * C':\n   - Activate automatic pipelining at the first and second loop levels.\n   - Permit automatic tiling factor selection at both loop levels.\n   - Initiate automatic parallelization at all loop levels, with the third loop executing a reduction on 'D'.\n\nMake sure to denote the loops with the corresponding pipeline, tile, and parallel pragmas with automatic factor determination, and specify that the parallel pragma in the reduction loops should apply to the 'tmp' and 'D' arrays, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` as a hardware-accelerated kernel; within it, automatically optimize the main loops for throughput, data locality, and parallel execution, allowing the compiler to choose the pipeline, tiling, and parallelism factors. Also, apply parallel reduction to the innermost loops to accumulate the matrix multiplication results for `tmp` and `D` matrices using the `alpha` and `beta` scaling factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes using natural language instructions, you can provide the following directive:\n\n\"Define a hardware-accelerated kernel function called 'kernel_2mm' which multiplies and accumulates matrices with configurable loop optimizations. Enable automatic pipelining, tiling, and parallelization for each nested loop to maximize throughput. Ensure data reduction parallelism is applied whenever partial results are combined across iterations.\"\n\nRespective #pragma directive translations for the instructions:\n\n- Define 'kernel_2mm' as a hardware kernel function: `#pragma ACCEL kernel`.\n- Enable automatic pipelining for optimizing loop latency: `#pragma ACCEL PIPELINE auto`.\n- Apply automatic tiling to break down loops into smaller chunks for efficient usage of local memory: `#pragma ACCEL TILE FACTOR=auto`.\n- Enable automatic parallelization to run loop iterations concurrently and maximize parallel execution: `#pragma ACCEL PARALLEL FACTOR=auto`.\n- Use data reduction parallelism for accumulating values into 'tmp' and 'D' matrices across iterations: `#pragma ACCEL PARALLEL reduction = <variable> FACTOR=auto`.\n\nNote: The `{__PIPE__Lx}`, `{__TILE__Lx}`, `{__PARA__Lx}` placeholders within pragmas are likely to be replaced by specific values during compilation depending on the directive's context and provided settings, but in a natural language instruction these details would be automatically managed by the compiler/toolchain.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function, defining it as a hardware acceleration kernel. Automatically optimize loop pipelining, tiling, and parallel processing using appropriate factors. For the first set of nested loops, initialize a temporary 2D array 'tmp' to zero and calculate its values based on matrices 'A' and 'B' with scaling by 'alpha', applying reduction on 'tmp' for parallel computations. For the second set of nested loops, scale the matrix 'D' by 'beta' and then update its values with the product of 'tmp' and 'C', again applying reduction on 'D' for parallel computations. Use automatic directive values for optimal factor assignments in both sets of nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis kernel named 'kernel_2mm' to compute a two-matrix multiplication operation with the following parameters: matrix dimensions `ni`, `nj`, `nk`, `nl`, scalar values `alpha` and `beta`, and input/output matrices `tmp`, `A`, `B`, `C`, and `D`. Enable automatic optimization for pipelining, tiling, and parallel execution for all loops, with specific pragmas for each loop nest letting the compiler decide the best factors. Additionally, use parallel reduction for the innermost loops that accumulate values in matrices `tmp` and `D`. The outermost two loops for intermediates and result computation are comprised of two sets, each with three nested loops. The first set calculates `tmp` with the intermediate results of `alpha*A*B`, and the second set updates `D` with `beta*D + tmp*C`. Use pragma directives to explicitly declare that each loop set should be optimized with automatic parallelization, pipelining, and tiling directives, and ensure that the addition operations that accumulate into `tmp` and `D` within the innermost loops are identified for parallel reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the provided `kernel_2mm` function by treating it as a hardware kernel, enabling automatic pipeline optimization for loop execution to improve throughput, dynamically setting the tile factors to control the amount of loop unrolling conducive to the hardware's available resources, and configuring automatic parallel factor determination to run loop iterations concurrently where possible, with parallel reduction to correctly accumulate values in arrays `tmp` and `D`. Use automatic settings for all pragma directives to allow the HLS tool to make optimization decisions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel intended for FPGA synthesis. Automatically optimize the loops - first for computing matrix multiplication and addition storing the result in a temporary matrix, and second for updating the final matrix with scaling and addition - with pipelining, tiling, and parallelization heuristics to enhance performance. Additionally, apply automatic parallel reduction to accumulate values across iterations for both matrix calculations. Use these optimizations without manually specifying the pragma factors, allowing the HLS tool to determine the best values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to operate as a hardware acceleration kernel, optimizing the nested loops by automatically applying pipelining, tiling, and parallelism where appropriate, and enable parallel reduction for specific array calculations, managing all optimization factors automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and for the nested loops inside the function, apply these high-level synthesis optimizations:\n\n1. Automatically determine the optimal pipelining level for each loop to improve data throughput.\n2. Automatically select the tile size for each loop to split it into smaller, more manageable chunks for processing without specifying an exact factor.\n3. Automatically determine the parallelization factor for each loop to enable concurrent processing where possible.\n4. For the innermost loops that accumulate results into 'tmp' and 'D' arrays, enable parallel reduction with an automatic factor to efficiently combine partial results generated in parallel.\n\nEnsure you use auto-tuning #pragma directives for pipelining, tiling, and parallelizing, and specify reduction for the pertinent accumulations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function called `kernel_2mm` as a hardware acceleration kernel to compute a 2mm algorithm, where `tmp` is the matrix product of `alpha * A * B` and `D` is updated as `beta * D + tmp * C`. Enable automatic loop pipelining, tiling, and parallel processing while allowing the HLS tool to decide the best factor for each. Additionally, for loops that accumulate values into `tmp` and `D`, use parallel reduction to improve performance, letting the HLS tool determine the optimal parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function named 'kernel_2mm' designed for matrix multiplication and matrix addition, implementing the equation D := alpha*A*B*C + beta*D for matrix dimensions 40x70, 70x50, 50x80 respectively. Let the HLS tool optimize the loops through automatic pipeline, tiling, and parallelization, including parallel reduction for accumulation operations, and specify those optimizations with respective pragmas.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel with automated optimization directives: Set the function as a hardware acceleration kernel; enable loop pipelining, tiling, and parallelization heuristics for all nested loops, utilizing automatic factor determination, and specify parallel reduction for accumulating operations on arrays 'tmp' and 'D' in the respective innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be optimized for hardware acceleration. Allow the compiler to automatically determine the optimal pipelining, tiling, and parallelization of loops for performance enhancement. Apply these optimizations at all loop levels within the function to improve the computation of the matrix expression 'D := alpha*A*B*C + beta*D', where 'tmp' is an intermediary matrix. Ensure that data dependencies are accounted for through appropriate reduction pragmas during accumulation in the 'tmp' and 'D' matrices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated kernel function called `kernel_2mm` that computes the matrix multiplication and addition `D := alpha*A*B*C + beta*D` with specified dimensions. Use automatic optimization for all loops, including pipelining, tiling, and parallelism, and employ reduction pragmas for temporary array `tmp` and resulting array `D` to accumulate values correctly within the parallelized inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, automatically optimize the processing of the loops, adaptively tile the loops for balanced fine-grain parallelism, and allow independent operations within the loop to be performed in parallel, automatically determining the appropriate parallelism factor. Additionally, direct the compiler to perform reduction operations on the temporary arrays in the nested loops while automating the optimization settings for the iterations within those loops. \n\nSpecifically for the HLS optimizations:\n\n1. Declare the 'kernel_2mm' function as a hardware acceleration kernel using `#pragma ACCEL kernel`.\n2. Enable automatic loop pipelining for efficient iteration with `#pragma ACCEL PIPELINE auto{__PIPE__L*}` wherever applicable, letting the compiler decide the pipelining strategy.\n3. Automatically tile each loop using `#pragma ACCEL TILE FACTOR=auto{__TILE__L*}` which lets the compiler decide on the tiling factor to generate balanced parallel chunks.\n4. Enable automatic parallel processing of loop iterations with `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L*}`, allowing the compiler to determine the factor for parallel execution to enhance loop execution performance.\n5. Automatically optimize reductions within the loops by using `#pragma ACCEL PARALLEL reduction = array_name FACTOR=auto{__PARA__L*}`, instructing the compiler to handle the parallel reduction of the 'tmp' and 'D' arrays, enhancing performance for accumulation patterns in the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) configuration for a function that performs a 2-matrix multiplication (2mm) with the following directives:\n\n1. Designate the entire function 'kernel_2mm', which takes matrix dimensions and matrices as inputs, as a hardware kernel for acceleration.\n2. For the first loop 'i' that iterates over matrix `tmp`, enable automatic optimization for pipelining, tiling, and parallel execution using HLS tool heuristics.\n3. Within the nested loop 'j', also enable automatic pipelining, tiling, and parallel execution, and specify that the parallel operation should include a reduction on the 'tmp' array.\n4. For the second set of loops (another 'i', and 'j') that iterates over matrix `D`, repeat the step 2 directives for automatic optimization.\n5. Within this set's nested loop 'j', enable the same optimizations as in step 3, but this time specify the reduction to occur on the 'D' array.\n\nSpecifically, use the following pragmas and placeholders for automatic values:\n\n- `#pragma ACCEL kernel` to indicate the function is a hardware kernel.\n- `#pragma ACCEL PIPELINE auto{__PIPE__Lx}` where 'x' is the unique loop level identifier to enable automatic determination of pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{__TILE__Lx}` to enable automatic determination for loop tiling factor.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__Lx}` to enable automatic determination of parallel factors.\n- `#pragma ACCEL PARALLEL reduction = array_name FACTOR=auto{__PARA__Lx}` to enable parallel reduction on the specified array within the loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate 'kernel_2mm' function as a hardware kernel to accelerate the matrix-matrix multiplication and addition operations. Within the function, automatically optimize the main loops 'i' and 'j' by allowing them to be pipelined, tiled, and parallelized, with the specific optimization parameters to be determined automatically by the HLS tool. For the nested loops, use automatic parallelization with reduction on 'tmp' and 'D' variables to accumulate the results, again with the HLS tool automatically determining the parallel factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the function `kernel_2mm` responsible for multiplying and adding matrices, automatically determining the levels of pipelining, tiling, and parallel processing for each loop, and including directives for appropriate parallel reduction on accumulation variables.\n\n- Declare the function `kernel_2mm` as a hardware acceleration kernel.\n- For the two-dimensional loop nest calculating `tmp[i][j]`, with loop indices `i` and `j`:\n  - Automatically decide the extent of pipelining for the outer loop `i` and tag it with a unique identifier.\n  - Automatically choose a tiling factor for the loop `i` and associate it with a specific identifier.\n  - Automatically determine the parallel processing factor for the outer loop `i` and label it uniquely.\n  - Apply the same automatic pipelining, tiling, and parallel processing to the inner loop `j`.\n  - For the innermost loop `k` that updates `tmp[i][j]`, introduce automatic parallel reduction optimizing the accumulation operation.\n- For the subsequent two-dimensional loop nest updating `D[i][j]`, with loop indices `i` and `j`:\n  - Introduce automatic pipelining for the outer loop `i`, marked with its identifier.\n  - Select an automatic tiling factor for the loop `i` and reference it with its tag.\n  - Set the outer loop `i` to automatically determine the parallel processing factor and ascribe it with a unique nomenclature.\n  - Apply corresponding directives for automatic pipelining, tiling, and parallel processing to the inner loop `j`.\n  - In the innermost loop `k` that affects `D[i][j]`, enable automatic parallel reduction to efficiently manage the accumulation.\n\n#Pragma Directives:\n- Treat function as a hardware kernel: `#pragma ACCEL kernel`\n- Pipelining for loops: `#pragma ACCEL PIPELINE auto{__PIPE__Lx}`\n- Tiling for loops: `#pragma ACCEL TILE FACTOR=auto{__TILE__Lx}`\n- Parallelization for loops: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__Lx}`\n- Parallel reduction for accumulation: `#pragma ACCEL PARALLEL reduction = variable_name FACTOR=auto{__PARA__Lx}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be synthesized as a hardware acceleration kernel. Optimize the loops within 'kernel_2mm' using automatic pipelining, tiling, and parallelism decisions made by the HLS tool, and apply automatic loop factors where appropriate for performance. Enable parallel reduction on the 'tmp' and 'D' arrays to accumulate their values concurrently within innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the function 'kernel_2mm' as a dedicated hardware acceleration kernel, enable automatic loop pipelining, tiling, and parallel processing where the tool determines optimal factors for each, as well as perform reduction optimizations on the specified variables within inner loops.\n\n- Mark the function 'kernel_2mm' to be synthesized as a dedicated hardware kernel using '#pragma ACCEL kernel'.\n- For the outermost loops of both major operations, allow the HLS tool to automatically decide on pipelining, tiling, and parallelization strategies by using '#pragma ACCEL PIPELINE auto', '#pragma ACCEL TILE FACTOR=auto', and '#pragma ACCEL PARALLEL FACTOR=auto' respectively. Tag these pragmas with unique identifiers like '__PIPE__L0', '__TILE__L0', and '__PARA__L0' for the first operation and '__PIPE__L1', '__TILE__L1', and '__PARA__L1' for the second.\n- Similarly, configure the second-level loops for automatic optimization with pragmas followed by identifiers like '__PIPE__L2', '__TILE__L2', and '__PARA__L2' for the first operation and '__PIPE__L3', '__TILE__L3', and '__PARA__L3' for the second operation.\n- In the innermost loops, use '#pragma ACCEL PARALLEL reduction = <variable> FACTOR=auto' with '__PARA__L4' and '__PARA__L5' to instruct the HLS tool to automatically optimize parallel computations that include reduction operations on the 'tmp' and 'D' arrays, fine-tuning for the best factor in each case.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function optimized for hardware acceleration. Mark the entire function as a kernel for acceleration. Apply automatic pipelining, tiling, and parallel processing optimizations at each loop level, allowing the compiler to decide the optimal factor for these optimizations. For the innermost loops, enable parallelization with automatic factor determination and add reduction on the 'tmp' and 'D' arrays to ensure that parallel updates to these arrays are handled correctly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the described HLS C code for a 2mm kernel, instruct the HLS tool as follows:\n\nDesignate the `kernel_2mm` function as a hardware acceleration kernel. For each multilevel nested loop, automatically determine and apply the optimal settings for pipelining, tiling, and parallel processing while allowing the highest level of compiler heuristics for decision making. Moreover, for the innermost loops, enable parallel reduction on the `tmp` and `D` arrays, respectively, and again let the compiler decide the optimal factor for parallel execution.\n\nSpecifically state in the instruction:\n- \"Declare 'kernel_2mm' as a hardware acceleration kernel.\"\n- \"Decide automatically the best pipelining opportunities for each loop, preferring the compiler's heuristic.\"\n- \"For tiling, use the compiler's auto-tuning ability to choose the optimal tiling factor for each loop.\"\n- \"Set parallel processing to automatically pick the factor for each loop level, relying on the compiler to optimize it.\"\n- \"In the two innermost loops where the `tmp` and `D` arrays are being updated, enable parallel reduction and let the compiler determine the parallelism factor.\"\n\nEssentially, the use of `auto` within each pragma suggests to the HLS tool that it should analyze the code and decide the specific values for PIPELINE, TILE, and PARALLEL directives to achieve the best acceleration and performance by itself.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration candidate, and optimize the nested loops within it using automatic pipelining, tiling, and parallelization, with specific details such as automatic factor adjustments and reduction operations on the 'tmp' and 'D' arrays to efficiently perform the matrix multiplication and addition operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate 'kernel_2mm' function as a hardware acceleration kernel, for the two main loops and their internal computing loops, instruct the HLS tool to automatically determine the optimal settings for pipelining, tiling, and parallelization, and for the innermost multiplication-accumulation computation, enable parallel reduction on 'tmp' and 'D' arrays with automatic factor determination. Use automatic loop optimization for the two matrix multiplication patterns within the 'kernel_2mm' function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS compiler to accelerate the function `kernel_2mm` as a hardware kernel, where `kernel_2mm` performs a two-matrix multiply (2MM) operation with A, B, C, and D being matrices, and alpha and beta are scalar values. The 2MM operation computes `D = alpha * A * B * C + beta * D`.\n\n1. For each of the outer loops running over `i` and `i` indices (processing rows of `D` and `tmp`), enable automatic selection for pipelining, tiling, and parallelism by setting respective pragma values to 'auto'. Use the custom pragmas with unique identifiers (such as `__PIPE__L0`, `__TILE__L0`, `__PARA__L0`) to allow the HLS tool to apply different optimization strategies for these loops.\n\n2. Inside the first outer loop, for the initialization of `tmp`, and then inside the second outer loop, for the multiplication with `C` and the accumulation into `D`, also enable automatic selection for pipelining, tiling, and parallelism by defining the respective pragmas with unique identifiers for each loop nest level (as demonstrated by `__PIPE__L2`, `__TILE__L2`, `__PARA__L2` for the first case and `__PIPE__L3`, `__TILE__L3`, `__PARA__L3` for the second case).\n\n3. For the deepest inner loops running over `k`, which aggregate results into `tmp` and `D`, apply automatic parallelization with the reduction operation on `tmp` and `D` respectively. Employ the `reduction` clause on the parallel pragma along with the 'auto' factor and tag the pragma with unique identifiers for each occurrence (as shown by `__PARA__L4` and `__PARA__L5`).\n   \nSummarizing, the pragmas should indicate to the HLS tool to consider the function `kernel_2mm` for hardware acceleration, and let the compiler automatically decide the best optimization strategies for pipelining, tiling, and parallelism for each loop and for managing reduction operations in aggregation loops, leading to potentially differentiated optimization configurations for each loop level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware-accelerated 2mm kernel that performs a matrix multiplication and accumulation operation with two main loops and nested inner loops. Use automatic optimization directives to inform the compiler to attempt to pipeline each loop at its discretion, tile the loops where beneficial, and parallelize where possible. Include pragmas for pipeline, tile, and parallel with automatic factor determination by the compiler to optimize execution. Additionally, indicate two inner loops with reduction operations on the 'tmp' and 'D' matrices, setting the parallelization pragmas to perform the reduction with automatically determined parallel factors by the compiler. The kernel should accept dimensions ni, nj, nk, nl, and multipliers alpha, beta, as well as the matrix operands A, B, C, D, and a temporary matrix tmp, with predetermined array sizes. Implement the matrix operation D := alpha*A*B*C + beta*D, ensuring that the top-level function is marked as a hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel and optimize loops for parallel computation, pipelining, and effective data partitioning with automatically determined factors.\n\n- Mark the function 'kernel_2mm' as a hardware acceleration kernel.\n- For each loop level:\n  - Enable automatic pipelining to potentially overlap loop iteration executions.\n  - Automatically determine and apply tiling to break down loops into smaller, more manageable blocks if beneficial.\n  - Set parallel execution factors automatically to allow concurrent execution of operations where possible.\n  - For reduction operations in the nested loops that accumulate into 'tmp' and 'D', enable parallel reduction with an automatically determined parallel factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the `kernel_2mm` function, which computes the matrix multiplication and addition, `D := alpha*A*B*C + beta*D`. Enable automatic determination of pipeline, tiling, and parallelism optimizations for the nested loops and apply a reduction clause for array updates within the innermost loops using the following HLS pragmas:\n\n- Treat the function `kernel_2mm` as a hardware acceleration kernel with `#pragma ACCEL kernel`.\n- For the first two nested loops, which initialize `tmp` and compute the partial product `alpha * A * B`, respectively:\n  - Automatically optimize the pipeline level with `#pragma ACCEL PIPELINE auto`.\n  - Automatically determine the tiling factor with `#pragma ACCEL TILE FACTOR=auto`.\n  - Automatically optimize the parallel execution factor with `#pragma ACCEL PARALLEL FACTOR=auto`.\n  - For the innermost loop within `tmp` initialization, use parallel reduction to accumulate the values into `tmp[i][j]`, automatically determining the reduction factor with `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto`.\n- For the second set of two nested loops, where `D` is scaled by `beta` and the final addition to incorporate `tmp * C` is performed:\n  - Share the same automatically determined values for pipeline, tiling, and parallel factors as previously mentioned.\n  - Apply parallel reduction for updating `D[i][j]` with automatically determined factor using `#pragma ACCEL PARALLEL reduction=D FACTOR=auto` within the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with appropriate pragmas, use the following instruction:\n\n\"Accelerate a 2mm kernel function with two matrix multiplication and addition operations. Apply automatic optimization pragmas for pipelining, tiling, and parallelization at each loop level to boost performance, allowing the compiler to determine the best factors. Specifically, for reduction operations in the innermost loops, enable parallel accumulations for the 'tmp' and 'D' matrices. Treat this entire function as an accelerated hardware kernel.\"\n\nYou need to insert the corresponding pragmas to automatically optimize the two main loop nests where matrix operations occur. Use automatic pipelining to allow iterations to be overlapped, automatic tiling to break down the iterations into smaller blocks for better data locality, and automatic parallelization to run loop iterations concurrently where possible, while handling reductions in data dependencies in the innermost loops with parallel accumulation. Here are the prime pragma values:\n\n- Treat the function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Automatically optimize pipelining: `#pragma ACCEL PIPELINE auto`\n- Automatically determine the tiling factor for loop unrolling: `#pragma ACCEL TILE FACTOR=auto`\n- Automatically determine the parallel factor for concurrent loop execution: `#pragma ACCEL PARALLEL FACTOR=auto`\n- Enable parallel reduction for the specified matrix: `#pragma ACCEL PARALLEL reduction=<matrix_name> FACTOR=auto`\n\nPlace these pragmas immediately before each respective for-loop to inform the compiler of how to optimize the loops. For the nested loops, you need to uniquely identify the level at which each pragma applies, typically done by using placeholders like `{__PIPE__L0}`, `{__TILE__L0}`, `{__PARA__L0}`, etc., to distinguish between different loop levels, albeit this part will often be automatically managed by the compiler or HLS tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To create the provided HLS C code, you would instruct the HLS tool as follows:\n\n\"Implement a 2mm kernel function that performs the operation `D := alpha*A*B*C + beta*D` using matrix multiplication and addition, where `A`, `B`, `C`, and `D` are multi-dimensional arrays and `alpha` and `beta` are scalar values. For the loops involved in the computation, enable automatic optimization for pipelining, tiling, and parallelization, letting the HLS tool determine the best possible values for these pragma settings. Moreover, allow the tool to automatically decide how many iterations of the inner loops can be processed in parallel, especially for reducing the matrix products into `tmp` and `D`. Utilize loop transformation directives to enable the potential reduction of partial results concurrently within the innermost loops, and assign the resulting function as a hardware kernel for acceleration using suitable pragmas.\" \n\nSpecific pragma directives and their descriptions are as follows:\n- `#pragma ACCEL kernel` - Label the function as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto` - Automatically optimize the loop to be pipelined with an unspecified initiation interval, letting the compiler decide the best value.\n- `#pragma ACCEL TILE FACTOR=auto` - Automatically optimize the loop to be tiled with an unspecified factor, letting the compiler choose the best value for how to break the loop into smaller chunks.\n- `#pragma ACCEL PARALLEL FACTOR=auto` - Automatically optimize the loop to run iterations in parallel with an unspecified factor, allowing the compiler to determine the best value for how many iterations to run concurrently.\n- `#pragma ACCEL PARALLEL reduction = variable` - Instruct the HLS tool to perform parallel reduction on the specified variable, optimizing the accumulation of partial results within the loop. \n\nNote that the placeholders `{__PIPE__LX}`, `{__TILE__LX}`, and `{__PARA__LX}` within the pragmas imply that these are template values to be replaced by the HLS tool with specific numeric values during the code generation process, based on its automatic optimization strategies.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a candidate for hardware acceleration, applying automatic optimization directives to enhance parallelism, pipelining, and data tiling for all nested loops within matrix multiplication and addition operations:\n\n1. Mark the function 'kernel_2mm' as a hardware acceleration kernel.\n2. For the first pair of nested loops where 'tmp' matrix is computed, automatically select pipelining, tiling, and parallelization options to improve loop execution, with specific parallel reduction for the 'tmp' matrix.\n3. For the second pair of nested loops where the 'D' matrix is updated, similarly automate pipelining, tiling, and parallelization choices, with specific parallel reduction for the 'D' matrix.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: flatten\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS-optimized C implementation for a 2mm kernel function that takes matrix dimensions and coefficients as inputs, and processes two matrix multiplications D = alpha * A * B * C + beta * D. Utilize automatic optimization choices for pipelining, tiling, and parallelism in all loops. Additionally, apply parallel reduction for accumulating values in the tmp and D matrices. Use pragmas to define the function as an acceleration kernel and to instruct the HLS tool to automatically determine the appropriate values for pipeline, tile, and parallel factors, as well as to specify the reduction variables.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: flatten\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the described HLS codes, the instruction would be:\n\n\"Define a function as a hardware acceleration kernel that performs a two-matrix multiplication (2mm) followed by their addition. For both computation stages, automatize the decision of pipelining, tiling, and parallelism factors to optimize the loop's performance. Additionally, apply parallel reduction to accumulate results within inner loops, letting the compiler decide the parallelism factor to efficiently exploit hardware resources.\" \n\nThis describes the overall function's purpose, the automatic optimization directives for pipelining, tiling, and parallel processing with '#pragma ACCEL PIPELINE auto', '#pragma ACCEL TILE FACTOR=auto', and '#pragma ACCEL PARALLEL FACTOR=auto', respectively, and the use of parallel reduction with '#pragma ACCEL PARALLEL reduction' followed by automatic parallelization factors specified within curly brackets \u2014 {__PARA__Lx} where 'x' corresponds to the loop level identifier.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a matrix multiplication and addition kernel that performs the operation D := alpha*A*B*C + beta*D. Define the sizes of the matrices A, B, C, and D with parameters ni, nj, nk, and nl respectively, and use 'double' data types for all matrices and alpha, beta scalars. Implement a two-level nested loop structure: the outer level processes the matrix multiplication of A, B, and C, and stores the intermediate result in matrix tmp; the inner level updates matrix D with the scaled addition of the intermediate result and the original matrix D values.\n\nFor both levels of loops, use automatic pipelining to enhance parallelism but allow the compiler to decide the optimal configuration. For full loop unrolling or tiling, let the compiler automatically decide loop tiling factors for partitioning the loops into smaller, evenly-distributed computational blocks to improve data locality and parallel processing. Similarly, allow the compiler to choose the parallelism factor for executing loop iterations concurrently where applicable.\n\nMoreover, apply reduction pragma to enable parallel reduction operations within the innermost loops while accumulating the multiplication results into the tmp and D matrices using parallel accumulators, again letting the compiler automatically set the reduction factor.\n\nSpecifically, assign the directives: \n- Treat the function as a hardware kernel with '#pragma ACCEL kernel'.\n- '#pragma ACCEL PIPELINE auto' to pipeline the significant loops, letting the compiler's heuristic choose if and how to pipeline them.\n- '#pragma ACCEL TILE FACTOR=auto' to allow the compiler to automatically determine the tiling factor for loop nest optimizations.\n- '#pragma ACCEL PARALLEL FACTOR=auto' to enable parallel execution of loops with the compiler automatically determining the factor.\n- '#pragma ACCEL PARALLEL reduction = matrix_name' with 'FACTOR=auto' on the innermost loops for parallel reduction, specifying the matrix (tmp or D) to which the reduction should be applied.\n\nRemember to use the symbolic constants __PIPE__Lx, __TILE__Lx, and __PARA__Lx to indicate where the specific #pragma values should be applied for each respective loop level, with 'x' representing the loop level's depth starting from 0.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is optimized for hardware acceleration by treating it as a hardware kernel, automatically deciding the best loop pipelining strategy, determining the optimal loop tiling factors, and adjusting the parallel execution strategies and reduction operations where necessary:\n\nDesignate the function 'kernel_2mm' to behave like a hardware acceleration kernel. Optimize the loops within this function by allowing the HLS tool to automatically configure pipelining, to intelligently decide on the tiling factors, and to adjust the parallel processing parameters. Specifically, in the first two-nested loops where 'tmp' matrix is computed, apply pipelining, tiling, and parallelism at both loop levels, and include a reduction clause for 'tmp' during the innermost loop's parallel computation. For the second pair of nested loops where the 'D' matrix is updated, also apply similar pipelining, tiling, and parallelism strategies, and include a reduction clause for 'D' matrix in the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the various HLS codes for the function `kernel_2mm`, apply the following instructions succinctly, detailing which #pragma values to use for each section of the code:\n\n1. Define the function as a hardware acceleration kernel with `#pragma ACCEL kernel`.\n\n2. Instruct the HLS tool to automatically determine the optimal loop pipeline settings for the first outer loop `i` by using `#pragma ACCEL PIPELINE auto{__PIPE__L0}`, and to automatically decide the loop's tiling factor and parallel factor with `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}` and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`, respectively.\n\n3. Within the first loop nest, automate the pipelining, tiling, and parallelism directives for the second loop `j` using `#pragma ACCEL PIPELINE auto{__PIPE__L2}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`. Then specify automatic parallel factor for the innermost loop `k`, which performs reduction on `tmp`, with `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}`.\n\n4. Employ similar automatic HLS directives for the second set of nested loops, indicating the same loop optimization strategy for the outer loops `i` and `j` with `#pragma ACCEL PIPELINE auto{__PIPE__L1}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`, `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`, `#pragma ACCEL PIPELINE auto{__PIPE__L3}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}`.\n\n5. Additionally, for the second loop nest, set the innermost loop `k` to perform a reduction on `D` while automatically determining the parallel factor with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}`.\n\nThe above directives guide the HLS tool to optimize each loop in the function with automatic decisions for pipelining, tiling, and parallelism, thus achieving a balance between performance and resource utilization based on the HLS tool's heuristics and the target architecture's characteristics.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following 2mm kernel implementation into a high-level synthesis (HLS) design for FPGA acceleration by applying appropriate directives to manage parallelism, pipelining, and data tiling. Use automatic optimization heuristics for all pragma factors, and explicitly parallelize the reduction operations within the nested loops to accumulate values of the `tmp` and `D` arrays. Additionally, interpret the main function as an acceleration kernel while giving the HLS tool freedom to determine the best factor values for the pragmas.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Design a hardware kernel for a 2mm algorithm with automatic pipelining, tiling, and parallel processing optimization for all loops. Ensure that intermediate accumulation on 'tmp' and 'D' arrays uses parallel reduction. Automatically configure these optimizations for best performance as assessed at compile time.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Apply automatic pipelining, tiling, and parallel execution where appropriate to loops that perform matrix operations while allowing the compiler to choose the specific factors. For reduction operations where temporary results accumulate in 'tmp' and 'D', enable automatic parallel processing without specifying a fixed factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function named 'kernel_2mm' as a hardware acceleration kernel that performs a matrix multiplication followed by a matrix addition. Enable automatic decision for pipelining, tiling, and parallelism at each level of the nested loops. Specifically:\n\n1. Set the entire function as a hardware kernel for acceleration.\n2. For the outermost two-level nested loops, iterate over the dimensions 40x50 to initialize the 'tmp' matrix to zero while automatically determining the pipeline, tiling, and parallelism factors using the corresponding pragmas `PIPELINE`, `TILE`, and `PARALLEL`.\n3. Inside the innermost loop, where 'tmp' is computed using 'alpha', 'A', and 'B', enable parallel computation with automatic factor determination and activate reduction for 'tmp' variable.\n4. For the next two-level nested loops, iterate over the dimensions 40x80 and update the 'D' matrix by scaling it with 'beta' while automatically determining the pipeline, tiling, and parallelism factors using the corresponding pragmas.\n5. Inside the innermost loop, where 'D' is updated with the product of 'tmp' and 'C', enable parallel computation with automatic factor determination and activate reduction for 'D' variable. \n\nEnsure all loop optimizations are automatically managed by providing the 'auto' keyword for `PIPELINE`, `TILE`, and `PARALLEL` pragmas. The reduction pragmas should be targeting the appropriate arrays being reduced ('tmp' and 'D').", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 14\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware acceleration kernel. Automatically optimize the pipeline, tiling, and parallelism with appropriate factors for each nested loop level within the function to enhance performance when computing the matrix multiplication and addition expression 'D := alpha * A * B * C + beta * D'. Utilize parallel reduction for the innermost loops where intermediate results are accumulated into 'tmp' and 'D' matrices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Provide directives to convert the function `kernel_2mm` into a hardware kernel, automatically optimize the loops for pipelining, tiling, and parallel processing using the best possible factors that the compiler can choose, and include reduction for array computations in nested loops. Specifically:\n\n- Declare `kernel_2mm` as a hardware acceleration kernel.\n- For the outermost loops `i` counting up to 40 for two separate loop nests, enable automatic pipelining, tiling, and parallelization.\n- In the subsequent nested loops `j`:\n  - For the first loop nest counting up to 50, automatically pipeline, tile, and parallelize it.\n  - For the second loop nest counting up to 80, also automatically pipeline, tile, and parallelize it.\n- Within the innermost loops `k`:\n  - For the first loop nest counting up to 70, enable automatic parallel reduction on the `tmp` array.\n  - For the second loop nest counting up to 50, enable automatic parallel reduction on the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the given matrix multiplication algorithm into a hardware-accelerated function with optimized dataflow. Specifically, label the function as an acceleration kernel and apply automated directive hints to achieve pipelining, tiling, and parallelization of the outer and inner loops with optimal factors decided by the tool, and perform parallel reduction to accumulate the result within innermost loops. Use automation to set the factor values for PIPELINE, TILE, and PARALLEL directives, including reduction for temporary arrays tmp and D during multiplication and addition operations respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the 'kernel_2mm' function, which performs a 2mm composite linear algebra kernel on matrices, for hardware acceleration. Automatically tune and allow the HLS tool to decide the best loop pipelining, tiling, and parallelization strategies. Specifically, allow the tool to determine when and how to unroll loops, and perform parallel reduction when appropriate.\n\n- Treat 'kernel_2mm' as a hardware kernel function.\n- For the first two-level nested loops that compute the product of 'alpha', 'A', and 'B' and store it in 'tmp', let the HLS tool automatically decide the pipelining, tiling, and parallelization factors, and perform parallel reduction on 'tmp'.\n- For the second two-level nested loops, where 'D' is updated by scaling with 'beta' and adding the product of 'tmp' and 'C', also let the HLS tool automatically choose pipelining, tiling, and parallelization factors, and perform parallel reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Enable automatic pipelining, tiling, and parallelization for all loops, with the specific strategy to be decided by the compiler. Employ parallel reduction for the innermost loops where 'tmp' and 'D' are the accumulation variables. The provided HLS pragma directives should adjust optimization factors dynamically as determined by the tool during compilation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS (High-Level Synthesis) codes written in C, use the following comprehensive but concise natural language instruction:\n\nCreate a high-level synthesis C function named 'kernel_2mm' that accelerates the calculation of the matrix expression 'D := alpha * A * B * C + beta * D' for matrix dimensions defined by 'ni', 'nj', 'nk', and 'nl'. Mark the function for hardware acceleration. Within the four nested loops, automate pipelining, employ automatic tiling, and parallelize where beneficial with appropriate factors, but let the HLS tool determine their values. Include parallel reduction for intermediate summation in the 'tmp' and 'D' matrices. Ensure you apply these optimization pragmas before each loop respectively:\n\n1. Use '#pragma ACCEL kernel' to mark the function for acceleration.\n2. Use '#pragma ACCEL PIPELINE auto' to enable automatic loop pipelining.\n3. Use '#pragma ACCEL TILE FACTOR=auto' to allow the tool to decide on loop tiling factors.\n4. Use '#pragma ACCEL PARALLEL FACTOR=auto' to let the tool decide on loop parallelization factors.\n5. Apply '#pragma ACCEL PARALLEL reduction' followed by '= tmp' or '= D' and 'FACTOR=auto' before the innermost loops doing matrix multiplication to enable parallel reduction for accumulating into 'tmp' and 'D'.\n\nEnsure that the HLS tool automatically manages pipelining, tiling, and parallel factors without explicit manual specification by utilizing the 'auto' keyword.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, enable automatic loop pipelining, tiling, and parallel processing for each nested loop structure, and apply automatic parallel reduction for loop-carried dependencies on the temporary and output accumulation arrays:\n\n- Use '#pragma ACCEL kernel' to specify that the function 'kernel_2mm' should be treated as a hardware kernel for acceleration.\n- Apply '#pragma ACCEL PIPELINE auto' with an auto-assigned label to each loop level to allow the tool to determine the optimal pipelining strategy.\n- Insert '#pragma ACCEL TILE FACTOR=auto' with an auto-assigned label to each loop level to permit the tool to automatically choose an appropriate tiling factor for breaking down the iteration space.\n- Set '#pragma ACCEL PARALLEL FACTOR=auto' with an auto-assigned label to each loop level to let the tool decide on the parallel processing factor for the iterations.\n- Specify '#pragma ACCEL PARALLEL reduction = [array name] FACTOR=auto' with an auto-assigned label within the k-loops to indicate that the tool should handle parallel reduction for updating the 'tmp' and 'D' arrays, automatically determining the reduction factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following computation into an optimized hardware kernel, using automatic settings for pipelining, tiling, and parallelizing the loops. Specifically, for each loop, adjust pipelining to allow overlapped execution of iterations when possible, choose an appropriate tiling factor to break loop iterations into smaller, more manageable blocks, and set parallel execution factors to run iterations concurrently, as resources permit. Additionally, apply reduction to accumulate values within certain loops for 'tmp' and 'D' matrices, to ensure correct handling of dependencies during concurrent operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, allowing the compiler to automate pipelining and tiling optimization for loops and to parallelize loop iterations with automatic factor determination, while performing reduction operations within the nested loops to cumulatively update the matrix multiplication results.\n\n- Use `#pragma ACCEL kernel` to indicate that the function 'kernel_2mm' should be treated as a hardware acceleration kernel.\n- Apply `#pragma ACCEL PIPELINE auto` with a unique label for each loop to let the compiler decide the pipeline strategy for each loop for efficient scheduling.\n- Use `#pragma ACCEL TILE FACTOR=auto` with unique labels for each loop to enable the compiler to determine the tiling factor for optimal data partitioning in hardware.\n- Employ `#pragma ACCEL PARALLEL FACTOR=auto` with unique labels for each loop to instruct the compiler on how many iterations of the loop to execute in parallel, where the factor is automatically determined by the compiler.\n- Integrate `#pragma ACCEL PARALLEL reduction` followed by the variable to be reduced and `FACTOR=auto` with a unique label for the nested inner loop to direct the compiler to perform parallel reductions on that variable across the iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, then apply automatic optimization directives to pipeline and tile all loops for parallel execution, and automatically parallelize innermost loop iterations with reduction on the 'tmp' and 'D' array accumulations. Use the placeholders `__PIPE__LX`, `__TILE__LX`, and `__PARA__LX` to let the HLS tool decide the best factors for pipelining, tiling, and parallelism for each loop level 'X' respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with the respective pragmas, please follow this instruction:\n\nDefine a function to be accelerated as a hardware kernel that performs a two-matrix multiplication followed by the addition of another product of two matrices to the result (2mm operation). The first multiplication takes a matrix A and multiplies it by matrix B, and then the result is multiplied by matrix C; the final results are then scaled by a constant alpha. Additionally, an existing matrix D is scaled by a constant beta and then added to the final result. Apply automatic optimization directives at each level of the nested loops, allowing the HLS tool to decide the best optimization strategies based on the available resources and target constraints. Specifically, for the outermost loops, enable automatic pipelining to improve throughput, use automatic tiling to break down large data arrays into smaller chunks for better data reuse, and automatically parallelize the loops to unroll them for concurrent execution, allowing for higher throughput and parallelism at each level of loop computations. Finally, apply automatic reduction to aggregate intermediate results while keeping a balance between parallelism and resource usage during accumulation operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the `kernel_2mm` function as a hardware kernel, automatically managing the pipelining, tiling, and parallelization of the nested loops. For the computation of `tmp` and updating of `D`, enable parallel reduction on the accumulator variable. Adjust the pipelining, tiling, and parallel factor pragmas as follows:\n\n- In the outermost loops (over `i`), automatically determine the optimal pipeline, tile size, and parallelization factor.\n- In the second loops (over `j`), automatically determine the pipeline, tile size, and parallelization factor for both computation of `tmp` and updating of `D`.\n- In the innermost loops (over `k`), enable automatic parallel reduction with the `tmp` and `D` variables, allowing the compiler to determine the optimal parallelization factor while performing the reduction operation. \n\nUse the 'auto' configuration to let the hardware compiler decide the optimal values for pipelining, tiling, and parallelization factors, marked by placeholders like `{__PIPE__L0}`, `{__TILE__L0}`, `{__PARA__L0}`, and so on for each respective loop level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel and employ pragma directives to automatically optimize the pipeline, tile, and parallel execution of the loops with the following specifications:\n\n1. For the outer loops labeled L0 and L1, enable automatic determination of pipeline settings to optimize for throughput, optimize tiling to balance parallelism and memory usage, and determine the parallel execution factors for balanced parallel processing and resource utilization.\n2. For the nested loops labeled L2 and L3, replicate the optimization approach used for the outer loops L0 and L1.\n3. For the innermost loops labeled L4 and L5, enable parallel execution with reduction on 'tmp' and 'D' arrays respectively, allowing the compiler to automatically choose the parallel execution factor while ensuring proper accumulation of values to prevent race conditions.\n\nEnsure that all 'auto' pragmas are dynamically assigned optimal values by the compiler for the respective loops L0 to L5.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code for a 2mm kernel function that processes matrix multiplication and addition with alpha and beta scaling factors, such that the function is treated as a hardware kernel for acceleration. Enable automatic decisions for pipelining, tiling and parallel processing of loops to optimize for performance. For the first level of nested loops, compute the product of matrices A and B scaled by alpha and store it in a temporary matrix. For this computation, apply automatic optimization techniques, including a reduction on the temp matrix. For the second level of nested loops, scale matrix D by beta and add the product of the temporary matrix and matrix C to it, again applying automatic optimization and reduction on the D matrix. Ensure that every #pragma directive let's the compiler decide the best factor value as the directives guide the compiler about the structure of the computation and memory access patterns.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the function 'kernel_2mm' for hardware execution, allowing the compiler to optimally pipeline all loops, automatically determine loop tiling factors, and parallelize computations where beneficial with automatic assignment of parallel factors. Additionally, apply reduction to accumulate values in the 'tmp' and 'D' arrays in the nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function where the operation `D := alpha * A * B * C + beta * D` is performed on matrix inputs. For each loop, automatically determine the optimal pipeline, tiling, and parallelism strategies using pragmas with 'auto' directive for the compiler to decide the best factor. In nested loops, also use parallelism with automatic factor and reduction clauses for the intermediate accumulation variables 'tmp' and 'D' respectively. Mark the function as a hardware kernel to be accelerated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated function 'kernel_2mm' that takes as input the dimensions 'ni', 'nj', 'nk', 'nl', and the scaler factors 'alpha' and 'beta', along with two-dimensional arrays 'tmp', 'A', 'B', 'C', and 'D'. Apply optimizations for pipelining, tiling, and parallelization with automatic determination of the optimal values. Include two nested loop blocks where the inner loops are parallelized with respect to 'tmp' and 'D' using automatic factor determination and reduction to accumulate the multiplications. The outer loops of both blocks should also have automatic optimization directives applied for pipelining, tiling, and parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with appropriate #pragma directives for optimization, use the following instruction:\n\n\"Create a hardware-accelerated function named 'kernel_2mm' that performs a modified matrix multiplication and addition, with matrices 'A', 'B', 'C', 'D', and intermediary 'tmp', parameterized by sizes 'ni', 'nj', 'nk', and 'nl', utilizing the factors 'alpha' and 'beta'. Enable automatic pipeline, tiling, and parallelization optimizations for each nested loop, adjusting factors as needed for the best performance. For the innermost loops, apply a parallel reduction on the target temporary matrices. Let HLS tools decide the optimization factors for pipeline, tile, and parallel by setting them to 'auto'.\"\n\nThis instruction details the use of the following #pragma values:\n- `#pragma ACCEL kernel`: This directive indicates to treat the 'kernel_2mm' function as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto`: This indicates to pipeline loops automatically, optimizing loop iterations to possibly overlap in execution.\n- `#pragma ACCEL TILE FACTOR=auto`: This hints to the HLS tool to automatically determine the tiling factor for loop iterations, breaking them into smaller, manageable chunks that can be scheduled independently.\n- `#pragma ACCEL PARALLEL FACTOR=auto`: This instructs the tool to automatically determine a factor by which the loop iterations can be executed in a parallel fashion.\n- `#pragma ACCEL PARALLEL reduction = target FACTOR=auto`: This directive applies automatic parallelization to recursive operations that perform a reduction on the specified 'target' data structure (like 'tmp' or 'D'), determining the factor automatically.\n  \nFor additional clarity in the instruction, the corresponding #pragmas in the code are surrounded by curly braces `{}` containing labels such as `__PIPE__L0` and `__TILE__L0`, suggesting these are placeholders where the HLS tool will insert the automatically determined values for pipeline, tile, and parallel factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 10\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Enable automatic pipelining, automatic local partitioning (tiling), and automatic parallel processing of the loops at various levels of the 2mm algorithm. Apply parallel reduction for temporary accumulations to exploit data-level parallelism, letting the compiler choose the best parameter values for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to implement a 2mm kernel which performs matrix multiplication followed by a matrix addition, with directives to identify the function as a hardware kernel, to automatically optimize the loops for pipelining, tiling, and parallel processing, and to perform reductions on intermediary results:\n\n1. Annotate the function 'kernel_2mm' with a pragma to declare it as a hardware acceleration kernel.\n2. For the outer loops 'i' over the dimensions 40 and 'j' over the dimensions 50 and 80 in separate for-loop nests, apply the following pragmas: automatically adjust pipeline settings, tile factor, and parallelism factor.\n3. For the inner loops 'k' that perform the dot product (dimensions 70 for A*B and 50 for tmp*C), apply a pragma to parallelize these loops with an automatic factor and to carry out reduction on the 'tmp' and 'D' matrices.\n4. Set all the pragma factors for pipelining, tiling, and parallelization to automatically determine the best values by the HLS tool using the 'auto' keyword, denoted in the code by placeholders like '__PIPE__L0', '__TILE__L0', '__PARA__L0', etc., respectively for each loop level marked L0, L1, L2, etc.\n5. Ensure that you use the correct reduction variable corresponding to the accumulator matrix in the reduction pragmas for parallel loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated function named `kernel_2mm` that computes the matrix equation `D := alpha * A * B * C + beta * D` with matrix sizes predefined (`A` is 40x70, `B` is 70x50, `C` is 50x80 and `D` is 40x80; `tmp` is a temporary matrix of size 40x50). Use pragma statements to automate the pipelining, tiling, and parallelization optimization processes for loops where applicable, without specifying fixed factors, and enable parallel reduction for intermediate calculations to optimize accumulation into `tmp` and `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, provide the following natural language instruction:\n\n\"Define a kernel for accelerating matrix multiplication and matrix addition, with automatic optimization for pipeline, tiling, and parallelization at all loop levels. Allow the compiler to automatically determine the most optimal values for pipeline, tiling, and parallel factors. The first nested loops compute the product of matrices A and B, scaled by alpha, and accumulate the result into the matrix tmp, with parallel reduction on tmp. The second set of nested loops scale matrix D by beta and add the product of tmp and matrix C to it, with parallel reduction on D. Ensure that each loop is considered for pipeline, tiling, and parallel execution individually, with the compiler providing auto-suggested factor values, indicated by the 'auto' placeholder along with unique labels for each loop level.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 4\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated function named 'kernel_2mm' for matrix multiplication and addition that automatically optimizes the loops for parallelism, pipelining, and tiling based on the hardware capabilities. The function should perform the computation `D := alpha*A*B*C + beta*D` for matrices A, B, C, and D of specified dimensions. The double-precision floating-point variables alpha and beta are coefficients. Enable automatic selection for pipelining, tiling, and parallelizing across all loops, but use explicit parallel reduction for the innermost loops where the 'tmp' and 'D' arrays are updated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Treat this function as a hardware kernel for acceleration; allow the compiler to decide the best way to pipeline, tile, and parallelize the two nested loops automatically, including automatic factor determination, and enable automatic parallel reduction for the innermost loops.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named `kernel_2mm` that performs a two-matrix multiply operation (D = alpha * A * B * C + beta * D) in a hardware-accelerated manner by automatically optimizing loops with pipeline, tile, and parallel pragmas for iteration space and data reduction, specifically including automatic decisions for all optimization factors and ensuring the function itself is marked for acceleration as a kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Automatically optimize loop pipelining, tiling, and parallelism for all loops. Apply reduction pragmas to the nested innermost loops to facilitate parallel accumulation into the 'tmp' and 'D' arrays respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated 2mm kernel function that takes various matrix dimensions (ni, nj, nk, nl) and scalars (alpha, beta), as well as matrices A, B, C, D, and a temporary matrix tmp. Allow automatic optimization for loop pipelining, tiling, and parallelization where appropriate. Specifically, for the first set of nested loops that calculate the matrix multiplication `tmp = alpha*A*B`, enable all forms of optimization on the outer loops (i and j) and parallelize with reduction on the innermost loop (k). Similarly, for the second set of nested loops that update matrix D as `D = beta*D + tmp*C`, enable optimization on the outer loops (i and j) and parallelize with reduction on the innermost loop (k). Use 'auto' for the tool to decide the optimal values for pipeline, tile, and parallel factors. Additionally, indicate the reduction variable in parallel pragmas where accumulation occurs.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided C function 'kernel_2mm' as a hardware acceleration kernel and apply the following optimizations using pragmas with automatic factor determination by the HLS tool:\n\n1. Enable automatic pipelining at three levels of nested loops - globally at the first loop level for both the calculation of 'tmp' and 'D' matrices, and then locally within each second loop level.\n2. Activate tiling at the outer two loop levels for each calculation to enable processing blocks of data in each iteration.\n3. Allow the HLS tool to automatically decide on the parallelization factors for each loop level, with a special emphasis on reduction for accumulation in the innermost loops that update 'tmp' and 'D' matrices respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 4\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C codes with specific pragma annotations, the instruction to the HLS tool would be:\n\n\"Define the function 'kernel_2mm' as a hardware acceleration kernel with loops that automatically optimize parallel execution, loop tiling, and pipelining, where the optimization parameters for each loop are to be automatically determined by the HLS tool. Apply parallel reduction to accumulate results within the innermost loops where 'tmp' and 'D' are being computed. Do not manually set numeric factors for tiling or parallelism phases\u2014the HLS tool should infer them automatically.\"\n\nSpecific explanations for each pragma directive used in the code are:\n\n- Treat 'kernel_2mm' as a hardware acceleration kernel with `#pragma ACCEL kernel`.\n- For the outer loop iterating over 'i', enable pipelining, tiling, and parallel processing with optimization factors to be automatically inferred by the tool with the respective pragmas:\n\n  `#pragma ACCEL PIPELINE auto{__PIPE__L0}`\n\n  `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}`\n\n  `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n\n- For the second outer loop iterating over 'j' within the first 'i' loop, apply the same set of automatic optimizations for pipelining, tiling, and parallel processing with:\n\n  `#pragma ACCEL PIPELINE auto{__PIPE__L2}`\n\n  `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`\n\n  `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}`\n\n- Within the innermost loop iterating over 'k' for computing 'tmp', use parallel reduction to combine results with the automatic factor determined by the HLS tool:\n\n  `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}`\n\n- Apply the same automatic optimization pragmas for the subsequent nested loops processing the multiplication and accumulation for the matrix 'D':\n\n  `#pragma ACCEL PIPELINE auto{__PIPE__L1}`\n\n  `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`\n\n  `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n\n  `#pragma ACCEL PIPELINE auto{__PIPE__L3}`\n\n  `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}`\n\n  `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}`\n\n- For the innermost loop iterating over 'k' for computing 'D', include a parallel reduction with:\n\n  `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}`\n\nThe key aspect to note is that all optimization factors such as pipeline, tile, and parallel factors are to be determined automatically by the HLS tool, indicated by 'auto{...}' in the respective pragma statements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive and very concise natural language instruction to generate the HLS codes provided:\n\n\"Define a hardware acceleration kernel that performs a 2mm (two-matrix multiply) operation on matrix dimensions defined by the input parameters. Initially, precalculate matrix product `A * B` scaled by `alpha` and store the result into `tmp`. Then, update matrix `D` by scaling it with `beta` and adding the product of `tmp * C`. Automatically optimize each loop with pipelining, tiling, and parallel processing directives where the specific optimization factors are to be automatically determined by the HLS tool for efficiency.\" \n\nRemember to specify automatic optimization factors within the compiler directives: for pipelining use `#pragma ACCEL PIPELINE auto`, for tiling `#pragma ACCEL TILE FACTOR=auto`, and parallel processing with `#pragma ACCEL PARALLEL FACTOR=auto`. Furthermore, include the appropriate reduction clauses for parallel accumulation in `tmp` and `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create hardware-accelerated implementation of the 2mm kernel function, allowing the compiler to automate decisions on how to pipeline and parallelize loops for efficient execution, and optimize data movement through tiling.\n\nSpecifically,\n1. Designate the whole `kernel_2mm` function for hardware acceleration.\n2. Enable automatic pipelining for both the first and second outer loop nests, letting the compiler determine the best pipeline settings.\n3. Allow the compiler to automatically choose tiling factors for both the first and second outer loop nests to optimize locality and memory access.\n4. For both outer loop nests, enable automatic parallelization to balance workload across processing elements.\n5. In the innermost loops, employ automatic parallelization with a reduction clause to safely accumulate values in the `tmp` and `D` arrays, while also letting the compiler determine the best factor for parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function named `kernel_2mm` as a hardware acceleration kernel to perform a two-matrix multiplication and addition operation with inputs `ni`, `nj`, `nk`, `nl`, `alpha`, `beta`, and matrices `tmp`, `A`, `B`, `C`, `D`. Enable automatic optimization for loop pipelining, tiling, and parallelization, and specify automatic reduction for intermediate results. Each loop and nested loop should be automatically optimized for pipelining, tiling, and parallel execution, and reduction should be applied where intermediate results are accumulated in the `tmp` and `D` matrices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS C code for a matrix multiplication and addition kernel to be used for acceleration in FPGA, specifying that the compiler may automatically determine the most suitable pipeline, tile, and parallel factor options for each loop in the function. For the innermost loops, employ parallelism with appropriate reduction on the accumulator arrays `tmp` and `D` to ensure efficient use of FPGA resources. Ensure that the top-level function is designated as a kernel for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to perform a 2mm kernel operation on matrix data where the function 'kernel_2mm' should be treated as a hardware kernel for acceleration. Apply automatic pipelining, tiling, and parallelism to the outer loops to optimize throughput, and use automatic factor determination for achieving good performance without explicitly specifying the factors. Enable parallel reductions on the innermost loop computations involved in matrix operations with automated determination of parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function named 'kernel_2mm' that performs a 2-matrix multiplication sequence where matrix `D` is set to `alpha*A*B + beta*D`. Define this function to take as parameters the sizes of the matrices `ni`, `nj`, `nk`, `nl`, the `alpha` and `beta` scalar values, and the matrices `A`, `B`, `C`, and `D` themselves. Ensure automatic optimization for the loops over all dimensions using pipelining, tiling, and parallel processing, and specify the use of reduction for accumulator variables `tmp` and `D`. Let the compiler determine the optimal values for pipelining, tiling, and parallel processing (denoted with 'auto' and placeholders like `__PIPE__L0`, `__TILE__L0`, and `__PARA__L0`) at each level of the nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Enable automatic optimization for the entire function treating it as a hardware acceleration kernel, and apply dynamic pipeline, tiling, and parallelization strategies with compiler-chosen factors to all nested loops. For the innermost loops, specify parallel execution with a compiler-decided factor and enable reduction on the accumulator arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to act as a hardware acceleration unit. Automatically configure the pipeline, tile, and parallel execution optimizations for the operation of the matrix formula D := alpha*A*B*C + beta*D. Apply reduction to accumulate partial results when required and set the optimization factors for loop unrolling, tiling, and pipelining to be determined automatically by the tool for each nested loop instance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel and apply automated optimizations to enhance loop performance and parallel execution. Specifically:\n\n1. Transform the entire computation block of the matrix multiplication and addition, which involves the loop nests for iterating over the array dimensions, ensuring the outermost loops `i` in both block segments are automatically pipeline optimized for sequential data flow.\n\n2. Automatically determine and apply the tiling factor for breaking down the loops into smaller, more manageable chunks to optimize data locality and reuse.\n\n3. Automatically configure the parallelization factor to unroll loops and execute multiple iterations in parallel where suitable, including the innermost loops `k` with parallel reduction on the `tmp` and `D` arrays to efficiently combine results across parallel iterations.\n\nApply these optimizations to all the pertinent loops in the function with the corresponding pragmas to allow for the HLS tool to adjust the pipelining, tiling, and parallelism automatically, as denoted by the `auto` keyword and placeholders for HLS tool-specific values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the specified C function as a candidate for acceleration in hardware and apply the following optimizations dynamically: initialize automated loop pipelining, define loop tiling, and set parallel loop execution to optimize for performance. Additionally, within specific nested loops, allow for data reduction to enhance the computation of cumulative operations.\n\n- Treat the 'kernel_2mm' function as a hardware kernel for acceleration.\n- For the outer loops 'i' in both main segments, enable automated pipelining, dynamic loop tiling, and automatic parallelization.\n- For the 'j' loops nested within, also enable automated pipelining, dynamic loop tiling, and automatic parallelization.\n- In the innermost 'k' loops where the multiplication and addition to 'tmp' occur, allow for automatic parallelization with data reduction on 'tmp'.\n- Similarly, in the other 'k' loop where 'D' is updated, also apply automatic parallelization and enable data reduction on 'D'.\n\nThe #pragma values to use are as follows:\n\n- `#pragma ACCEL kernel` to specify hardware acceleration for the function.\n- `#pragma ACCEL PIPELINE auto` for all loops you wish to pipeline (`__PIPE__Lx` are placeholders for specific loop labels, which could be specified if required).\n- `#pragma ACCEL TILE FACTOR=auto` for all loops you wish to tile (`__TILE__Lx` are placeholders for specific loop labels).\n- `#pragma ACCEL PARALLEL FACTOR=auto` for all loops you want to parallelize (`__PARA__Lx` are placeholders for specific loop labels).\n- `#pragma ACCEL PARALLEL reduction = variable_name FACTOR=auto` where `variable_name` is 'tmp' or 'D', depending on the loop context for parallelization with data reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel, applying optimizations to the nested loops for efficient parallel execution. Automatically determine the best pipeline settings for the first level of nested loops, dynamically choose tiling factors to portion loops into smaller blocks, and automatically decide on the parallel execution factors for each loop level. Enable parallel reduction on specific inner loops to enhance data aggregation operations while following the compiler's automatic factor selection, ensuring that the intermediate and final results use optimal parallelism for the matrix multiplication and addition calculations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 10\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the described HLS codes, here is a comprehensive but concise instruction:\n\nTransform the function `kernel_2mm` into a hardware-accelerated kernel that computes a two-matrix multiplication operation with scaling factors alpha and beta. For each nested loop, let the compiler decide the best way to pipeline, tile, and parallelize the loop by using the pragma values 'auto'. Enable parallel reduction to accumulate the inner multiplication results into the `tmp` and `D` arrays. Apply these optimizations:\n\n1. For the first two levels of nested loops where the `tmp` array is computed, pipeline and parallelize both loops and enable parallel reduction for the innermost loop on the `tmp` array with automatically determined factors.\n2. For the second two levels of nested loops where the `D` array is updated, again pipeline and parallelize both loops and enable parallel reduction for the innermost loop on the `D` array with automatically determined factors.\n\nHere are the respective #pragma values to include:\n\n- Use `#pragma ACCEL kernel` to mark the function for hardware acceleration.\n- Inside each loop nest, use `#pragma ACCEL PIPELINE auto` to let the compiler automatically pipeline the loops.\n- Apply `#pragma ACCEL TILE FACTOR=auto` within each set of nested loops to let the compiler decide how to tile the loops automatically.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto` for each loop to enable automatic parallelization by the compiler.\n- Employ `#pragma ACCEL PARALLEL reduction` along with automatic factor determination `{__PARA__L4}` or `{__PARA__L5}` for the innermost loops to allow parallel reduction on the `tmp` and `D` arrays, respectively.\n\nThis set of pragmas instructs the HLS tool to optimize the function for hardware acceleration, focusing on efficient data manipulation and parallel execution while leaving specific optimization decisions to the compiler's heuristics.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS optimized kernel function named `kernel_2mm` for a 2mm algorithm using the given matrices `A`, `B`, `C`, and `D`, and coefficients `alpha` and `beta`. Apply hardware acceleration pragmas to optimize the computation by:\n\n1. Automatically determining whether to pipeline the four nested loops for possible concurrent execution.\n2. Automatically selecting the optimal chunk size for tiling the four nested loops to efficiently utilize local memory.\n3. Automatically managing parallel execution of the four nested loops, including any necessary reductions for the accumulation within the loops.\n4. Designating the function as a target for hardware acceleration.\n\nSpecifically, for each of the four nested loops, add the corresponding pragmas:\n- `#pragma ACCEL PIPELINE auto` to suggest automatic pipelining decisions.\n- `#pragma ACCEL TILE FACTOR=auto` to allow automatic tiling factor selection.\n- `#pragma ACCEL PARALLEL FACTOR=auto` to enable automatic parallel factor decisions, and for loops that include an accumulation (reduction), add `reduction` with the appropriate variable targeted for the accumulation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the 'kernel_2mm' function to perform a 2mm computation as a hardware acceleration kernel, applying an automated optimization strategy for loop pipelining, tiling, and parallelization. The outer loops over dimensions 40 and 50 should be automatically optimized for pipelining, tiling, and parallelism. Include a parallel reduction directive in the inner loops to accumulate results for 'tmp' and 'D' arrays, allowing the compiler to automatically determine the factor of parallelism for these reductions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function 'kernel_2mm' as a hardware acceleration kernel. Automatically optimize the outer loops for parallel execution, pipelining, and appropriate partitioning of the data. Within each outer loop, initialize a matrix 'tmp' or scale matrix 'D' and then perform a matrix multiplication with automatic parallelism, reduction, and loop optimizations. Specifically, instruct the compiler to decide the best factors for pipelining, tiling, and parallel execution for each loop, and conduct reduction operations to accumulate results into 'tmp' or 'D' during the matrix multiplications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into an optimized hardware kernel using High-Level Synthesis (HLS) by allowing the compiler to automatically decide the optimal pipelining, tiling, and parallelism factors. Perform matrix operation 'D = alpha * A * B * C + beta * D' where 'D', 'A', 'B', 'C' are matrices and 'alpha', 'beta' are scalars. Use reduction parallelism for accumulations on 'tmp' and 'D' matrices.\n\nIn terms of specific #pragma instructions:\n\n- Declare the function as a hardware acceleration kernel.\n- For outer matrix multiplication loop blocks, enable automatic optimization for pipeline, tiling, and parallelism.\n- Inside the first two-level loop nest where the 'tmp' matrix is calculated, automatically optimize pipelining and tiling for the outer loop, and apply automatic parallelism factor with reduction on 'tmp' for the innermost loop.\n- Inside the second two-level loop nest where the 'D' matrix is updated, enable automatic pipelining and tiling for the outer loop, and use automatic parallelism factor with reduction on 'D' for the innermost loop.\n\nEnsure that the 'auto' option is used for the compiler to infer the best possible values seamlessly for each pragma directive.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function that accelerates the computation of the expression D = alpha * A * B * C + beta * D. Use automatic pipeline optimization on all loops, allowing the compiler to choose the best settings. Automatically tile and parallelize all loops to enable parallel execution and data optimization, with the compiler determining the factor for each loop. Apply a parallel reduction to the innermost loops where temporary arrays are involved in accumulation. Define the entire function as a hardware kernel to be accelerated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel and optimize the nested loops within for parallelism, pipelining, and tiling, allowing the compiler to automatically determine the optimal values for each pragma directive. Specifically, enable automatic pipelining for the first level loops `i` in both the multiply and add sections, automating tiling and parallelizing in all loops, and perform reduction optimizations on the innermost `k` loops for the temporary result arrays 'tmp' and 'D' to ensure efficient accumulation of values in hardware.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function 'kernel_2mm' to be accelerated in hardware, which computes matrix multiplication and addition with two separate loops following the equation `D := alpha*A*B + beta*D`. Enable automatic optimization choices for loop pipelining, tiling, and parallel processing. Apply reduction to the appropriate variables in the innermost loops to accumulate values correctly. Use pragmas for pipeline, tile, and parallel with 'auto' settings to let the tool determine the best factors for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 16\n__PARA__L4: 4\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, enable automatic optimization for the processing of loops and data tiling, and also facilitate parallel execution while maintaining the opportunity for reduction operations within the nested loops. Specifically, allow the compiler to determine the optimal pipeline settings for each loop, the best tile size for inner loop unrolling, and the most efficient factor for parallelizing loop iterations, while ensuring the correct accumulation of results in the 'tmp' and 'D' arrays during the multiplication and addition operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the current HLS codes, you should direct the compiler to consider the `kernel_2mm` function as a dedicated hardware acceleration kernel. For each loop, automatically choose the best configuration for pipeline optimization, tiling, and parallel execution, while ensuring that the operations within the innermost loops that update the `tmp` and `D` matrices respectively use parallel reduction. Also, enable automatic determination of the optimal factor by which to tile, pipeline, or parallelize each loop. Specifically, you might say:\n\n\"Treat the `kernel_2mm` function as a dedicated acceleration hardware kernel. For each loop within `kernel_2mm`, let the compiler automatically decide the optimal settings for pipelining, tiling, and parallelization to improve performance. For the innermost loops that update the `tmp` and `D` matrices, use parallel reduction with an automatically determined factor to accelerate the associative and commutative reduction operations.\"\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated implementation for matrix multiplication and addition using the 2mm algorithm, with automatic optimization pragma settings for pipelining, tiling, and parallelization. Specifically, define a kernel function 'kernel_2mm' with parameters for input and output matrix sizes and the scalar values alpha and beta. Within the kernel, there are two main computational blocks. First, calculate the product of matrices A and B, scale it by alpha, and store it in temporary matrix tmp. Then add the product of matrix tmp and matrix C to the matrix D scaled by beta. For each of the four loops within these blocks, automatically optimize the loop execution by applying pipelining, tiling, and parallelization, and use reduction to accumulate results in the parallel sections. Allow the compiler to determine the appropriate factors for these optimizations with the 'auto' setting. Apply parallelization with automatic factor determination and specify reduction on the output matrix temporaries within the innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes, use the following natural language instruction:\n\n\"Designate the function 'kernel_2mm' as a hardware kernel to be accelerated. For each nested loop within this function, apply automatic pipelining, tiling, and parallelization, adjusting the pragmas' factors for optimal performance based on the compiler's decision. Additionally, implement parallel reduction for the innermost loops that accumulate values into 'tmp' and 'D' arrays, letting the compiler decide the parallelization factor for these reductions.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, where it performs a matrix multiplication and matrix addition. Automatically optimize the loop pipelining, tiling, and parallelization for all nested loops. Apply automatic parallel reduction to the temporary accumulations within the innermost loops for both matrix operations.\n\n- Use \"Designate this function as a hardware acceleration kernel\" for `#pragma ACCEL kernel`.\n- Use \"Optimize this loop with automatic pipelining\" for each instance of `#pragma ACCEL PIPELINE auto`.\n- Use \"Automatically determine the optimal tiling factor for this loop\" for each instance of `#pragma ACCEL TILE FACTOR=auto`.\n- Use \"Automatically determine the optimal parallelization factor for this loop\" for each instance of `#pragma ACCEL PARALLEL FACTOR=auto`.\n- Use \"Apply automatic parallel reduction to this loop with the specified target variable\" for each instance of `#pragma ACCEL PARALLEL reduction`.\n\nRemember to replace placeholders like `{__PIPE__L0}` with the actual label of the loop to which the pragma applies, following the same for `{__TILE__L0}`, `{__PARA__L0}`, `{__PIPE__L2}` and so on for other loops. These placeholders suggest that the pragmas are to be applied automatically with possibly predetermined optimization parameters that are specific to each loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with appropriate pragmas, apply the following instructions in natural language to each part of the code respectively:\n\n1. Treat the 'kernel_2mm' function as target for hardware acceleration with: \"Mark the 'kernel_2mm' function as a hardware kernel for acceleration.\"\n\n2. Enable automatic pipelining analysis to potentially overlap loop iterations at the first 'for' loop level inside the 'kernel_2mm' function with: \"Automatically determine the optimal unrolling for the first layer of loops to improve throughput without explicit factor provided.\"\n\n3. Similarly, allow the tool to automatically determine the optimal tile size for the first 'for' loop level with: \"Allow the compiler to automatically select the best chunk size for the first layer of loops.\"\n\n4. Also for the first 'for' loop level, permit automatic parallel factor determination with: \"Permit the compiler to decide the best level of parallelism for the first layer of loops.\"\n\n5. Repeat the automatic pragma instructions (pipeline, tile, and parallel) for the second layer 'for' loop (the one nested directly inside the first 'for' loop) with: \"Apply the same optimization strategies of automatic pipelining, tiling, and parallelization for the second nested loop layer as with the first loop layer.\"\n\n6. In the innermost loop (third layer) inside the first 'for' loop block, use a `reduction` operation to handle the accumulation into the 'tmp' array efficiently: \"Automatically parallelize the innermost loop where 'tmp' array is being accumulated, allowing concurrent reduction operations.\"\n\n7. Lastly, apply the same set of automatic optimizations used for the first 'for' loop block to the second 'for' loop block which computes the update of 'D' matrix: \"Repeat the automatic optimization strategies (pipeline, tile, and parallel) used for the first loop block to the second loop block with the update of the 'D' matrix, and similarly apply a reduction operation for the nested innermost loop.\"\n\nThis comprehensive set of instructions caters to the provided HLS C code with directives for automatic optimization decisions made by the HLS tool without specifying explicit pragma values for factors, relying on the 'auto' keyword to give the HLS tool the freedom to choose values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the 'kernel_2mm' function with automated directive optimizations: auto-tile and auto-pipeline the two outermost loops, and auto-parallelize all loops. Enable reduction for partial results in 'tmp' and 'D' arrays within their innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize a 2-matrix multiplication function followed by an addition to a third matrix with scalars for acceleration in hardware by using automatic optimization directives for pipelining, data tiling, and parallel processing, while also applying automatic parallel reduction over designated loop-carried dependencies:\n\n1. Mark the 'kernel_2mm' function as a hardware acceleration kernel.\n2. Automatically apply pipelining, tiling, and parallelism directives at the appropriate nesting levels of the 'for' loops responsible for computing the matrix operations.\n3. Utilize automatic parallel reduction to handle accumulations in the 'tmp' and 'D' matrices efficiently.\n4. Let the compiler determine the optimal values for pipeline stages, tile sizes, and parallelism factors by specifying 'auto'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function for hardware acceleration where the matrix multiplication and addition of `D := alpha*A*B*C + beta*D` is computed. Enable automatic pipelining, tiling, and parallel processing optimizations for all loops. Utilize parallel reduction on the innermost loops where the `tmp` and `D` matrices are being updated respectively. Let the compiler determine the optimal values for these optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_2mm' function as a hardware kernel, optimize it for parallelization by automatically selecting the best pipeline settings, tile sizes, and parallel factors for each loop, and apply reduction to accumulate the sums in the 'tmp' and 'D' arrays. Use 'auto' placeholders in pragmas where automatic optimization is desired.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 14\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the following hardware-accelerated kernel using automatic optimization directives for pipelining, tiling, and parallelizing at different loop levels, while ensuring a reduction operation accumulates correctly in two separate loops:\n\n1. Declare the function 'kernel_2mm' as a hardware-accelerated kernel.\n2. For the two top-level loops iterating over the 'i' index, apply automatic optimization for pipelining, tiling, and parallelizing without specifying particular factor values.\n3. Inside the first 'i' loop for the 'j' index, apply automatic pipelining, tiling, and parallelizing, and within its innermost 'k' loop, enable automatic parallel factor optimization with a reduction clause for accumulating into the 'tmp' array.\n4. In the second 'i' loop for the 'j' index, also apply automatic pipelining, tiling, and parallelizing, and within its inner 'k' loop, enable automatic parallel factor optimization with a reduction clause for accumulating into the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, allowing for automatic optimization of loop pipelining, memory tiling, and parallel execution with factor decisions deferred to the HLS tool. Specifically:\n\n1. Treat the entire function 'kernel_2mm' as a target for hardware acceleration.\n2. Automatically determine the best loop pipelining, tiling, and parallel execution strategies for the first pair of nested loops iterating over indices 'i' and 'j', which initialize the 'tmp' array and calculate its values by multiplying 'alpha' with the matrices 'A' and 'B'.\n3. Apply an automatic parallel reduction to the innermost loop over 'k' that accumulates the 'tmp' array elements.\n4. Apply the same automatic optimization strategies for the second set of nested loops that update the 'D' matrix by scaling it with 'beta' and then adding the product of 'tmp' and 'C'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Label the following function as a hardware kernel, automatically optimize the loops for pipelining, tiling, and parallel execution with system-determined optimal factors, and enable a reduction pattern for the temporary accumulation arrays in nested loops:\n\n1. Declare a function named `kernel_2mm` that takes in dimensions `ni`, `nj`, `nk`, `nl`, multiplication factors `alpha`, `beta`, and four 2D array pointers `tmp`, `A`, `B`, `C`, `D`.\n2. Within the first two nested loops, initialize array `tmp` with zeroes and calculate the matrix multiplication of `A` and `B`, scaled by `alpha`.\n3. Within the second set of two nested loops, scale matrix `D` by `beta`, then add the matrix multiplication of `tmp` and `C`.\n4. For each nested loop structure, use automatic pipelining, tiling, and parallelization directives, assigning unique identifiers to each loop's pragmas like `__PIPE__L0`, `__TILE__L0`, and `__PARA__L0`.\n5. Apply the parallel pragma with a reduction clause for the `tmp` and `D` arrays where the sum is accumulated, with system-determined optimal parallel factors using placeholders like `__PARA__L4` and `__PARA__L5`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function `kernel_2mm` that performs a 2-matrix multiply (2MM) operation, where matrix `D` is updated based on the equation `D := alpha*A*B + beta*D`. The function takes six parameters `ni`, `nj`, `nk`, `nl`, along with scalars `alpha` and `beta`, and it operates on matrices `tmp`, `A`, `B`, `C`, and `D`. Optimize the nested loops by automatically determining the pipeline, tile, and parallel pragma parameters for the most efficient hardware implementation, with reductions specified where necessary to manage data dependencies.\n\nHere are the HLS pragma instructions corresponding to the above directive:\n\n1. Designate the entire function as a hardware kernel using `#pragma ACCEL kernel`.\n\n2. For each loop level (0, 1, 2, 3, 4, and 5), apply the following optimization pragmas:\n   - `#pragma ACCEL PIPELINE auto{__PIPE__L#}`: Let the compiler automatically decide if and how to pipeline the loop with label `L#`, where `#` corresponds to the loop level.\n   - `#pragma ACCEL TILE FACTOR=auto{__TILE__L#}`: Let the compiler automatically determine the tiling factor for the loop at level `L#`.\n   - `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L#}`: Allow the compiler to automatically figure out the parallelization factor for threading the loop at level `L#`.\n\n3. On the innermost loops (levels 4 and 5) where a reduction operation occurs (the accumulation into `tmp[i][j]` and `D[i][j]`), use:\n   - `#pragma ACCEL PARALLEL reduction = array_name FACTOR=auto{__PARA__L#}`: Instruct the compiler to implement a parallel reduction on the specified array `array_name` with an automatic factor for loop level `L#`, to ensure data integrity during parallel operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the specified function as a hardware acceleration kernel, allow the compiler to decide how to pipeline each loop for best performance, let the compiler choose how to tile and parallelize the loop iterations, and specify that the partial results in certain variables should be combined when parallelizing their respective loops. Use the 'auto' keyword to let the compiler determine the appropriate factor for pipelining, tiling, and parallelism for each loop, and apply reduction clauses to gather results from parallel executions for the variables 'tmp' and 'D' during their updates.\n\nHere is the instruction breakdown:\n\n- Treat 'kernel_2mm' as a hardware acceleration kernel.\n- For loop L0, L1, L2, and L3: Allow the compiler to automatically decide the pipelining strategy.\n- For loop L0, L1, L2, and L3: Let the compiler automatically determine the tiling factor for the loops.\n- For loop L0, L1, L2, and L3: Let the compiler automatically determine the parallelization factor for the loops.\n- In loop L4: Designate 'tmp' for reduction to accumulate results when parallelizing and let the compiler decide the parallelization factor 'auto'.\n- In loop L5: Designate 'D' for reduction to accumulate results when parallelizing and let the compiler decide the parallelization factor 'auto'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following 2mm kernel function into a hardware-accelerated implementation using High-Level Synthesis (HLS) directives to optimize for parallelization and pipelining, ensuring to automatically determine the optimal factor for tiling, parallel execution, and pipelining in each nested loop level, while also specifying reduction operations in the innermost loops where the sum of products is accumulated in the `tmp` and `D` matrices. Additionally, explicitly mark the function as an acceleration kernel:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni, int nj, int nk, int nl, double alpha, double beta, double tmp[40][50], double A[40][70], double B[70][50], double C[50][80], double D[40][80]) {\n  // Matrix multiplication and addition for 2mm kernel\n  // Optimizations with HLS directives: pipeline the loops, automatically determine tiling factors and parallel execution factors, and specify reductions for accumulation.\n\n  // D := alpha * A * B * C + beta * D\n  for (int i = 0; i < 40; i++) {\n    for (int j = 0; j < 50; j++) {\n      // Initialize tmp matrix to 0 and compute alpha * A * B\n      tmp[i][j] = 0.0;\n      for (int k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n\n  for (int i = 0; i < 40; i++) {\n    for (int j = 0; j < 80; j++) {\n      // Multiply matrix D by beta and add the result of tmp * C\n      D[i][j] *= beta;\n      for (int k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n}\n```\n\nFor each loop, use the HLS `PIPELINE`, `TILE`, and `PARALLEL` pragmas with the 'auto' value to let HLS tools determine the optimal scheduling, parallelism, and tiling. Identify reduction points within inner loops where temporary results are being summed into `tmp` and `D` matrices and optimize these with the `PARALLEL` pragma specifying reduction clauses.\n\nPlease replace `{__PIPE__L0}`, `{__PIPE__L1}`, `{__PIPE__L2}`, `{__PIPE__L3}`, `{__TILE__L0}`, `{__TILE__L1}`, `{__TILE__L2}`, `{__TILE__L3}`, `{__PARA__L0}`, `{__PARA__L1}`, `{__PARA__L2}`, `{__PARA__L3}`, `{__PARA__L4}`, and `{__PARA__L5}` with the respective loop identifiers to correctly map each pragma to its corresponding loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, optimize its nested loops for pipelining, tiling, and parallelism automatically, and employ reduction to accumulate values in the arrays `tmp` and `D` during the multiplication processes. Use the 'auto' directive with placeholders for the HLS tool to decide the optimal factors for pipelining, tiling, and parallel execution, and handle the concurrent updates to the `tmp` and `D` arrays safely within the innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 10\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the `kernel_2mm` function for hardware acceleration by employing automatic directive decisions for pipelining, tiling, and parallelization. Ensure that loops are pipelined to allow for concurrent execution of iterations where possible, apply tiling to break down large data sets for improved on-chip memory usage and management, and leverage parallelism to distribute computations across multiple hardware resources simultaneously. Additionally, facilitate reduction operations within nested loops to accumulate results efficiently.\n\nSpecifically, for the top-level loops at lines `L0` and `L1`, enable automatic pipelining, tiling, and parallelization. For the second nested loops at lines `L2` and `L3`, also employ automatic pipelining, tiling, and parallelization. Within the innermost loops at lines `L4` and `L5`, automatically determine parallel factors and use reduction to handle dependencies in the accumulation of values in `tmp` and `D`, respectively. Lastly, mark the entire function as a dedicated hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel to perform a 2mm (two matrix multiply) operation, where the operation 'D := alpha * A * B * C + beta * D' is computed for matrices with known dimensions. The main computation is split into two major parts, one that computes 'tmp = alpha * A * B', and another that computes 'D = tmp * C + beta * D'.\n\nFor the first part, automatically optimize the loop nest that populates the 'tmp' matrix: pipeline the outer loop 'i' and 'j' to efficiently overlap iterations, tile the loop iteration space to optimize local memory usage, and parallelize the inner loop 'k' with automatic factor determination and reduction to accumulate the product into the 'tmp' matrix.\n\nFor the second part, perform a similar automatic optimization for the loop nest that updates the 'D' matrix: pipeline the outer loop 'i' and 'j', tile their iteration space for memory efficiency, and parallelize the inner loop 'k' with a reduction operation on the 'D' matrix, allowing automatic factor determination for potential parallel implementations.\n\nSpecialize the loop optimizations by using placeholders such as 'auto{__PIPE__LX}', 'auto{__TILE__LX}', and 'auto{__PARA__LX}' where 'X' corresponds to each loop level, indicating that the specific pragma values for pipelining, tiling, and parallel factors should be automatically determined during the compilation process.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel to optimize matrix-matrix multiplication and addition operations, using automated pipelining, tiling, and parallelization to enhance performance and efficiency in the loops, and enable reduction to accumulate values correctly in the operations on arrays `tmp` and `D`.\n\n- Declare the function `kernel_2mm` as a hardware kernel for acceleration.\n- For each loop nest, allow the HLS tool to determine:\n  - The pipelining strategy for steady data flow using `pragma ACCEL PIPELINE auto`.\n  - The tiling factors to partition loops for better data locality using `pragma ACCEL TILE FACTOR=auto`.\n  - The parallelization factors to specify how loop iterations can be executed concurrently using `pragma ACCEL PARALLEL FACTOR=auto`.\n- Enable a parallel reduction for the loops that accumulate into the `tmp` and `D` arrays with `pragma ACCEL PARALLEL reduction` and let the HLS tool decide the parallelization factor automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 16\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated 2mm kernel function which performs a two-matrix multiplication followed by a matrix addition. Apply automatic pipelining, tiling, and parallelization pragmas to optimize all loops. Enable parallel reduction for intermediate results on pertinent accumulative loops. Automatically determine the optimal factors for these pragmas. Specifically, for the outer loops (loop level 0 and level 1), the pragmas for pipeline, tile, and parallel should optimize the iterations with automatically determined factors, and the same should be done for the inner loops (level 2 and level 3). For the innermost loops (level 4 and level 5), apply parallel reduction on the temporary and resultant matrix with automatic factor determination to optimize the accumulation operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration target, and enable automatic optimization for loop pipelining, tiling, and parallelization, with compiler-determined factors. Additionally, for reduction operations within the nested loops, activate parallel processing with compiler-selected factors for specific intermediate array computations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware acceleration kernel. For the 'kernel_2mm' function, which performs a 2mm computation updating matrix D with the equation `D := alpha * A * B * C + beta * D`, apply the following HLS (High-Level Synthesis) directives: \n\n- Automatically manage the pipelining of the outer loops iterating over `i` for the matrix multiplication steps, choosing the unroll factor dynamically at compile time.\n- Determine automatically at compile time the tiling factor for partitioning the loops that iterate over `i`.\n- For the loops that iterate over `j`, automatically decide the parallel execution factor, partitioning these loops for parallel execution while maintaining data reduction for intermediate results in 'tmp' and 'D' matrices.\n- Enable dynamic selection at compile time for managing the pipelining, tiling, and parallel execution of the innermost loops iterating over `k`.\n\nEnsure to specify that these loops should not be pipelined ('off'), should not be tiled (factor of 1), should run iterations sequentially (parallel factor of 1), and that the outer function should be treated as an acceleration kernel. Use placeholders 'auto{__PIPE__L#}', 'auto{__TILE__L#}', and 'auto{__PARA__L#}' where the '#' corresponds to the loop level they apply to, allowing for the automation of the respective pragma value determination.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 4\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function 'kernel_2mm' as a hardware acceleration kernel. In performing the matrix multiplication and addition operations within this kernel, automate the pipeline, tiling, and parallelism pragmas for the loops to enable the HLS tool to optimize them for speed and area. Specifically, employ automated pipelining for better throughput, automated tiling to divide computation into optimal sub-blocks, and automated parallelism to maximize concurrent execution where applicable. Additionally, enable automated parallelism with reduction for the innermost loop to accumulate results efficiently into the temporary matrix 'tmp' and matrix 'D'. Use the 'auto' keyword to let the HLS tool select the best factors for pipelining, tiling, and parallelism, and apply reduction to manage dependencies arising from the accumulation into matrices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given multi-dimensional matrix multiplication and addition algorithm into High-Level Synthesis (HLS) C code suitable for hardware acceleration. Enable automatic optimization decisions for pipelining, tiling, and parallelism at various loop levels to maximize data throughput and performance using the respective pragmas. Additionally, implement directives to facilitate parallel reduction on the innermost loops where accumulation occurs, letting the compiler decide the optimal factor. Ensure that the outermost function is designated as a hardware kernel for execution on an FPGA or similar hardware accelerator.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a C high-level synthesis code for a 2-matrix multiplication kernel function that includes the following directives:\n\n1. Designate the function as a hardware acceleration kernel.\n2. Enable automatic loop pipelining for all for-loops.\n3. Allow the compiler to automatically determine the tiling factors for all for-loops.\n4. Permit the compiler to automatically choose the parallelism factors for all for-loops.\n5. Utilize parallel reduction for matrix 'tmp' and matrix 'D' within their respective innermost loops.\n\nSpecifically, for loops nested under the 'i' loop iterating over 'tmp' array, apply automatic parallel reduction on 'tmp', and for those iterating over the 'D' array, apply automatic parallel reduction on 'D'. The compiler is allowed to decide the optimal settings for pipelining, tiling, and parallel execution strategies.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Optimize the loop nests by allowing the compiler to automatically decide the pipeline settings, tiling factors, and parallel execution factors. For the accumulation in both pairs of nested loops, enable parallel reduction on the 'tmp' and 'D' arrays respectively to expedite the matrix-multiplication and matrix-addition operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware accelerator kernel, and within this function, apply automated pipelining, tiling, and parallel processing to the outer loops iterating over dimensions 'i' and 'j', and to their respectively nested loops. Additionally, for the third loop levels that perform the multiplications and accumulations, use automatic parallelization with reduction on the 'tmp' and 'D' arrays respectively to enhance concurrency while preserving dependencies during accumulation. Use the auto option for all #pragmas to enable the HLS tool to determine the optimal pipelining, tiling, and parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, enable automatic optimization for pipelining and processing elements within each loop, and also automate the decision for tiling factor parallel factor. Specify the reduction variable in the innermost loops to perform parallel reduction accumulations. Use pragma annotations to handle these respective optimizations and reductions appropriately.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the provided 2mm kernel into HLS C code with directives that optimize for parallel execution and efficient memory tiling. The outer two loops should be parallelized, pipelined, and tiled automatically for maximum throughput. The internal reduction loops should be parallelized with automatic factor determination, ensuring the intermediate results (tmp and D) are appropriately reduced across iterations. Define the kernel 2mm function for hardware acceleration. Make sure to include directives for loop pipelining, tiling, and parallel processing with automatic pragma values to enable the HLS tool to optimize these parameters, and to perform reduction operations within the innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel and optimize the nested loops for parallel processing and pipelining. Use automated optimization factors for both pipelining and parallel processing in all loops, and also automatically tile the two outermost loops for efficient memory access. Additionally, apply parallel reduction to the inner loops 'k' to accelerate accumulation operations in arrays 'tmp' and 'D'. Use auto values for specifying the degree of tiling and parallelizing via pragmas, allowing the HLS tool to determine the optimal factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to optimize the function 'kernel_2mm' for hardware acceleration by applying automatic pipelining, tiling, and parallelization to the appropriate nested loops, and to employ reduction for accumulation in the innermost loops. Specifically, declare the function as a hardware kernel, enable automatic determination of pipeline stages for the first and third 'for' loop nests as well as their respective inner loops, use automatic tiling for all loop nests, and apply automatic parallelization to all loop nests. Additionally, for the accumulations within the innermost loops 'k' for variables 'tmp' and 'D', perform parallel reduction with automatic factor determination.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a 2mm composite matrix-matrix multiplication kernel with automated optimization directives. Define the function 'kernel_2mm' with parameters for matrix dimensions 'ni', 'nj', 'nk', 'nl', the scalar factors 'alpha' and 'beta', and matrices 'tmp', 'A', 'B', 'C', and 'D'. Annotate the function for hardware acceleration and optimize all loops: For the first pair of nested loops, calculate the matrix product alpha*A*B and store the result in 'tmp'. In the second pair of nested loops, update matrix 'D' by scaling it with 'beta' and adding the product of 'tmp'*C to it. Use automatic optimization for pipelining, tiling, and parallelism where indicated by 'auto' and specify the reduction clause on appropriate variables during parallel loop transformations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration candidate. Enable automatic optimization for pipeline, tiling, and parallel execution of loops within the function, with specific factors assigned during compilation. Apply parallel reduction for partial results summation in nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` as a hardware accelerated kernel that computes the matrix multiplication and addition `D := alpha*A*B + beta*D` with the matrices `A`, `B`, `C`, and `D` of given dimensions `ni`, `nj`, `nk`, and `nl` respectively, where intermediary results are stored in matrix `tmp`. Apply automatic optimization techniques for pipelining, tiling, and parallelization at every nested loop level, and enable parallel reduction for the intermediary accumulation in `tmp` and final accumulation in `D`. Use compiler directives to automatically determine the best pipeline stages, tile sizes, and parallel factors for all nested loops and reductions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware accelerator kernel, and for various loops within this kernel, apply automated optimization directives that allow the HLS tool to determine the best pipelining, tiling, and parallel processing strategies, as well as enable parallel reduction for efficient accumulation in specific loops.\n\nSpecifically:\n1. Declare the function 'kernel_2mm' as a hardware kernel to be accelerated.\n2. For the outermost loops (loops over `i` indexing 40 iterations and the second occurrence of loop over `i` indexing 40 iterations again), apply automatic pipelining, tiling, and parallel processing. Let the HLS tool decide the appropriate factors for these optimizations.\n3. Within the first nested loop (loop over `j` indexing 50 iterations), again apply automatic pipelining, tiling, and parallel processing, and for the innermost loop (loop over `k` indexing 70 iterations), apply parallel reduction on the 'tmp' array allowing the HLS tool to select the parallel factor.\n4. For the second batch of nested loops, similarly, apply automatic pipelining, tiling, and parallel processing for the loop over `j` indexing 80 iterations, and enable parallel reduction on the 'D' array within the innermost loop (loop over `k` indexing 50 iterations), with the HLS tool determining the parallel factor.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 4\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be accelerated in hardware. For all loops within 'kernel_2mm', allow the compiler to automatically optimize pipelining, tiling, and parallelization, and for reduction computations in the nested loops, enable parallel reduction on the 'tmp' and 'D' arrays with automatic factor determination.\n\nFor more specific instructions:\n\n- Indicate the function 'kernel_2mm' as a hardware kernel for potential acceleration purposes.\n- For the top-level loops iterating over 'i' and 'j', allow the HLS tool to automatically handle pipelining to maximize throughput.\n- Permit the HLS tool to choose the best tiling factor to split these loops into smaller statically-scheduled chunks for efficient data management.\n- Enable parallel execution of loop iterations with an automatically selected factor to allow concurrent processing of independent iterations.\n- In the nested loops performing operations on 'tmp' and 'D', specify parallel reduction on these arrays, letting the HLS tool automatically decide the appropriate parallel factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code to represent the matrix-matrix multiplication and addition operation as a hardware acceleration kernel with automated optimization directives: \n\n- Declare the function 'kernel_2mm' as a hardware acceleration kernel. \n- Apply automatic loop pipelining, automatic loop tiling, and automatic parallel processing factor to the outer two loops 'i-j' for 'tmp' calculations.\n- Inside the innermost loop for 'tmp', employ parallel reduction on 'tmp' with an automatic factor.\n- Repeat the automatic optimization directives for the outer two loops 'i-j' for the 'D' matrix calculations.\n- Inside the innermost loop for 'D', apply parallel reduction on 'D' with an automatic factor.\n\nEnsure each pragma statement is placed directly before the loop or block it modifies and configure pragma values to automatically determine the appropriate factor for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the following 2mm kernel function for hardware acceleration by allowing the compiler to automatically determine the best pipeline, tiling, and parallel processing strategies for the nested loops while also specifying where to use reduction to combine elements efficiently:\n\nMark the function as a hardware acceleration kernel, enable automatic pipelining optimization to potentially allow overlapped execution of loop iterations where appropriate. Apply automatic tiling optimization which allows the compiler to break loops into smaller, more manageable chunks to improve data locality and potential parallel execution. Enable automatic parallel processing optimization to potentially perform loop iterations concurrently where data dependencies allow. Specify reduction for the inner loops where elements of the 'tmp' and 'D' arrays are being accumulated to ensure efficient combination of partial results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function that implements a two-matrix multiplication and an addition of a third matrix, with automatic optimization directives for pipelining, tiling, and parallel processing:\n\n1. Designate the kernel function for hardware acceleration.\n2. For the first nested loop that initializes and calculates matrix `tmp`, enable automatic pipeline optimization, automatic tiling, and automatic parallel factor at each loop level respectively to ensure the compiler determines the best parameters for each directive.\n3. Within the innermost loop that computes the elements of `tmp`, apply automatic parallelization with a reduction clause on `tmp` to optimize the summation across iterations.\n4. For the second nested loop that updates matrix `D` and accumulates the product of `tmp` and `C`, again enable automatic pipeline optimization, automatic tiling, and automatic parallel factor at each loop level.\n5. Within the inner loop handling updates to `D`, enable automatic parallelization with a reduction clause on `D` to optimize for accumulation of results.\n6. Leave the loop bounds and array dimensions hardcoded as provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and optimize it by automatically determining the best loop pipelining, tiling, and parallelization strategies for all loops. Specifically, for the first loop nest, automate the decision for the loop pipelining (#pragma ACCEL PIPELINE auto), the tiling factor (#pragma ACCEL TILE FACTOR=auto), and the parallelization factor (#pragma ACCEL PARALLEL FACTOR=auto) for loops 'i' and 'j', with an additional directive to handle reductions on the 'tmp' array with automated parallelization within the innermost loop 'k'. For the second loop nest, apply identical automated optimizations for loops 'i' and 'j', and again handle reductions on the 'D' array with automated parallelization in the inner loop 'k'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given code into a high-level synthesis (HLS) directive instruction by describing each directive and its intended effect:\n\n\"Declare the function 'kernel_2mm' as a hardware acceleration kernel. Within this function, enable automatic optimization for pipelining, tiling, and parallel processing for all loops. Specifically, in the first pair of nested loops, initialize the matrix 'tmp' by computing the matrix multiplication of 'A' and 'B' scaled by 'alpha', and in the second pair of nested loops, update matrix 'D' by scaling with 'beta' and adding the result of 'tmp' multiplied by matrix 'C'. Use automatic factor determination for pipelining, tiling, and parallel processing in each loop nest, and for reduction operations within the innermost loops for both 'tmp' and 'D' matrix updates.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a kernel function called 'kernel_2mm' that calculates the result of the equation D = alpha * A * B * C + beta * D with input parameters ni, nj, nk, nl, alpha, beta, and matrices A, B, C, D. Use auto for pipeline, tile factor, and parallel factor values in #pragma directives for loops. Also, apply reduction pragma with tmp and D matrices for parallel loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 32\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the hardware compiler to optimize a two-matrix multiplication function (kernel_2mm) with dimensions ni, nj, nk, nl by accelerating the main computation function as a hardware kernel, automatically determining optimal pipelining, tiling, and parallelization strategies for the loops. Specifically, enable automatic pipelining and parallelization for all loops to improve throughput, and apply automatic tiling to optimize for spatial locality and memory bandwidth. Utilize reduction to accumulate the scalar product in parallel for the innermost loops, where temporary results are summed into tmp and D arrays for intermediate and final computations.\n\n- Specify that the entire kernel_2mm function is to be treated as a hardware acceleration kernel.\n- For the outer loop that iterates over the i-th dimension of tmp and D arrays, enable automatic pipeline optimization, automatic tiling, and apply automatic parallelization to effectively balance the workload across parallel compute units.\n- In the following loop iterating over the j-th dimension of the tmp array, apply similar automatic pipelining, tiling, and parallel factors, and for the inner k-th loop, activate parallel reduction on the tmp array to concurrently perform accumulation.\n- Replicate this loop optimization pattern for the outer loop over the i-th and inner loops over the j-th and k-th dimensions of the D array, enabling automatic pipelining and tiling for the outermost loops and parallel reduction on the D array when accumulating intermediate products.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, give the following comprehensive but concise instruction:\n\n\"Define a hardware kernel function named 'kernel_2mm' to calculate the matrix operation D = alpha * A * B * C + beta * D using matrices with predefined sizes, where A is 40x70, B is 70x50, C is 50x80, and D is 40x80. Utilize automatic optimization pragmas for pipelining, tiling, and parallelization within the nested loops such that the compiler determines the best factors. Also, apply automatic parallel reduction for the innermost loops that accumulate values into the 'tmp' and 'D' matrices. Label the outer and inner loops pragmas with unique identifiers, such as '__PIPE__Ln', '__TILE__Ln', and '__PARA__Ln', where 'n' is a loop level indicator starting with 0 for the outermost loops and incrementing for each nested level.\"\n \nIn specific #pragma terms:\n\n1. Treat \"kernel_2mm\" as a hardware acceleration kernel:\n   `#pragma ACCEL kernel`\n\n2. For all loops, use the following pragmas with `auto` values, ensuring loop identifiers are unique for each level (L0, L1, L2, etc.):\n\n   - Enable the compiler to decide if and how to pipeline each loop, with pragmas placed right before the loop definitions:\n     `#pragma ACCEL PIPELINE auto{__PIPE__Ln}`\n\n   - Allow the compiler to automatically determine the tiling factor for loop iterations, with pragmas placed right before the loop definitions:\n     `#pragma ACCEL TILE FACTOR=auto{__TILE__Ln}`\n\n   - Enable automatic parallelization of the loops with the compiler determining the parallelization factor:\n     `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__Ln}`\n\n3. For the innermost loops where the `tmp` and `D` arrays are being reduced (accumulation), apply automatic parallel reduction with the compiler determining the appropriate factor:\n   `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}`\n   `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}`\n\nThis instruction encapsulates the directives necessary to inform the HLS tool on how to handle loop optimizations and parallelism for the given matrix operation code, with an emphasis on compiler-determined automation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Consider this function 'kernel_2mm' as a candidate for acceleration in hardware, optimize all the loops within for parallel execution with automatic determination of the best optimization factors, while ensuring that data dependencies are resolved through suitable pipelining, tiling, and parallel execution strategies. Specifically, apply pipelining to the outer two loops, automatically determine the tile sizes for efficient data access, parallelize the innermost loops with an automatic selection of the number of parallel units, and perform reduction operations on the 'tmp' and 'D' arrays in their respective nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the function 'kernel_2mm' for hardware acceleration using HLS pragmas to improve parallel computation, pipelining, and tiling. Specifically: Treat 'kernel_2mm' as a hardware acceleration kernel, automatically determine the pipelining strategy for loop level 0, allow the compiler to automatically choose the tile factor for the same loop to unroll it without breaking into smaller chunks, and use an automatic factor to define the parallelization strategy. Within the first nested loop, apply auto pipelining for loop level 2, tile it with an automatic factor, and automatically parallelize the k-loop with a reduction on 'tmp'. Then, for the second nested loop, automatically pipeline it at loop level 1, configure the tile factor automatically for loop level 3, and parallelize the second k-loop with a reduction on 'D', with the factor again chosen automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a candidate for hardware acceleration, then, for each nested loop within this function, automatically optimize the loop's pipelining, tiling, and parallelism to enhance performance while respecting data dependencies. Use auto-tuning for the specific factor values of each pragma by inserting placeholders to allow for automated optimization of pipeline, tile, and parallelism factors separately for each loop level, and employ parallel reduction for the intermediate computation arrays 'tmp' and 'D' to enable concurrent accumulation of values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. Automatically optimize both the processing of matrix multiplications `tmp[i][j] += alpha * A[i][k] * B[k][j]` and `D[i][j] += tmp[i][k] * C[k][j]` for parallel hardware execution. For each loop layer, use the following strategies:\n\n1. Enable pipeline optimization with automatic determination of the pipeline initiation interval.\n2. Enable loop tiling with an automatic selection of the tile size.\n3. Enable loop parallelism with automatic determination of the parallelization factor.\n4. In the innermost loops performing reductions in `tmp` and `D`, apply parallel reduction pattern with an automatic determination of the parallelization factor.\n\nFor clarity, here is the instruction matched with the corresponding `#pragma` directives:\n\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Automatically determine and apply pipeline optimization: `#pragma ACCEL PIPELINE auto{__PIPE__L<x>}`\n- Automatically determine and apply loop tiling: `#pragma ACCEL TILE FACTOR=auto{__TILE__L<x>}`\n- Automatically determine and implement loop parallelism: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L<x>}`\n- For reduction operations inside loops, employ parallel reduction with an automatic factor: `#pragma ACCEL PARALLEL reduction=<array_name> FACTOR=auto{__PARA__L<x>}`\n\nReplace `<x>` with the appropriate loop label (`L0`, `L1`, `L2`, etc.), corresponding to each loop level. Each `pragma` should be placed directly before the loop it is intended to optimize.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 2\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS directives to optimize the function `kernel_2mm` performing matrix operations, as follows: treat the function as a hardware acceleration kernel, automatically determine pipelining, tiling, and parallelization factors for all loops, and apply reduction to accumulate the results correctly for the specified temporary matrices within the nested loops.\n\nHere's the instruction in bullet points to describe each directive:\n\n- Treat the function `kernel_2mm` as a hardware acceleration kernel.\n- For both the `i-j` and `i-j-k` nested loops structure, let the compiler automatically decide the pipelining, tiling, and parallelization strategies:\n    - Automatically manage the pipelining of the loops to overlap loop iteration executions.\n    - Automatically select the tiling factor for loop iterations to break them into smaller chunks without altering the loop's behavior.\n    - Automatically determine the parallelization factor to enable running loop iterations in parallel where possible.\n- Within the innermost `k` loops, apply a parallel reduction on the `tmp` and `D` matrices to properly accumulate the multiplication results, allowing simultaneous updates to the same matrix element from different iterations. Let the compiler choose the appropriate parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function `kernel_2mm` as a hardware acceleration kernel, and within it, for each nested loop, automatically determine the optimal settings for pipelining, tiling, and parallelization to improve performance, and also perform parallel reduction on specific accumulation operations.\n\n- Use hardware kernel acceleration for `kernel_2mm`.\n- For each loop layer (`L0`, `L1`, `L2`, `L3`), let the HLS tool choose the best pipelining, tiling, and parallelization factors.\n- Within the innermost loops of each matrix operation (`L4`, `L5`), perform parallel reduction on the `tmp` and `D` arrays, respectively, allowing the HLS tool to determine the optimal parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration function. Automatically optimize the loops within it for pipeline, tiling, and parallel processing, allowing the compiler to decide the optimization factors. Specifically:\n\n1. Auto-optimize the pipelining of the outer `i` loop and the subsequent outer `i` loop in code blocks L0 and L1 to concurrently execute loop iterations where possible.\n2. Auto-optimize the tiling of both sets of `i` loops (L0 and L1) to process portions of the loop's iteration space in smaller, more manageable blocks.\n3. Enable automatic parallel processing within the inner `j` loop and the subsequent inner `j` loop in code blocks L2 and L3 to execute iterations simultaneously.\n4. Allow the compiler to automatically manage the parallel reduction for the array `tmp` in the innermost `k` loop within the first `j` loop and for the array `D` in the innermost `k` loop within the second `j` loop, choosing the appropriate parallel factors in blocks L4 and L5, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, allowing the compiler to automatically optimize pipeline execution, tiling, and parallelism for each nested loop construct, as well as applying parallel reduction operations on temporary arrays within the innermost loops where specified.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 32\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the instruction:\n\n\"Designate the function `kernel_2mm` as a hardware acceleration kernel, automatically optimize all loops within this function for parallel execution, pipelining, and tiling. Specifically, for the first two nested loops, automatically decide the pipelining, tiling, and parallelization settings while summing the product of `alpha`, `A[i][k]`, and `B[k][j]` into a temporary array `tmp`. In a separate pair of nested loops, multiply matrix `D` by `beta` and add the product of the temporary matrix `tmp[i][k]` and `C[k][j]`, with automatic optimization for reducing the output matrix `D`. Use dynamic pragma insertion for all optimization factors.\" \n\nAlign this with the following pragmas:\n\n- Start with `#pragma ACCEL kernel` to indicate that function `kernel_2mm` is for hardware acceleration.\n- Use `#pragma ACCEL PIPELINE auto{__PIPE__L0}` and similarly placed pragmas in other loops to allow the compiler to determine the optimal pipeline settings.\n- Apply `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}` and corresponding pragmas in other loops for the compiler to automatically decide on tiling.\n- Insert `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` and equivalents in the rest of the loops for automatic parallel factor decision making.\n- For the innermost loops, add `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}` to perform a reduction on `tmp` and `D` arrays, with automatic parallelization factors.\n\nEach `auto` keyword indicates the compiler will decide the best factor to apply for pipelining, tiling, or parallelism, and `{__PIPE__LX}`, `{__TILE__LX}`, and `{__PARA__LX}` serve as placeholders for these compiler-determined values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration module. For every nested loop inside the function, enable automatic optimization for loop pipelining, data tiling, and parallel loop execution with the optimal factors determined by the HLS tool. Specifically, in the loop that computes the temporary matrix 'tmp', include an instruction to allow parallel reduction on the 'tmp' array. Similarly, in the loop that updates matrix 'D', include a parallel reduction instruction for the 'D' array. Use automatic factor values for tiling and parallelization, which will be substituted during compilation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 32\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be optimized as a hardware accelerator module. Within this module, for both outer loops indexed by 'i' iterating over dimensions 40 and 50, enable automatic potential pipelining, automated chunking for data locality enhancement, and possible parallel execution with optimal factors derived by the compiler. Similarly, for the innermost loops indexed by 'k', enable automatic parallel processing with compiler-determined factors, ensuring the reduction of partial results for 'tmp' and 'D' matrices is handled correctly to avoid data hazards.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a 2mm kernel function treating it as a hardware acceleration kernel, with automatic optimization directives applied to two nested loops. For the first pair of nested loops, enable loop pipelining, tiling, and parallel processing with automatic factor determination for computing the product of three matrices (A, B, and C) scaled by alpha and storing the result in a temporary array. Implement a parallel reduction in the innermost loop for accumulating into the temporary array. For the second pair of nested loops, again apply pipelining, tiling, and parallel directives with automatic factor determination for updating matrix D by multiplying it with beta and then accumulating the product of the temporary array with matrix C into D, including a parallel reduction directive in the innermost loop for the accumulation into matrix D.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware accelerator, for the execution of a two-matrix multiplication (2mm) algorithm, enabling automatic optimization decisions for pipelining, data tiling, and parallel processing at each nested loop level, with automatic unrolling factors where applicable, and register reduction for accumulation variables `tmp` and `D`. Use the respective `auto` values for each loop directive to let the tool determine optimal settings.\n\nHere's an outline for generating the associated HLS code:\n\n- Apply the `#pragma ACCEL kernel` directive to instruct the compiler that the following function, `kernel_2mm`, is to be treated for acceleration in dedicated hardware.\n- Employ the `#pragma ACCEL PIPELINE auto` directive for each loop where pipelining is to be optimized by the compiler, allowing for simultaneous loop iteration executions when possible.\n- Use the `#pragma ACCEL TILE FACTOR=auto` directive to enable the compiler to decide on the best tiling factor for chunking iterations in loops, to optimize spatial locality and efficient usage of memory bandwidth.\n- Insert the `#pragma ACCEL PARALLEL FACTOR=auto` directive to indicate that the loop iterations may be automatically parallelized, allocating hardware resources to execute multiple iterations concurrently.\n- Incorporate the `#pragma ACCEL PARALLEL reduction` directive along with automatic factor settings for loops performing accumulation operations into `tmp` and `D` arrays, allowing the compiler to apply reduction techniques and optimize for potential resource conflicts.\n\nFollowing these instructions translates to applying pragmas for kernel acceleration, pipelining, tiling, auto-parallelization, and parallel reductions at the appropriate code sections, using auto factors to hand off optimization decisions to the HLS tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the 'kernel_2mm' function, which computes matrix multiplication and addition, by using the following strategies: for all loops, enable automatic pipelining, automatic tiling, and automatic parallelism with unspecified factors; for the innermost loops, perform a parallel reduction on the temporary matrices 'tmp' and 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the specified function as a target for hardware acceleration and apply automated optimization strategies for pipelining, data tiling, and parallelism, as well as loop-specific parallel reduction for the innermost loops, with the applicable variables and iteration factors determined automatically by the compiler.\n\n- Mark 'kernel_2mm' as a hardware acceleration target.\n- Automatically determine and apply pipelining strategies at the first loop level for `i` with respect to loops L0 and L1.\n- Automatically determine and apply data tiling factors at the first loop level for `i` with respect to loops L0 and L1.\n- Automatically determine and apply parallelism factors at the first loop level for `i` with respect to loops L0 and L1.\n- Within the nested loop where `j` iterates from 0 to 50, enable automatic pipelining, tiling, and parallelism strategies with respect to loops L2.\n- Within the innermost loop where `k` iterates from 0 to 70, apply automatic parallel reduction for `tmp` using auto-determined factors.\n- Within the nested loop where `j` iterates from 0 to 80, enable automatic pipelining, tiling, and parallelism strategies with respect to loops L3.\n- Within the innermost loop where `k` iterates from 0 to 50, apply automatic parallel reduction for 'D' using auto-determined factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function 'kernel_2mm' for matrix operations with the following pragmas:\n\n- Promote the function to a hardware kernel for acceleration using '#pragma ACCEL kernel'.\n- For the first pair of nested loops initializing the 'tmp' matrix, enable automatic loop pipelining, tiling, and parallel processing with optimizations determined at compile-time, applying parallel reductions on the innermost loop to accumulate the matrix product.\n- Similarly, for the second pair of nested loops updating the 'D' matrix, also enable automatic loop pipelining, tiling, and parallel processing with compiler-optimized settings, with parallel reductions to accumulate the results of the matrix multiplication and addition.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Automatically optimize the loop nests within the function by allowing the compiler to decide the best pipeline strategy, tiling factor, and level of parallelism, adjusting these loop optimizations automatically. Additionally, facilitate reduction operations on the 'tmp' and 'D' arrays within the innermost loops, permitting the compiler to automatically determine the appropriate factor for parallel reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function to be optimized as a hardware accelerator, allowing the compiler to autonomously decide the optimal pipelining, tiling, and parallel processing strategies for each loop within that function to achieve enhanced parallelism and performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. For the computation of `D := alpha*A*B*C + beta*D`, apply automatic pipeline optimization to the outer loops, allow automatic determination of loop tiling and parallelization factors for all nested loops, and perform reduction operations in parallel for temporary arrays `tmp` and `D`. Use dynamic #pragma values for all optimization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is a singular, comprehensive instruction to generate the HLS code provided:\n\n\"Designate a function to serve as a hardware acceleration kernel that multiplies and adds matrices following the equation `D = alpha * A * B * C + beta * D`, with all outer and inner loop iterations optimized automatically, including pipelining, tiling, and parallelization, while also applying automatic reduction for parallel loop iterations that update the `tmp` and `D` matrices, and let the compiler choose the optimal values for pipeline, tile, and parallel factor pragmas.\"\n\nTo break down the specific #pragma values and directives:\n\n1. `#pragma ACCEL kernel`: Designate the function as a hardware acceleration kernel.\n2. `#pragma ACCEL PIPELINE auto`: Automatically optimize the loop with pipelining to enhance the throughput by overlapping loop iteration executions.\n3. `#pragma ACCEL TILE FACTOR=auto`: Let the compiler automatically choose the tiling factor to efficiently utilize hardware resources by partitioning the loop iteration space.\n4. `#pragma ACCEL PARALLEL FACTOR=auto`: Automatically select the factor for parallel loop execution to improve runtime by executing loop iterations concurrently.\n5. `#pragma ACCEL PARALLEL reduction = variable`: Automatically apply parallelization with reduction to combine results across iterations that update either `tmp` or `D` matrices.\n\nThe placeholders `{__PIPE__Lx}`, `{__TILE__Lx}`, and `{__PARA__Lx}` within the pragmas signify that the compiler should automatically derive the optimal values for each of the pipeline, tile, and parallel factors for the loops labeled Lx.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel and set optimization strategies using automatic determination for pipeline, tile, and parallel computation in the two key computational blocks of the function, namely the matrix multiplication and the matrix addition.\n\nSpecifically, treat 'kernel_2mm' as a hardware kernel, and then apply automatic pipelining, automatic tiling, and automatic parallel processing to the outer and inner loops within both matrix multiplication and matrix addition stages of the algorithm, with automatic factor determination for each directive. Utilize parallel reductions to accumulate results across iterations in innermost loops where temporary arrays 'tmp' and 'D' are updated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel, and apply optimization pragmas to enable automatic determination of the pipeline, tile, and parallel factors, as well as specify reductions for the given nested loop combinations used in matrix multiplication and addition:\n\n- Use '#pragma ACCEL kernel' to specify that the function 'kernel_2mm' should be treated as a hardware acceleration kernel.\n- Automate the pipelining of the outermost loops 'i' in both major block sections by using '#pragma ACCEL PIPELINE auto' followed by a unique label placeholder for each loop.\n- Instruct the compiler to automatically choose tiling factors for these loops with '#pragma ACCEL TILE FACTOR=auto' and corresponding unique label placeholders.\n- Set up automatic parallelization of the same loops by using '#pragma ACCEL PARALLEL FACTOR=auto' with matching unique label placeholders.\n- For the innermost loops 'k' that perform the matrix multiplications and additions, also enable automatic parallelization with reduction on the 'tmp' and 'D' arrays respectively, by using '#pragma ACCEL PARALLEL reduction = tmp|D FACTOR=auto' and subsequently adding unique label placeholders for each occurrence.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a kernel function `kernel_2mm` responsible for performing a matrix multiplication and matrix addition, where matrix `D` is updated as `D = alpha * A * B * C + beta * D`. To optimize this function for hardware acceleration, apply automatic pipelining, tiling, and parallel processing to all loops, allowing the compiler to decide the best factors. Additionally, enable parallel reduction for the innermost loops that accumulate values in the `tmp` and `D` matrices, ensuring that the summations can be computed concurrently. Use automated factor determination for all these optimizations by specifying 'auto' alongside the HLS pragmas.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel, optimize for throughput in all loops by allowing the compiler to automatically decide the pipelining, tiling, and parallelization factors while ensuring correct handling of data dependencies through reduction on 'tmp' and 'D' variables.\n\nSpecifically, you might use the following instructions:\n\n- Convert 'kernel_2mm' into a hardware kernel suitable for acceleration.\n- Allow the compiler to automatically optimize the outer loops 'i' in both main block sections through its proprietary methods with no specific pipelining, tiling, or parallelization values given.\n- For the first loop nest, within each iteration over 'j', initialize 'tmp[i][j]' to zero before performing a reduction operation across 'k'\u2014again, let the compiler decide the factor of parallelization automatically while maintaining the correct accumulation into 'tmp[i][j]'.\n- In the second loop nest, apply a scalar multiplication factor 'beta' to 'D[i][j]', then carry out a reduction operation that considers 'tmp' and 'C' matrices, with parallelization handled by the compiler's automatic factor determination.\n\nEnsure all #pragma directives concerning loop optimization are marked 'auto' to entrust the HLS tool with determining the most appropriate values for pipelining, tiling, and parallel factors. Use the specific reduction directive for 'tmp' and 'D' when automatic parallelization is applied within innermost loops to avoid race conditions during summations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware-accelerated kernel, and enable the compiler to decide the optimal pipelining, tiling, and parallelization parameters for all loops. For the innermost loop where the variable 'tmp' is being reduced, apply an automatic parallel reduction pragma for 'tmp', and similarly for the loop where 'D' is being reduced, apply an automatic parallel reduction pragma for 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel that optimizes the execution of the two matrix-multiply operations involved in the `2mm` benchmark. The first operation computes the product of matrices `A` and `B` and scales the result by `alpha`, storing it in `tmp`. The second operation computes the product of `tmp` and `C`, scales the existing matrix `D` by `beta`, and adds the result to `D`. For each major loop, enable automatic loop pipelining, automatically determine tiling factors to split loops into smaller chunks, and choose the level of loop parallelism automatically. Additionally, apply automatic parallelism with reduction for the innermost loops that accumulate into `tmp` and `D`. Use dynamic pragma replacements for the compiler to identify optimal values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with the correct #pragma annotations, issue the following comprehensive yet concise instruction:\n\n\"Translate the two-level matrix-matrix multiplication algorithm expressed in C to an HLS-optimized kernel, ensuring that the highest-level loops for both matrix computations are automatically pipelined, tiled, and parallelized for optimum performance. Additionally, within the nested loops, automate the pipelining, tiling, and parallelism pragmas, and specifically apply reduction to the parallel pragma for accumulation within the innermost loops. Use automatic factor determination for all pragmas.\"\n\nThis instruction encompasses the process of creating the provided HLS C code with the appropriate #pragma values, Using 'auto' where specified, ensure to: \n- Treat the function as an acceleration kernel.\n- Optimize the two separate nested loop blocks by setting the pragmas to automatically determine the best pipeline, tile, and parallel factor values for the loops at indexes L0, L1, L2, and L3.\n- Apply automatic parallelization with reduction for the accumulation of temporary results in the innermost loops at indexes L4 and L5, which are important for the matrix multiplication and accumulation steps.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 32\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function named 'kernel_2mm' that operates on matrices with predefined sizes, and employs loop optimization directives to automatically optimize the pipeline, tiling, and parallelization of the nested loops. Enable automatic determination of the best pipelining, tiling, and parallel factors for the outer and inner loops with directives applicable to the matrix multiplication and addition steps, and include directives for automatic parallel reduction for temporary array accumulation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the given matrix multiplication and addition function using hardware optimization directives, applying automatic pipelining, tiling, and parallelization strategies for loop iterations, and ensuring the operations for accumulating products into the `tmp` and `D` arrays are performed with parallel reductions.\n\nSpecifically, declare the function as a hardware kernel, enable auto-tuning for pipelining, tiling, and parallel factors for each loop nest, and set up parallel reduction for the innermost loops that accumulate values into `tmp` and `D`, allowing HLS to decide the optimization parameters.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 8\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function 'kernel_2mm' as a dedicated hardware acceleration kernel, allowing for automated optimization of loop pipelining, loop tiling, and loop parallelism based on the HLS tool's heuristics to maximize performance within hardware constraints. Specifically, implement techniques that allow for the concurrent execution of operations within the loops, automatic pipelining of loop iterations for throughput maximization, and intelligent partitioning of loops into smaller, optimally-sized computational blocks to exploit data locality. Additionally, ensure that reductions within the loops that accumulate into the 'tmp' and 'D' arrays are correctly managed to prevent data hazards while exploiting parallelism. Use 'auto' for the HLS tool to automatically determine the best #pragma values for each loop's pipelining, tiling, and parallelism factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "In order to generate the described HLS codes in C with the appropriate pragmas, the following comprehensive instruction should be given:\n\n\"Implement a hardware accelerated 2mm kernel function named 'kernel_2mm' which computes a matrix multiplication and addition with scalar multiplication. The inputs are integer dimensions ni, nj, nk, and nl, two scalar values alpha and beta, and four 2D arrays A, B, C, and D with specified sizes. Initialize a temporary 2D array 'tmp'. Let the HLS tool automatically decide the pipelining, tiling, and parallelization factors at each loop nest level by using 'auto'. At the innermost loop levels for the multiplications, include parallel reduction on the 'tmp' and 'D' arrays, allowing the tool to automatically determine the parallel factor. Mark the function as a hardware kernel to specify its role for hardware acceleration.\"\n\nSpecifically, use the following pragmas for their respective purposes:\n\n- `#pragma ACCEL kernel` to tell the HLS tool that the 'kernel_2mm' function is a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto` followed by an identifier like `{__PIPE__Lx}` at each loop level to let HLS automatically decide whether to pipeline the loop and at what degree if so.\n- `#pragma ACCEL TILE FACTOR=auto` followed by an identifier like `{__TILE__Lx}` at each loop level to let the HLS tool automatically determine the tiling factor for the loop, if applicable.\n- `#pragma ACCEL PARALLEL FACTOR=auto` followed by an identifier like `{__PARA__Lx}` to let the HLS tool automatically determine the level of parallelism for the loop iterations.\n- `#pragma ACCEL PARALLEL reduction = array_name` followed by `FACTOR=auto` and an identifier like `{__PARA__Lx}` within the inner reduces to specify that parallel reduction should be applied to the specified array with the degree of parallelism determined automatically. \n\nReplace `array_name` with 'tmp' and 'D' for the two inner reductions and assign different identifiers to the `auto` for the pipeline, tile, and parallel pragmas to denote that they are for different loop levels (shown as `L0`, `L1`, `L2`, `L3`, `L4`, and `L5` in the provided code).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be optimized for hardware acceleration, allowing the compiler to employ various optimization techniques such as loop pipelining, tiling, and parallel processing, while automatically determining the optimal values for these strategies. Specifically, for both main matrix multiplication loops and their nested loops, enable automatic pipeline optimization, allow the compiler to choose the best loop tiling factor, and permit automatic determination of the parallelism factor. Additionally, within the innermost loops, apply automatic parallel reduction to facilitate the accumulation of the 'tmp' and 'D' matrices, optimizing for concurrency and reduction of latency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function to perform a two-matrix multiply (2mm) operation. Enable compiler-determined optimization strategies for pipelining and loop transformation within the function while ensuring the compiler automatically determines the parallelization factor and tiling factor for each loop. Apply parallel reduction to aggregate results correctly when performing summation across iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 8\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given HLS (High-Level Synthesis) C code directives and configurations into natural language instructions:\n\n1. \"Designate this function as a target for hardware acceleration.\"\n   - This refers to `#pragma ACCEL kernel`.\n\n2. \"Optimize the loops for parallel execution, but let the HLS tool determine the degree of pipelining, tiling, and parallelization automatically.\"\n   - This applies to the pragmas with `auto`:\n     - `#pragma ACCEL PIPELINE auto{__PIPE__Lx}`\n     - `#pragma ACCEL TILE FACTOR=auto{__TILE__Lx}`\n     - `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__Lx}`\n   - The index `Lx` (where `x` is a number) refers to the loop label to which the pragma is applied.\n\n3. \"Explicitly perform a sum reduction on designated arrays within the loop constructs, with the tool automatically determining the parallel factor.\"\n   - Reflected by:\n     - `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}`\n     - `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}`\n   - The arrays `tmp` and `D` are being reduced, respectively, indicating loop-carried dependencies that must be considered in the reduction process.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 10\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the following function for hardware acceleration by treating it as a kernel, allowing the loops to be automatically pipelined, tiled, and parallelized according to the tool's discretion, and apply reduction to accumulate into matrix variables 'tmp' and 'D' efficiently.\n\nNote: Ensure that the optimization pragmas are set to automatically determine the most appropriate values for pipelining, tiling, and parallel factors by using 'auto' and specify the respective loop identifiers where the pragmas should be applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 1\n__PARA__L4: 4\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, the comprehensive but very concise natural language instruction would be:\n\n\"Define a hardware-accelerated kernel to perform a 2mm operation, where the first multiplication of matrices A, B, and scalar alpha is accumulated in a temporary matrix tmp, and the second multiplication with matrix C is then scaled by beta and accumulated in matrix D. Automatically optimize the inner loop iterations for parallel execution, pipeline them as suited for the target architecture, and tile them if beneficial for performance. Also, perform reduction operations to accumulate results in tmp and D matrices, allowing for parallel updates where possible. Let the HLS tool decide the optimal factor values for parallelism, pipelining, and tiling.\"\n\nSpecifically, use the following pragma instructions:\n\n- Treat the following function as a hardware kernel for acceleration, indicated by `#pragma ACCEL kernel`.\n\n- For each loop nest:\n  - Enable the compiler to automatically decide whether to pipeline this loop with `#pragma ACCEL PIPELINE auto`.\n  - Allow the compiler to automatically choose the loop tiling factor with `#pragma ACCEL TILE FACTOR=auto`.\n  - Permit automatic selection of the parallel execution factor with `#pragma ACCEL PARALLEL FACTOR=auto`.\n  - For reduction loops, specify the reduction variable and allow the parallelism factor to be automatically determined with `#pragma ACCEL PARALLEL reduction = <variable> FACTOR=auto`.\n\nReplace `<variable>` with the appropriate matrix (tmp or D) being reduced in each nested loop. Use unique identifiers within curly braces (e.g., `{__PIPE__L0}`, `{__TILE__L0}`, `{__PARA__L0}`, etc.) to allow the compiler to distinguish between different loops when determining pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 4\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare a function to be parallelized into FPGA hardware, implementing matrix multiplication and addition with tiling, pipelining, and parallelization optimization directives for loops, where optimization factors are determined by the HLS tool. Specifically, in the nested loops, direct the HLS tool to choose whether to pipeline the loops, tile loops using optimal chunk sizes, and execute loop iterations in parallel. Additionally, apply parallel reduction on inner loops where accumulators update a matrix, letting the HLS tool decide the factor for parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 16\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the `kernel_2mm` function as a hardware kernel, automatically determine the optimal loop transformation for pipelining, tiling, and parallelism while ensuring all iterations within the innermost loops that update `tmp` and `D` arrays are executed in parallel with automatic factor selection and appropriately perform reduction to accumulate the results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and automatically manage the pipelining, tiling, and parallelization of the computations inside its loops for optimizing the multiplication and addition of matrices A, B, C, and scaling of matrix D. Apply automatic data reduction strategies for temporary storage during matrix computation. Use the appropriate automatic optimization strategies for each nested loop level within the function.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the enclosed function to operate as a hardware acceleration kernel, allowing automatic optimization for sequential and parallel operations within its loops. Specifically, permit the HLS tool to decide the optimal pipeline settings, task partitioning granularity, and parallel execution parameters for each loop level, while also enabling automatic parallel reduction for specified temporary storage variables used in computations within the nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS (High-Level Synthesis) C code to implement a 2mm kernel function, which represents a two-matrix multiply kernel used in linear algebra, with the following characteristics and directive annotations for optimization:\n\n1. Designate the entire 2mm function as a hardware kernel targeted for acceleration.\n2. For each of the two major nested loop blocks, apply the following optimizations:\n   - Enable automatic pipeline optimization to allow overlapped execution of loop iterations.\n   - Utilize automatic tiling optimization to break down the loop iterations into smaller, more manageable blocks that can be executed more efficiently.\n   - Use automatic parallelization to allow concurrent execution of independent loop iterations.\n3. Within the innermost loop of the first major loop block, where the temporary matrix `tmp` is computed, enable parallel reduction on `tmp`, allowing parallel accumulation of the results.\n4. Similarly, in the innermost loop of the second major loop block, where the final output matrix `D` is updated, enable parallel reduction on `D`.\n\nFor all optimizations, use the `auto` option to let the HLS tool decide the best factor or setting, which should be denoted by placeholder tokens such as `{__PIPE__L0}`, `{__TILE__L0}`, `{__PARA__L0}` for the first loop, `{__PIPE__L1}`, `{__TILE__L1}`, `{__PARA__L1}` for the second loop, and so on, where '`Lx`' corresponds to the loop level.\n\nHere is the instruction in natural language:\n\nCreate a 2mm kernel function in C for HLS, employing auto-tuning pragmas to guide the synthesis tool in optimizing the loop structure for pipelining, tiling, parallelization, and parallel reduction in order to effectively accelerate the computation. Define the function as a hardware kernel and for each nested loop level, include appropriate pragma directives with auto factors to inform the HLS tool about the preferred optimization strategies, being mindful of the need for placeholders that allow the tool to automatically determine the optimal values during synthesis.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function named `kernel_2mm` that accelerates the calculation of `D := alpha*A*B*C + beta*D`. For the outer two loops iterating over dimensions 40x50 and 40x80, enable automatic selection for pipelining, tiling, and parallelizing the iterations with optimal factors. Ensure the nested innermost loops that compute the dot product are also parallelized with automatic factor selection, including reduction on `tmp` and `D` arrays to avoid data hazards. Use respective pragmas for pipelining, tiling, parallelization with 'auto' for the tool to determine the best values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 32\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the function `kernel_2mm` by designating it as a hardware acceleration kernel and automatically managing the parallelization, pipelining, and tiling of the loops within, with the optimization level for each pragma determined at compile-time.\n\n- Treat `kernel_2mm` as a hardware kernel to accelerate computation.\n- For each of the four nested loops (two major outer loops and two inner reduction loops), automatically determine the optimization factors for pipelining, tiling, and parallel processing.\n- For the 'k' indexed loops, apply a parallel reduction to the `tmp` and `D` arrays, allowing for concurrent calculation of sum reductions with automatically determined factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated HLS kernel in C language for the 2mm matrix multiplication algorithm, with 4 nested loops. For each level of the loops, automatically optimize pipelining, tile factor, and parallel factor. Additionally, apply parallelization with reduction for the inner loops (loops 4 and 5) while calculating 'tmp' and 'D' matrices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate HLS directives for the provided C code which performs a 2mm kernel operation on matrices, follow these instructions:\n\n1. Designate the entire function `kernel_2mm` as a hardware acceleration kernel.\n2. Within both top-level loops (`i` loops), enable automatic optimization techniques by pipelining, tiling, and parallelizing the loops with the appropriate factors that the HLS tool should automatically determine.\n3. For the innermost loops (`j` and `k` loops), similarly enable pipelining, tiling, and parallelization with autogenerated factors.\n4. For the reduction operations within the nested `k` loops that accumulate into `tmp[i][j]` and `D[i][j]`, apply parallel reduction with automatically determined factors.\n\nThe specific HLS pragmas that correspond to these instructions are:\n\n- `#pragma ACCEL kernel` to designate the function for hardware acceleration.\n- `#pragma ACCEL PIPELINE auto{__PIPE__L<level>}` to enable automatic loop pipelining where `<level>` is the loop nest level.\n- `#pragma ACCEL TILE FACTOR=auto{__TILE__L<level>}` to allow automatic loop tiling with an optimal factor determined by the HLS tool at each loop level.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L<level>}` to enable automatic parallel processing with an optimal factor determined by the HLS tool at each loop level.\n- `#pragma ACCEL PARALLEL reduction = <variable> FACTOR=auto{__PARA__L<level>}` to enable automatic parallel reduction on the specified `<variable>` with an optimal parallelism factor automatically determined for the loops that perform the reduction.\n\nThe `{__PIPE__L<level>}`, `{__TILE__L<level>}`, and `{__PARA__L<level>}` are placeholders that the HLS tool will replace with specific factors during the optimization process, and `<level>` corresponds to the particular nested loop level in the code structure.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 2\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated implementation of the 2mm (two-matrix multiply) kernel, where the function 'kernel_2mm' should be treated as a hardware kernel. Implement the following steps with automatic optimization decisions for the pipeline, tiling, and parallelization:\n\n1. For the outer two loops (labeled L0 and L1), allow automatic optimization to enhance loop pipelining, loop tiling, and loop parallelization as needed without any predetermined factors.\n2. Within the inner loops (labeled L2 and L3 respectively), again allow automatic optimization for pipelining, tiling, and parallelization.\n3. In the innermost loops, where the temporary matrix 'tmp' and matrix 'D' are being computed (labeled L4 and L5), automatically optimize parallel execution and specify reduction for the 'tmp' and 'D' variables to ensure correct accumulation of values. \n\nUse the relevant #pragma directives to instruct the HLS tool to apply these optimizations automatically without explicitly specifying the factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS accelerated kernel function named 'kernel_2mm' to perform a 2-matrix multiplication operation with matrix dimensions `ni x nj x nk x nl`, scaling factors `alpha` and `beta`, and input/output matrices `A`, `B`, `C`, `D`, and an intermediate matrix `tmp`. Configure the outer two loops 'i' (with loop labels __PIPE__L0 and __PIPE__L1) and the inner loops 'j' (with loop labels __PIPE__L2 and __PIPE__L3) for automatic pipelining, tiling, and parallel processing optimizations. In the reduction loops 'k' (with loop labels __PARA__L4 and __PARA__L5), enable parallel accumulation with auto-tuning for factors while specifying the reduction variable as `tmp` and `D` respectively. Additionally, leave the specific optimization values for pipeline, tile, and parallel execution to be automatically determined by the HLS tool utilizing the 'auto' directive where placeholders are indicated by `__PIPE__Lx`, `__TILE__Lx`, and `__PARA__Lx`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 8\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as hardware acceleration target, and optimize loop iterations at different loop depths with automatic determination of the best pipeline, tile, and parallel factor configurations for the FPGA architecture while performing data reductions on temporary arrays where needed.\n\n- Use the 'kernel' directive to indicate 'kernel_2mm' as an acceleration function.\n- Employ the 'PIPELINE', 'TILE', and 'PARALLEL' directives with the 'auto' keyword to enable the compiler to automatically decide the optimization parameters for loop pipelining, loop tiling, and parallel loop execution at each loop level.\n- Apply the 'PARALLEL' directive with a reduction clause on 'tmp' and 'D' arrays for the innermost loops which accumulate values across iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 8\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the specified function as a hardware acceleration kernel, apply automatic optimization to improve the execution flow of outer loops, and within nested loops allow for simultaneous computation and local optimization. Specifically, for the first set of nested loops computing the matrix product `alpha*A*B`, automatically determine the optimal pipelining, tiling and parallel processing strategies, and for the innermost loop, apply automatic parallelism with a reduction on 'tmp'. For the second set of nested loops calculating `beta*D + tmp*C`, repeat the same automatic optimization strategies for pipelining, tiling, and parallel processing, and similarly apply automatic parallelism with a reduction on 'D' for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the following C function using hardware directives by annotating it as a kernel for high-level synthesis, enabling the compiler to automatically determine the optimal pipelining, tiling, and parallelization factors for the nested loops while ensuring the reduction operations on the 'tmp' and 'D' arrays are handled correctly to prevent race conditions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: flatten\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) design for a 2mm kernel function which treats the function itself as a hardware kernel targeted for acceleration. The kernel performs a two-matrix multiplication operation and the addition of another matrix, scaled by coefficients alpha and beta respectively. \n\nFor the outer loops computing temporary matrix 'tmp' and the result matrix 'D', employ automatic optimization strategies that allow the HLS tool to decide on pipelining, tiling, and parallel execution parameters. For the innermost loops responsible for the actual computation of the matrix products, apply parallel processing with automatic factor determination and enable reduction on the 'tmp' and 'D' matrices to accumulate results properly.\n\nSpecifically:\n1. Enable the function to be treated as a hardware kernel using `#pragma ACCEL kernel`.\n2. For the first set of nested loops (calculating 'tmp'):\n    - Apply automatic pipelining to the outer loop `i` using `#pragma ACCEL PIPELINE auto{__PIPE__L0}`.\n    - Use an automatic tile factor for the outer loop `i` with `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}`.\n    - Handle the outer loop 'i' with automatic parallel factor using `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`.\n    - Apply automatic pipelining, tiling, and parallel processing similarly to the subsequent loop 'j' using the appropriate pragmas with markers `__PIPE__L2`, `__TILE__L2`, and `__PARA__L2`.\n    - For the inner loop 'k', enable parallel processing with an automatic factor, and particularly specify reduction for the 'tmp' 2D array using `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}`.\n3. For the second set of nested loops (updating 'D'):\n    - Utilize the same automatic optimization strategies for pipelining, tiling, and parallel execution as above, noting the respective loop markers `__PIPE__L1`, `__TILE__L1`, and `__PARA__L1` for loop 'i', and `__PIPE__L3`, `__TILE__L3`, and `__PARA__L3` for loop 'j'.\n    - Similar to 'tmp', grant parallelization with reduction for matrix 'D' in the innermost loop 'k', with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}`.\n\nEnsure that all the pragmas are inserted in the correct positions relative to their corresponding loops and that the automatic parameters are set to allow the HLS tool to determine the best optimization values for pipelining, tiling, and parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 32\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be synthesized as a hardware kernel, automatically optimize the loops for pipelining, tiling, and parallel execution with compiler-determined factors, and perform reduction operations in parallel while ensuring correct accumulation of results in the 'tmp' and 'D' matrices. Use the respective '#pragma ACCEL' directives to allow the HLS tool to decide the best optimization parameters for all loops except the outermost, which should not be pipelined explicitly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 32\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the provided C code, use the following natural language instructions to generate the respective HLS pragmas:\n\n1. Designate the function 'kernel_2mm' as a hardware acceleration kernel using the pragma '#pragma ACCEL kernel'.\n2. Enable automatic decision on loop pipelining optimization for both high-level loops and the nested loops using the pragma '#pragma ACCEL PIPELINE auto' for each loop, with unique identifiers for each instance like '__PIPE__L0', '__PIPE__L1', '__PIPE__L2', and '__PIPE__L3'.\n3. Allow the compiler to automatically determine the optimal tiling factor for all loops to balance parallelism and resource usage with pragmas such as '#pragma ACCEL TILE FACTOR=auto', paired with unique identifiers for each loop '__TILE__L0', '__TILE__L1', '__TILE__L2', and '__TILE__L3'.\n4. Entrust the compiler with choosing the best parallelism factor for all loops using '#pragma ACCEL PARALLEL FACTOR=auto', using unique identifiers for each loop '__PARA__L0', '__PARA__L1', '__PARA__L2', and '__PARA__L3'.\n5. Use parallel reduction to accumulate values in the 'tmp' and 'D' arrays by enhancing inner loop parallelism with pragmas '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto' and '#pragma ACCEL PARALLEL reduction = D FACTOR=auto', with identifiers '__PARA__L4' and '__PARA__L5' for the respective loops where the reductions are used.\n\nNote: The 'auto' value specified for PIPELINE, TILE, and PARALLEL FACTOR pragmas implies that the HLS tool should automatically choose the best values for the loop optimizations based on the given constraints and target architecture. The unique identifiers such as '__PIPE__L0', '__TILE__L0', '__PARA__L0', etc., distinguish between different loops where the pragmas are applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated kernel function `kernel_2mm` for a 2mm matrix multiply and addition routine, applying automatic optimization decisions for pipeline and parallelism, and enable tiled processing where appropriate:\n\n1. Designate the function `kernel_2mm` as a hardware acceleration kernel.\n2. Automatically optimize pipelining of the outer loops (`i` loops) to improve loop execution throughput.\n3. For both `i` loops, automatically decide on tiling factors to partition loops into smaller, more manageable blocks.\n4. Automatically determine parallelism factors to execute loop iterations concurrently where possible for `i` and `j` loops.\n5. For the innermost `k` loops within the `tmp` and `D` matrix computations, apply automatic parallelism to speed up the reduction operation on the `tmp` and `D` elements.\n\nEnsure that all optimization directives (`PIPELINE`, `TILE` and `PARALLEL`) use the 'auto' setting to allow the HLS tool to infer the optimal parameter values for the given hardware context. Use the following pragma annotations just before each loop or block of code they are optimizing:\n\n- `#pragma ACCEL kernel` before the function declaration\n- `#pragma ACCEL PIPELINE auto` with appropriate `__PIPE__Lx` tag before each `i` loop\n- `#pragma ACCEL TILE FACTOR=auto` with the corresponding `__TILE__Lx` tag before each `i` loop\n- `#pragma ACCEL PARALLEL FACTOR=auto` with the relevant `__PARA__Lx` tag before each `i` loop and `j` loop\n- `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto` with the corresponding `__PARA__Lx` tag before each `k` loop, specifying `tmp` or `D` as the variable being reduced.\n\nWhere `x` in `Lx` corresponds to the loop's nesting level in the function, with `L0` for the outermost loop and increasing by one for each nested loop level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: flatten\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate 'kernel_2mm' function as the target for hardware acceleration. Ensure the outer loops 'i' for both the initial matrix multiplication of A and B and the subsequent accumulation with C and D are pipelined with decision for pipelining parameters left to the compiler. Similarly, enable tiling and parallelization for these loops, letting the compiler choose the tiling sizes and parallelization factors. Apply parallel reduction across the loop 'k' to optimize the intermediate accumulation into 'tmp' and final accumulation into 'D', again allowing the compiler to determine the parallel factors automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: flatten\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function to be optimized for hardware acceleration, letting the HLS tool automatically decide on the level of pipelining, tiling, and parallelism for the appropriate loops, while also enabling specific parallel reductions on partial loop results.\n\n- Mark the 'kernel_2mm' function as a hardware acceleration target.\n- For the two outermost 'i' loops, permit the HLS tool to automatically optimize pipelining, with each loop having its own settings by using 'auto'.\n- Enable automatic tiling optimization on each 'j' loop and the directly nested 'k' loops within both 'i' loops, allowing the HLS tool to determine ideal chunk sizes.\n- Allow the compiler to automatically decide on the degree of parallelism for each loop nest, setting 'FACTOR=auto' on the 'i', 'j', and 'k' loops.\n- For the reduction across the 'k' loop when accumulating 'tmp[i][j]' and 'D[i][j]', indicate that the reduction should be parallelized, and let HLS automatically decide on the parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, you should instruct the HLS tool to:\n\n- Treat the function `kernel_2mm` as a hardware acceleration kernel.\n- Apply automatic optimization directives for pipelining, tiling, and parallelism for all loops, letting the HLS tool decide the optimal values for factors based on the context.\n- Employ parallel reduction for specific inner loops that accumulate values into the `tmp` and `D` matrices, allowing the HLS tool to choose the factor of parallelism pragmatically.\n\nHere's the comprehensive, concise instruction combining all the directives:\n\n\"Compile function 'kernel_2mm' into a dedicated hardware acceleration kernel. For each loop nest within the function, enable automatic pipelining, automatic loop tiling, and automatic parallelism optimization so the tool determines the best parameters. Use parallel reduction with automatic factor selection for accumulations into matrices 'tmp' and 'D'.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function to multiply and add matrices. In this kernel, enable automatic loop pipelining, tiling, and parallel processing for all loops to improve performance. For the calculation of the `tmp` and `D` matrices, use reduction to accumulate the results in parallel while automatically determining the optimal factor for pipelining, tiling, and parallelism. Apply parallel reduction to the innermost loops that accumulate into `tmp` and `D` to enable concurrent execution of reduction operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel with automated optimization directives for pipelining, tiling, and parallelism in the loops. Specifically, apply optimizations to each loop level to allow the compiler to automatically determine the best pipelining strategy, tiling factors, and parallelization factors, including automated factor values for the parallel reduction in the innermost loops where temporary arrays `tmp` and `D` are being updated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the function 'kernel_2mm' for hardware acceleration by treating it as a hardware kernel, and automatically determine the optimal parameters for pipelining, tiling, and parallelizing each nested loop to improve data flow and parallel execution while considering data dependencies. Specifically, utilize automatic pipelining to enhance loop execution efficiency, automatically calculate the best tile sizes to decompose loops for better parallel processing, and dynamically assign parallel factors to execute loop iterations concurrently, including managing accumulations in reduction patterns.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated function named `kernel_2mm` to compute a 2mm kernel with matrix multiplication and addition, with the following directives:\n\n- Treat the entire function as a hardware kernel by annotating it with a pragma for kernel acceleration.\n- For the two outermost loops labeled `L0` and `L1`, enable automatic pipelining, tiling, and parallelism, allowing the HLS tool to optimize the loop structures based on available resources.\n- For the second level loops labeled `L2` and `L3`, similarly enable automatic pipelining, tiling, and parallelism.\n- In the innermost loops labeled `L4` and `L5`, use parallelism with automatic factor determination to accumulate values into the `tmp` and `D` matrices while enabling automatic reduction to handle dependencies.\n\nSpecifically, apply the following pragmas with `auto` directives where the HLS tool should automatically choose the values:\n\n- For the function, use `#pragma ACCEL kernel`.\n- For outermost loops `i` in both major computation sections, use:\n  - `#pragma ACCEL PIPELINE auto{__PIPE__L0}` and `#pragma ACCEL PIPELINE auto{__PIPE__L1}` respectively.\n  - `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}` and `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}` respectively.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` respectively.\n- For second level loops `j`, use:\n  - `#pragma ACCEL PIPELINE auto{__PIPE__L2}` and `#pragma ACCEL PIPELINE auto{__PIPE__L3}` respectively.\n  - `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}` and `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}` respectively.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}` respectively.\n- For the innermost loops `k`, apply parallelism with reduction for the `tmp` and `D` updates using:\n  - `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}`\n  - `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 32\n__PARA__L4: 70\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, automatically optimize the loop pipelining, tiling, and parallelism for all nested loops, and apply automatic parallel reduction for partial sums in specific loops.\n\nHere's the instruction breakdown for the HLS pragmas:\n\n1. `#pragma ACCEL kernel`: Designate the `kernel_2mm` function as a hardware acceleration kernel.\n2. `#pragma ACCEL PIPELINE auto{__PIPE__Lx}`: Automatically determine the pipelining strategy for each loop at levels L0, L1, L2, and L3.\n3. `#pragma ACCEL TILE FACTOR=auto{__TILE__Lx}`: Automatically determine the tiling factor to split loops at levels L0, L1, L2, and L3 into smaller chunks for better parallelism.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__Lx}`: Automatically determine the parallelism factor for loops at levels L0, L1, L2, L3 and L4 without explicitly specifying the factor to parallelize loop iterations.\n5. `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}`: Apply parallel reduction to accumulate values into `tmp` and `D` arrays within loops at levels L4 and L5, with the parallelism factor automatically determined.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 8\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and within this function, automatically optimize each loop for parallel processing, pipelining, and loop tiling based on the loop's characteristics. For the innermost loops that perform reduction operations on 'tmp' and 'D', enable parallelism with automatic factor determination and specify the reduction variable to optimize the accumulation pattern in hardware.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 8\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS codes, use the following comprehensive but concise natural language instruction that describes the entire code generation process and the appropriate pragma values:\n\n\"Designate the function 'kernel_2mm' as a hardware acceleration kernel, applying matrix multiplication and addition operations on matrices A, B, C, and D with scaling factors alpha and beta. Optimize the loops with automated pipelining, tiling, and parallelization factors, allowing the HLS tool to determine the optimal values. Inside nested loops, the outermost loops i and j should be optimized for pipeline and parallel execution, while the innermost loop k should also perform parallel reduction on the 'tmp' and 'D' matrices with automatic optimization. Ensure that the products of matrices A and B are stored in 'tmp,' which is then scaled and added to matrix D after being multiplied by matrix C.\" \n\nThis instruction correlates with the pragmas in the provided code, indicating:\n\n- \"#pragma ACCEL kernel\": Treat the specified function as a hardware kernel for acceleration.\n- \"#pragma ACCEL PIPELINE auto\": Automatically determine pipelining strategy for loops (applied to the outer loops i and j).\n- \"#pragma ACCEL TILE FACTOR=auto\": Let the HLS determine the optimal tiling factor for loops (applied to the outer loops i and j).\n- \"#pragma ACCEL PARALLEL FACTOR=auto\": Allow HLS to automatically deduce the parallel factor for loops (applied to all loops i, j, and k).\n- \"#pragma ACCEL PARALLEL reduction = tmp\" / \"#pragma ACCEL PARALLEL reduction = D\": Perform parallel reduction on tmp and D matrices, respectively, with automatic optimization on the innermost loop k.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Optimize the provided 2mm kernel function for hardware acceleration using automatic decisions for pipelining, data tiling, and parallelization within loop nests to enhance its performance on an FPGA. Ensure the matrix operations follow the specified alpha-scaled multiplication of matrices A and B, followed by beta-scaled accumulation with matrix D, and use automatic parallel reduction for intermediary matrix multiplication results. Explicitly declare this function as a hardware acceleration kernel.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the following function as a hardware kernel, leveraging automatic optimization strategies for pipeline, tile, and parallelization for the various loops while ensuring appropriate reduction for partial results:\n\nMark 'kernel_2mm' function as a hardware kernel. Automatically pipeline the two main nested loop structures, which involve matrix initializations and calculations, along with their respective inner loops. Automatically choose the tiling factors for better data reuse and locality in these loops. Apply automatic parallelism to each loop level, including reducing operations on the 'tmp' and 'D' matrices for concurrent execution of partial sum calculations. Use the given pragmas to facilitate these optimizations, allowing the HLS tool to determine the best factors and settings for pipeline, tile, and parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, and within it, for all loops, enable automatic optimization decisions for pipelining, tiling, and parallel execution, with special consideration for parallel accumulations.\n\nSpecifically, the outer loops iterating over i and j should be considered for automatic pipelining, tiling, and parallel processing, while the innermost loops performing reductions on 'tmp' and 'D' should enable automatic parallelism with reduction. The exact pragma values for the optimization factors are to be determined automatically by the compiler tools.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 5\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for a 2mm computation with four nested loops. Automatically optimize the outer two loops for parallel pipelining and tile partitioning, and their inner loops respectively, allowing the compiler to determine the best factor for pipelining, tiling, and parallelism. Enable parallel reduction on temporary accumulation variables 'tmp' and 'D' in the innermost loops, again letting the compiler choose the optimal factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 16\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a target for hardware acceleration and optimize the nested loops within for parallel execution and efficient data tiling, allowing the compiler to automatically determine the optimal pipeline settings, tiling factors, and parallelization factors for each loop level. Additionally, enable parallel reduction on the temporary arrays 'tmp' and 'D' during their respective accumulation steps to ensure efficient summation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 20\n__PARA__L4: 4\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function that first computes a matrix multiplication of alpha * A * B and stores the result in a temporary matrix tmp, and then computes another matrix multiplication of the temp matrix and matrix C, adding that result to beta * D, with directives for loop optimization:\n\n1. Designate the function `kernel_2mm` as a hardware kernel for acceleration.\n2. For each pair of nested loops, set the outermost loop to automatically determine the best pipeline strategy, tile size, and parallel factor.\n3. For the innermost loop, enable parallelism with automatic factor determination and specify the `reduction` variable relevant to the loop operation.\n4. Ensure that the matrices are sized according to the given dimensions (40x50 for tmp, 40x70 for A, 70x50 for B, 50x80 for C, and 40x80 for D) with no changes in their processing order.\n\nThe code structure and pragmas provided in the task are correct; use 'auto' to allow the HLS tool to choose the specific values for the pipeline, tile factor, and parallel factor pragmas. Use the placeholder format such as `{__PIPE__L0}`, `{__TILE__L0}`, `{__PARA__L0}`etc., to indicate where the HLS tool should insert the appropriate values automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the mentioned HLS codes in C, the instruction would be:\n\n\"Define a hardware acceleration kernel function called 'kernel_2mm' that computes the matrix expression 'D := alpha*A*B*C + beta*D', where 'alpha' and 'beta' are scalar values, and 'A', 'B', 'C', and 'D' are matrices with dimensions 40x70, 70x50, 50x80, and 40x80 respectively. Utilize the following optimization directives: automatically determine the most efficient strategy for pipelining and parallelizing the loops, and tiling the loops where optimal factors are to be decided by the compiler. Additionally, apply reduction to accumulate values to the 'tmp' and 'D' matrices in the innermost loops with automated parallelization factors.\"\n\nSpecifically, the #pragma directives with their values would be:\n\n- '#pragma ACCEL kernel': Treats the subsequent function as a hardware kernel for acceleration.\n  \n- '#pragma ACCEL PIPELINE auto': Allows the compiler to automatically decide if and how to pipeline a loop for parallel execution of iterations.\n\n- '#pragma ACCEL TILE FACTOR=auto': Permits the compiler to automatically determine the optimal tile size to break a loop into smaller, more manageable chunks.\n\n- '#pragma ACCEL PARALLEL FACTOR=auto': Directs the compiler to automatically decide the factor by which loop iterations should be parallelized.\n\n- '#pragma ACCEL PARALLEL reduction = <matrix_name> FACTOR=auto': Specifies that parallel execution should be employed considering dependencies imposed by reduction operations on the specified matrix, allowing the compiler to determine the parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use the following HLS pragmas to optimize the 2-matrix multiplication (2mm) kernel function for hardware acceleration:\n\n1. Apply hardware kernel acceleration to the entire function with `#pragma ACCEL kernel`.\n2. For the first two nested loops that compute the intermediate matrix product `tmp = alpha * A * B`, allow the tool to automatically determine the optimal pipeline, tiling, and parallel factor settings using `#pragma ACCEL PIPELINE auto`, `#pragma ACCEL TILE FACTOR=auto`, and `#pragma ACCEL PARALLEL FACTOR=auto` respectively at each loop level.\n3. For calculating the temporary matrix `tmp`, apply a parallel reduction on the innermost loop over `k` with `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto`.\n4. For the second set of nested loops that update matrix `D = beta * D + tmp * C`, similar to the first part, allow the tool to automatically decide on the best loop optimizations for pipeline, tile, and parallel factor settings using `#pragma ACCEL PIPELINE auto`, `#pragma ACCEL TILE FACTOR=auto`, and `#pragma ACCEL PARALLEL FACTOR=auto` at the corresponding loop levels.\n5. Finally, for the accumulation into matrix `D`, apply a parallel reduction to the inner loop over `k` with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto`.\n\nThis set of instructions aims to provide the HLS tool with maximum flexibility to optimize the function while also directing parallel reductions where accumulation takes place.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 32\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' for hardware acceleration, and automatically optimize the loops within it using pipelining, tiling, and parallel processing pragmas with their factors set for runtime determination based on the compiler's decision. Apply reductions appropriately for accumulation operations within the nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare that the function 'kernel_2mm' should be optimized for hardware acceleration. Automatically optimize the function's outer loops for pipelining, appropriate tiling, and parallelization based on hardware constraints. Within the first nested loop, initialize the 'tmp' matrix and automatically optimize the loop body for parallel reduction on 'tmp'. In the second nested loop, scale matrix 'D' by 'beta' and then automatically optimize the loop body for parallel reduction on 'D', accumulating the product of 'tmp' and matrix 'C'. Use automatic optimization choices for pipelining, tiling, and parallel factors at all loop levels.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the following function for hardware acceleration by enabling automatic pipelining, tiling, and parallelization for loops where appropriate, applying reductions on temporary arrays when performing summation within the innermost loops, and identifying the entire function as a hardware kernel.\n\nThe process and #pragma instructions:\n\n1. Identify the function `kernel_2mm` as the target for hardware acceleration.\n    - `#pragma ACCEL kernel`\n\n2. For all loops, enable the following:\n    - Automatic pipelining - The loop iterations may be overlapped in hardware, potentially improving loop execution time.\n        - `#pragma ACCEL PIPELINE auto`\n    - Automatic tiling - The loop operations are divided into smaller, more manageable blocks that can improve data locality and parallelism.\n        - `#pragma ACCEL TILE FACTOR=auto`\n    - Automatic parallelization - The loop iterations are executed in parallel when possible.\n        - `#pragma ACCEL PARALLEL FACTOR=auto`\n\n3. For the innermost loops performing the summation into the `tmp` and `D` arrays, apply a reduction to facilitate concurrent accumulation of values into these arrays.\n    - `#pragma ACCEL PARALLEL reduction = <array> FACTOR=auto`\n\nCombine these optimization directives with the loop hierarchies to guide the high-level synthesis tool in generating optimized hardware for the 2mm kernel function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be accelerated in hardware. Automate the pipelining, tiling, and parallel processing of the loops within the function to optimize performance as determined by the compiler. For the 'tmp' and 'D' matrix computations, enable parallel reductions to sum up the elements efficiently. Automatically adjust these optimization parameters based on the specific requirements of each loop, as indicated by the placeholders 'auto{__PIPE__Lx}', 'auto{__TILE__Lx}', and 'auto{__PARA__Lx}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel and apply automatic optimization directives to improve the performance of the nested loops calculating matrix multiplication and addition. Specifically, enable the compiler to automatically determine the optimal pipelining, tiling, and parallelization factors for each loop level and employ parallel reduction for temporary accumulations with respective automatic factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated function named 'kernel_2mm' for performing a 2mm computation on matrices, following these guidelines:\n\n1. Declare the function 'kernel_2mm' with integer arguments 'ni', 'nj', 'nk', 'nl', double precision scalars 'alpha' and 'beta', and 2D arrays 'tmp', 'A', 'B', 'C', and 'D' with dimensions 40x50, 40x70, 70x50, 50x80, and 40x80 respectively.\n2. Add a 'kernel' pragma to indicate the function is a hardware acceleration kernel.\n3. Inside the function, implement two nested for-loop blocks:\n   - In the first block, initialize array 'tmp' and calculate its values using 'alpha', 'A', and 'B'. Use pipeline, tile, and parallel pragmas before the loops, and set their factors to auto-tuning placeholder values that optimize the loop iterations for pipelining, tiling, and parallelism.\n   - In the second block, scale array 'D' by 'beta' and calculate its final values by adding the product of 'tmp' and 'C'. Use the same approach with pragma directives as in the first block, but ensure to include a parallel pragma with a reduction clause for array 'D' to properly handle the accumulation within the innermost loop. Again, set their factors to auto-tuning placeholder values.\n4. After each block of loops, configure the pragma auto-tuning placeholders for pipeline, tile, and parallel factors to be replaced by the HLS tool with specific values during compilation. These values are denoted as '{__PIPE__L0}', '{__PIPE__L1}', '{__PIPE__L2}', '{__PIPE__L3}', '{__TILE__L0}', '{__TILE__L1}', '{__TILE__L2}', '{__TILE__L3}', '{__PARA__L0}', '{__PARA__L1}', '{__PARA__L2}', and '{__PARA__L3}' accordingly. The placeholder values allow the HLS compiler to optimize the loops for performance.\n5. Include reduction clauses in the parallel pragmas where appropriate, to ensure correct accumulation of values in the arrays 'tmp' and 'D', mapping the reduction clauses to '{__PARA__L4}' and '{__PARA__L5}' respectively.\n\nMake sure to enclose the entire computational region within '#pragma scop' and '#pragma endscop' directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, then optimize it using automatic pipelining, tiling, and parallelization for the outer two loops, and within these, for the inner loops, automatically optimize the pipelining, tiling, and, specifically for the innermost loop, enable parallel reduction for the output arrays 'tmp' and 'D', allowing the compiler to decide the best factor for each pragma.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 70\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a hardware-accelerated 2mm kernel, which performs a two-matrix multiply-add operation (D := alpha * A * B * C + beta * D) with the following specifications:\n\n1. Designate the function `kernel_2mm` with integer arguments `ni`, `nj`, `nk`, and `nl` as well as double precision arguments `alpha`, `beta` and matrices `tmp`, `A`, `B`, `C`, and `D` as a hardware kernel for acceleration.\n2. Implement two nested loops for the matrix multiplication and accumulation `tmp = alpha * A * B`. The outer loop iterates 40 times, and the inner loop runs 50 times.\n3. Automate pipeline optimization for both loops with `#pragma ACCEL PIPELINE auto`, allowing the compiler to determine the optimal pipeline settings.\n4. Automate tile optimization for both loops with `#pragma ACCEL TILE FACTOR=auto`, permitting the compiler to set the tiling factor for optimal data reuse and locality.\n5. Automate parallelization for both loops with `#pragma ACCEL PARALLEL FACTOR=auto`, giving the compiler the ability to decide the degree of parallelism.\n6. Optimize the reduction operation within the innermost loop, automatically adjusting parallelization for the reduction of the `tmp` matrix with `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto`.\n7. Implement another two nested loops for the matrix addition `D = beta * D + tmp * C`. The outer loop iterates 40 times, and the inner loop 80 times, with pipeline, tile, and parallel optimizations automatically determined by the compiler similar to the first set of loops.\n8. Optimize the reduction operation within the innermost loop of the second set, automatically adjusting parallelization for the reduction of the `D` matrix with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto`.\n9. Ensure all optimizations are done within the scope of the kernel function but outside any loop-specific constructs.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the following function is recognized as a dedicated hardware acceleration point. Within the function, use automatic determination for pipelining, tiling, and parallelization strategies for each loop, and allow for automatic factor determination. Additionally, utilize parallel reduction on temporary arrays in nested loops to optimize matrix-matrix multiplication and subsequent accumulation stages.\n\nHere is a breakdown for the specific required `#pragma` directives:\n\n1. Declare the 'kernel_2mm' function as a hardware acceleration kernel.\n2. For the outer loops 'i' in both major loop nests, enable automatic pipelining, automatic tiling, and automatic parallelization.\n3. For the inner loop 'j', similarly enable automatic pipelining, automatic tiling, and automatic parallelization.\n4. In the innermost loop where 'tmp' or 'D' arrays are accumulated, enable parallel reduction with automatic factor selection to efficiently compute the product and sum of matrix elements.\n\nMake sure that the strategies for each loop are automatically decided by the HLS toolchain by using \"auto\" as the value wherever \"FACTOR=auto\" is specified. Use placeholders like `{__PIPE__LX}`, `{__TILE__LX}`, and `{__PARA__LX}` where \"LX\" corresponds to the loop level to which the pragma is applied. This maintains readability and allows specific iterations of loop optimization by the HLS during compilation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 32\n__PARA__L4: 70\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel named 'kernel_2mm' that takes as parameters dimensions of matrices, scalar multipliers, and four matrices as input. This kernel performs the operation D := alpha*A*B*C + beta*D. Automatically manage the pipelining depth and the parallel factor of the outer Loops (__PIPE__L0 and __PARA__L0). Use the same automatic pragma directives for the inner Loops (__PIPE__L2 and __PARA__L2). Incorporate a 'reduction' pragma for variable 'tmp' in the innermost loop (__PARA__L4), in order to correctly accumulate the sum across multiple hardware threads. Similarly, manage the pipelining depth and the parallel factor of the subsequent outer and inner loops (__PIPE__L1, __PARA__L1, __PIPE__L3, and __PARA__L3) followed by a reduction for the variable 'D' in the innermost loop (__PARA__L5). The 'ACCEL TILE' pragma is used for data tiling in loops, where value is assigned automatically in each case (__TILE__L0, __TILE__L2, __TILE__L1, __TILE__L3).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate this mathematical matrix operation kernel into an HLS-accelerated kernel function, automatically optimizing the pipelining of the loops, tiling for partitioning the data into smaller chunks, and parallelizing the iterations where possible. Also, implement reduction to accumulate values correctly when optimizing parallel loop iterations. Autotune the parameters for pipelining, tiling, and parallelization instead of using fixed values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: flatten\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function to be synthesized into a hardware kernel and iterate through specific multi-dimensional computations, using dynamic optimization techniques such as pipelining, tiling, and parallel processing, with each loop's optimization parameters set by the compiler automatically.\n\nThis instruction corresponds to the inclusion of the kernel pragma for the function, and the use of automatic settings for pipelining, tiling, and parallel processing in nested loops, which instructs the compiler to choose the most effective values on its own. Reduction pragmas are also used to handle accumulation in parallel loops efficiently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel to compute `D := alpha*A*B*C + beta*D`. Auto-tune pipelining, tiling, and parallel processing for the entire function and individual loops to enhance performance without manual specification of factors, and automatically handle reduction operations within the nested loops to accumulate matrix multiplication results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_2mm' as a hardware accelerator kernel, automatically optimize the loops for pipelining, tiling, and parallel processing, and apply reduction to accumulate values into 'tmp' and 'D' arrays with the appropriate optimization factors determined automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 70\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the given 2mm kernel function for hardware acceleration by treating it as a hardware kernel, letting the compiler automatically decide the optimal pipeline, tiling, and parallelization strategies for each loop nest while also applying reduction to accumulate the results of specific variables.\n\n- Apply the kernel pragma to the function to designate it as a hardware acceleration point.\n- For the first pair of nested loops, allow automatic loop pipelining, automatic loop tiling, and automatic parallelization for the outer and inner loops, applying parallel reduction for the accumulation of the `tmp` array.\n- For the second pair of nested loops, apply similar automatic optimizations as the first and use parallel reduction for updating the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Perform optimization on the loops within 'kernel_2mm' by: allowing the compiler to determine if pipelining should be applied and to what extent; letting the compiler choose the tiling factor for potential loop unrolling and array partitioning; and enabling the compiler to decide the parallelism factor for loop iterations. Further, apply automatic parallelism to reduction operations on the 'tmp' and 'D' arrays within their respective loops to potentially improve performance through concurrent execution of reduction statements. Use automatic settings for these optimizations so that the compiler can analyze and decide them at compile time.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 70\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` for acceleration, enable automatic optimization for pipelining, tiling, and parallelization for all loops, and apply reduction for matrix operations contained within the nested loops, where HLS will determine the optimal factors for these optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 32\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the `kernel_2mm` function by treating it as a hardware kernel, automatically enabling pipeline optimization for each loop, determine optimal tiling and parallelization factors, and apply parallel reduction for specified arrays within the loop bodies.\n\n- Treat the function `kernel_2mm` as a hardware kernel for acceleration.\n- For all loops labeled `L0`, `L1`, `L2`, and `L3`, enable pipeline optimization automatically and let the compiler decide on the optimal configuration.\n- For these same loops, allow the compiler to automatically determine the optimal tile size and parallelization factor.\n- Specifically for loop `L4`, enable parallel reduction on the `tmp` array and let the compiler automatically choose the parallelization factor.\n- Similarly, for loop `L5`, enable parallel reduction on the `D` array with an automatically chosen parallelization factor by the compiler.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with appropriate pragmas, instruct the generator as follows: \"Create a function that acts as a hardware kernel to accelerate the computation of a 2mm (two-matrix multiplication) operation involving matrix multiplication and addition, optimized for FPGA. The outer loops should be automatically optimized for pipelining, tiling, and parallelism, these optimizations should be declared with `auto` to let the tool determine the best values. Inner loop over 'k' requires assignment of a parallel pragma with reduction on the 'tmp' matrix for the first multiplication and on the 'D' matrix for the addition, with optimization factors set to automatic. Each loop level should have its unique pipeline, tile, and parallel pragmas with auto-tuned factors.\" \n\nSpecific pragma instructions for each part are:\n\n- \"#pragma ACCEL kernel\": Indicates that the function is to be treated as a hardware kernel for acceleration.\n- \"#pragma ACCEL PIPELINE auto\": Enables automatic pipelining optimization of the loop by the HLS tool.\n- \"#pragma ACCEL TILE FACTOR=auto\": Allows the HLS tool to automatically choose the tiling factor to break the loop into smaller, more manageable chunks that can be executed in parallel.\n- \"#pragma ACCEL PARALLEL FACTOR=auto\": Directs the HLS tool to automatically decide the parallelism factor, which handles how many loop iterations are to be run in parallel.\n- \"#pragma ACCEL PARALLEL reduction = [matrix] FACTOR=auto\": Applies the parallel pragma with automatic factor and specifies the reduction operation on the given matrix ('tmp' or 'D'), which properly accumulates the results across parallel operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, enable automatic loop pipelining, tiling, and parallel processing with optimization heuristics for the loops involved in matrix multiplication and addition. Specifically, employ automatic pragma settings to improve pipelining, tiling, and parallel processing for each loop nest during the computation of the D matrix as the result of `alpha * A * B * C + beta * D`, while also performing automatic optimization for reduction operations involving the `tmp` and `D` arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm (two-matrix multiply) kernel function taking matrix dimensions and scalar values `alpha` and `beta` as parameters, along with matrices `tmp`, `A`, `B`, `C`, and `D`. Identify the function as a hardware acceleration kernel. Within the function, use automated pipelining, tiling, and parallelization for the outer two loops and their respective inner loops, with the parallelization including a reduction pragma applied to the innermost loop for accumulating results in `tmp` and `D`. Optimize the loop operations using pragmas with automatic factor determination, and ensure dependencies are maintained for correct reductions in matrix operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 70\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function with automatic performance optimization pragmas for pipeline, tiling, and parallelism. The code includes directives for a function that multiplies and accumulates matrices with a nested loop structure while applying scaling factors alpha and beta. Implement automatic pipelining for each loop to optimize throughput, apply automatic tiling to generate smaller computation blocks from the original loops, and automatically determine the factor for parallel loop execution, including specifying reduction operations for the innermost loops to ensure parallel summations. The outer loops iterate over the rows of matrix `tmp` and matrix `D`, while the inner loops handle the computation of these matrices, considering the given scaling of matrix `D` and the matrix multiplication and accumulation for `tmp`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Make the following function optimized for hardware acceleration, allowing the compiler to automatically determine the optimal points for pipelining, tiling, and parallel execution of loops, and ensure intermediate results are accumulated in a parallel-reduced manner for matrix computation.\n\nSpecifically, the function 'kernel_2mm' performs a two-matrix-multiply operation on matrices A, B, C, and D using scaling factors alpha and beta. Every outer and inner loop should be considered for automatic pipelining, tiling, and parallel execution, but the compiler should automatically determine the factors. Additionally, within the innermost loops, use parallel reduction to accumulate the products for elements of the temporary matrix 'tmp' and the result matrix 'D', allowing simultaneous updates across iterations. The compiler should decide the reduction factor as well.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a target for hardware acceleration while allowing the compiler to determine the best parallelization, tiling, and pipelining strategies automatically for the nested loops involved in matrix multiplication and addition, noting that the parallel pragma should also accommodate for reduction on arrays 'tmp' and 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes to create a hardware kernel function called `kernel_2mm` that will perform a two-matrix multiplication operation followed by a matrix addition. Allow the compiler to automatically decide the best pipelining, tiling, and parallelism strategies for all loops by appending the respective pragmas with 'auto'. Also, apply reduction to accumulate values in `tmp` and `D` arrays within the parallel nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named `kernel_2mm` that accelerates the calculation of the matrix expression `D := alpha * A * B * C + beta * D` with dimensions `ni=40`, `nj=50`, `nk=70`, and `nl=80` using the following directives:\n\n- Treat the `kernel_2mm` function as a hardware kernel suitable for acceleration.\n- For the outermost loops `i` iterating over dimension 40, allow the compiler to auto-determine the optimal settings for pipelining, tiling, and parallel processing.\n- Inside the `i` loop, another nested loop `j` over dimension 50 should also be optimized automatically in terms of pipelining, tiling, and parallel processing, but in addition, enable automatic parallel reduction for the temporary matrix `tmp`.\n- Repeat the auto-optimization strategies for pipelining, tiling, and parallel processing for another pair of nested loops: the outer `i` loop iterating over dimension 40, followed by the `j` loop iterating over dimension 80. In the innermost loop, enable automatic parallel reduction for matrix `D`.\n\nEach optimization strategy uses the placeholder `auto{__PIPE__Ln}`, `auto{__TILE__Ln}`, and `auto{__PARA__Ln}` where `Ln` corresponds to the loop nesting level, to let the compiler insert the appropriate values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a kernel function `kernel_2mm` for hardware acceleration that performs a 2mm matrix multiplication and addition computation (`D := alpha*A*B*C + beta*D`) with automatic optimization directives for pipelining, tiling, and parallel execution, as well as utilizing reduction clauses for partial results in the nested loops, letting the HLS compiler determine the best factors for parallelism, pipelining, and tiling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please accelerate the `kernel_2mm` function with pragmatic directives to enable hardware optimization. Specifically, allow the tool to determine the optimal settings for loop pipelining, tiling, and parallelization. In the first loop nest, automatically pipeline the outer two loops and use automatic factors for parallelizing the innermost loop with a reduction on the `tmp` array. In the second loop nest, follow the same pattern of pipelining and parallelization, but with a reduction on the `D` array in the innermost loop. The function performs a matrix multiplication followed by a matrix addition, where `D` is updated with the operation `alpha * A * B * C + beta * D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the matrix multiplication and addition function 'kernel_2mm' for FPGA by allowing the compiler to determine the optimal pipelining, tiling, and parallel execution strategies, while also maintaining reductions on temporary calculations for the two nested loop sequences.\" \n\nFor each loop, you should apply the following HLS pragmas with automatic factor determination:\n- Enable automatic pipelining with `#pragma ACCEL PIPELINE auto{__PIPE__L#}`\n- Allow the compiler to select the tile factor with `#pragma ACCEL TILE FACTOR=auto{__TILE__L#}`\n- Apply automatic parallelization with `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L#}`\n- Use parallel reduction to optimize accumulation in the innermost loops with `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{__PARA__L#}`, where 'variable' is `tmp` for the first triply nested loop and `D` for the second.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function that accelerates a standard two-matrix multiply-add computation, where `D := alpha * A * B * C + beta * D`. The function will take six parameters specifying the scalars (`alpha` and `beta`), input matrices `A`, `B`, and `C`, the resultant intermediate matrix `tmp`, and output matrix `D`, with appropriate dimensions for each. Declare all matrices statically sized and the sizes of the dimensions as macros where relevant. \n\nFor each nest of loops, enable automatic pipeline optimization. For the innermost loops where the reduction operations take place to accumulate values into `tmp` and `D`, the pragmas should allow automatic factorization for parallelization of the reductions. Also, allow automatic tile factor assignment for loop chunking as needed for locality optimizations on the two matrix multiply-add computations\u2014first accumulating into `tmp` and then using `tmp` to update `D`. \n\nIn detail:\n- Mark the function as a hardware kernel to be accelerated.\n- Autonomously decide the pipelining of loops `i` and `j` in both the matrix multiplication stages to balance parallel execution and resource usage.\n- Specify tile sizes autonomously for both loops `i` and `j` in both matrix computation stages.\n- Specify parallelization factors autonomously for reduction on `tmp` and `D` accumulation. \n\nEnsure that the pragmas you describe use 'auto' tokens (`{__PIPE__L*}`, `{__TILE__L*}`, `{__PARA__L*}`) to instruct the HLS tool to automatically optimize the pipelining, tiling, and parallelization factors for each respective loop level. \n\nIn natural language, your instruction should be something like: \n\n\"Generate a hardware-accelerated 2mm kernel function with automatic optimization pragmas for pipelining, tiling, and parallel reductions for efficient FPGA synthesis.\u201d", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a matrix-matrix multiplication and addition kernel with the following optimizations: Enable the function to be synthesized as a dedicated hardware kernel. For the first-level loops (`i` loops operating over the `tmp` and `D` arrays), apply automatic loop pipelining optimization, automatic loop tiling, and automatic parallel processing optimization. Within the second-level `j` loops, apply the same set of automatic optimizations as the first level. Furthermore, for the innermost loops (`k` loops), enable automatic parallel processing with reductions for the `tmp` and `D` accumulations. Use the corresponding pragma directives with the 'auto' keyword to allow the HLS tool to determine the optimal factor for each pragma setting.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a kernel function named `kernel_2mm` that accepts several parameters - four integers (ni, nj, nk, nl), two double variables (alpha, beta), and 4 two-dimensional double arrays (tmp, A, B, C, D). This function will update the array D as `alpha*A*B*C + beta*D`. Process each loop in an optimized auto manner without specific orders, by using the following three #pragmas respectively: `ACCEL PIPELINE auto` to enable automatic loop pipelining, `ACCEL TILE FACTOR=auto` to specify automatic optimization of the execution sequence of loop iterations, and `ACCEL PARALLEL FACTOR=auto` for automatically optimizing concurrent execution of loop iterations. Inside the first nested loop, you will initialize the elements of array `tmp` to 0 and then, use the third loop to calculate the sum of `alpha * A[i][k] * B[k][j]` for each `k` and store this sum to `tmp[i][j]`. Ensure this operation has a parallel execution which is automatically optimized with a reduction operation applied to `tmp`. Then in the second main loop, you will multiply each element of array `D` by `beta` and then, use the third loop to calculate the sum of `tmp[i][k] * C[k][j]` for each `k` and add this sum to `D[i][j]`. Ensure also this operation has a parallel execution which is automatically optimized with a reduction operation applied to `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 8\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel using the appropriate pragma. To optimize the execution of the main component loops within the function, apply automatic pipeline, tiling, and parallelization directives with the `auto` keyword, and ensure that variable reductions in nested loops are also parallelized. Adjust these optimizations for each unique loop nest within the function, using distinct parameter placeholders for the pragma values to enable separate tuning for each loop level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to optimize a 2mm kernel function that performs matrix multiplication and scaling, accommodating the following conditions:\n\n1. The function should be recognized as a distinct hardware module.\n2. For all loops, automatically manage the pipeline settings to achieve enhanced throughput without specifying explicit pipeline stages.\n3. For each loop nesting level, permit the tool to automatically determine the optimal tiling factor for processing loop iterations in smaller, more manageable blocks.\n4. Where applicable, enable parallel execution of loop iterations with the tool selecting the optimal factor, also allow for a reduction operation on the 'tmp' and 'D' arrays within the innermost loops with an automatic parallel factor.\n5. The 'tmp' array is initially zeroed out and then used to hold the intermediate result of `alpha * A * B`.\n6. The 'D' array is first scaled by `beta`, and then the result of `tmp * C` is accumulated into it.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code, please use the following instruction:\n\n\"Define a function named 'kernel_2mm' that accelerates the matrix-matrix multiplication and addition algorithm on hardware. Treat the entire function as a hardware acceleration kernel. Inside the function, employ hardware optimization directives to improve performance. Automatically optimize loop pipelining, tiling, and parallelization for all loops. Moreover, apply automatic parallel reduction optimizations to the innermost loops, where partial results of the 'tmp' and 'D' matrices are accumulated.\"\n\nSpecifically, the pragmas should:\n\n1. Make the 'kernel_2mm' function a hardware kernel using `#pragma ACCEL kernel`.\n2. Enable automatic loop pipelining using `#pragma ACCEL PIPELINE auto`, which you should place right before each loop.\n3. Automatically optimize loop tiling using `#pragma ACCEL TILE FACTOR=auto`.\n4. Allow the loops to run iterations in parallel automatically with `#pragma ACCEL PARALLEL FACTOR=auto`.\n5. For the innermost loops involving the 'tmp' and 'D' array updates, perform parallel reductions automatically by using `#pragma ACCEL PARALLEL reduction=target_array FACTOR=auto`, where `target_array` is either 'tmp' for the first triple-nested loop or 'D' for the second triple-nested loop.\n\nThe placeholders `{__PIPE__L0}`, `{__TILE__L0}`, `{__PARA__L0}`, etc., should be replaced with the appropriate loop labels as per the code's loop hierarchy if needed. The 'auto' value assigned to each optimization directive should enable the compiler to infer the best possible optimization strategy for the target hardware architecture.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 8\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function 'kernel_2mm' as a hardware acceleration kernel, and for each loop, enable automatic optimization for pipelining, data tiling, and parallel execution, adjusting these optimizations automatically for performance. Also, specify automatic parallelization with a reduction on the 'tmp' and 'D' arrays where applicable.\n\n- Treat the function 'kernel_2mm' as an accelerator kernel.\n- Automatically optimize the outer loops 'i' labeled L0 and L1 for pipeline execution.\n- Allow the compiler to automatically determine the tiling factors for these loops, marked L0 and L1.\n- Enable automatic optimization of the parallel execution factor for both outer loops, marked L0 and L1.\n- For the second-level loops 'j,' labeled L2 and L3, enable automatic pipelining, determine the tiling factors, and optimize the parallel execution factor automatically.\n- In the innermost loop 'k,' labeled L4 and L5, apply parallel optimization with automatic factor determination, and use reduction on respective arrays 'tmp' and 'D' to calculate additions concurrently within the loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS pragmas to accelerate a double matrix multiplication (2mm) kernel that performs `D := alpha*A*B*C + beta*D`. Automatically optimize the loops for pipelining, tiling, and parallel execution, where the tool automatically determines the appropriate factors. Additionally, specify pragma for parallel reduction to accumulate results in the 'tmp' and 'D' arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a matrix multiplication followed by an addition with alpha and beta scaling factors. The function 'kernel_2mm' should be treated as a hardware kernel that accelerates the computation of D = alpha * A * B * C + beta * D, where A, B, C, and D are two-dimensional arrays of appropriate dimensions. Optimize the loops with automatic pipelining, tiling, and parallel processing, allowing the compiler to decide the optimal values dynamically. Additionally, enable parallel reduction on the 'tmp' and 'D' arrays within their respective loops to improve performance. Use the respective #pragma commands to instruct the compiler to apply these optimizations, while leaving the innermost 'k' loop factors to be automatically determined by the compiler for all instances it occurs.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 8\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a target for hardware acceleration and auto-optimize it using directives to enable pipelining, tiling, and parallel processing while performing a matrix-multiplication and addition computation. Specifically, in the first nested loop, set the pipelining, tiling, and parallelism of the outer two loops to automatic factors, while applying parallel reduction to the innermost loop to cumulatively sum up multiplication results in `tmp`. In the second nested loop, also automatically manage pipelining, tiling, and parallelism, and similarly use parallel reduction in the innermost loop to incrementally calculate the matrix operation `D = beta*D + tmp*C`. Indicate that the appropriate pragma factors should be determined automatically and embedded within each loop, allowing the HLS tool to decide the best optimization strategies for performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, allowing the HLS tool to use its optimization strategies on this function. Employ automatic decision-making for pipelining, tiling, and parallelization for the nested loops that calculate the matrix-matrix multiplication and addition, while also automatically figuring out the appropriate degree of parallelizing the reduction operations inside the innermost loops. Use pragmas to enable these optimizations, with automatic factor determination placeholders that will be filled by the HLS tool during compilation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the matrix multiplication and addition function 'kernel_2mm' using HLS C pragmas by designating it as a hardware kernel, automating the pipeline, tiling, and parallelization of the outer and inner loops with unspecified factors for optimization, and applying parallel reduction to accumulate matrix product results. Use specific pragmas to indicate that the loops' optimization factors are to be automatically determined by the tool, and ensure to apply parallel reduction where necessary with the 'reduction' keyword for the 'tmp' and 'D' arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and employ automatic optimization strategies for pipelining, tiling, and parallelization of loop operations involved in matrix-matrix multiplication and addition within the hardware design. These optimizations should automatically determine the best values for pipeline stages, tile sizes, and parallel factors, as well as perform parallel reduction on intermediate results to enhance parallel execution within the loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the `kernel_2mm` function as a hardware kernel for acceleration, and automatically optimize all nested loops for parallelization, pipelining, and loop tiling with factors determined by the HLS tool. Additionally, for the accumulation operations within the loops, apply a parallel reduction to enable concurrent summation for the `tmp` and `D` matrices, with factors and specific optimizations again automatically determined by the HLS tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: flatten\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration function named 'kernel_2mm' that performs a series of matrix multiplications and additions: it computes `D` as the sum of `alpha` times the multiplication of matrices `A`, `B`, and `C`, and `beta` times `D` itself, all with automatic optimization for pipeline, tiling, and parallelism. \n\nEnable the compiler to decide the best pipeline strategy for each of the four main loops, which iterate over `i` and `j` for two distinct operations. \n\nFor the first operation, initialize matrix `tmp` to zero while allowing automatic decisions on how to parallelize reductions on `tmp` within its nested loops. \n\nFor the second operation, scale matrix `D` by `beta` and sum it with the product of `tmp` and `C`, and similarly allow automatic optimization for reductions on `D` within its nested loops. \n\nUse the `#pragma ACCEL` directives with 'auto' specified, letting the compiler identify optimal `FACTOR` values for `PIPELINE`, `TILE`, and `PARALLEL` in all loops, with loop-specific tags `{__PIPE__Lx}`, `{__TILE__Lx}`, and `{__PARA__Lx}` to distinguish the loops and enable fine-grained control over each loops' optimization parameters.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate hardware-accelerated code to perform a 2-matrix multiplication followed by another matrix multiplication and accumulation, according to the equation D := alpha * A * B * C + beta * D. Allow the compiler to automatically optimize pipelining, tiling, and parallelism at each nested loop level, while also making use of reduction optimizations for the temporary accumulation arrays. Specifically, apply the following pragmas:\n\n1. Treat the `kernel_2mm` function as a hardware kernel for acceleration.\n2. Automatically optimize pipeline settings at all loop levels where applicable.\n3. Automatically optimize the tile factor for loop unrolling to potentially improve parallelism at all loop levels where applicable.\n4. Automatically optimize the parallel factor for all loops, allowing the compiler to decide how many iterations of the loops can be executed in parallel.\n5. Apply parallel reduction to the accumulation arrays `tmp` and `D` where inner loops modify their values through additions, allowing potential parallel computations of partial sums. \n\nEnsure you properly annotate the start and end of the optimization scope and bind the appropriate loop variables to each pragma's automated values. Use `auto{__PIPE__L*}`, `auto{__TILE__L*}`, and `auto{__PARA__L*}` placeholders, where `*` is replaced with the corresponding loop level identifier to instruct the compiler on automated optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code to implement a 2mm kernel used for matrix multiplication and accumulation, applying specific optimization directives as follows:\n\n1. Designate the function `kernel_2mm` as a hardware kernel that will be accelerated.\n2. Apply automatic pipelining to the outermost loops for computing temporary matrix `tmp` and the accumulation on matrix `D`, optimizing loop execution by overlapping iterations when possible.\n3. Apply automatic loop tiling for these loops to spatially partition the computation into smaller, manageable chunks that can be executed in parallel efficiently.\n4. Apply automatic parallelization to all loops, allowing independent iterations to be computed simultaneously where hardware resources permit.\n5. For the innermost loops updating the `tmp` and `D` matrices, include a directive that allows for the reduction operation to be performed in parallel, enhancing performance.\n\nUse the `#pragma ACCEL` directive followed by the optimization type and use \"auto\" as the pragma value to allow the tool to automatically infer the best factor. For loop pipelining, use the `PIPELINE` keyword followed by the `auto` value within curly braces including an identifiable loop tag (e.g., `{__PIPE__L0}`). Similarly, use `TILE` and `PARALLEL` keywords with `auto` value for loop tiling and parallelization, each with their respective identifiable tags (e.g., `{__TILE__L0}` and `{__PARA__L0}`). Additionally, specify the reduction variable and apply the automatic factor using the `reduction` keyword followed by the variable name and `FACTOR=auto` with identifiable tags (e.g., `reduction = tmp FACTOR=auto{__PARA__L4}`).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the 'kernel_2mm' function as a hardware kernel, allowing the HLS tool to automatically manage pipelining, tiling, and parallelization for all loops. Specifically, perform matrix multiplication and accumulation on the temporary matrix 'tmp' and the result matrix 'D' with an auto-determined degree of parallelism that includes reduction, where the intermediate results of 'tmp' and 'D' are combined using specified factors of alpha and beta, correspondingly.\n\nIn detail:\n- Indicate that the function 'kernel_2mm' is a hardware acceleration kernel.\n- For the initial matrix multiplication loops that populate 'tmp', automatically manage pipelining, tiling, and parallelization with reduction on 'tmp'.\n- For the following loops that update 'D', automatically manage pipelining, tiling, and parallelization with reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function performing a two-matrix multiply operation, with automatic loop optimization directives, by following these instructions:\n\n1. Define the function as a hardware kernel for acceleration.\n2. Within the function, apply automatic pipeline optimizations to the outermost loops that compute the temporary matrix `tmp` and update matrix `D`.\n3. Use automatic tiling to improve data reuse for the same outer loops, and apply automatic parallel processing to maximize computational throughput.\n4. For the inner loops that accumulate the products into `tmp` and `D`, enable parallel processing with automatic factor selection, and ensure the compiler recognizes these loop variables as targets for reduction to prevent data hazards.\n\nSpecifically, declare the following pragmas with auto factors where applicable:\n\n- At the top of the function, use the kernel pragma to indicate hardware acceleration.\n- For each of the two outer loops (iterating over `i`), enable pipelining, tiling, and parallelism with the pragmas, setting to auto to let the compiler determine the best values.\n- For the innermost loops (iterating over `k`), which perform the reduction operation, enable parallelism and specify the reduction variable (either `tmp` or `D`), again with the factor set to auto to let the compiler choose the optimal value.\n\nThe resulting HLS C directives should inform the compiler to optimize the given nested loops with pipeline, tile, and parallel pragmas, all set to automatic factors, to generate efficiently accelerated hardware.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For the loops contained within, automatically optimize pipelining, tiling, and parallel processing settings, allowing the compiler to choose the best values. Additionally, enable parallel reduction for temporary arrays to facilitate combining results across iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C with specific pragmas:\n\nInstruct to designate the function 'kernel_2mm' as a hardware acceleration kernel for optimization. Then configure the compiler to automatically determine the most appropriate optimization strategies for pipelining, data tiling, and parallel processing for each loop nest within the function. Additionally, employ parallel reduction on specific loop iterations where accumulations occur on arrays 'tmp' and 'D', letting the compiler automatically select the optimization factor, to effectively handle dependencies and maximize throughput.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel with auto-tuning for the loops to maximize dataflow efficiency, where inner loop iterations perform multiplication and accumulation with parallelism, and the reduction in temp arrays is also enabled with automatic factor selection for pipelining, tiling, and parallel loop execution based on design analysis.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, then apply the following loop optimization strategies: \n\n1. For the top-level loops over the indices 'i' and 'j' in both the matrix multiplication and the addition process, automatically choose whether to pipeline the loop operations based on an analysis of the loop dependency graph, inherently deciding on the amount of parallelism that can be applied.\n\n2. Automatically determine the optimal size of loop tiles for the first tier loops to generate smaller, more manageable computations, which could be run in parallel or to fit into on-chip resources.\n\n3. Automatically determine the degree of parallel loop execution for the top-level loops to improve data throughput by executing multiple iterations simultaneously, as long as data dependencies allow it.\n\n4. Within the nested 'k' loops for the matrix multiplication and addition process, enable parallel reduction on the 'tmp' and 'D' arrays, respectively, where the loop iterations can be processed in a reduction tree pattern to accumulate the sum. Automatically calculate the factor of parallelism to apply to these loops to optimize parallel reduction.\n\nTo achieve this, use:\n\n1. `#pragma ACCEL kernel` to treat the 'kernel_2mm' function as an acceleration kernel.\n\n2. `#pragma ACCEL PIPELINE auto` with placeholders `__PIPE__L0`, `__PIPE__L1`, `__PIPE__L2`, and `__PIPE__L3` for automatic loop pipelining optimization at different loop levels.\n\n3. `#pragma ACCEL TILE FACTOR=auto` with placeholders `__TILE__L0`, `__TILE__L1`, `__TILE__L2`, and `__TILE__L3` for automatic determination of tile sizes.\n\n4. `#pragma ACCEL PARALLEL FACTOR=auto` with placeholders `__PARA__L0`, `__PARA__L1`, `__PARA__L2`, and `__PARA__L3` for auto-calculating the degree of parallelism for the main loops.\n\n5. `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto` with placeholders `__PARA__L4` and `__PARA__L5` for the inner 'k' loops that perform reduction on 'tmp' and 'D', for automatic optimization tuning of the reduction operation's parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_2mm' function as a hardware acceleration kernel and within it, for each loop, automatically optimize the loop's pipeline, tiling, and parallelism pragmas for best performance; also use parallel reduction where appropriate for intermediary arrays to ensure efficient synthesis. Use wildcard placeholders to let the tool decide the specific factor values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware-accelerated kernel, automatically optimizing each loop for pipelining, tiling, and parallel execution based on the given loop structure and data dependencies. Apply parallel reduction patterns for matrix multiplications where appropriate, allowing for automatic factor determination by the HLS tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a matrix multiplication and addition function named 'kernel_2mm' with the following characteristics:\n\n1. Designate the function `kernel_2mm` to be a hardware kernel for acceleration.\n2. For the first loop nest, which computes the intermediate product `tmp[i][j]`, enable automated pipelining, tiling, and parallelization for the outer loops `i` and `j` but make sure parallelization for the innermost loop `k` also includes an automatic reduction on the `tmp` array.\n3. For the second loop nest, which computes the final matrix `D[i][j]` by scaling it with `beta` and adding the product of `tmp` and `C`, similarly enable automated pipelining, tiling, and parallelization for the outer loops `i` and `j`, and ensure that the innermost loop `k` has automated parallelization with a reduction on the `D` array.\n4. Use placeholders `__PIPE__Lx`, `__TILE__Lx`, and `__PARA__Lx` in the pragmas to indicate where the compiler is free to automatically determine the optimal values for pipeline stages, tiling factors, and parallelization factors, respectively, where 'x' corresponds to the loop level.\n\nIn natural language, the instruction would be: \n\n\"Create a two-matrix multiplication and addition hardware acceleration function 'kernel_2mm' that automatically pipelines, tiles, and parallelizes loops for optimal performance, with automatic parallel reduction for the inner product accumulation on the temporary matrix 'tmp' and the output matrix 'D'.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function named 'kernel_2mm' to be treated as a hardware acceleration kernel that performs a 2-matrix multiply and then multiplies and adds the result to another matrix (D := alpha*A*B*C + beta*D). The function takes integer dimensions ni, nj, nk, nl and doubles alpha and beta, along with double-precision floating-point two-dimensional square arrays 'tmp', 'A', 'B', 'C', and 'D' of sizes defined by these dimensions. Automatically optimize the top-level loop nests for performance using pipelining, tiling, and parallelism, utilizing compiler heuristics to determine the appropriate factor values. For the innermost loops of both top-level loop nests, utilize automatic parallelism with reduction on the 'tmp' and 'D' arrays respectively to enable concurrent accumulation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply automatic hardware acceleration instructions for efficient parallel execution and resource allocation for the matrix multiplication and addition kernel, with automated optimization suggestions for pipelining, tiling, and parallelization, as well as designated parallel reductions for intermediate and output matrices.\n\nSpecifically:\n- Declare the function as a hardware acceleration kernel.\n- Enable automatic pipelining, tiling, and parallel factor suggestions at both loop levels for iterating over tmp, A, B, C, and D matrices.\n- Utilize parallel reduction for accumulating the products into tmp and D matrices, allowing automatic determination of the parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 7\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes with pragmas corresponding to automated performance optimization for a 2mm kernel in C, use the following natural language instruction:\n\n\"Compile the given matrix multiplication kernel for a hardware accelerated implementation, automatically optimizing the loops for pipeline, tiling, and parallel processing according to the compiler's analysis. In the first pair of nested loops, perform alpha times A times B operation and store the result in a temporary matrix. In these loops, automate the allocation of pipeline stages, memory tiling, and parallel computation, also applying a reduction to accumulate results in the temporary matrix. In the second pair of nested loops, scale matrix D by beta and add the product of the temporary matrix and matrix C to D. Similar to the first loops, automate the optimization for pipeline, tiling, and parallelism, with a reduction for the accumulation in matrix D. Ensure all computations adhere to the best possible loop transformation suggested by the HLS tool without manual specification, thereby scaling the loop optimization factors dynamically.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function named 'kernel_2mm' that takes dimensions and matrix coefficients as inputs. Apply a matrix multiplication and addition algorithm to compute `tmp = alpha * A * B` and `D = beta * D + tmp * C`, optimizing the loops with automatic pipelining, tiling, and parallelization directives while also including reduction clauses for the innermost loops to accumulate the results properly. Use the automatic setting for all pragma factors to enable the compiler's heuristics to decide the level of parallelism, pipelining, and tiling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code by annotating a hardware-accelerated kernel function that computes the operation `D := alpha * A * B * C + beta * D` where `A`, `B`, `C`, and `D` are matrices. Allow the compiler to automatically decide on pipelining, tiling, and parallel processing optimizations for each loop; use the appropriate pragmas for automatic decision. For the reduction loops over `k`, employ parallel reductions on the `tmp` and `D` arrays while also letting the compiler choose the parallelization factor. Use 'auto' placeholder within braces to denote automatic pragma value decisions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an optimized 2D matrix-multiply-and-add hardware accelerator function called 'kernel_2mm' which performs the operation `D := alpha*A*B*C + beta*D` for matrix sizes as follows: `A` is 40x70, `B` is 70x50, `C` is 50x80, `D` and `tmp` are 40x80 and 40x50 respectively. Utilize automatic pipelining, tiling, and parallelization optimization strategies for all loops. Within the innermost loops, perform parallel reduction on the `tmp` and `D` arrays, with automatic factor determination.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the 2mm algorithm into a hardware-accelerated kernel, where data dependencies allow, apply automatic loop pipelining, tiling, and parallelization pragmas to optimize the loops for performance, allowing the tool to determine the optimal factors for pipelining, tiling, and parallelism, and incorporate parallel reduction for the temporary and result matrices within the innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Enable hardware acceleration for the entire `kernel_2mm` function, allowing the tool to consider it for hardware synthesis. Optimize the performance of the loops through automatic pipelining, tiling, and parallelization where the specifics will be managed by the tool based on the platform's capabilities. Introduce reduction to combine elements during iterations for both `tmp` and `D` matrices with auto-determined parallelism factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel and optimize it using automatic loop optimization directives that decide whether to pipeline, tile, and parallelize each loop, including the appropriate use of reduction for accumulations within inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and apply suitable optimization pragmas to improve the performance of the matrix multiplication and addition operations involved. For the first pair of nested loops that compute the product of matrices 'A' and 'B', set the tool to automatically determine the best pipelining, tiling, and parallelization strategies using respective pragmas. In the innermost loop, enable a parallel reduction on 'tmp' with automatic determination of the parallel factor. Similarly, for the second pair of nested loops that update matrix 'D' with the product of 'tmp' and 'C', followed by scaling with 'beta', also automatically infer the optimal pipelining, tiling, and parallelization factors. Ensure that for the innermost loop, a parallel reduction is performed on 'D' with an automatic parallel factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 32\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function named 'kernel_2mm' that performs matrix multiplication and addition. Apply the following optimizations:\n\n1. Treat the entire function as a hardware kernel for acceleration.\n2. Use automatic pipeline optimization for the outer loops of matrix operations.\n3. Automatically tile the loops for partitioning data into smaller, more manageable chunks.\n4. Optimize the loops for parallel execution with automatic factor determination, and use reduction for accumulation in specific loops.\n5. For the first matrix operation, multiply alpha by elements of matrices A and B, and accumulate the results in a temporary array 'tmp'.\n6. In the second matrix operation, scale matrix D by beta and then add the result of the first matrix operation multiplied by matrix C.\n\nInclude respective #pragma directives with 'auto' placeholders to allow the HLS tool to automatically determine the appropriate pragma values: {__PIPE__L0}, {__TILE__L0}, {__PARA__L0}, etc., for pipeline, tile, and parallel factors at each loop level. Use reduction pragmas where appropriate for accumulation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given HLS task, please use the following comprehensive but concise natural language instruction to describe the necessary #pragma annotations:\n\n\"Define 'kernel_2mm' as a hardware acceleration kernel, and within it, enable automatic optimization for pipelining, tiling, and parallelizing each nested loop, while allowing the HLS tool to determine the specific factors for these optimizations. Additionally, employ automatic parallel reduction for the 'tmp' and 'D' arrays within the innermost loop iterations to enable concurrent accumulation operations, with the optimization factors being automatically determined as well.\" \n\nThis instruction implies the use of '#pragma ACCEL kernel' for the function and several 'auto' specified #pragma directives ('PIPELINE', 'TILE', and 'PARALLEL' with 'FACTOR=auto' and 'reduction') for loop optimizations. The placeholders '{__PIPE__Lx}', '{__TILE__Lx}', and '{__PARA__Lx}' should be replaced by the HLS tool with appropriate values automatically determined based on the specific loop optimization needs.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function that treats the function as a hardware kernel for acceleration, and is optimized for throughput with the flexibility for automatic adjustment of pipelining, tiling, and parallelism in the nested loops. Specifically, enable automatic configuration for pipeline optimization at each loop level to enhance loop execution overlap, automatic selection of tiling factor to control loop unrolling and data partitioning, and automatic adjustment of parallelism factor with reduction optimization for accumulator arrays 'tmp' and 'D' to promote concurrent execution of independent iterations, while ensuring correct accumulation of results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with specific pragma annotations, you can use the following comprehensive but concise natural language instruction:\n\n\"Generate a kernel function that performs a two-matrix multiplication (2MM) operation, optimizing it for hardware acceleration by automatically determining the best pipelining, data tiling, and parallelism levels for all loops. Declare the kernel function with appropriate data types and array dimensions, ensuring to define dimensions as constants where needed. The 2MM operation involves multiplying matrix A with matrix B and scaling it by alpha, then multiplying the result by matrix C, adding the product to a matrix D scaled by beta. Use reduction to accumulate the results in the output matrix. Enable hardware-level performance optimization for the loops by automatically setting pipeline, tile, and parallel factors. For all loops, let the toolset choose optimal factors, including the innermost loop which reduces into the temporary matrix and, later, into matrix D. Use comments or pragmas to mark the start and end of the computational region of interest.\"\n\nHere is what each pragma value denotes based on the instruction:\n- `#pragma ACCEL kernel`: Treat the following function as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto{__PIPE__Lx}`: Automatically decide the best pipelining strategy for loop level x to maximize throughput.\n- `#pragma ACCEL TILE FACTOR=auto{__TILE__Lx}`: Let the compiler automatically select the tiling factor for loop level x to enhance memory access patterns and data reuse.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__Lx}`: Enable the compiler to automatically adjust the parallelism factor for loop level x to exploit parallel execution capabilities.\n- `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{__PARA__Lx}`: Specify that the designated loop should be parallelized with an automatic parallel factor, and use reduction to safely accumulate values into a potentially shared variable such as 'tmp' or 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, then set the pipeline, tiling, and parallel pragma directives for each loop to be determined automatically by the compiler with the aim to enhance performance. For the two innermost loops, enable parallelism and reduction on the 'tmp' and 'D' arrays respectively, also set automatically by the compiler.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code using natural language instructions, use the following comprehensive but concise directive:\n\n\"Designate the `kernel_2mm` function as a hardware acceleration kernel. For the outer loops `i` in both the main computational blocks, allow the compiler to automatically determine if and how to pipeline and parallelize, and similarly decide on tiling strategies for improved data locality. Within the first nested loop `j`, automate pipeline, tile, and parallel strategies, and apply parallel reduction to the accumulation of `tmp` to enable concurrent computation of the sum. Duplicate this directive pattern for the second main computational block, substituting the accumulation of `D` instead of `tmp`. Permit the compiler to automatically select factors for pipelining, tiling, and parallelization for all loops, denoted by the `auto` specifier followed by a placeholder for compiler-assigned values (e.g., `{__PIPE__L0}`). Ensure the innermost loops `k` in both blocks are also automatically parallelized with a focus on reducing the specific array being accumulated in each.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following function into HLS code with specified pragmas for performance optimization: Define the function `kernel_2mm` as a hardware kernel for acceleration. This function performs a matrix multiplication and addition operation on matrices `A`, `B`, `C`, and `D` scaled by factors `alpha` and `beta` respectively, storing intermediate results in matrix `tmp`. For the outer two loops iterating over the dimensions of matrix `tmp` (`i` and `j` indices), enable automatic optimization for pipelining, tiling, and parallel processing. Similarly, optimize the next two loops iterating over the dimensions of matrix `D` (`i` and `j` indices) for pipelining, tiling, and parallel execution. For the innermost loops (`k` index) that accumulate values in `tmp` and `D`, enable parallel processing with automatic factor and specify that these loops perform a reduction operation on `tmp` and `D` respectively. Use automatic factor determination for all tiling and parallel processing directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel which computes the matrix operation D := alpha*A*B + beta*D for given matrix dimensions. Utilize automatic optimization for loop pipelining, tiling, and parallelization, and apply parallel reduction on the intermediate calculation results for arrays 'tmp' and 'D'. Implement these optimizations by auto-tuning the associated #pragma factors at the respective loop levels for maximizing throughput and dataflow performance in the hardware synthesis process.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Transform the provided 2mm kernel matrix operation function into a hardware-accelerated implementation by automatically optimizing loop pipelining, tiling, and parallelism for all nested loops, and by applying reductions for matrix summation operations.\" \n\nSpecifically, for every nested loop:\n\n1. Enable automatic loop pipelining to improve the initiation interval between loop iterations.\n2. Apply automatic loop tiling to partition loops into smaller chunks for better data locality and potential parallel execution.\n3. Allow automatic parallel factor determination to decide the number of loop iterations to be executed in parallel, taking into account potential concurrency and resource constraints. \n4. Use automatic parallel reductions on the variables `tmp` and `D` to efficiently aggregate the results of concurrent matrix computations.\n\nUse appropriate #pragma directives to indicate these optimizations for each nested loop level, and ensure the function is treated as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated 2mm kernel function that calculates the matrix expression `D := alpha*A*B*C + beta*D`. Declare the function as a hardware kernel and apply automatic optimization pragmas for pipelining, tiling, and parallel processing at each loop level, with specific pragma values not predefined but set to be determined automatically by the HLS tool. At the innermost loops, use the reduction pragma for parallel computation on the `tmp` and `D` arrays respectively. The outer two loops iterate over the first dimension of `tmp` and `D`, with the third and fourth loops iterating over their second dimensions, and the innermost loops calculate the matrix multiplication and addition with scalars `alpha` and `beta`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. Automatically optimize parallelism, pipelining, and tiling for the outer two loops (`i` loops) and their respective inner `j` loops, while also applying automatic optimization within the innermost `k` loops for both matrix multiplication sequences, with a focus on parallel reduction for the `tmp` and `D` matrices.\n\nThe specific #pragma instructions per loop level should unlock the respective performance attributes of the HLS tool's automatic optimization while keeping the structure of the loops largely intact for the compiler to analyze. Use reduction pragmas on the innermost loops to emphasize accumulation into the targeted matrix (`tmp` or `D`).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as one for hardware acceleration, with automatic optimization for pipelining, tiling, and parallel processing at each loop level. Utilize dynamic optimization factors determined at runtime for each optimization technique, and perform reduction operations to accumulate results in shared data structures within innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Automate the optimization process for the acceleration of a 2D matrix multiplication kernel that updates matrix 'D' with the formula `D = alpha * A * B * C + beta * D` using loop tiling, pipelining, and parallel processing with auto-tuned factors. Additionally, apply reduction to accumulate values in 'tmp' and 'D' within their respective nested loops. Treat the entire function as a hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the embedded function as a hardware acceleration kernel and optimize the loops within it for parallel processing using intelligent automatic factor selection. Specifically, enable automatic pipelining, tiling, and parallelization for each loop with optimal factors determined by the compiler, along with the appropriate reduction operations to accumulate matrix multiplication results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel treating the function as a hardware acceleration kernel, and automatically optimizing loops for pipelining, tiling, and parallelization with compiler-determined factors, while performing a reduction for the temporary array computation and the final matrix computation.\n\nFor the nested loops in the sequence:\n\n1. Pipeline the outer two loops with automatic factor determination, allowing for potentially multiple iterations to be executed simultaneously at different stages of completion.\n2. Tile the outer two loops with automatic factor determination, partitioning them into smaller computational blocks that can potentially be executed more efficiently.\n3. Parallelize the outer two loops with automatic factor determination, enabling simultaneous execution of independent loop iterations.\n\nInside each of the outer loops:\n\n1. For the loop that accumulates into tmp, apply parallelization with automatic factor determination, and specify reduction on the tmp array to ensure correct summation when iterations are executed concurrently.\n2. For the loop that accumulates into D, apply parallelization with automatic factor determination, and specify reduction on the D array to ensure correct summation during concurrent execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 32\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS C code for matrix-matrix multiplication and addition with pipelining and parallelization enabled at appropriate levels, and automatic tiling based on a heuristic. Specifically:\n\n1. Define a function named 'kernel_2mm' to accelerate matrix operations on hardware, taking as arguments the dimensions 'ni', 'nj', 'nk', 'nl', and matrices 'A', 'B', 'C', 'D', as well as scalars 'alpha' and 'beta'. The matrices 'tmp', 'A', 'B', 'C', and 'D' are two-dimensional arrays with predetermined sizes.\n\n2. Declare loop variables 'i', 'j', and 'k'.\n\n3. Implement matrix multiplication and addition according to the equation `D := alpha*A*B*C + beta*D`. \n\n4. For each loop that iterates over 'i', 'j', and 'k', apply the following pragmas:\n   - Enable pipelining with automatic determination of initiation intervals.\n   - Enable loop tiling with factors automatically determined by the HLS tool.\n   - Enable loop parallelization also with factors automatically determined, including specifying a reduction pragma for the parallelized nested 'k' loops for correct accumulation in 'tmp' and 'D'.\n\nUse auto directive values preceded by double underscores (e.g., `auto{__PIPE__L0}`) for the HLS tool to fill in the optimization parameters during compilation. Also, ensure that the first loop pair computes the intermediate matrix 'tmp' and the second loop pair updates matrix 'D' with the final results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes presented, use the following concise instruction outlining the processing of the code along with the specific `#pragma` directives:\n\n\"Define a function as a hardware acceleration kernel that performs a two-matrix multiplication operation; the first result is then used in addition to a scaled second input matrix to update that second matrix. Enable the compiler to automatically optimize the loop pipelining, tiling, and parallel processing, with specific attention to parallel reduction for summation within the innermost loops. All pragmas should be set to allow the compiler to automatically determine the best possible values.\" \n\nHere are the specific #pragma directives to describe and their values per loop:\n\n1. `#pragma ACCEL kernel` - This instructs the compiler to treat `kernel_2mm` function as a hardware acceleration kernel.\n2. `#pragma ACCEL PIPELINE auto{__PIPE__L0}` and other similar pipeline pragmas - These direct the compiler to optimize loop pipelining at various loop levels automatically.\n3. `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}` and other similar tile pragmas - These instruct the compiler to automatically select the tiling factors for loops to break them into smaller, more manageable chunks for better parallel processing.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` and other parallel pragmas - These commands tell the compiler to automatically determine the parallel processing factors, which dictate how many iterations of a loop can be executed in parallel.\n5. `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}` - These specify parallel reduction for the `tmp` and `D` arrays, respectively, allowing the compiler to optimize the summation within the inner loops by determining the best approach to parallelize the reduction automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 8\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the function `kernel_2mm` for hardware acceleration by automatically determining the best pipeline settings, loop tiling, and parallelization for each loop nest, while ensuring that the intermediate `tmp` array and the `D` array reductions are optimized for parallel computation with automatic factor selection. Additionally, declare the function itself as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS C code for a 2mm kernel computing `D := alpha * A * B * C + beta * D` with optimization pragmas by using the following instructions:\n\n- Mark the function as a hardware acceleration kernel.\n- Enable automatic optimization for loop pipelining, tiling, and parallelization based on the hardware resources available while respecting the dependency rules, with all optimization factors set to automatically determine the best values by the compiler.\n- In the first loop nest, perform a reduction on the `tmp` array with automatic parallel factor, allowing concurrent computations when accumulating the results.\n- In the second loop nest, perform a reduction on the `D` array with an automatic parallel factor, allowing concurrent updates to the `D` array.\n- Use automatic pipelining, tiling, and parallelization for all loops, as provided by the compiler.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the function 'kernel_2mm' into a hardware kernel for acceleration. Apply automatic optimization for pipelining, tiling, and parallelism at each loop level, and enable automatic loop unrolling with parallel reduction for the innermost loops to accumulate the results for 'tmp' and 'D'. Assign pragma placeholder tokens to allow for automatic factor determination by the HLS tool during compilation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For all loops, enable pipeline optimization, automatic tiling, and parallel processing with pragmas set to automatically determine the most efficient values. Upon each loop nest, specify parallel reduction for arrays 'tmp' and 'D' to accumulate results across iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 16\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a candidate for hardware acceleration and dynamically optimize its loops for parallelism, pipelining, and loop tiling based on the high-level synthesis tool's automatic analysis.\n\n- Initialize the hardware acceleration for the 'kernel_2mm' function.\n- Automatically apply pipelining to the outer loops labeled '__PIPE__L0' and '__PIPE__L1' to improve loop iteration latency.\n- Enable the HLS tool to automatically determine the tiling factors for the loops tagged '__TILE__L0', '__TILE__L1', '__TILE__L2', and '__TILE__L3' for better data locality and reduced memory bandwidth.\n- Configure the outer loops '__PARA__L0', '__PARA__L1', '__PARA__L2', and '__PARA__L3' for automatic parallel execution, allowing concurrent iterations where possible.\n- Within the first inner loop where 'tmp' is calculated, use automatic parallel reduction on 'tmp' with pragma '__PARA__L4' to aggregate values across iterations efficiently.\n- Similarly, within the second inner loop where 'D' is updated, apply automatic parallel reduction on 'D' with pragma '__PARA__L5' to accumulate the results concurrently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Design a hardware-accelerated kernel function for performing a two-matrix multiplication (2mm) operation with double precision floating point numbers. Use automatic optimization decisions for pipelining, tiling, and parallelization of the nested loops while ensuring that data dependencies are handled correctly through appropriate reductions. Apply loop transformations and memory optimizations to exploit parallelism, and enable effective pipelining and tiling based on the context. Specifically, use automatic hardware optimization settings for each nested level in the primary loop nests corresponding to the computation of matrix products and their accumulations, as well as the scaling and addition operations on the resultant matrix.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a target for hardware acceleration, automatically optimize the entire body of the function, and all embedded loops using available hardware optimization directives with implementation-specific factors that are determined during compilation:\n\nUse the '#pragma ACCEL kernel' directive to mark the function 'kernel_2mm' as the hardware acceleration target.\n\nInside the function 'kernel_2mm', optimize the loop that initializes 'tmp' array: automatically manage how deeply this loop is pipelined, enable tiling to split the loop into blocks, setting both the pipelining and tiling factors to be chosen by the compiler.\n\nAutomatically choose the parallelization factor to run the iterations concurrently within the third loop (variable 'k') in the initialization of 'tmp', and specify that values being written into 'tmp' should undergo a reduction operation to properly accumulate the results in parallel.\n\nFor the loop that updates 'D', repeat the loop optimization directives as for the 'tmp' initialization loop: automatically pipeline the loop, tile it, and set the respective factors to be chosen by the compiler.\n\nIn the innermost loop that updates 'D', similar to the 'tmp' initialization, automatically determine the parallelization factor, and indicate a reduction operation for 'D' to handle accumulating results in parallel.\n\nTo reflect these instructions, the corresponding HLS pragmas with 'auto' placeholders need to be inserted where the factors will be automatically determined by the HLS tool during compilation:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni, int nj, int nk, int nl, double alpha, double beta, double tmp[40][50], double A[40][70], double B[70][50], double C[50][80], double D[40][80]) {\n  ...\n  // Automated pipelining, tiling and parallelizing for the loops with 'auto' factors\n  ...\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function named `kernel_2mm` to perform a modified matrix multiplication and addition with scalars. Automate the pipelining, tiling, and parallelization optimization strategies for the loops within the function. When optimizing, automatically determine the pipeline stages, tile sizes, and the number of loop iterations to run in parallel for each loop to maximize performance, while also applying reduction to accumulate values for `tmp` and `D` matrices where appropriate. Use all available pragma directives to optimize the outer and inner loops of the double-nested loop constructs.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function that is designated for hardware acceleration. The function will perform a matrix multiplication followed by addition, scaling two matrices D and tmp with factors beta and alpha, respectively. The temporary matrix tmp is calculated by multiplying matrix A with matrix B, and then the result of this operation is multiplied by matrix C and added to the scaled matrix D. Implement optimizations as follows:\n\n1. Set the 2mm kernel function as a target for hardware acceleration.\n2. Automatically optimize loop pipelining at the top levels of the two primary for-loop nests.\n3. Let the compiler decide the optimal tiling factors for all loops.\n4. Enable automatic parallelization for all loops with the compiler determining the appropriate factor, except for reductions within the innermost loops, which should be parallelized with reduction clauses on the tmp and D matrices.\n5. The loop bounds should be specified statically with 40 for 'i' loops, 50 for the second-tier 'j' loops in the first nest and 80 for the 'j' loops in the second nest, and 70 for the 'k' loop in the first nest and 50 for the 'k' loop in the second nest.\n\nRemember to encapsulate the compute region with appropriate pragmas to indicate the beginning and end of the region that should be considered for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use automatic optimization for the function treating it as a hardware kernel, automatically determining the parallelism, pipelining, and tiling parameters for each loop level. Specifically, for the operation `D := alpha*A*B*C + beta*D`, enable automatic pipelining, tiling, and parallel processing for the outer loops iterating over matrix dimensions. For inner loops, ensure that the temporary reduction variable `tmp` or `D` can be written simultaneously by multiple iterations, using automatic factor determination for parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the provided C function `kernel_2mm` into a hardware accelerated kernel by marking it for hardware execution. Apply automatic loop pipelining, tiling, and parallelization to all loops, allowing the compiler to choose the best factors for optimization. Apply parallel reduction for the intermediate arrays `tmp` and `D` to allow for concurrent summation within respective loop nests.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, and optimize nested loops for parallel execution and pipelining, with the degree of parallelization, pipelining, and tiling determined automatically by the compiler for each loop, and apply reduction to accumulate results in an array within a loop.\n\nSpecific instructions for each pragma:\n\n1. Treat the 'kernel_2mm' function as one that will be accelerated in hardware using the corresponding pragma.\n\n2. For the outer loop 'i' iterating over the range [0,39], enable automatic pipeline optimization, automatic loop tiling, and automatic parallelism optimizations.\n\n3. Within the first level of nested loop 'j' iterating over the range [0,49], also enable automatic pipeline optimization, automatic loop tiling, and automatic parallelism with loop-independent reductions on the 'tmp' array.\n\n4. For the second outer loop 'i', once again enable automatic pipeline optimization, automatic loop tiling, and automatic parallelism optimizations.\n\n5. Inside the second level of nested loop 'j' iterating over the range [0,79], enable automatic pipeline optimization, automatic loop tiling, and automatic parallelism with loop-independent reductions on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the `kernel_2mm` function as a hardware kernel, where you should automatically manage the pipelining, tiling, and parallelization of the loops to optimize performance. Specifically, for the outer loops `i` and inner loops `j`, enable the compiler to decide the pipelining strategy, tiling factor, and parallelization factor. Additionally, for the innermost loops `k` within both the computation of `tmp` and `D`, perform a parallel reduction on `tmp` and `D` with an automatically determined factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 80\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a high-level synthesis (HLS) hardware kernel, and optimize all loops using automatic pipelining, tiling, and parallelization pragmas with compiler-determined optimal factors. Additionally, enable parallel reduction for the innermost loops to accumulate results efficiently on the 'tmp' and 'D' arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS C code provided, use the instruction: \"Create a hardware kernel function for 2mm (two matrix multiplications) algorithm with all loops automatically optimized for parallelization, pipelining, and tiling as determined by the compiler, including appropriate reductions for the accumulation statements in the innermost loops.\"\n\nSpecific pragma value descriptions:\n\n- Treat the 'kernel_2mm' function as a hardware kernel to be accelerated.\n- For all loops, allow the compiler to automatically decide the degree of pipelining, tiling, and parallelization.\n- For the innermost accumulation loops where 'tmp[i][j]' and 'D[i][j]' are being updated, apply a parallel reduction pragma and allow the compiler to automatically choose the factor.\n\nIn the provided code, `{__PIPE__Lx}`, `{__TILE__Lx}`, and `{__PARA__Lx}` are placeholders indicating that the specific values for pipeline, tile, and parallel factors are to be determined by the compiler tool, and the 'x' in 'Lx' corresponds to different loop levels.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and for each of the nested loops within this function, automatically optimize the loop pipelining, tiling, and parallel processing. Specifically, for the reductions that compile temp and D matrices, add parallelization with automatic factor decision while accumulating results to prevent resource conflicts.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to optimize a two-matrix multiplication kernel using automatic pipelining, tiling, and parallelization for loops where appropriate, with the aim of accelerating the computation on hardware. Specifically, declare the function 'kernel_2mm' as a hardware kernel for acceleration. Inside the 'kernel_2mm', enable automatic hardware pipeline optimization for the outer matrix multiplication loops and apply automatic data partitioning for the inner loops with parallel processing, including reduction operations, leaving the tool to decide the optimal factor for each optimization pragmas based on the surrounding code context and hardware constraints.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, automatically optimize the top-level loops for pipelining, tiling, and parallelism, and within the nested loops, enable parallel reduction on the output matrices `tmp` and `D` while auto-tuning the pipelining, tiling, and parallel factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 16\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel by treating the function as a hardware acceleration kernel, enabling automatic optimization for pipelining, tiling, and parallelization where appropriate. Use distinct pragma settings for different loop nests to explore optimal parallelism and tiling factors, and use parallel reduction for accumulations within the innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as suitable for hardware acceleration. Within this function, use tuning pragmas to optimize the first loop nest by automatically determining the best settings for pipelining, tiling, and parallelism. Inside the innermost loop, employ a parallel reduction on the 'tmp' array. Apply the same optimization strategy to the second loop nest but this time with a parallel reduction on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 16\n__PARA__L4: 70\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, optimize the loops with automatic pipelining, tiling, and parallelization strategies to increase throughput and ensure data dependencies are preserved with reduction pragmas in the nested loops that accumulate into 'tmp' and 'D' arrays. Adjust the optimization level automatically for each loop with respective unique identifiers.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function performing a two-matrix multiply and add operation, where both the matrices and the intermediate/temporary arrays have fixed dimensions. Use automatic optimization directives to enhance performance without manual configuration of pipeline, tile, and parallelization factors. Apply pipeline optimization to the outer loops and their immediate inner loops to allow concurrent execution of iterations where possible. Inside the multiplication loops, enable parallel computation with an automatic factor and specify reduction on the partial result arrays to ensure correct accumulation of results. Define the entire function to be synthesized as a dedicated hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and apply automatic optimization directives for pipelining, tiling, and parallelization to the loops within the function. Specifically, enable automatic pipelining to enhance loop execution throughput, set tile size automatically for loop tiling to divide the loops into smaller, more manageable blocks for efficient memory usage, and determine parallelization factors automatically to exploit concurrent execution where possible. Additionally, include reduction clauses for arrays `tmp` and `D` to perform parallel reduction operations with automatic factor determination, ensuring that shared resources are updated correctly in concurrent environments.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS accelerated kernel function named `kernel_2mm` for matrix operations with automatic optimization directives. It should perform the computation `D := alpha*A*B*C + beta*D` on matrices, with specified dimensions using nested loops. Apply automatic pipelining, tiling, and parallelism optimization with the compiler's discretion to all outer loops and their immediate inner loops. Apply automatic parallel reduction to the innermost loops responsible for partial sum updates of `tmp` and `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following 2mm kernel function into a hardware-accelerated component, optimizing each loop with automatic pipeline, tiling, and parallelization factors, and enable parallel reductions for the innermost loops that accumulate into 'tmp' and 'D' matrices.\n\n1. Mark the function as a hardware acceleration kernel.\n2. For each loop nest starting with 'for (i = 0; i < 40; i++)', optimize as follows:\n   a. Automatically determine and apply the most efficient pipelining.\n   b. Automatically determine and apply the most efficient tiling factor.\n   c. Automatically determine and apply the most efficient parallelism factor.\n3. For the innermost loop that contains 'tmp[i][j] += alpha * A[i][k] * B[k][j];', enable parallel reduction on 'tmp' with an automatically determined factor.\n4. For the innermost loop that contains 'D[i][j] += tmp[i][k] * C[k][j];', enable parallel reduction on 'D' with an automatically determined factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 2\n__PARA__L4: 10\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function with automatic optimizations enabling pipelining, tiling, and parallelization as follows:\n\n- Designate the function `kernel_2mm` as a hardware kernel for acceleration using the appropriate pragma.\n- Enable automatic pipelining for the outer two for-loops, which iterate over the matrices' dimensions, allowing the HLS tool to determine the optimal pipeline stages based on the input size and available resources.\n- Allow the HLS tool to automatically deduce tiling factors for the outer loops with an auto placeholder, which is meant to be replaced by specific values calculated by the HLS tool during the process of optimization.\n- Set up parallel processing for all the loops with an automatic factor decided by the HLS tool, and specify automatic parallel reduction for the innermost loops performing the matrix multiplications, aiming to reduce the summation into the `tmp` and `D` arrays efficiently.\n- Utilize placeholders within the auto directives (like `__PIPE__L0`) to indicate where the tool should replace with calculated values for pipelining, tiling, and parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 32\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to implement a hardware-accelerated 2mm kernel function which multiplies a matrix `A` with two other matrices `B` and `C`, adds the result to a scaled matrix `D`, and stores the result back in `D`. Utilize automatic loop pipelining, tiling, and parallelization to optimize for throughput, with specific optimization factors determined by the HLS tool. The outer loops iterating over matrix dimensions should be pipelined and parallelized to improve performance, while the innermost loops responsible for the actual computation should employ parallel reduction to accumulate results efficiently. Implement reduction to handle the parallel accumulation within the loops that calculate matrix products. Make sure the top-level function is designated as the hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a dedicated hardware acceleration kernel, and optimize the execution of all nested loops by automatically determining the optimal pipeline, tiling, and parallelization strategies, while specifically enabling parallel accumulation for partial results within the innermost loops.\n\nHere is a breakdown of the #pragma directives and their corresponding verbal instructions:\n\n- Treat the following function, 'kernel_2mm', as a hardware acceleration kernel with the instruction '#pragma ACCEL kernel'.\n- For each outer loop labeled with L0 and L1, allow the compiler to automatically decide the best pipelining, tiling, and parallelization strategies, with the instructions:\n  - '#pragma ACCEL PIPELINE auto' \n  - '#pragma ACCEL TILE FACTOR=auto' \n  - '#pragma ACCEL PARALLEL FACTOR=auto'\n- Within the inner loops labeled with L2 and L3, again allow the compiler to automatically decide the optimal configurations for pipelining, tiling, and parallelization.\n- Specifically for the innermost loops labeled L4 and L5, enable parallel reduction on the 'tmp' and 'D' arrays to accumulate the intermediate multiplication results, directing the compiler to determine the best parallel factor automatically:\n  - '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto'\n  - '#pragma ACCEL PARALLEL reduction = D FACTOR=auto'\n\nThis combination of #pragma directives will prompt the HLS tool to automatically infer the best possible hardware implementation for the given function, managing the trade-off between parallelism and resource usage to maximize performance and efficiency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code that designates a function `kernel_2mm` as a hardware acceleration kernel to perform a 2mm computation on matrices, with dimensions provided by the parameters `ni`, `nj`, `nk`, and `nl`. The function computes the matrix expression `D := alpha*A*B*C + beta*D` with input matrices `A`, `B`, `C`, `D`, and scalars `alpha` and `beta`. Optimizations should automatically determine pipeline, tiling, and parallelism factors. Specifically, apply the following design directives:\n\n1. Use automatic pipelining for the first and second loop nests, surrounding the assignments of `tmp` and `D` matrices, respectively. Indicate these with placeholders `{__PIPE__L0}` and `{__PIPE__L1}`.\n2. Automatically select tiling factors for the aforementioned loop nests, using placeholders `{__TILE__L0}` and `{__TILE__L1}`.\n3. Choose automatic parallelism factors for the same loops, with placeholders `{__PARA__L0}` and `{__PARA__L1}`.\n4. Within the loop that computes `tmp[i][j]`, apply automatic pipelining, tiling, and parallelism with reduction on the `tmp` array using `{__PIPE__L2}`, `{__TILE__L2}`, and `{__PARA__L4}` placeholders.\n5. Within the loop that updates `D[i][j]`, similarly apply automatic pipelining, tiling, and parallelism with a reduction on the `D` array using placeholders `{__PIPE__L3}`, `{__TILE__L3}`, and `{__PARA__L5}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 80\n__PARA__L4: 14\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be accelerated as a hardware kernel, employing automatic pipelining, tiling, and parallel execution strategies for optimal performance across all loops, with loop-specific pragmas for these optimizations. Allow automatic factor tuning for all optimization pragmas and parallel reductions for accumulations within the inner loops to enhance computation efficiency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 10\n__PARA__L4: 16\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and in this function, optimize the nested loops for efficient hardware synthesis by adjusting pipelining, tiling, and parallelizing automatically, also enabling parallel reductions on the specific innermost loops where sums are accumulated in the `tmp` and `D` arrays. Use automatic deduction of the optimum pragma values for each loop where `factor=auto` is specified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the function 'kernel_2mm' which performs a two-matrix multiplication followed by a matrix addition, with dimensions ni x nj x nk x nl and scaling factors alpha and beta, by treating it as a hardware kernel. Use automatic optimization for pipelining and parallelizing the top-level loops to improve performance without user-specified values. Additionally, automate tiling to partition the data and compute for performance gains. Apply parallel reduction to the innermost loops to enable concurrent accumulation into the arrays 'tmp' and 'D' respectively, but let the HLS tool determine the optimal parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 14\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Allow the compiler to optimize the loop pipelining, tiling, and parallel processing automatically for each nested loop, ensuring efficient FPGA resource use and performance. For reduction operations on the 'tmp' and 'D' arrays within the innermost loops, enable the compiler to automatically determine the most effective parallel factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 14\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS code for a 2mm kernel function that accelerates the computation of the mathematical operation `D := alpha * A * B * C + beta * D`, where `D`, `A`, `B`, `C` are matrices, and `alpha`, `beta` are scalar values. Apply automatic pipeline, tiling, and parallelization optimizations at each level of nested loops, with specific factors to be determined by the HLS tool. Additionally, enable parallel reduction on temporary variables within the innermost loops to efficiently accumulate the results. Define the top-level function as a hardware kernel for acceleration purposes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated 2mm kernel function with automated optimization directives for pipelining, tiling, and parallelism where appropriate to process matrix operations D = alpha * A * B * C + beta * D. Use automatic pipelining for loops to boost data throughput, automatically calculate optimal tiling factors to split loops for spatial computation efficiency while fitting memory constraints, and apply automatic parallel factors for concurrent loop iteration execution \u2014 with parallel factor and reduction pragmas for associated accumulation variables to harness data-level parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Turn the function `kernel_2mm` into a hardware acceleration kernel to perform matrix multiplication and addition with alpha and beta scalars scaling. Automate the pipeline, tile and parallel optimizations for all loops, and for the deepest loop, where the actual multiplication and reductions occur, use parallel reduction on the `tmp` array in the first block and on the `D` array in the second block, letting the compiler automatically choose the optimization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel that computes the matrix operation `D := alpha * A * B * C + beta * D`. Enable automated pipelining, tiling, and parallelizing optimization strategies for all loops, with specific optimization factors to be determined by the compiler. For the accumulation loops within both the 'tmp' and 'D' matrix calculations, apply parallel reduction to enable concurrent summation, with the compiler determining the parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, applying loop pipelining, tiling, and parallelism optimizations automatically where appropriate. Specifically, for the outermost loops 'i', enable dynamic optimization for pipelining, tiling, and parallel execution. For the nested 'j' loops, also enable automatic pipelining, tiling, and parallelism. In the innermost 'k' loop within the first double-nested loop construct, introduce a parallel reduction on the 'tmp' array, with automatic optimization of the parallel factor. Apply the same concepts to the second double-nested loop construct, but with the parallel reduction applied to the 'D' array in the innermost 'k' loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 10\n__PARA__L4: 16\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate High-Level Synthesis (HLS) code to perform the 2mm function, which involves two matrix-matrix multiplication operations. Use pragmas to optimize the design for FPGA acceleration:\n\n1. Declare the function as a hardware kernel.\n2. Enable automatic pipelining for the first loop over the 'i' index, allowing iterations to be potentially carried out in an overlapping fashion for increased throughput.\n3. Automate the tiling optimization for the first loop, which partitions the iteration space into smaller blocks to facilitate better usage of local memory resources.\n4. Allow automatic parallelization for the first loop, letting the compiler identify opportunities to execute multiple iterations simultaneously if possible.\n5. Again, automatically handle pipelining, tiling, and parallelization for the second loop over the 'j' index for the first part of the 2mm computation, which calculates the product of matrices A and B and scales it by alpha.\n6. Use parallel reduction pragma with automatic factor setting for the inner loop over 'k' when accumulating into the temporary matrix 'tmp', to allow concurrent reduction operations on parallel threads.\n7. Similarly, configure automatic pipelining, tiling, and parallelization for the third loop over 'i' for the second part of the 2mm computation.\n8. Repeat the automatic optimization pragmas for the fourth loop over 'j', focusing on scaling matrix D by beta and accumulating the product of the temporary matrix 'tmp' and matrix C into matrix D.\n9. Use parallel reduction pragma with automatic factor setting for the inner loop over 'k' when accumulating into matrix D, to allow concurrent reduction operations on parallel threads.\n\nThe specified HLS pragmas will enable concurrency at various levels of the computation, potentially improving the performance on FPGA hardware by optimizing the usage of its parallel computation resources and memory hierarchy.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, apply automatic pipelining, tiling, and parallel processing where appropriate within the two nested loop structures, and specify reduction operations for output arrays 'tmp' and 'D' in their respective innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 14\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm computation kernel where the first task is to multiply matrix A with matrix B and then with matrix C, scaling by alpha, and the second task is to scale matrix D by beta and add the result of the first task to it. Apply automatic pipeline optimization for all loops, allow the compiler to decide the tiling and parallel processing factors, and ensure parallel reduction for intermediate accumulation variables 'tmp' and 'D'. Use 'auto' for pragma values where the compiler should decide the best parameter. Treat the code within the function as a hardware kernel suitable for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 10\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Consider the following natural language instruction to generate the HLS C code provided:\n\n\"Define a hardware-accelerated kernel function to perform a two-dimensional matrix-multiply-accumulate operation, `D = alpha*A*B + beta*D`. In this function, named `kernel_2mm`, automatically optimize the loops for parallel execution, pipelining, and tiling, with specific values determined by the compiler. For each loop, apply loop tiling to break the computation into smaller, more manageable blocks, enable pipeline optimization to allow for loop iterations to be processed in an overlapped fashion, and use parallel processing to allow for simultaneous execution of independent iterations, with the degree of parallelism and the exact tiling factors to be automatically tuned. Additionally, enable parallel reduction on the temporary matrix `tmp` and the output matrix `D` to ensure efficient accumulation of results during the matrix multiplication stages.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following High-Level Synthesis (HLS) directives for a 2mm kernel into natural language instructions:\n\n1. Declare a hardware acceleration function for the matrix multiplication and addition of 2D arrays.\n2. Automate the pipelining, tile factor, and parallelization for the outer loops processing the first matrix multiplication (alpha * A * B), but do not specify fixed values\u2014let the compiler decide the optimization levels.\n3. Within the innermost loop that computes the matrix multiplication result, apply parallel reduction to the temporary array with an unspecified factor, allowing the compiler to determine parallelism.\n4. Similarly, automate pipelining, tile factor, and parallelization for the outer loops processing the second matrix multiplication (tmp * C) and addition with the scaled array (beta * D).\n5. In the innermost loop of the second operation, use parallel reduction on the array 'D' with the compiler determining the parallel factor.\n\nPlease note that for all the pragmas where 'auto' is specified with curly braces placeholders (e.g., {__PIPE__L0}), you should leave them as 'auto' for the compiler to automatically determine the best values for pipeline stages, tiling factors, and parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 80"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function named kernel_2mm that accelerates a matrix multiplication and addition operation, with parameters `ni`, `nj`, `nk`, `nl`, non-local double arrays `tmp`, `A`, `B`, `C`, `D`, and two double scalar values `alpha` and `beta`. Automatically determine the pipelining, tiling, and parallelism strategies for all loops. For the two outermost loops in each of the two main block operations, allow the compiler to optimize them by automatically setting pipelining, tiling, and parallelism factors. For the innermost loop of each block, use parallelization with automatic factor determination to perform a reduction operation on the `tmp` array in the first block and on the `D` array in the second block.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be synthesized as a hardware acceleration kernel, and apply automatic optimization strategies for pipelining, tiling, and parallelizing each nested loop within the function to improve performance. This involves automatic determination of the pipelining of loop's iterations, the selection of loop tiling factor for partitioning the loops into smaller, more manageable blocks, and the setting of parallel processing factors to control the number of loop iterations to be processed concurrently, with additional instructions to perform reductions on 'tmp' and 'D' arrays where parallel accumulations occur.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 4\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the provided 2mm algorithm into a high-level synthesis (HLS) accelerated kernel with configurable optimization directives for pipelining, tiling, and parallel processing. Apply these directives to the outer and inner loops to enable the compiler to determine the best factor for each optimization strategy automatically. Employ pipeline optimization to allow consecutive loop iterations to overlap in execution. Utilize tiling to partition loops into sub-loops for improved data locality and potential parallel execution. Implement parallel loop processing with automatic factor selection to enhance concurrent execution of iterations where possible. Additionally, ensure that data dependencies are respected by employing reduction clauses for the accumulation variables 'tmp' and 'D' in the corresponding innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 80\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Enable automatic decision on the pipeline, tiling, and parallelization factors for the loop nest optimizing the matrix computation D = alpha*A*B + beta*D with matrices A (40x70), B (70x50), C (50x80), and D (40x80). Specifically, for the outer two loops of each major block, allow the compiler to automatically control the pipelining, tiling, and parallelism. For the innermost loops, employ automatic parallelization with a reduction clause on the 'tmp' and 'D' matrices respectively to ensure proper accumulation of values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a matrix multiplication and addition function called `kernel_2mm` with the following instructions:\n\n1. Accelerate the entire function `kernel_2mm` by declaring it as a hardware kernel.\n2. Automatically optimize the pipeline for the top-level loops iterating over the first two dimensions of the matrices by enabling pipeline optimization with an unspecified factor (`auto`).\n3. For these same top-level loops, also enable automatic loop tiling optimization and automatic parallelism with unspecified factors (`auto`) for each loop, letting the compiler decide the best strategy.\n4. Within the innermost loop of the first matrix multiplication, enable automatic parallel reduction on `tmp` array with an unspecified factor (`auto`) to allow concurrent accumulation of the product results.\n5. Apply the same automatic pipeline, tiling, and parallel optimizations to the second top-level loop responsible for multiplying the temporary matrix `tmp` with the matrix `C` and adding it to the scaled matrix `D`, with an unspecified factor (`auto`).\n6. Similar to step 4, enable automatic parallel reduction for the `D` array within the innermost loop of the second operation with an unspecified factor (`auto`).\n\nNote: Replace `auto` with the appropriate `__PIPE__L*`, `__TILE__L*`, and `__PARA__L*` placeholders respectively for each loop level, where * corresponds to the loop level identifier.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: flatten\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Automatically optimize the loop execution, data tiling, and parallelism in the given function 'kernel_2mm', which performs matrix multiplication and addition (D := alpha * A * B * C + beta * D), while considering the function as a hardware acceleration kernel and enabling reduction for the specified loop variables 'tmp' and 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Automatically optimize the pipelining, tiling, and parallelism of loops using pragma directives with keywords set to 'auto', enabling the compiler to determine the optimal values. Specifically, apply pipeline optimization to the loops iterating over 'i' and 'j' for both the computation of 'tmp' and the update of 'D'; use tiling and parallel pragma directives on the same loops to enhance data locality and parallel execution. Moreover, enable parallel reduction for the 'k' loop within both the computation of 'tmp' and the update of 'D' to efficiently aggregate results across loop iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 5\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function to perform a 2mm matrix-multiplication algorithm, with optimizations for pipelining, tiling, and parallel execution automatically determined by the HLS tool on the loops where matrix computation occurs. Specifically, allow the outermost two loops of each matrix computation block to be pipelined and tiled, and parallelize the innermost loop with automatic factor determination, applying a reduction pattern on the temporary and resulting matrices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 7\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the provided 2mm kernel function using HLS pragmas to optimize each nested loop with automatic factor determination for pipelining, tiling, and parallel processing, while performing reduction on the 'tmp' and 'D' arrays within the innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 8\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the HLS code provided, the comprehensive yet very concise instruction in natural language would be:\n\n\"Define a hardware-accelerated kernel function named `kernel_2mm` for a 2mm (two matrix multiply) operation with alpha scaling on A*B and beta scaling on D. For each stage and loop within the function, delegate optimization decisions for pipelining, tiling, and parallel processing to the compiler's heuristics, and perform parallel reduction on partial loop results to accumulate into the `tmp` and `D` matrices.\"\n\nSpecific #pragma instructions and corresponding natural language directives would be:\n\n- `#pragma ACCEL kernel`: Indicate that the function `kernel_2mm` is intended to be synthesized into dedicated hardware (a kernel) for acceleration.\n- `#pragma ACCEL PIPELINE auto`: Enable automatic loop pipelining to allow concurrent execution of loop iterations when beneficial.\n- `#pragma ACCEL TILE FACTOR=auto`: Delegate decision for tiling the loop to the HLS tool to divide loops into sub-loops (tiles) for better parallelization or to fit in on-chip memory.\n- `#pragma ACCEL PARALLEL FACTOR=auto`: Enable automatic parallelization of loops by determining an appropriate factor to replicate blocks of loop iterations.\n- `#pragma ACCEL PARALLEL reduction = variable`: Specify that a parallel reduction operation should be used to combine values computed in parallel into the specified `variable` (either `tmp` or `D`).\n\nWhere \"{__PIPE__L0}\", \"{__TILE__L0}\", \"{__PARA__L0}\", \"{__PIPE__L2}\", etc., are placeholders that would be replaced with specific values generated by the compiler's optimization passes. The `auto` keyword indicates that the compiler automatically selects the appropriate value.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be compiled as a hardware accelerator, where in the first operation, a matrix 'tmp' is computed as alpha times the matrix multiplication of 'A' and 'B', and in the second operation, the matrix 'D' is updated as the product of 'D' scaled by beta and then incremented by the matrix multiplication of 'tmp' and 'C'. Signify to the HLS tool to decide the optimal loop unrolling, tiling, and pipelining strategies for each loop to enhance performance, and enable parallel reduction for each innermost loop to accumulate the partial results efficiently into 'tmp' and 'D'. Use automatic factor assignment for all #pragmas driving parallelism, tiling, and pipelining. \n\nHere are the corresponding #pragma instructions that need to be applied:\n\n- Treat 'kernel_2mm' as a hardware kernel function: `#pragma ACCEL kernel`\n- For the top-level loops (`i` loops) over the 'tmp' and 'D' matrices, enable automatic loop pipelining, tiling, and parallelization: \n  - `#pragma ACCEL PIPELINE auto{__PIPE__L0}` for the first 'i' loop over 'tmp', and `#pragma ACCEL PIPELINE auto{__PIPE__L1}` for the second 'i' loop over 'D'.\n  - `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}` for the first 'i' loop, and `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}` for the second 'i' loop.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` for the first 'i' loop, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` for the second 'i' loop.\n- For the second-level (`j` loops) within each `i` loop, apply pipelining, tiling, and parallelization pragmas to manage data flow and computation efficiently: \n  - `#pragma ACCEL PIPELINE auto{__PIPE__L2}` for the first 'j' loop over 'tmp', and `#pragma ACCEL PIPELINE auto{__PIPE__L3}` for the second 'j' loop over 'D'.\n  - `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}` for the first 'j' loop, and `#pragma ACCEL TILE FACTOR=auto{__TILE__L3}` for the second 'j' loop.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` for the first 'j' loop, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}` for the second 'j' loop.\n- For the innermost loops (`k` loops), enable automatic parallel reduction to handle the computation of matrix products. This includes the accumulation to the 'tmp' and 'D' matrices:\n  - `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}` for accumulating into 'tmp'.\n  - `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}` for accumulating into 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 10\n__PARA__L4: 32\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a 2mm kernel function that performs a matrix multiplication followed by another matrix multiplication and scales the result with given alpha and beta factors. Define the kernel function with appropriate data types and sizes for input matrices A, B, C, D, and intermediary matrix tmp, where A is 40x70, B is 70x50, C is 50x80, D is 40x80, and tmp is 40x50. Begin with a kernel pragma to indicate hardware acceleration. Apply pipelining, tiling, and parallelism pragmas automatically (let HLS tool decide the best factors) to each loop nest with placeholders for specific factors. Additionally, for the innermost loops that accumulate into tmp and D, employ parallel reduction to allow concurrent summing into the accumulating elements. Ensure that no pragmas are set to explicitly turn off optimizations, and that the entire double-nested loop system for the matrix computations is included within the scop and endscop pragma markers.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated function named 'kernel_2mm' that performs a matrix-matrix multiplication with specified size parameters and scaling factors. Apply all optimizations, including pipelining, tiling, and parallel processing at every loop level, using automatic determination of the best factor for each. Ensure that the reduction is parallelized for appropriate loop levels and that temporary array results are properly accumulated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function `kernel_2mm` as a hardware acceleration kernel. Automatically determine the optimal pipeline, tiling, and parallelization factors for various loops, and for the innermost loops, which perform reduction operations, specify the array to be reduced with the parallelization pragmas.\n\nIn natural language, you would provide the following instructions:\n\n1. Treat the `kernel_2mm` function as a hardware kernel for acceleration.\n2. For all loops within the function, automatically optimize the pipelining, tiling, and parallel processing without specific numerical values for the various factors; let the compiler or tool decide the best configurations.\n3. For the reduction loops that accumulate values into the `tmp` and `D` arrays, explicitly indicate that `tmp` and `D` are targets for reduction and allow the compiler to automatically determine the optimal parallel reduction factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel to improve performance when processing the matrix multiplication and addition formula D := alpha * A * B * C + beta * D. Apply automatic optimization to each loop in the function by enabling pipelining, tiling, and parallelization where appropriate. This includes automatic determination of optimal factors for these respective optimizations. Furthermore, for the innermost loops where the reductions are performed on arrays 'tmp' and 'D', enable parallel reductions to aid in accumulating the results efficiently. Use automatic factor determination for parallel reductions as well to ensure balanced resource usage and optimal performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Enable compile-time automatic optimization for pipelining, tiling, and parallel processing on relevant loops within the \"kernel_2mm\" function, which performs a series of matrix operations. Use pragma annotations to optimize the function specified as a hardware kernel, ensuring that the loop for initializing 'tmp' and the loop for computing the final matrix 'D' are both efficiently accelerated, by letting the compiler choose optimal values for pipelining, tiling, and parallelization, as well as applying reduction to accumulate values correctly in 'tmp' and 'D' during their respective update phases.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a kernel function that performs a 2mm operation on multi-dimensional arrays using optimized hardware acceleration. Within the kernel, apply automatic pipelining, automatic tiling, and automatic parallelization factor pragmas to all the nested loops. Additionally, incorporate automatic parallel reduction for matrix multiplications to efficiently accumulate the results in 'tmp' and 'D' matrices, ensuring that each loop operation is appropriately optimized for parallel execution in hardware.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated 2mm kernel function named 'kernel_2mm' that takes dimensions 'ni', 'nj', 'nk', 'nl', and scalar values 'alpha' and 'beta' along with double-precision matrices 'tmp', 'A', 'B', 'C', and 'D'. Configure the HLS tool to automatically determine the pipeline depth, tiling factor, and parallelization factor where relevant. Add parallel accumulation to the matrix multiplication loops using the 'reduction' clause. Apply these configurations to two nested loop sequences, the first computing the product of 'alpha', 'A', and 'B' stored in 'tmp', and the second adding the product of 'tmp' and 'C' to 'beta' scaled matrix 'D', effectively implementing the operation 'D := alpha*A*B*C + beta*D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 14\n__PARA__L5: 10\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration target, then enable automatic optimization for loop pipelining, tiling, and parallelization with specific yet adjustable factors for future tuning for each loop in the function. Ensure that the parallel pragma includes a reduction clause appropriate for the accumulation operations performed within the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 10\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, apply automatic optimization for pipelining, tiling, and parallel processing at each loop level, and enable parallel reduction for specific array computations within the innermost loops.\n\nSpecifically:\n\n- Treat the 'kernel_2mm' function as a hardware acceleration target.\n- For the first two-level nested loops, enable automatic pipelining, automatic loop tiling, and automatic parallel factor determination for the outer loop 'i' as well as automatic pipelining, tiling, and parallel factor for the inner loop 'j'.\n- Apply automatic selection of parallelism for the innermost loop 'k' that computes the matrix multiplication and accumulation into 'tmp', with a parallel reduction on the 'tmp' array.\n- For the next two-level nested loops, enable automatic optimization strategies for the outer loop 'i' and inner loop 'j' similar to the first two.\n- Again, apply automatic parallel factor determination with parallel reduction for the innermost loop 'k', focusing on updating matrix 'D' with the result from 'tmp' and matrix 'C'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Define a hardware-accelerated kernel to perform a two-matrix multiplication (2mm) operation, where each loop is automatically optimized for parallelism, pipelining, and data tiling, as well as allowing for reductions on the partial sums of `tmp` and `D` matrices. Let the compiler decide the optimization factors for all the loops.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the function 'kernel_2mm' with automatic optimization for the nested loops. Apply automatic pipelining, tiling, and parallelization for each level of loop nest where appropriate, and enable parallel reduction for accumulation within the innermost loops. Use pragmas to reflect these optimizations and allow for automatic factor determination.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function with automatic hardware acceleration directives by following these instructions:\n\n- Designate the function `kernel_2mm` to be compiled as a hardware kernel.\n- For the outermost loop that multiplies `A` and `B` matrices, apply automatic pipelining, tiling, and parallelization without specifying factors, allowing the HLS tool to determine the optimal values.\n- Similarly, for the nested loop calculating the product of `alpha`, `A[i][k]`, and `B[k][j]`, apply automatic parallelization with reduction on `tmp`, letting the HLS tool decide the factor.\n- For the outermost loop that computes the final expression `alpha*A*B*C + beta*D`, also apply automatic pipelining, tiling, and parallelization directives without specified factors to decide optimization levels.\n- In the nested loop updating `D` with the product of `tmp` and `C` as well as scaling by `beta`, also apply automatic parallelization with reduction on `D`, with the factor to be determined by the HLS tool.\n- Ensure all HLS pragmas inserted for pipelining, tiling, parallel factors, and reductions use 'auto' arguments, which directs the HLS tool to use its own analysis for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel to accelerate a matrix multiplication and addition operation, automatically optimizing the pipelining, tiling, and parallel execution of the loops. Use automatic factors for optimization as appropriate. In the first loop nest, calculate the product of matrices A and B, scaled by alpha, and store the result in a temporary matrix. Apply automatic parallel reduction to the innermost loop iterating over k. In the second loop nest, scale matrix D by beta and add the product of the temporary matrix and matrix C to it, using parallel reduction for the innermost loop as well.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function with pragmas to optimize for hardware acceleration. Label the function as a hardware kernel. For the computation of `tmp = alpha * A * B`, auto-optimize the outer loops `i` and `j` for pipelining, tiling, and parallel processing. Apply a parallel reduction to `tmp` for the inner loop `k`. For the computation of `D = beta * D + tmp * C`, apply the same optimization strategies for `i` and `j` loops and parallel reduction to `D` for the inner loop `k`. Set all optimization pragma factors to automatic determination.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, use the following instruction:\n\n\"Define a function called 'kernel_2mm' as a hardware acceleration kernel that takes matrix dimensions 'ni', 'nj', 'nk', 'nl', and scalar values 'alpha' and 'beta', along with double-precision floating-point matrices 'tmp', 'A', 'B', 'C', and 'D' as inputs. The function performs a two-matrix multiplication where 'D' is updated with 'alpha' times the matrix product of 'A' and 'B', followed by the product of the result with 'C', and summed with 'beta' times 'D'. Apply automatic pipelining, tiling, and parallelism pragmas to both the outer and inner loops, and use automatic parallelism with reduction pragmas on the innermost loops to accumulate the products into 'tmp' and 'D'. The pragmas should be generic such that the optimization factors are automatically determined by the compiler.\" \n\nSpecific #pragma values for each directive:\n\n1. `#pragma ACCEL kernel`: Treat the 'kernel_2mm' function as a Hardware Kernel for acceleration.\n2. `#pragma ACCEL PIPELINE auto`: Enable compiler-determined pipelining on the loop immediately following this pragma to improve throughput by overlapping loop iteration executions when possible.\n3. `#pragma ACCEL TILE FACTOR=auto`: Allow the compiler to automatically determine the tiling factor, partitioning the loop into smaller chunks to be executed concurrently, improving data locality and parallelism.\n4. `#pragma ACCEL PARALLEL FACTOR=auto`: Enable compiler-determined parallelism on the loop immediately following the pragma to increase concurrent execution of loop iterations when appropriate.\n5. `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto`: For the specified variable, apply automatic factorization to enable parallel computation of the reduction variable within the loop, while the compiler determines the optimal parallelization factor.\n\nReplace `variable` in the reduction pragma with the relevant variable name ('tmp' or 'D') for each loop context. And make sure to let the compiler automatically determine the appropriate factorization by using `auto{__PIPE__L#}`, `auto{__TILE__L#}`, `auto{__PARA__L#}` where `#` denotes the specific loop level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 1\n__PARA__L4: 2\n__PARA__L5: 25\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Identify the function 'kernel_2mm' for acceleration, and apply automatic hardware optimization directives for pipelining, tiling, and parallelization with custom factors for all loops within the function, along with parallel reduction for the 'tmp' and 'D' arrays to accumulate results effectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The instruction to generate the provided HLS C code with specific pragma commands for optimization and parallelization would be:\n\n\"Define a matrix multiplication and addition kernel which accelerates the operation D = alpha * A * B * C + beta * D for matrices A, B, C, and D with given dimensions. Automatically optimize the loop pipelines, tiling, and parallelization using pragrams inserted before each loop level. Enable loop pipelining and parallel reduction for the innermost loops to aggregate results efficiently. Suggest that the compiler determines the best factors for tiling and parallelization automatically, while all outer loops are explicitly optimized for high-throughput pipelining, memory access patterns, and parallel execution.\" \n\nFor each pragma directives, use the respective descriptions:\n\n- `#pragma ACCEL kernel`: Identify `kernel_2mm` function as a kernel for hardware acceleration.\n- `#pragma ACCEL PIPELINE auto{__PIPE__L0} / #pragma ACCEL PIPELINE auto{__PIPE__L1}...`: Advise the compiler to automatically choose the optimal pipelining strategy for each marked loop (L0, L1, L2, L3).\n- `#pragma ACCEL TILE FACTOR=auto{__TILE__L0} / #pragma ACCEL TILE FACTOR=auto{__TILE__L1}...`: Allow the compiler to automatically decide the optimal tiling factor for the loops (L0, L1, L2, L3).\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0} / #pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}...`: Indicate that the compiler should automatically fine-tune the parallel factor for each loop level (L0, L1, L2, L3) to utilize hardware parallelism effectively.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}` / `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}`: Use parallel reduction on the variable `tmp` or `D` within the nested loop (L4, L5 respectively) to combine results across iterations in parallel, with the compiler selecting the parallelization factor.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This is a guide to creating a hardware accelerated 2-dimensional matrix multiplication kernel in C, using pragmas for HLS directives. The kernel takes in two matrices A and B, multiplies them, then adds the result to matrix D. The matrix calculations are scaled by two factors, alpha and beta.\n\nTo start off, the function you need to create will be tagged as a hardware kernel using '#pragma ACCEL kernel'. \n\nTwo triple-nested for loops are used to perform the matrix calculations. \n\n1. The outermost loop of each, iterating over 'i', should be processed in an automated pipeline using '#pragma ACCEL PIPELINE auto{__PIPE__L0}' for the first loop and '#pragma ACCEL PIPELINE auto{__PIPE__L1}' for the second loop. It should not be divided into smaller chunks ('#pragma ACCEL TILE FACTOR=auto{__TILE__L0}' and '#pragma ACCEL TILE FACTOR=auto{__TILE__L1}') and should be run sequentially, not in parallel ('#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}').\n\n2. The second level loops, iterating over 'j', need similar pragmas: '#pragma ACCEL PIPELINE auto{__PIPE__L2}' and '#pragma ACCEL PIPELINE auto{__PIPE__L3}', '#pragma ACCEL TILE FACTOR=auto{__TILE__L2}' and '#pragma ACCEL TILE FACTOR=auto{__TILE__L3}', '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}' for each loop respectively.\n\n3. Inside each of the second level loops, a third level loop iterating over 'k' aggregates the calculation of each cell in the output matrices. Here, you use '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}' in the first group of nested loops, which reduces all the results through addition into variable 'tmp'. In the second group of nested loops, use '#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}' to aggregate the calculations into matrix D. \n\nRemember, by specifying 'auto' as the FACTOR in each pragma, the HLS tool decides the loop unrolling or pipelining factors for optimization automatically based on the structure and requirements of the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 4\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function that processes matrix-matrix multiplications and additions subject to various optimization directives:\n\n- Treat the whole function as a hardware acceleration kernel.\n- For both top-level matrix operations, apply automatic pipelining to enable the possibility of overlapping loop iterations for potential throughput improvement.\n- For the nested loops, enable automatic determination of tiling to break down the problem into smaller, more manageable chunks that could be executed more efficiently on hardware.\n- For the innermost loops, enable parallel execution with an unspecified factor, hinting at a parallel reduction operation on the 'tmp' and 'D' matrices, to exploit data level parallelism while performing summations.\n- Leave the optimization factors such as the pipelining depth, tiling size, and the number of parallel units for the compiler to decide, marking them as 'auto'.\n\nUse the corresponding HLS pragmas with 'auto' attributes where appropriate to ensure that the HLS tool has the flexibility to optimize these loops without explicit user-defined values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate a 2mm kernel computation by enabling automatic optimization and parallelization features for each of the nested loops, utilizing compiler-assisted values for pipelining, tiling, and parallel processing, and enable reduction for the innermost loops where partial results are accumulated into the `tmp` and `D` matrices. Make sure to treat the `kernel_2mm` function as a dedicated hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function that accelerates the computation of matrix multiplication followed by another matrix multiplication, as per the following specifications:\n\n1. Mark the entire function as a hardware acceleration kernel.\n2. For the first pair of nested loops (labeled L0 and L2), where the matrix 'tmp' is computed as the product of matrices 'A' and 'B' scaled by 'alpha':\n    - Allow the HLS tool to decide on automatic loop pipelining for potential performance optimization, with automatically determined pipeline initiation intervals.\n    - Let the tool choose the optimal tiling factor for the loops, for better use of resources and memory bandwidth.\n    - Permit the HLS tool to decide on the amount of loop parallelism, potentially unrolling loops for better throughput.\n    - Specify that the innermost loop (labeled L4) that accumulates the product 'tmp[i][j]' should be parallelized with a reduction on 'tmp', letting the HLS tool decide on the parallel factor.\n\n3. For the second pair of nested loops (labeled L1 and L3), where the matrix 'D' is updated by scaling with 'beta' and adding the matrix multiplication result of 'tmp' and 'C':\n    - Same instructions for automatic loop pipelining, tile factor, and parallel factor as in the first pair of nested loops.\n    - Ensure that the innermost loop (labeled L5) accumulating to 'D[i][j]' should be parallelized with a reduction on 'D', with the parallel factor left to the HLS tool's choice.\n\nNote: Use the 'auto' keyword to let the HLS tool automatically determine the values for pipeline, tile, and parallel factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 10\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following C function as a hardware acceleration kernel, apply automated pipelining, tiling, and parallel processing optimizations with compiler-determined optimal values to each nested loop, and employ parallel reduction to aggregate results within the specified arrays, ensuring all loops are fully optimized for concurrent execution with respect to the dependencies present in matrix multiplication and addition computations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 32\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function called `kernel_2mm` that performs a sequence of matrix-matrix multiplications on matrices `A`, `B`, `C`, and `D`, scaling `D` by `beta` and the result of `A*B` by `alpha`, and summing these products into `D`. Apply automatic optimization pragmas for pipelining, tiling, and parallelizing on all loops. Additionally, parallelize the reduction steps within the innermost loops with automatic factor determination, ensuring the reduction operations on `tmp` and `D` are properly handled.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Apply automated pipelining, tiling, and parallelization strategies to all loops, inferring the optimal values. Additionally, perform parallel reduction on 'tmp' and 'D' arrays within the innermost loops to facilitate concurrent computations that accumulate results. Use the following natural language instructions to annotate the HLS code:\n\n1. Treat 'kernel_2mm' function as a dedicated hardware acceleration unit.\n2. For all loops, enable automatic determination of pipeline settings to improve instruction throughput without specifying the exact behavior.\n3. For all loops, enable automatic calculation of tiling factors to partition the loops for better data locality and parallel execution without specifying the exact size of the tiles.\n4. For all loops, activate automatic determination of parallel execution factors to run loop iterations concurrently without specifying the degree of parallelism.\n5. Within the inner loops where 'tmp' and 'D' arrays are updated, apply automatic parallel reduction to enable concurrent accumulation of results without specifying the exact factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Optimize the `kernel_2mm` function which performs a 2mm computation on input matrices A, B, C, and D, to run efficiently on hardware by automatically determining pipelining, tiling, and parallelization strategies at each nested loop level, and enabling parallel reduction for certain temporary accumulations, while treating the entire function as a standalone hardware acceleration kernel.\"\n\n- For the `kernel_2mm` function, ensure it is recognized as a hardware acceleration kernel with `#pragma ACCEL kernel`.\n- At each loop level, enable automatic pipelining with `#pragma ACCEL PIPELINE auto`, which lets the HLS tool decide whether and how to pipeline each loop.\n- Apply automatic tiling to divide the loops into smaller chunks through `#pragma ACCEL TILE FACTOR=auto`, allowing the tool to choose appropriate tiling factors.\n- Use automatic parallelization with `#pragma ACCEL PARALLEL FACTOR=auto` for exploiting data-level parallelism, allowing the tool to determine the optimal factor.\n- For loops performing reductions on `tmp` and `D`, utilize parallel reduction with `#pragma ACCEL PARALLEL reduction` followed by the array being reduced, specifying an automatic factor to improve performance of accumulation steps.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 10\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, you can give the following comprehensive but concise instruction:\n\n\"Create a two-part matrix multiplication and addition hardware accelerator kernel where both parts have the outer loops automatically optimized for pipeline, tile, and parallel execution with pragmas auto-detected settings, and include parallel reduction for the innermost loops with the corresponding matrix accumulation variables, allowing for dynamic optimization based on the provided matrix dimensions and avoiding manual pipelining, tiling, or parallel factors.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 32\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function that accelerates matrix multiplication and addition with automatic optimization directives. The matrices involved are A (40x70), B (70x50), C (50x80), and D (40x80). Begin by treating the kernel function for hardware acceleration. For each of the two major nested loop blocks, enable automatic pipeline optimization, automatic loop tiling, and automatic parallel factor detection. In the innermost loop of each block, apply a parallel reduction optimization directive with auto factor detection to the 'tmp' array for the first block and the 'D' array for the second block, aiming for efficient parallel accumulation of the products.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function `kernel_2mm` as a hardware kernel to accelerate the process of calculating `D = alpha*A*B*C + beta*D` with matrices `A`, `B`, `C`, and `D`, and scalars `alpha` and `beta`. Enable automatic optimization for all loops where possible by applying pipeline, tile, and parallel pragmas with auto-tuned factors. Specifically, for first-level nested loops, apply auto-tuning to create parallel blocks, pipeline stages, and tiling factors. In the second level of nested loops, automatically tune the loop unrolling and parallelization for optimal execution, including reducing parallel loops to accumulate values into `tmp` and `D` matrices. Ensure intermediate matrix `tmp` is computed before calculating the final `D` matrix, and apply respective pragmas to optimize the loop-carried dependency reduction for both `tmp` and `D` updates.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 7\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be synthesized into a hardware kernel, instructing the following optimizations for the two main computation blocks:\n\n1. For the block performing 'tmp = alpha * A * B', enable automatic decision-making for loop pipelining, data partitioning, and inner loop parallelization with appropriate parallel reduction for the 'tmp' array. \n2. For the block updating 'D = D * beta + tmp * C', similarly enable automatic loop pipelining, data partitioning, and parallelism with parallel reduction for the 'D' array.\n\nEnsure that each loop's #pragma directives are set to automatically determine the best values for pipeline stages, tile sizes, and parallel factors. The `auto` sub-directives should leverage tags such as `__PIPE__Lx`, `__TILE__Lx`, and `__PARA__Lx` to indicate their association with specific loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 20\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, allowing the compiler to determine the best loop unrolling, tiling, and pipelining strategies automatically for all loops to optimize performance and resources. Additionally, explicitly enable parallel computation with automatic factor determination to optimize potential reductions within the nested loops.\n\nSpecifically:\n- Mark the function 'kernel_2mm' as a hardware acceleration kernel.\n- For each loop level (L0, L1, L2, L3, L4, L5), let the compiler automatically decide the best pipelining, tiling, and parallelization factors.\n- Within the innermost loops at levels L4 and L5, allow parallelization with automatic factor determination and specify that the reduction operations on the 'tmp' and 'D' arrays should be optimized for parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code segments with appropriate pragmas, the following comprehensive but concise set of instructions would be used:\n\n1. Designate the `kernel_2mm` function as a hardware acceleration kernel using `#pragma ACCEL kernel`.\n\n2. Apply automatic optimization to pipeline all top-level loops (`for (i = 0; i < 40; i++)`) with `#pragma ACCEL PIPELINE auto`.\n\n3. Instruct the HLS tool to automatically determine the optimal tiling factor for all top-level loops with `#pragma ACCEL TILE FACTOR=auto`.\n\n4. Instruct the HLS tool to automatically decide the parallel execution factor for all top-level loops with `#pragma ACCEL PARALLEL FACTOR=auto`.\n\n5. Enable pipeline optimization within the second level `for (j = 0; j < 50; j++)` and `for (j = 0; j < 80; j++)` loops using `#pragma ACCEL PIPELINE auto`.\n\n6. Request automatic determination of tiling factor and parallelization factor at the second level of loops (`for (j = 0; j < 50; j++)` and `for (j = 0; j < 80; j++)`) via `#pragma ACCEL TILE FACTOR=auto` and `#pragma ACCEL PARALLEL FACTOR=auto`.\n\n7. For the innermost loops (`for (k = 0; k < 70; ++k)` and `for (k = 0; k < 50; ++k)`) that have reduction operations on `tmp` and `D`, use `#pragma ACCEL PARALLEL reduction` to specify that these loops are to be parallelized with a reduction operation, and set the factor to automatically optimize by placing `FACTOR=auto`.\n\nThese instructions would guide the HLS compiler to produce a design that is potentially optimized for parallel execution at various loop levels. Note that 'auto' allows for the HLS tool to use its heuristics or algorithms to figure out the best possible factors leading to a trade-off between the hardware resources used and the performance achieved.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as the target for hardware acceleration, and apply optimizations to improve the performance of matrix multiplication and addition operations within this function. Specifically, enable the automatic determination of the pipeline stage settings, tile sizes for breaking down large computations, and parallel factor settings for concurrent execution in two separate sets of nested loops. Furthermore, incorporate parallel reduction for accumulation within the innermost loops to enhance the efficiency of the summation of products. Use compiler directives to automatically assign the optimal values for these optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 32\n__PARA__L4: 32\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, optimize the loops using pipelining, tiling, and parallel processing strategies as applicable, where specific optimization factors are automatically determined by the compiler. For reduction operations within the loops, apply parallelization with automatic factor selection and specify the array being reduced:\n\n1. Apply the kernel directive to the entire function to target hardware acceleration.\n2. Enable automatic pipelining, tiling, and parallelization for the outermost loops (labeled L0 and L1), allowing the compiler to select the optimization factors.\n3. Within the first nested loop (labeled L2), automatically pipeline and configure tiling and parallelization.\n4. For the first innermost loop (labeled L4), enable parallel reduction on the 'tmp' array with an automatically selected factor.\n5. For the second outer loop (labeled L3), automatically enable pipelining, tiling, and parallelization.\n6. In the second innermost loop (labeled L5), enable parallel reduction on the 'D' array with an automatically selected factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function by treating the function as a hardware acceleration kernel and applying automatic optimization techniques such as pipelining, tiling, and parallelization with auto-determined factors to all nested loops while enabling parallel reduction for matrix operations within the innermost loops. Specifically, apply automatic pipelining to the first and second level outer loops, auto-tile them for spatial partitioning, and set parallelization factors automatically. In the innermost loops where matrix multiplication occurs, enable parallel reduction with the target array (either `tmp` or `D`) to facilitate accumulation in parallel, again with an automatic factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel called 'kernel_2mm' to perform a 2-matrix multiplication followed by the addition of another matrix, with the first operation scaling by 'alpha' and the second by 'beta'. Allow the high-level synthesis tool to determine the optimal pipeline, tiling, and parallelization factors for the different nested loops. Specifically, enable automatic pipelining, automatic tiling, and automatic parallelization at each loop level, and perform parallel accumulation within the innermost loops on the 'tmp' and 'D' matrices.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 4\n__PARA__L4: 16\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function 'kernel_2mm' as a hardware acceleration kernel, and within this kernel, for both of the primary nested loops, enable automatic pipeline optimization, automatic loop tiling, and automatic loop parallelization. Specifically, for the loop responsible for calculating the product of 'alpha', 'A', and 'B', apply a parallelization strategy with automatic factorization that additionally performs reduction on the 'tmp' array. For the second primary nested loop, that scales 'D' by 'beta' and then accumulates the product of 'tmp' and 'C', apply a parallelization strategy with automatic factorization that performs reduction on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 20\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive yet concise natural language instruction that encompasses all the given HLS codes and #pragma values:\n\nDesignate the function 'kernel_2mm' as a hardware kernel for acceleration, which performs a two-matrix multiplication operation. For all loops, enable automatic optimization such as pipelining, tiling, and parallelism where the compiler decides the best strategy. The first nested loop initializes the 'tmp' matrix with the product of 'alpha', 'A' and 'B', considering dependencies during accumulation. The second nested loop updates matrix 'D' with the scaled sum of itself multiplied by 'beta' and the product of 'tmp' and 'C'. Apply reductions to safely handle potential data hazards within the innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 32\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: flatten\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate High-Level Synthesis (HLS) directives for optimizing a matrix multiplication and addition kernel as follows:\n\n1. Designate the function `kernel_2mm` itself as a hardware acceleration kernel. _(Use `#pragma ACCEL kernel` before the function definition.)_\n\n2. For each loop layer within the function, enable pipeline optimization automatically, allowing the HLS tool to decide the unrolling factor for improved throughput. _(Apply `#pragma ACCEL PIPELINE auto` before both outer `i` loops and both `j` inner loops.)_\n\n3. For both the outer `i` and inner `j` loops, allow automatic tiling optimization to break the loops into smaller, more manageable chunks if the HLS tool finds it beneficial for performance or resource usage. _(Use `#pragma ACCEL TILE FACTOR=auto` before both outer `i` loops and both `j` inner loops.)_\n\n4. Enable automatic parallel loop execution wherever possible to improve data processing concurrency, letting the HLS decide on the appropriate factor. _(Add `#pragma ACCEL PARALLEL FACTOR=auto` before both outer `i` loops and both `j` inner loops, as well as before the innermost `k` loops.)_\n\n5. For the innermost `k` loops that perform the actual computation of matrix multiplication and addition, set up reduction to accumulate results safely in the presence of parallel computations, targeting the `tmp` and `D` matrices. _(Include `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto` for the respective loops where these matrices are modified.)_\n\nThese directives should enable the HLS tool to fully optimize the loop nest structure for high-performance hardware generation subject to the resource constraints and design goals of the target platform.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 4\n__PARA__L5: 16\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 8"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel that performs a matrix multiplication followed by a matrix addition. For the outer loops (indexed by 'i' over 40 iterations and 'j' over 50 iterations), the matrix 'tmp' is initialized and calculated using the elements from matrices 'A' and 'B'. Then, in another set of outer loops (indexed by 'i' over 40 iterations and 'j' over 80 iterations), the matrix 'D' is updated. Both sets of outer and inner loops should be automatically optimized for parallel execution, pipelining, and tiling based on the design space exploration but without defining specific factors. Additionally, enable automatic parallel reduction for intermediate results stored in 'tmp' and 'D' while performing their respective innermost computations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 1\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Enable automatic hardware optimization of the 2mm (two-matrix multiply) algorithm by treating the function `kernel_2mm` as a hardware kernel for acceleration, automatically pipelining all the loops to improve the throughput where possible, using the most suitable factor determined by the compiler for loop tiling and parallel execution to optimize the loop iterations and data partitioning, and reducing the partial results of the multiply-accumulate operations on the matrices `tmp` and `D` to allow concurrent accumulation in hardware.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 32\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS accelerated kernel function named 'kernel_2mm' that takes ni, nj, nk, nl, alpha, beta, tmp, A, B, C, and D as parameters. Begin by declaring loop variables i, j, and k. Implement main loop (i) for both D computation and tmp intermediate value computation using pipeline, tile factor, and parallel factor set to auto. Inside the main loop, implement nested loops (j) using pipeline, tile factor, and parallel factor set to auto. Finally, implement nested loops (k) with parallel pragma reduction on 'tmp' and 'D' and auto factors. The kernel function performs D := alpha * A * B * C + beta * D operation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 4"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Automatically optimize the nested loops in the matrix multiplication kernel function for hardware acceleration by pipelining and parallelizing where possible, setting tile sizes and parallel factors based on automated profiling, and using reduction to prevent data hazards while accumulating values in the tmp and D matrices. Apply these optimizations comprehensively across all loop nests, and treat the whole function as a hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 32\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a kernel function named 'kernel_2mm' that performs matrix multiplication and addition with matrices A, B, C, and D, scaled by factors alpha and beta respectively. For the first loop nest, automatically manage pipelining, tiling, and parallelism for the 'i' and 'j' loops with pragma directives, and within the innermost 'k' loop, apply parallel reduction on 'tmp' matrix with automatic factor selection. For the second loop nest, again manage pipelining, tiling, and parallelism automatically for the 'i' and 'j' loops, and within the innermost 'k' loop, apply parallel reduction on 'D' matrix with automatic factor selection. Use the 'auto' option for all pragma directives to enable the HLS tool to decide the appropriate values for pipeline stages, tile sizes, and the number of parallel loop iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "I will provide you with a single, comprehensive natural language instruction that encompasses the process and pragma values for the HLS code provided:\n\n\"Treat the function 'kernel_2mm' as a hardware kernel to be accelerated, and for each nested loop within this function, enable automatic optimization of pipeline, tiling, and parallel execution with factors determined by the compiler. Additionally, where there are reductions, apply parallel reduction to the specified variable with compiler-determined factors for optimal performance.\"\n\nThis single sentence encapsulates the pragmas (`#pragma ACCEL kernel`, `#pragma ACCEL PIPELINE auto`, `#pragma ACCEL TILE FACTOR=auto`, `#pragma ACCEL PARALLEL FACTOR=auto`, and `#pragma ACCEL PARALLEL reduction`) and the intent of optimization with compiler-determined factors, which correspond to `auto{__PIPE__Lx}`, `auto{__TILE__Lx}`, and `auto{__PARA__Lx}` for the loops and parallel reductions in the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 20\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Designate the function 'kernel_2mm' as a hardware acceleration kernel that computes the matrix multiplication and addition expression 'D := alpha * A * B * C + beta * D', with matrices 'A', 'B', 'C', 'D', and intermediate matrix 'tmp'. For each of the five nested loops, automatically optimize pipelining, tiling, and parallel processing, and for the innermost loops that accumulate into 'tmp' and 'D', respectively, perform parallel reduction operations.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 4\n__PARA__L4: 32\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Automatically optimize the function 'kernel_2mm' for hardware acceleration by setting it as a kernel and intelligently determine pipeline, tiling, and parallelism pragmas for each loop nest to improve performance while managing parallel reductions for the matrix multiplications involved.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 32\n__PARA__L4: 7\n__PARA__L5: 4\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate high-level synthesis (HLS) directives to optimize the kernel function for a 2mm operation with the following specifications:\n\n1. The entire function should be optimized as an acceleration kernel.\n2. For the two main nested loops, enable automatic pipelining, tiling, and parallel factor determination to optimize performance while maintaining hardware feasibility.\n3. Within the innermost loops, use parallelization with automatic factor determination for the reduction operation on the 'tmp' and 'D' arrays to enable concurrent accumulations.\n4. Ensure that the parallelized reduction maintains the integrity of the output data for 'tmp' and 'D' by using the appropriate pragma.\n\nHere are the specific directives to use:\n\n- Treat the entire function as a hardware kernel for acceleration.\n  - `#pragma ACCEL kernel`\n\n- For the first pair of nested loops calculating `tmp[i][j]`, enable:\n  - Automatic pipelining: `#pragma ACCEL PIPELINE auto{__PIPE__L0}`\n  - Automatic tiling factor determination: `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}`\n  - Automatic parallel factor determination: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`\n  - For the innermost loop performing the reduction to calculate `tmp[i][j]`, use parallel reduction with automatic factor determination: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{__PARA__L4}`\n\n- For the second pair of nested loops updating `D[i][j]`, enable:\n  - Automatic pipelining: `#pragma ACCEL PIPELINE auto{__PIPE__L1}`\n  - Automatic tiling factor determination: `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`\n  - Automatic parallel factor determination: `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`\n  - For the innermost loop performing the reduction to update `D[i][j]`, use parallel reduction with automatic factor determination: `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{__PARA__L5}`\n\nThis configuration allows the compiler to automatically analyze and decide the most efficient way to perform pipelining, tiling, and parallelization for the given operation, along with the data reductions required for the 'tmp' and 'D' arrays, while abstracting the specific numerical values of factors away from the user.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 2\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, automatically optimize all loops for pipelining, allow dynamic resource allocation (tiling) to break large data blocks into smaller chunks, and run loop iterations in parallel with appropriate factors, applying reduction to accumulate partial results where needed. Specifically, declare the function 'kernel_2mm' with the #pragma directive for kernel acceleration, use the auto option for pipelining, tiling, and parallelism to let the compiler decide optimal factors, denote parallel reduction for the 'tmp' and 'D' arrays to efficiently combine intermediate results within their respective third-level loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, automatically optimize the loop schedules for pipelining, tiling, and parallel processing, and enable reduction for the accumulation in the specified loop nests:\n\n- Treat 'kernel_2mm' as an acceleration function.\n- For the top-level loops 'i' iterating over 'tmp' and 'D' matrices, enable automatic optimization choices for pipelining, tiling, and parallelism.\n- For the 'j' loop nested within the top-level 'i' loop iterating over 'tmp', automatically decide the pipelining, tiling, and parallelism, and specify the reduction variable 'tmp' to accumulate values.\n- For the 'j' loop nested within the top-level 'i' loop iterating over 'D', automatically decide the pipelining, tiling, and parallelism, and specify the reduction variable 'D' to accumulate values.\n- For every innermost 'k' loop, which governs the multiplication and accumulation, automatically determine parallelism strategies, including the reduction, to efficiently accumulate results in 'tmp' and 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 1\n__PARA__L5: 5\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` with ni, nj, nk, nl, alpha, beta, and 2D arrays tmp, A, B, C, and D as a dedicated hardware kernel. Within this function, dynamically optimize two nested loops: the first computing `tmp` as the matrix multiplication of `alpha`, `A`, and `B`, and the second updating `D` with a scaled self-addition and the matrix multiplication of `tmp` and `C`, both accelerated by automatic pipeline, tiling, and parallel processing directives. Utilize automatic factor determination for all directives and enable parallel reduction for the innermost multiplication loops affecting `tmp` and `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel. In this function, implement a matrix multiplication followed by matrix addition, both of which use the matrices A, B, C, D, and the scalar values alpha and beta. Set the hardware to automatically optimize the loop pipelining, loop tiling, and loop parallelization with unspecified factors. Additionally, apply parallel reduction to the accumulation operations within the nested loops to improve performance. Make sure that these optimizations are indicated with automatic determination of the appropriate factors and control the parallel reduction on specific array elements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 4\n__PARA__L4: 32\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: \n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` to act as a dedicated hardware module to accelerate the computational task of updating matrix `D` with the formula `D := alpha * A * B * C + beta * D`. Automatically optimize the loops within the function through pipelining, loop tiling, and parallel processing by setting the respective pragmas to auto-tune the values for pipeline stages, tile sizes, and parallel factors. Specifically, indicate the need to reduce the variable `tmp` within the nested loop calculating `tmp[i][j]` and the variable `D` within the loop updating `D[i][j]` using automatic parallel factor adjustment.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 32\n__PARA__L4: 32\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, automatically optimize loop pipelining with the option to adjust pipeline settings, apply automatic loop tiling optimization where the tiling factor can be determined dynamically, and auto-tune the parallelization settings for each loop, including setting the appropriate reduction variables for parallel accumulations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 8\n__PARA__L4: 14\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function 'kernel_2mm' for 2mm matrix-matrix multiplication as a hardware acceleration kernel. For the first portion of the computation, automatically optimize all levels of looping through 'tmp', 'A', and 'B' matrices with pipelining, tiling, and parallel processing. For 'tmp', apply a parallel reduction. Proceed similarly with the second part involving 'D', 'tmp', and 'C' matrices. Use automatic optimization factors, letting the HLS tool decide the specifics.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 20\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS C code for a 2mm kernel function designed for FPGA acceleration using the following pragmas and settings: Declare the function as a hardware kernel, optimize nested loops by combining automatic pipelining, tiling, and parallelization with automatic factor determination, and apply reduction optimization for intermediary arrays during matrix multiplication.\n\nHere is the concise instruction to achieve that:\n\n\"Create a hardware-accelerated 2mm kernel performing matrix multiplications and additions on fixed-size arrays, with all loops fully optimized using automatic pipelining, automatic tiling, and automatic parallelization. Additionally, enable automatic selection of parallel reduction factors for the intermediate results in the matrix multiplications.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 2\n__PARA__L4: 70\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and configure it to automatically optimize the loop pipelining, tiling, and parallelism factors for all loops within. Apply parallel reduction to effectively use resources for sum calculations in specific loops.\n\n- Declare `kernel_2mm` as a hardware acceleration kernel.\n- Enable automatic optimization for pipelining, tiling, and parallel factors for each loop nest, specifically for loops labeled L0, L1, L2, and L3.\n- For loop nests L0 and L1 which iterate over `i`, and loop nests L2 and L3 which iterate over `j`, instruct the HLS tool to determine the best possible pipelining, tiling, and parallelism settings automatically.\n- In the `k` loops nested within loops L2 and L3 (specifically L4 and L5), automatically optimize the parallelism factors and additionally enable parallel reduction on the `tmp` and `D` arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 10\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: off\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, instruct as follows: \n\n\"Define a function to serve as a hardware kernel that accelerates the computation of \\( D := \\alpha \\cdot A \\cdot B \\cdot C + \\beta \\cdot D \\). First, initialize a temporary 2D array by performing a matrix multiplication of \\( A \\) and \\( B \\), scaled by \\( \\alpha \\), and store the result in the temporary array. In the process, enable automatic optimization for pipeline, tile, and parallel execution for all loops, using the respective `auto` options which should be replaced with the appropriate numerical values by the accelerator directives at compilation. Additionally, apply a reduction on the temporary array to correctly accumulate the results during the first matrix multiplication. Afterwards, perform an element-wise operation to scale matrix \\( D \\) by \\( \\beta \\), and add the product of the temporary array and matrix \\( C \\) to it, again with automatic optimization directives for pipeline, tile, and parallel execution, and use reduction for parallel accumulation on matrix \\( D \\). Within the function, apply the optimization pragmas just before each loop and ensure that the reduction of partial sums is correctly handled for parallel loops.\" \n\nCorresponding HLS Pragmas to include before each loop:\n- `#pragma ACCEL PIPELINE auto{__PIPE__Lx}`: Enable the compiler to automatically determine the pipeline granularity for a loop labeled as `Lx`.\n- `#pragma ACCEL TILE FACTOR=auto{__TILE__Lx}`: Allow the compiler to decide the best tiling factor for partitioning a loop labeled as `Lx` into smaller, more manageable chunks.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__Lx}`: Instruct the compiler to decide how many iterations of the loop labeled as `Lx` can be executed in parallel.\n- `#pragma ACCEL PARALLEL reduction = <array> FACTOR=auto{__PARA__Lx}`: Apply automatic parallelization with reduction on the specified array to correctly sum partial results in a loop labeled as `Lx`.\n\nEnsure the pragmas are placed before each of the loops in the function, adjusting the labels (`L0`, `L1`, `L2`, `L3`, `L4`, `L5`) according to their position in the nested loop hierarchy.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Automatically determine the optimal pipelining, tiling, and parallelism strategies for the loops within this function, aimed at matrix multiplication and accumulation operations. Enable parallel reduction with automatic factor determination for the 'tmp' array within the innermost loop of the first set of nested loops, and for the 'D' array within the innermost loop of the second set of nested loops, ensuring intermediate results are accurately aggregated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 4\n__PARA__L4: 32\n__PARA__L5: 2\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware accelerated kernel, and for the loops inside, automatically manage pipelining, tiling, and parallelization strategies, with specific optimization factors dynamically determined by the HLS tool. Use parallel reduction pragmas for the innermost accumulation operations on 'tmp' and 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 5\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel computation that accelerates the matrix-matrix multiplication and the matrix addition in the form of `D := alpha*A*B*C + beta*D` using hardware optimization directives. Adopt automatic optimizations for the pipeline, tiling, and parallel processing parameters. Allow for parallel reduction in certain loop nests to ensure efficient aggregation of results. Explicitly define the function as a hardware kernel to be accelerated. Apply the following specific pragma directives at each level of the loop nests:\n\n1. Define the function `kernel_2mm` as a hardware kernel with the pragma `#pragma ACCEL kernel`.\n2. For the top-level loop over index `i` that computes `tmp[i][j]`, use the automatic pipeline, tiling, and parallel directives (with unique tokens to be replaced by the compiler or the user).\n3. Inside the nested loop over index `j` in the computation of `tmp`, use the automatic pipeline, tiling, and parallel directives again.\n4. For the nested inner loop over index `k` that calculates the element-wise multiplication and addition for `tmp`, enable parallel reduction on `tmp` with automatic factor determination.\n5. For the next top-level loop over index `i` concerning the update of `D[i][j]`, repeat the step of applying automatic pipeline, tiling, and parallel directives.\n6. In the subsequent nested loop over index `j` for updating `D`, replicate the use of automatic pipeline, tiling, and parallel directives.\n7. Finally, for the innermost loop over index `k` that accomplishes the element-wise multiplication and addition for `D`, enable parallel reduction on `D` with an automatic factor.\n\nEnsure that the pragmas related to pipeline, tile, and parallel directives are followed by an `auto` keyword and placeholders for compiler-generated values (e.g., `{__PIPE__L0}`, `{__TILE__L0}`, `{__PARA__L0}`). The placeholders should be uniquely labeled for each loop level, suggesting that the optimization factors may differ between loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 2\n__PARA__L3: 8\n__PARA__L4: 70\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate High-Performance FPGA Acceleration C code for a 2mm kernel computation that multiplies and accumulates matrices with the following specifics: Mark the function as a hardware acceleration kernel, enable automatic optimization for pipelining, tiling, and parallelizing all loops with compiler-determined factors, and employ parallel reduction for the summations within the innermost loops.\n\n- Treat the entirety of the 'kernel_2mm' function as an independent hardware computation unit.\n- Apply automated pipeline optimization for both the top-level loops iterating over the 'tmp' and 'D' matrices.\n- Introduce automatic tiling optimization for every nested loop to potentially enhance data locality and reduce memory access times.\n- Implement automatic parallel processing on all loops including the innermost summations in 'tmp' and 'D' matrices, with additional parallel reduction for accumulating results.\n- Use automatically assigned factors for pipeline, tile, and parallel pragmas determined by the HLS tool for optimal hardware performance, indicated by 'auto' and unique placeholder tags such as '{__PIPE__L0}', '{__TILE__L0}', '{__PARA__L0}', etc., corresponding to each loop\u2019s optimization settings.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 16\n__PARA__L4: 16\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function that accelerates the matrix multiplication and addition operation of the form D := alpha*A*B + beta*D. Set up the kernel with the following optimizations: \n\n1. Annotate the function as a hardware acceleration kernel.\n2. Apply automatic pipelining for loop levels L0, L1, L2, and L3, where the HLS tool should choose optimal pipeline settings.\n3. Apply automatic tile optimization for loop levels L0, L1, L2, and L3, allowing the HLS tool to partition loops into smaller, more manageable operations.\n4. Apply automatic parallelization for loop levels L0, L1, L2, L3, and L5 with a pragma option to allow the HLS tool to execute iterations concurrently where possible.\n5. Specifically for the innermost loops at levels L4 and L5, enable parallel reduction on the 'tmp' and 'D' arrays, respectively, to allow for concurrent accumulation of sum values.\n\nUse `auto` and placeholder tokens `__PIPE__Lx`, `__TILE__Lx`, and `__PARA__Lx` for pipelining, tiling, and parallelization factors, respectively, allowing the HLS tool to automatically determine the appropriate factors for optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 20\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel acceleration function with specific optimization directives by following these instructions:\n\n1. Declare the function as a hardware kernel using `#pragma ACCEL kernel`.\n2. Inside the first two-nested loop region, where the `tmp` matrix is computed:\n   - Enable automatic loop pipelining with `#pragma ACCEL PIPELINE auto`.\n   - Allow the compiler to automatically determine loop tiling factors using `#pragma ACCEL TILE FACTOR=auto`.\n   - Permit automatic parallelization factor determination with `#pragma ACCEL PARALLEL FACTOR=auto`.\n   - In the innermost loop for calculating `tmp`, apply a parallel reduction on the `tmp` array with an automatic factor using `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto`.\n3. Inside the second two-nested loop region, where the `D` matrix is updated:\n   - Again, apply automatic pipelining, tiling, and parallelization to the loops using the respective pragmas with `auto` option as in step 2.\n   - For the innermost loop where the `D` matrix update happens, use parallel reduction on the `D` array with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto`.\n\nEnsure that all pragmas contain the respective loop identifiers {__PIPE__L0}, {__TILE__L0}, {__PARA__L0}, etc., as placeholders for compiler-defined values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 10\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: \n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define this function as a hardware acceleration kernel, and within the matrix multiplication algorithm, apply automatic optimization strategies for pipelining, tiling, and parallel execution across appropriate loop levels, with a special emphasis on maximizing data reuse and ensuring reduction operations for the 'tmp' and 'D' arrays are correctly handled in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 50\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel with automated optimization pragmas for pipelining, tiling, and parallelization: Calculate the matrix multiplication of 'A' and 'B', then multiply by 'alpha'. Next, perform matrix multiplication between the result and 'C', sum it with 'beta' times 'D', and store the final result back in 'D'. Utilize automatic loop optimization pragmas for each nested loop, adjusting pipeline depth, tile size, and parallel factor, and include automatic parallel reduction for temporary accumulations in 'tmp' and 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 1\n__PARA__L3: 4\n__PARA__L4: 1\n__PARA__L5: 32\n__PIPE__L0: off\n__PIPE__L1: \n__PIPE__L2: flatten\n__PIPE__L3: off\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 2"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a target for hardware acceleration, automatically optimize the loops for pipelining, tiling, and parallelization with their respective settings set to be determined by the compiler. Additionally, enable parallel reduction for arrays 'tmp' and 'D' within their innermost loops to sum the results efficiently, letting the compiler decide the factor of parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n------------------\nDesign point:\n------------------\n__PARA__L0: 1\n__PARA__L1: 1\n__PARA__L2: 4\n__PARA__L3: 80\n__PARA__L4: 1\n__PARA__L5: 1\n__PIPE__L0: off\n__PIPE__L1: off\n__PIPE__L2: flatten\n__PIPE__L3: flatten\n__TILE__L0: 1\n__TILE__L1: 1\n__TILE__L2: 1\n__TILE__L3: 1"}
