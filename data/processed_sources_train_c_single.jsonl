{"instruction": "Generate HLS code with the following instructions:", "input": "This code provides functions for the encryption process using AES-256 (a symmetric block cipher with a 256-bit key). The AES 256-bit key consists of 32 characters where each character is an 8-bit unsigned char amounting to a total of 256 bits. The encryption process involves round key scheduling, round-based encryption, byte substitution, shift rows, mix columns, and a final stage of byte substitution and shift rows that is different from the process used in all previous rounds. \n\nA pragma directive indicates that the function \"aes256_encrypt_ecb\" should be treated as a hardware kernel for FPGA-based acceleration. \n\nAlso, two loops, identified by the \"pragma ACCEL PIPELINE auto\", are set to be automatically pipelined for further optimization. \n\nFinally, \"pragma ACCEL TILE FACTOR=auto\" is used regarding the loop \"ecb3\", responsible for the AES encryption rounds, suggesting that the compiler should analyze this loop and define an optimal chunk size to partition this loop for parallel execution.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code defines a function named 'ellpack' which is to be accelerated using dedicated hardware. It includes nested loops to perform multiplication and accumulation operations on specific elements of provided arrays. The HLS compiler pragmas are used for optimization:\n\n- '#pragma ACCEL kernel' is used to explicitly define 'ellpack' as a hardware kernel.\n\n- '#pragma ACCEL PIPELINE auto' instructs the compiler to automatically pipeline the first loop, enabling execution of multiple loop iterations concurrently with potential overlaps to enhance performance.\n\n- '#pragma ACCEL TILE FACTOR=auto' instructs the compiler to automatically configure data tiling for the first loop, breaking data into smaller chunks to make better use of the on-chip memory.\n\n- '#pragma ACCEL PARALLEL FACTOR=auto' instructs the compiler to automatically determine the parallelization factor for the first loop, indicating how many loop iterations can be executed simultaneously\n", "output": "#pragma ACCEL kernel\n\nvoid ellpack(double nzval[4940],int cols[4940],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  ellpack_1:\n  for (i = 0; i < 494; i++) {\n    double sum = out[i];\n    ellpack_2:\n    for (j = 0; j < 10; j++) {\n      Si = nzval[j + i * 10] * vec[cols[j + i * 10]];\n      sum += Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code defines a kernel function for a 2D Seidel algorithm which is treated as a hardware kernel function for acceleration. The function takes three parameters: the number of timesteps, the size of the matrix, and the 2D matrix itself. Three nested loops initialized with '#pragma ACCEL' commands are used, each with three sub-commands under '#pragma ACCEL': PIPELINE, TILE and PARALLEL. \n\nIn all loops, the PIPELINE command is set to 'auto', thus the compiler determines how to pipeline the iterations of each loop. For the TILE command, which divides each loop into smaller segments, it is also set to 'auto', allowing the compiler to pick the optimal segment size for each loop. For the PARALLEL command, which defines how many loop iterations to process in parallel, 'auto' setting also allows the compiler to define these parameters. \n\nThe nested loops iterate over the timesteps and the elements of the 2D matrix, updating each element of the matrix based on the surrounding elements. The loops for 'i' and 'j' exclude the first and last elements of the matrix. The parallel processing for each 'j' loop is also automatically managed by the compiler.", "output": "#pragma ACCEL kernel\n\nvoid kernel_seidel_2d(int tsteps,int n,double A[120][120])\n{\n  int t;\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (t = 0; t <= 39; t++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (i = 1; i <= 118; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n      for (j = 1; j <= 118; j++) {\n        A[i][j] = (A[i - 1][j - 1] + A[i - 1][j] + A[i - 1][j + 1] + A[i][j - 1] + A[i][j] + A[i][j + 1] + A[i + 1][j - 1] + A[i + 1][j] + A[i + 1][j + 1]) / 9.0;\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code written in C language describes a function named 'kernel_adi' which is marked as a hardware kernel for acceleration using '#pragma ACCEL kernel'. The function takes in several parameters including scalar values 'tsteps' and 'n' and 2D arrays 'u', 'v', 'p', and 'q'. There are several variables initialized within this function, after which it enters several nested loops.\n\nFor each of the outer most loops, they begin with a pipeline pragma, '#pragma ACCEL PIPELINE auto'. This directive implies that the compiler should automatically determine the optimal pipeline loop for better concurrency. \n\nFollowed by this directive, there's '#pragma ACCEL TILE FACTOR = auto', indicating that the compiler should automatically choose an appropriate tiling factor to partition the loop iterations into smaller chunks for data locality enhancement. \n\nThen there's '#pragma ACCEL PARALLEL FACTOR = auto', which instructs the compiler to automatically determine to what extent the loop should be run in parallel. \n\nInside these outer loops, there are multiple similar loop structures, still using those pragmas to enable efficient parallelization, pipelining and data locality optimizations. \n\nIn conclusion, with these pragma directives, the compiler is capable of automatically managing the pipelining, data partitioning and parallel execution of the loops to maximize performance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C code defines a function 'gemm' that performs General Matrix Multiply (GEMM) operation on two input arrays 'm1' and 'm2' of double values and saves the result in a 'prod' array. The function is marked with '#pragma ACCEL kernel' to be treated as a hardware kernel for acceleration. \n\nIn the function, three nested loops (outer, middle, inner) handle the matrix multiplication operation. For each loop, three pragma directives are used: \n1. '#pragma ACCEL PIPELINE auto': instructs the HLS tool to automatically pipeline the loop to improve throughput. \n2. '#pragma ACCEL TILE FACTOR=auto': instructs the HLS tool to automatically determine the best tiling factor (breaking the iterations of loop into smaller chunks) for optimising memory usage and data locality. \n3. '#pragma ACCEL PARALLEL FACTOR=auto': instructs the HLS tool to automatically determine the best parallel factor (amount of loop iterations executed in parallel) to optimise the loop execution. \n\nAdditionally, in the innermost loop, a reduction operation is performed to sum the result of the multiplication operation, which is indicated by '#pragma ACCEL PARALLEL reduction=sum'. The HLS tool is directed to automatically manage the parallelization of the reduction. \n\nThe loop indexes 'i', 'j', and 'k' are used to navigate through matrix elements. Results are stored back into 'prod' array.", "output": "#pragma ACCEL kernel\n\nvoid gemm(double m1[4096],double m2[4096],double prod[4096])\n{\n  int i;\n  int j;\n  int k;\n  int k_col;\n  int i_col;\n  double mult;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  outer:\n  for (i = 0; i < 64; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    middle:\n    for (j = 0; j < 64; j++) {\n      i_col = i * 64;\n      double sum = (double )0;\n      \n#pragma ACCEL PARALLEL reduction=sum FACTOR=auto{__PARA__L2}\n      inner:\n      for (k = 0; k < 64; k++) {\n        k_col = k * 64;\n        mult = m1[i_col + k] * m2[k_col + j];\n        sum += mult;\n      }\n      prod[i_col + j] = sum;\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code defines a function 'kernel_heat_3d'. We treat this function as a hardware kernel for acceleration with '#pragma ACCEL kernel'. This function performs a heat dissipation calculation in a 3D grid for a certain number of time steps. \n\nThe major operation is split into two parts, each involving four nested loops. We use '#pragma ACCEL PIPELINE auto', '#pragma ACCEL TILE FACTOR=auto' and '#pragma ACCEL PARALLEL FACTOR=auto' to direct HLS to optimize the execution of these loops.  \n\nThe compiler is instructed to automatically pipeline each loop to overlap the execution of loop iterations with 'auto' in '#pragma ACCEL PIPELINE'. The tiling factor is left for the HLS compiler to determine with 'auto' in '#pragma ACCEL TILE FACTOR' which allows the loop to be broken down into smaller pieces for better parallelism. \n\nThe same way, HLS compiler is directed to parallelize the loop iterations with 'auto' in '#pragma ACCEL PARALLEL FACTOR' which allows multiple iterations to be executed simultaneously. \n\nIn each loop, a grid cell's value is updated depending on its own current value and the values of its neighbors in the grid. This sequence of operations is performed repeatedly for 'tsteps' number of times.", "output": "#pragma ACCEL kernel\n\nvoid kernel_heat_3d(int tsteps,int n,double A[20][20][20],double B[20][20][20])\n{\n  int t;\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (t = 1; t <= 40; t++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (i = 1; i < 20 - 1; i++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n      for (j = 1; j < 20 - 1; j++) {\n        for (k = 1; k < 20 - 1; k++) {\n          B[i][j][k] = 0.125 * (A[i + 1][j][k] - 2.0 * A[i][j][k] + A[i - 1][j][k]) + 0.125 * (A[i][j + 1][k] - 2.0 * A[i][j][k] + A[i][j - 1][k]) + 0.125 * (A[i][j][k + 1] - 2.0 * A[i][j][k] + A[i][j][k - 1]) + A[i][j][k];\n        }\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    for (i = 1; i < 20 - 1; i++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L4}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L4}\n      for (j = 1; j < 20 - 1; j++) {\n        for (k = 1; k < 20 - 1; k++) {\n          A[i][j][k] = 0.125 * (B[i + 1][j][k] - 2.0 * B[i][j][k] + B[i - 1][j][k]) + 0.125 * (B[i][j + 1][k] - 2.0 * B[i][j][k] + B[i][j - 1][k]) + 0.125 * (B[i][j][k + 1] - 2.0 * B[i][j][k] + B[i][j][k - 1]) + B[i][j][k];\n        }\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code declares a function named \"kernel_gemver\", which will be considered as a hardware kernel for acceleration due to the pragma 'ACCEL kernel'. The function performs a series of matrix operations involving addition, multiplication and reduction.\n\nThere are four main loops in this function. For each loop, certain directives are applied:\n\n1. The first loop, running from i=0 to i=399, is enhanced with automatic pipelining, tiling and parallelization, as defined by the respective pragmas. Within this loop, there is a nested loop that works on each matrix element A[i][j], and performs addition and multiplication with the u1, v1, u2 and v2 arrays. This nested loop also uses an automatic parallel directive with a reduction on 'A'.\n\n2. The second loop performs similar operations as the first loop, but operates on the 'x' array and uses 'beta', 'A' and 'y' in its calculations. It also has automatic pipelining, tiling, and nested parallel reduction.\n\n3. The third loop, which only contains a single iteration running over 'x', has automatic parallelization, adding 'z[i]' to each 'x[i]'.\n\n4. The final loop operates on the 'w' array and also includes automatic pipelining, tiling, and nested parallel reduction, this time with the 'alpha', 'A[i][j]' and 'x[j]'.\n\nIn each instance, \"auto\" in '#pragma ACCEL' lets the HLS tool to determine the best factor or settings for optimization automatically.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemver(int n,double alpha,double beta,double A[400][400],double u1[400],double v1[400],double u2[400],double v2[400],double w[400],double x[400],double y[400],double z[400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction=A FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 400; j++) {\n      A[i][j] += + u1[i] * v1[j] + u2[i] * v2[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction=x FACTOR=auto{__PARA__L5}\n    for (j = 0; j < 400; j++) {\n      x[i] += beta * A[j][i] * y[j];\n    }\n  }\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 400; i++) {\n    x[i] = x[i] + z[i];\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction=w FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 400; j++) {\n      w[i] += alpha * A[i][j] * x[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C code defines a function called 'kernel_2mm'. This function is decorated with the pragma '#pragma ACCEL kernel', indicating that it should be treated as a hardware accelerated function. \n\nThe function performs a 2mm algorithm over multi-dimensional arrays. It first calculates alpha multiplied by the matrices A and B, and stores the result in a temporary array. Then, it multiplies the matrix D by beta. Finally, it adds the elements in temporary array multiplied with the matrix C to corresponding elements in matrix D.\n\nThe pragma directive '#pragma ACCEL PIPELINE auto' followed by an execution unit '__PIPE__Lx' is used around loops, suggesting that these loops are to be automatically pipelined, which checks whether loop iterations can be executed in parallel to speed up computation.\n\nThe '#pragma ACCEL TILE FACTOR=auto' followed by an execution unit '__TILE__Lx' is used to break up computation into smaller, manageable blocks, where 'auto' lets the accelerator determine the appropriate block size.\n\nThe '#pragma ACCEL PARALLEL FACTOR=auto' followed by an execution unit '__PARA__Lx' allows parallel computation across loop iterations, where factor 'auto' lets the accelerator determine the optimal number of iterations to be performed in parallel.\n\nWorkings inside the loop are also subject to further parallelism with '#pragma ACCEL PARALLEL reduction'. This directive is used to denote that all iterations of the loop can be safely computed in parallel, and results can be fetched using reduction operation, which combines all the elements of the array into a single value. The reduction operation here is addition being performed inside the loop, and the result is stored in the 'tmp' array and 'D' array respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{__PARA__L4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{__PARA__L5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The code starts by defining a hardware acceleration kernel function named 'kernel_fdtd_2d'. It takes in several arguments: tmax, nx, and ny as integer values, 3 two-dimensional arrays named ex, ey, and hz, and a one-dimensional array named '_fict_'. It is marked for processing by an FPGA or similar accelerator with `#pragma ACCEL kernel`.\n\nThe computations within the kernel are distributed across multiple for loops. The `#pragma ACCEL PIPELINE` directives are telling the compiler to optimize each of these loops by arranging their execution in a pipeline manner, allowing it to calculate commands from multiple loop iterations simultaneously when possible.\n\nThe `#pragma ACCEL TILE FACTOR` directives are informing the compiler to partition the computation for each loop into smaller chunks or \"tiles\", according to a factor automatically determined by the compiler.\n\nThe `#pragma ACCEL PARALLEL` directives are used to instruct the compiler to execute iterations of each loop in parallel if possible. Here, the parallel factor is automatically determined by the compiler.\n\nThe loops are nested and include calculations involving array elements, with some dependencies on previous values. We have a loop running for 't' iterations, loops running over 'j' and 'i' indices with various operation performed on the array's elements in each of these loops. For instance, within the first 'j' loop, array element `ey[0][j]` is being set to `_fict_[t]` and in the following 'i' and 'j' nested loop; `ey[i][j]` is being updated depending on its current value and values from the hz array. The process continues for ex and hz array updates.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_fdtd_2d(int tmax,int nx,int ny,double ex[200][240],double ey[200][240],double hz[200][240],double _fict_[100])\n{\n  int t;\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (t = 0; t < 100; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 240; j++) {\n      ey[0][j] = _fict_[t];\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (i = 1; i < 200; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n      for (j = 0; j < 240; j++) {\n        ey[i][j] = ey[i][j] - 0.5 * (hz[i][j] - hz[i - 1][j]);\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 200; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n      for (j = 1; j < 240; j++) {\n        ex[i][j] = ex[i][j] - 0.5 * (hz[i][j] - hz[i][j - 1]);\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L4}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 200 - 1; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7}\n      for (j = 0; j < 240 - 1; j++) {\n        hz[i][j] = hz[i][j] - 0.7 * (ex[i][j + 1] - ex[i][j] + ey[i + 1][j] - ey[i][j]);\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code block defines a hardware acceleration kernel named \"kernel_symm\". It calculates the value of the matrix C using a multi-nested for loop with specific settings enforced by the pragma keywords ACC PIPELINE, ACCEL TILE and ACCEL PARALLEL. Each pragma keyword has a factor set to 'auto' allowing the compiler to determine the optimal factor for pipelining, tiling and parallelizing. In the nested loop, a double variable named 'temp2' collects a sum of calculated results and it's used in the ACC PARALLEL pragma with the parameter reduction, thereby participating in a parallel computation. The outer loop and inner loop follow the same pragma settings. This HLS kernel performs an in-place computation on the matrix C, where C is updated for each iteration of the loops. The operations within the block of the 'k' loop are calculated in both serial and parallel manner depending on the condition 'k < i'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_symm(double alpha,double beta,double C[60][80],double A[60][60],double B[60][80])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//SIDE = 'L'\n//UPLO = 'L'\n// =>  Form  C := alpha*A*B + beta*C\n// A is MxM\n// B is MxN\n// C is MxN\n//note that due to Fortran array layout, the code below more closely resembles upper triangular case in BLAS\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 60; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 80; j++) {\n      double temp2 = ((double )0);\n      \n#pragma ACCEL PARALLEL reduction=temp2 FACTOR=auto{__PARA__L2}\n      for (k = 0; k < 60; k++) {\n        if (k < i) {\n          C[k][j] += alpha * B[i][j] * A[i][k];\n          temp2 += B[k][j] * A[i][k];\n        }\n      }\n      C[i][j] = beta * C[i][j] + alpha * B[i][j] * A[i][i] + alpha * temp2;\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code is for a function 'bbgemm' which calculates the multiplication of two arrays 'm1' and 'm2' and stores the result in 'prod'. The '#pragma ACCEL kernel' at the start marks this function as a hardware kernel for acceleration. For each of the nested loops within this function, different #pragma directives are provided. \n\nThe '#pragma ACCEL PIPELINE auto' tells the compiler to automatically determine whether to pipeline the processing of the loop iterations for optimized hardware resource usage. This auto feature expedites the computation by enabling concurrent execution of various loop iterations.\n\nThe '#pragma ACCEL TILE FACTOR=auto' tells the compiler to automatically decide the block size for the loop. This optimizes data reuse in hardware and helps in efficient local memory usage.\n\nIn the third nested loop, a '#pragma ACCEL PARALLEL FACTOR=auto' is specified which tells the compiler to automatically decide how many loop iterations can be executed in parallel. \n\nEach loop uses a combination of these directives for optimal execution on the hardware accelerator.", "output": "#pragma ACCEL kernel\n\nvoid bbgemm(double m1[4096],double m2[4096],double prod[4096])\n{\n  int i;\n  int k;\n  int j;\n  int jj;\n  int kk;\n  int i_row;\n  int k_row;\n  double temp_x;\n  double mul;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  loopjj:\n/* Standardize from: for(jj = 0;jj < 64;jj += 8) {...} */\n  for (jj = 0; jj <= 7; jj++) {\n    int _in_jj = 0 + 8L * jj;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    loopkk:\n/* Standardize from: for(kk = 0;kk < 64;kk += 8) {...} */\n    for (kk = 0; kk <= 7; kk++) {\n      int _in_kk = 0 + 8L * kk;\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n      loopi:\n      for (i = 0; i < 64; ++i) {\n        \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n        \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n        loopk:\n        for (k = 0; k < 8; ++k) {\n          i_row = i * 64;\n          k_row = (k + _in_kk) * 64;\n          temp_x = m1[i_row + k + _in_kk];\n          loopj:\n          for (j = 0; j < 8; ++j) {\n            mul = temp_x * m2[k_row + j + _in_jj];\n            prod[i_row + j + _in_jj] += mul;\n          }\n        }\n      }\n    }\n    kk = 56 + 8L;\n  }\n  jj = 56 + 8L;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS (High-Level Synthesis) code is implementing a symmetrical matrix multiplication hardware accelerated kernel function, specifically adapted for FPGAs, written in C. The function 'kernel_symm' performs multiplication of the matrices A and B, scaling the result by a factor of 'alpha', and adds it to the product of original matrix C and a factor 'beta'.\n\nThe pragma directives before and inside the loops use PIPELINE, TILE, and PARALLEL optimizations. These pragmas are set to 'auto' allowing the HLS tool to choose the best factor based on the loop structure, operation count, and data dependencies. \n\nThere are three nested for-loops. The outer-most loop handles rows of matrix B and regulates access to elements of matrix A. The second loop corresponds to columns of matrix C and the inner loop performs the operations of 'if' condition, and multiplication and addition of corresponding elements of matrix A and B. Pragma optimizations are applied to these loops to increase the computation parallelism and throughput.\n\nIn the inner loops, it uses reduction operation on 'temp2' variable to compute a sum of products of matrices B and A and assigns the final result to matrix C. \n\nHere are more details on the individual pragmas:\n\n1. '#pragma ACCEL kernel' instructs the compiler that the following function should be treated as a hardware kernel for FPGA acceleration.\n2. '#pragma ACCEL PIPELINE' is set to 'auto', specifying all the loops to be pipelined automatically. Pipelining improves throughput by allowing next iteration of the loop to start before the previous one has completed.\n3. '#pragma ACCEL TILE FACTOR=auto' allows the compiler to automatically select the best tile factor or loop unroll factor for the loops. Loop unrolling can reduce the loop control overhead and increase parallelism.\n4. '#pragma ACCEL PARALLEL FACTOR=auto' enables the parallel execution of iterations in the loops. It allows concurrent execution of multiple iterations for more efficient utilization of hardware resources and improved throughput.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_symm(double alpha,double beta,double C[60][80],double A[60][60],double B[60][80])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//SIDE = 'L'\n//UPLO = 'L'\n// =>  Form  C := alpha*A*B + beta*C\n// A is MxM\n// B is MxN\n// C is MxN\n//note that due to Fortran array layout, the code below more closely resembles upper triangular case in BLAS\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 60; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 80; j++) {\n      \n      double tmp = B[i][j];\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n      for (k = 0; k < 60; k++) {\n        if (k < i) {\n          C[k][j] += alpha * tmp * A[i][k];\n        }\n      }\n\n      double temp2 = ((double )0);\n#pragma ACCEL PARALLEL reduction=temp2 FACTOR=auto{__PARA__L3}\n      for (k = 0; k < 60; k++) {\n        if (k < i) {\n          temp2 += B[k][j] * A[i][k];\n        }\n      }\n      C[i][j] = beta * C[i][j] + alpha * B[i][j] * A[i][i] + alpha * temp2;\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code defines a kernel function `kernel_doitgen` by using '#pragma ACCEL kernel', where calculations are performed on multidimensional arrays 'A' and 'C4' and the results are stored in 'sum'. \n\nIn the function, there are nested loops iterating through the various dimensions of these arrays. The HLS software is instructed to automatically manage the pipelining of these loops by using '#pragma ACCEL PIPELINE auto'. \n\nEach loop is also instructed to have its iterations split into chunks by using '#pragma ACCEL TILE FACTOR=auto'. \n\nFinally, within the innermost loop, a reduction operation is performed on 'sum_tmp' using '#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto'. This involves a series of operations (additions and multiplications in this case) to accumulate a single reduced output. \n\nThe pragmas are specified automatically with the 'auto' specifier, allowing the HLS compiler to determine the optimal settings for parallelism, tiling, and pipelining.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        double sum_tmp = 0.0;\n#pragma ACCEL PARALLEL reduction=sum_tmp FACTOR=auto{__PARA__L3}\n\tfor (s = 0; s < 30; s++) {\n          sum_tmp += A[r][q][s] * C4[s][p];\n        }\n\tsum[p] = sum_tmp;\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The provided HLS C code carries out a Molecular Dynamics (MD) simulation. The function 'md_kernel' is treated as a hardware kernel function for acceleration using '#pragma ACCEL kernel'. It calculates the forces acting on each particle within a three-dimensional space (x,y,z coordinates).\n\nThe variables delx, dely, and delz calculate the relative distance between two particles i and j. These distances are then used to calculate the forces acting upon particles under consideration. The final forces are subsequently updated for each particle after all their respective neighbors are accounted for.\n\nThree specific #pragma values are used in the loop operations. Firstly, '#pragma ACCEL PIPELINE auto{__PIPE__L0}' enables automatic pipeline optimization, which may allow concurrent execution of loop iterations for improved performance. \n\nSecondly, '#pragma ACCEL TILE FACTOR=auto{__TILE__L0}' applies an automatic tiling optimization factor, which helps manage data locality for improved memory access efficiency.\n\nFinally, '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' allows iteration of the loop to operate in parallel, improving the performance by executing multiple iterations simultaneously, depending on the automatically determined optimal factor for parallelism.", "output": "#pragma ACCEL kernel\n\nvoid md_kernel(double force_x[256],double force_y[256],double force_z[256],double position_x[256],double position_y[256],double position_z[256],int NL[4096])\n{\n  double delx;\n  double dely;\n  double delz;\n  double r2inv;\n  double r6inv;\n  double potential;\n  double force;\n  double j_x;\n  double j_y;\n  double j_z;\n  double i_x;\n  double i_y;\n  double i_z;\n  double fx;\n  double fy;\n  double fz;\n  int i;\n  int j;\n  int jidx;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  loop_i:\n  for (i = 0; i < 256; i++) {\n    i_x = position_x[i];\n    i_y = position_y[i];\n    i_z = position_z[i];\n    fx = ((double )0);\n    fy = ((double )0);\n    fz = ((double )0);\n    loop_j:\n    for (j = 0; j < 16; j++) {\n// Get neighbor\n      jidx = NL[i * 16 + j];\n// Look up x,y,z positions\n      j_x = position_x[jidx];\n      j_y = position_y[jidx];\n      j_z = position_z[jidx];\n// Calc distance\n      delx = i_x - j_x;\n      dely = i_y - j_y;\n      delz = i_z - j_z;\n      r2inv = 1.0 / (delx * delx + dely * dely + delz * delz);\n// Assume no cutoff and aways account for all nodes in area\n      r6inv = r2inv * r2inv * r2inv;\n      potential = r6inv * (1.5 * r6inv - 2.0);\n// Sum changes in force\n      force = r2inv * potential;\n      fx += delx * force;\n      fy += dely * force;\n      fz += delz * force;\n    }\n//Update forces after all neighbors accounted for.\n    force_x[i] = fx;\n    force_y[i] = fy;\n    force_z[i] = fz;\n//printf(\"dF=%lf,%lf,%lf\\n\", fx, fy, fz);\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS (high-level synthesis) code in C defines a hardware kernel named `kernel_doitgen`. In this kernel, four nested loops are processed. They are pipelined and automatically tiled with pipeline and tile pragma directives, `#pragma ACCEL PIPELINE auto` and `#pragma ACCEL TILE FACTOR=auto`, respectively. In these loops, the 3-dimensional array `A` is multiplied by the 2-dimensional matrix `C4` and stored into the array `sum`. The product is then reassigned to the array `A`. Auto pragma for pipeline and tile optimizations allows the HLS compiler to determine the suitable factor for each loop to achieve optimal performance, provided by the following `auto{__PIPE__L0}`, `auto{__TILE__L0}`, `auto{__PIPE__L1}`, `auto{__TILE__L1}`, `auto{__PIPE__L2}`, and `auto{__TILE__L2}` directive values. To allow this, the `auto` value is set. The entire function is considered for hardware acceleration with the `#pragma ACCEL kernel` directive.", "output": "#pragma ACCEL kernel\n\nvoid kernel_doitgen(int nr,int nq,int np,double A[25][20][30],double C4[30][30],double sum[30])\n{\n  int r;\n  int q;\n  int p;\n  int s;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  for (r = 0; r < 25; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (q = 0; q < 20; q++) {\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n      for (p = 0; p < 30; p++) {\n        sum[p] = 0.0;\n        for (s = 0; s < 30; s++) {\n          sum[p] += A[r][q][s] * C4[s][p];\n        }\n      }\n      for (p = 0; p < 30; p++) {\n        A[r][q][p] = sum[p];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The HLS code provided defines a hardware-accelerated kernel function named 'kernel_mvt' which takes five 2D arrays as parameters: x1, x2, y_1, y_2, and A. \n\nThis kernel has two primary loops with each loop containing a nested loop. \n\nThe first #pragma tag 'ACCEL PIPELINE auto{__PIPE__L0}' describes that the loop process should be pipelined in an automated manner, the HLS tool decides an optimal stage of the pipeline. \n\nThis is followed by a \"#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\" directive, which indicates that the loop should be tiled with the optimal size chosen by the HLS tool.\n\nNext, '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0} indicates that HLS tool should decide the best/optimal value for executing loop iterations in parallel. \n\nThe loop iterates through the 'x1' array, performing a reduction operation which is signified by '#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}' where the reduction factor is automatically determined by the accelerator.\n\nThe second loop set has a similar structure, just as the first loop set, using the respective #pragma tags as mentioned earlier, the only difference is the reduction operation is performed on 'x2' array, which is indicated by '#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}' directive.", "output": "#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[120], double x2[120], double y_1[120], double y_2[120], double A[120][120])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 120; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 120; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 120; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 120; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The code corresponds to a hardware accelerated function, calculated to find the covariance of a dataset. The '#pragma ACCEL kernel' directive is used to denote that the function definition that follows has been optimized. Inside the function:\n\n- Firstly, it calculates the mean of each column of the dataset using an automatically pipelined, tiled, and parallelized loop. The 'reduction=mean' directive signifies mean value is being calculated as an aggregate result of the loop. The 'FACTOR=auto{__PARA__L3}' will automatically decide the factor of parallelism.\n\n- Then, it adjusts the dataset by subtracting the respective column means, using another automatically pipelined, tiled, and parallelized loop with 'reduction=data' directive.\n\n- Lastly, it calculates the covariance for each pair of columns using a set of three nested loops with the same automatic optimizations. The innermost loop calculates the sum of pairwise multiple of data elements, giving us the covariance. Again, the factor of parallel operations and tiling factor are configured automatically by the compiler.\n\nBy using this pragma directives, the loops are automatically optimized in a way that exploits parallelism and locality for maximizing the performance on a particular machine. This is achieved by pipelining, tiling, and performing operations in parallel \u2013 where appropriate.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_covariance(int m,int n,double float_n,double data[100][80],double cov[80][80],double mean[80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=mean FACTOR=auto{__PARA__L3}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL reduction=data FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    for (j = i; j < 80; j++) {\n      cov[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction=cov FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 100; k++) {\n        cov[i][j] += data[k][i] * data[k][j];\n      }\n      cov[i][j] /= float_n - 1.0;\n      cov[j][i] = cov[i][j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code defines a hardware accelerated kernel known as \"kernel_jacobi_2d\". The computations in the provided 2-dimensional arrays of data (A and B) are automatically pipelined, tiled, and parallelized in the inner loops for optimization purposes using the listed pragmas. The provided pragmas use a 'auto' flag which allows HLS to automatically determine the best strategy for pipelining, tiling and parallelizing. With the pragma PIPELINE, the loops are executed in an overlapped manner for efficient use of hardware resources. The pragma TILE helps in breaking data into smaller chunks to exploit data locality and the pragma PARALLEL helps in parallelizing the loop iterations. This kernel performs a jacobi computation over the input arrays, implementing a numerical method for solving differential equations. It involves nested loops where each cell in the matrix A and B is used to compute an associated value in the next iteration. The pragmas apply to each loop to accelerate the computations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_2d(int tsteps,int n,double A[90][90],double B[90][90])\n{\n  int t;\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (i = 1; i < 89; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n      for (j = 1; j < 89; j++) {\n        B[i][j] = 0.2 * (A[i][j] + A[i][j - 1] + A[i][1 + j] + A[1 + i][j] + A[i - 1][j]);\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (i = 1; i < 89; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n      for (j = 1; j < 89; j++) {\n        A[i][j] = 0.2 * (B[i][j] + B[i][j - 1] + B[i][1 + j] + B[1 + i][j] + B[i - 1][j]);\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code is a hardware acceleration kernel function named \"kernel_fdtd_2d\". It takes several parameters including arrays \"ex\", \"ey\", and \"hz\" and computes Finite Difference Time Domain (FDTD) operations over them in 2D space.\n\nThere are various sections with different '#pragma ACCEL' directives. Each time such a directive is encountered, it's an instruction to the compiler.\n\n- `#pragma ACCEL PIPELINE auto`: This is instructing the compiler to pipeline iterations of the loop, with the determination of execution details left to the compiler's autopilot feature.\n\n- `#pragma ACCEL TILE FACTOR=auto`: It suggests dividing the work into smaller portions or \"tiles\" to be processed separately. The size of these portions is decided automatically by the HLS compiler.\n\n- `#pragma ACCEL PARALLEL FACTOR=auto`: It allows parallel execution of various iterations, with autopilot determining the level of parallelism.\n\nThese directives are used multiple times for different loops (L0, L0_0, L0_1, L0_2, and L0_3) in the function, aiming to take advantage of parallel computation and pipelining to speed up the execution of iterations in each loop based on auto-generated factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_fdtd_2d(int tmax,int nx,int ny,double ex[60][80],double ey[60][80],double hz[60][80],double _fict_[40])\n{\n  int t;\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_0}\n    for (j = 0; j < 80; j++) {\n      ey[0][j] = _fict_[t];\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L0_1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0_1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_1}\n    for (i = 1; i < 60; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_1_0}\n      for (j = 0; j < 80; j++) {\n        ey[i][j] = ey[i][j] - 0.5 * (hz[i][j] - hz[i - 1][j]);\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L0_2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0_2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_2}\n    for (i = 0; i < 60; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_2_0}\n      for (j = 1; j < 80; j++) {\n        ex[i][j] = ex[i][j] - 0.5 * (hz[i][j] - hz[i][j - 1]);\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L0_3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0_3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_3}\n    for (i = 0; i < 59; i++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0_3_0}\n      for (j = 0; j < 79; j++) {\n        hz[i][j] = hz[i][j] - 0.7 * (ex[i][j + 1] - ex[i][j] + ey[i + 1][j] - ey[i][j]);\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C code represents an HLS kernel which performs the computation for symmetric matrix multiplications. The 'kernel_symm' function is a hardware-accelerated kernel, which is indicated by the pragma 'ACCEL kernel' directive. In the function, there are three nested loops with various pragmas inside them.\n\nThree different pragmas are used to direct the HLS compiler:\n\n1. 'ACCEL PIPELINE auto' indicates that pipelining of the loops should be performed optimally by the compiler, improving overall execution speed.\n\n2. 'ACCEL TILE FACTOR=auto' indicates that the compiler should auto-tune the tiling factor of the loops to ensure optimal data locality.\n\n3. 'ACCEL PARALLEL FACTOR=auto' directs the compiler to execute multiple iterations of the loop concurrently based on its own analysis.\n\nIn the innermost loops, 'ACCEL PARALLEL reduction' is used to ensure the correct partial summation operation when the iterations are run in parallel.\n\nRoughly, the function performs, C matrix as alpha * A matrix * B matrix + beta * C matrix, where A is a symmetric matrix. For an operation, it performs this for when k is less than i, this is to ensure operations are performed on elements on and above the main diagonal of the symmetric A matrix. Furthermore, partial summations are stored in variables tmp and temp2 to assist the computation. Overall, this kernel leverages hardware acceleration to optimize matrix multiplication operations.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_symm(double alpha,double beta,double C[200][240],double A[200][200],double B[200][240])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//SIDE = 'L'\n//UPLO = 'L'\n// =>  Form  C := alpha*A*B + beta*C\n// A is MxM\n// B is MxN\n// C is MxN\n//note that due to Fortran array layout, the code below more closely resembles upper triangular case in BLAS\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 240; j++) {\n      double tmp = B[i][j];\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L2}\n      for (k = 0; k < 200; k++) {\n        if (k < i) {\n          C[k][j] += alpha * tmp * A[i][k];\n        }\n      }\n      double temp2 = (double )0;\n      \n#pragma ACCEL PARALLEL reduction=temp2 FACTOR=auto{__PARA__L3}\n      for (k = 0; k < 200; k++) {\n        if (k < i) {\n          temp2 += B[k][j] * A[i][k];\n        }\n      }\n      C[i][j] = beta * C[i][j] + alpha * B[i][j] * A[i][i] + alpha * temp2;\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code defines a hardware accelerated function called 'kernel_jacobi_1d' which is used to perform the Jacobi 1D computation. It has used several pragma directives for optimization purposes. \n\nFirstly, '#pragma ACCEL PIPELINE auto{__PIPE__L0}' means that this code segment should be automatically pipelined. The accelerator assesses and implements the optimal degree of pipelining. \n\nSecondly, using '#pragma ACCEL TILE FACTOR=auto{__TILE__L0}', the tool is instructed to automatically tile the loop which essentially breaks the loop into smaller chunks and processes these chunks in isolation to improve data locality and parallelism.\n\nNext, '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' tells the compiler to automatically parallelize this loop. The accelerator determines an appropriate degree of parallelism.\n\nWithin the primary loop, there are two nested loops. Both loops calculate a discrete 1D convolution and are parallelized using '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' respectively, instructing the compiler once again to determine the optimal degree of parallelism. \n\nIn short, these directives make the accelerator understand how to optimize the function for better performance.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_jacobi_1d(int tsteps,int n,double A[120],double B[120])\n{\n  int t;\n  int i;\n//#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (t = 0; t < 40; t++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (i = 1; i < 120 - 1; i++) {\n      B[i] = 0.33333 * (A[i - 1] + A[i] + A[i + 1]);\n    }\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (i = 1; i < 120 - 1; i++) {\n      A[i] = 0.33333 * (B[i - 1] + B[i] + B[i + 1]);\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code defines a function 'kernel_gesummv', which is being treated as a hardware kernel to boost its processing efficiency. The function performs a combination of multiplication and addition tasks on two 250x250 element matrices (A and B) and vector (x), it outputs the result to vectors 'tmp' and 'y'. The kernel uses three types of pragmas to optimize the loop operations: Pipeline, Tile, and Parallel. \n\nThe outer 'i' loop and inner 'j' loop are wrapped with Pragma ACCEL PIPELINE, TILE and PARALLEL directives. The PIPELINE directive ensures that the operations within the loop are scheduled automatically so as to make optimal utilisation of resources and reduce the latency. The TILE directive breaks the loop into smaller blocks to improve memory access patterns and hence the performance. The PARALLEL directive allows the iterations of the 'i' and 'j' loops to be executed concurrently where possible. The values for these directives are set to 'auto', enabling the compiler to automatically decide the optimal factor values. \n\nIn detail, for each 'i', 'tmp' and 'y' are first initialized to 0. Then in the inner 'j' loop, 'tmp' is added with the product of elements from matrix 'A' and vector 'x' while 'y' is added with the product of elements from matrix 'B' and vector 'x'. Finally, 'y' is updated with a linear combination of 'alpha', 'tmp', 'beta' and 'y' itself.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_gesummv(double alpha,double beta,double A[250][250],double B[250][250],double tmp[250],double x[250],double y[250])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 250; i++) {\n    tmp[i] = 0.0;\n    y[i] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 250; j++) {\n      tmp[i] += A[i][j] * x[j];\n      y[i] += B[i][j] * x[j];\n    }\n    y[i] = alpha * tmp[i] + beta * y[i];\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code describes a kernel function `kernel_bicg` that works on two-dimensional matrix `A` and vectors `s`, `q`, `p` & `r`. The `#pragma ACCEL kernel` directive is used to identify this function as a hardware-accelerated kernel. This code initializes all elements of `s` and `q` array to 0. The loop for initializing `s` array has the optimization directive `#pragma ACCEL PARALLEL FACTOR=auto`, which means iterations of the loop get parallelized for performance enhancement. \n\nNext comes the nested loop where two operations are carried out. First, each element `s[j]` is incremented with the product of `r[i]` and `A[i][j]`. Second, each `q[i]` is incremented with the product of `A[i][j]` and `p[j]`. \n\nThe outer loop (`i` loop) is pipelined with `#pragma ACCEL PIPELINE auto` for automatic optimization of loop dataflow with maximum concurrency. Fine-grained memory partitioning is applied using `#pragma ACCEL TILE FACTOR=auto` to automatically break down the array into smaller chunks enhancing parallel execution. The `#pragma ACCEL PARALLEL FACTOR=auto` assures parallel execution of the iterations of the outer loop. The inner loop (`j` loop) is further optimized with `#pragma ACCEL PARALLEL reduction FACTOR=auto` which uses parallel execution and leverages reduction optimization where simultaneous read/write operation occurs. All `auto` tags indicate automatic factor determination for optimal performance by the compiler.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[410][390],double s[390],double q[410],double p[390],double r[410])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 390; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 410; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 390; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code is a hardware accelerator kernel defined by the '#pragma ACCEL kernel' directive in C language. The function, named 'kernel_atax', takes integer inputs 'm' and 'n', and arrays 'A', 'x', 'y' and 'tmp' of type double. \n\nInitially, it sets all the elements of the 'y' array to zero. Then, an outer loop iterates over each element in the 'tmp' array, setting all values to 0.0.\n\nFollowing this, there are two nested loops. The first nested block is marked by '#pragma ACCEL PARALLEL reduction=tmp' that instructs ACCEL to execute the loop iterations in parallel. The loop is designed to handle overall reduction operation on the 'tmp' array, where each 'tmp' array index is computed as the sum product of 'x' array with the 'A' array.\n\nSubsequently, the second block is denoted by the '#pragma ACCEL PARALLEL reduction=y'. It tells ACCEL to perform parallel computations on the 'y' array, where each 'y' array index is computed as the sum product of 'tmp' array with the 'A' array. \n\nThe pragma directives 'PIPELINE' and 'TILE' set to 'auto' direct ACCEL to automatically handle pipelining and tiling decisions, while the 'PARALLEL' factor also auto-tuned to handle parallelizing decisions for efficient hardware acceleration. Each nesting level of the loop is assigned a unique identifer, represented as '{__PARA__L0_0}' and '{__PARA__L0_1}', signifying different levels of computation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_atax(int m,int n,double A[116][124],double x[124],double y[124],double tmp[116])\n{\n  int i;\n  int j;\n//#pragma scop\n  for (i = 0; i < 124; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 116; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{__PARA__L0_0}\n    for (j = 0; j < 124; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{__PARA__L0_1}\n    for (j = 0; j < 124; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C code is defining an acceleration hardware kernel function named 'kernel_gesummv'. Here, two nested for-loops are implemented with automatic pragma values based on various performance metrics for optimization. \n\nThe '#pragma ACCEL PIPELINE auto{__PIPE__L0}' allows the compiler to automatically determine the pipelining of the first loop. '#pragma ACCEL TILE FACTOR=auto{__TILE__L0}' lets the compiler decide how to break the code into smaller, manageable pieces. \n\nThen, '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' is used on the outer loop i to determine the parallel factor automatically for better performance.\n\nIn the nested loop, the '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' directive is applied for the inner loop j, instructing the compiler to decide the level of parallelization for this loop. \n\nIn these loops, two array calculations occur, and finally, elements of the 'y' array are updated using calculated 'tmp' values with given 'alpha' and 'beta' coefficients.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gesummv(int n,double alpha,double beta,double A[90][90],double B[90][90],double tmp[90],double x[90],double y[90])\n{\n  int i;\n  int j;\n  \n#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 90; i++) {\n    tmp[i] = 0.0;\n    y[i] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 90; j++) {\n      tmp[i] = A[i][j] * x[j] + tmp[i];\n      y[i] = B[i][j] * x[j] + y[i];\n    }\n    y[i] = alpha * tmp[i] + beta * y[i];\n  }\n  \n#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The code is a kernel function named 'kernel_bicg' in C language which uses parallel computing to accelerate the computations. This function applies High Level Synthesis (HLS) pragmas.\n\nMark the function as a hardware kernel for acceleration with `#pragma ACCEL kernel`. \n\nIt initializes the values of array 's' to zero in parallel, which is denoted by `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`. The `FACTOR=auto` lets the compiler determine the optimal degree of parallelism.\n\nThen, the code applies three pragmas: `#pragma ACCEL PIPELINE auto{__PIPE__L1}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`. These pragmas optimize the following loop, which multiplies elements of matrix 'A' and vectors 'r' and 'p', and stores to vectors 's' and 'q'. \n\nThe `PIPELINE` pragma automatically manages the concurrent execution of iterations, and `TILE` pragma breaks the loop iterations into smaller groups or tiles to be processed. `PARALLEL` pragma allows iterations of this loop to be processed concurrently. Final pragma, `#pragma ACCEL PARALLEL reduction FACTOR=auto{__PARA__L2}` performs parallel reduction operation on the enclosed loop. The `FACTOR=auto` allows the HLS tool to automatically decide the number of parallel processing elements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[124][116],double s[116],double q[124],double p[116],double r[124])\n{\n  int i;\n  int j;\n//#pragma scop\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 116; i++) {\n    s[i] = ((double )0);\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 124; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 116; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given HLS code in C generates a hardware kernel for a `kernel_syr2k` function with parameters of type double including `alpha` & `beta` and 2D arrays `A`, `B` & `C`. Inside the function, it utilizes 3 nested loops with specific ACCEL directives or pragmas for each loop.\n\nThe first loop iterates over the elements of the array 'C' and if the condition 'j <= i' is true, each element of 'C' is multiplied by 'beta'. The loop is optimized with pipeline, tile and parallel pragmas with auto factors.\n\nThe second loop performs a similar operation as the first loop with array 'C', but this time it involves arrays 'A', 'B', 'C' and the variables 'alpha', 'beta'. If the condition 'j <= i' is met, the elements of 'C' are increased by the product of corresponding elements of 'A' and 'B' multiplied by 'alpha', and summed with the product of 'B', 'A' and 'alpha'. This loop is also optimized with pipeline, tile and parallel pragmas with auto factors. \n\nThe '#pragma ACCEL kernel' is used to specify that the function is to be treated as a hardware kernel for acceleration. '#pragma ACCEL PIPELINE auto' is used to enable automatic pipeline optimization. '#pragma ACCEL TILE FACTOR=auto' is used to indicate that the factor for loop tiling is automatically determined. '#pragma ACCEL PARALLEL FACTOR=auto' is used to indicate that the factor for loop parallelization is automatically determined.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_syr2k(double alpha,double beta,double C[80][80],double A[80][60],double B[80][60])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//UPLO  = 'L'\n//TRANS = 'N'\n//A is NxM\n//B is NxM\n//C is NxN\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 80; j++) {\n      if (j <= i) {\n        C[i][j] *= beta;\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 60; k++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 80; j++) {\n        if (j <= i) {\n          C[i][j] += A[j][k] * alpha * B[i][k] + B[j][k] * alpha * A[i][k];\n        }\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given HLS code defines a hardware acceleration function named `kernel_bicg`. This function takes two integers, `m` and `n`, and five two-dimensional arrays `A`, `s`, `q`, `p`, and `r` as input. The kernel declaration informs the compiler to treat this function as a hardware kernel. Furthermore, the function initializes each element of array `s` to 0.\n\nThen the kernel uses the automated pipeline and parallel processing technique to optimize the iteration for the next 410 elements of array `s`: `s[i]` each are initialized as 0. These steps utilize hardware resources for better throughput and performance.\n\nThe 'TILE' pragma tells the compiler to break the workload into smaller chunks for optimized scheduling.\n\nThe inner loop of `j` is parallelly processed by applying a reduction operation to every 390 iterations, and auto-assigns the parallel factor for hardware resource allocation.\n\nThe reduction operation computes the sum of `r[i]*A[i][j]` and adds it to `s[j]`, while simultaneously it computes the sum of `A[i][j]*p[j]` and adds it to `q[i]`. This makes use of the parallel processing potential of faster data computation.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_bicg(int m,int n,double A[410][390],double s[390],double q[410],double p[390],double r[410])\n{\n  int i;\n  int j;\n  for (i = 0; i < 390; i++) \n    s[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 410; i++) {\n    q[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 390; j++) {\n      s[j] += r[i] * A[i][j];\n      q[i] += A[i][j] * p[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code, written in C, depicts a kernel function named 'kernel_atax' which accepts four arrays - A, x, y, and tmp, as input parameters. The task of this function is to perform special operations on these arrays based on a series of nested for-loops.\n\nThe kernels are defined and then called with the '#pragma ACCEL kernel' directive stating that the consequent function will be treated as a hardware kernel for acceleration. Initially, the 'y' array is reset, with each element assigned to 0. \n\n'#pragma ACCEL PIPELINE auto{__PIPE__L0}', '#pragma ACCEL TILE FACTOR=auto{__TILE__L0}', and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' directives are then used in the first loop. These mean that the compiler will decide how to automatically pipeline this loop, how to treat this loop in its optimal form regarding dividing it into smaller chunks or tiles, and how to decide the number of iterations that should be executed in parallel.\n\nThe 'tmp' array is also reset to 0 in the first loop. In the nested loops, the elements of the 'tmp' array and 'y' array are updated in each iteration by using the results of dot product operations on the 'A' and 'x' matrices, and on the 'A' and 'tmp' matrices respectively. \n\nTowards this end, '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{__PARA__L1}' and '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{__PARA__L2}' directives are used to create a parallelized sum-of-products computation, with automatic parallelization factor decided by the compiler. The 'reduction' keyword here signifies that a particular array (either 'tmp' or 'y') should do a reduction operation, in this case, a sum, over the iterations.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given code is a kernel function, named \"kernel_correlation\", which is meant to be accelerated in a hardware implementation. This function takes as inputs a double type variable \"float_n\", a 2D array \"data[100][80]\", two 1D arrays \"mean[80]\" and \"stddev[80]\". It produces a 2D array \"corr[80][80]\" which is the result of a correlation computation.\n\n1. At the start, integer variables i, j, and k are declared along with a double epsilon eps set to 0.1.\n\n2. Pragma ACCEL directives are used to facilitate auto-tuning for the processing of the two outer loops. The first primary loop iterates over j from 0 to 80, calculating the mean for each column of the data. The ACCEL PIPELINE, ACCEL TILE FACTOR, and ACCEL PARALLEL FACTOR are set to \"auto\", allowing the compiler to determine the optimal values. An internal loop on i sums the column values, with parallel processing also auto-tuned by the pragma directive.\n\n3. Another set of loops computes the standard deviation for each column, again applying the directives for auto-tuned parallel processing, tiling, and pipelining.\n\n4. In the next section, the code \"centers and reduces\" the column vectors of the matrix, meaning it subtracts the mean from each data point and divides the result by the standard deviation times the square root of float_n.\n\n5. Finally, a correlation matrix for the data is computed. The correlation is calculated for each pair of columns in the data. All the ACCEL directives are used inside this section of the loops to assist auto-tuning.\n\nIn summary, the HLS code is used to generate a correlation matrix for a given dataset, where it employs several Pragma ACCEL directives for auto-tuning the parallelization, tiling, and pipelining of the for-loops to optimize the computation process.", "output": "#include <math.h>\n#pragma ACCEL kernel\n\nvoid kernel_correlation(double float_n,double data[100][80],double corr[80][80],double mean[80],double stddev[80])\n{\n  int i;\n  int j;\n  int k;\n  double eps = 0.1;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (j = 0; j < 80; j++) {\n    mean[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (i = 0; i < 100; i++) {\n      mean[j] += data[i][j];\n    }\n    mean[j] /= float_n;\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (j = 0; j < 80; j++) {\n    stddev[j] = 0.0;\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (i = 0; i < 100; i++) {\n      stddev[j] += pow(data[i][j] - mean[j],(double )2);\n    }\n    stddev[j] /= float_n;\n    stddev[j] = sqrt(stddev[j]);\n/* The following in an inelegant but usual way to handle\n         near-zero std. dev. values, which below would cause a zero-\n         divide. */\n    stddev[j] = (stddev[j] <= eps?1.0 : stddev[j]);\n  }\n/* Center and reduce the column vectors. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 100; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 80; j++) {\n      data[i][j] -= mean[j];\n      data[i][j] /= sqrt(float_n) * stddev[j];\n    }\n  }\n/* Calculate the m * m correlation matrix. */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n  for (i = 0; i < 80 - 1; i++) {\n    corr[i][i] = 1.0;\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L7}\n    for (j = i + 1; j < 80; j++) {\n      corr[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7_0}\n      for (k = 0; k < 100; k++) {\n        corr[i][j] += data[k][i] * data[k][j];\n      }\n      corr[j][i] = corr[i][j];\n    }\n  }\n  corr[80 - 1][80 - 1] = 1.0;\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Begin by marking the function 'kernel_gemm' as a hardware kernel for acceleration. This function is intended to perform matrix multiplication using a series of nested 'for' loops, where 'ni', 'nj', and 'nk' represent dimensions of matrices A, B, and C respectively, and 'alpha' and 'beta' are constant factors. \n\nFor the outermost loop, use plugins for pipeline, tile, and parallel factor pragmas to automate the performance tuning. The '#pragma ACCEL PIPELINE auto{__PIPE__L0}', '#pragma ACCEL TILE FACTOR=auto{__TILE__L0}', and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' directives instruct the compiler to automatically determine the optimal values for pipelining, tile factor, and parallel factor for the loop that iterates over the variable 'i'. \n\nFor the second loop, use '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' to automatically decide the level of parallelization for the loop handling the variable 'j'. This loop scales every element of matrix C by the factor 'beta'.\n\nFor the third 'for' loop, use the automated pipeline, tile, and parallel factor plugins. These directives instruct the compiler to automatically optimize the loop iterating over 'k'', which performs the matrix multiplication of A and B and accumulates the sum into matrix C. \n\nThen for the innermost loop, use '#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}' to automatically determine the best level of parallelization for the loop running over 'j'. This loop performs pairwise element multiplication of matrices A and B, scales it by the factor 'alpha', and incrementally adds the results to the corresponding element in matrix C. \n\nFinally, indicate the closing of the scope for pragma optimization by using '#pragma endscop'. This informs the compiler to stop applying any further pragma directives beyond this point.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[60][70],double A[60][80],double B[80][70])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 60; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 70; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 80; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 70; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The provided code is a hardware accelerated implementation of the SYRK (Symmetric Rank-k update) algorithm in BLAS (Basic Linear Algebra Subprograms). First, it utilizes `#pragma ACCEL kernel` to signify that the following function, `kernel_syrk()`, is denoted as a hardware acceleration kernel. Two double precision values `alpha` and `beta`, along with two 2D double precision matrices `C` and `A`, are inputs to this function. The algorithm forms `C` based on the formula `C := alpha*A*A**T + beta*C`, with `A` being an NxM matrix and `C` being an NxN matrix.\n\nThe outermost code loop is pipelined, tiled, and parallelized with auto factors, as dictated by `#pragma ACCEL PIPELINE auto{__PIPE__L0}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}`, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}` respectively, allowing the compiler to decide the best parameters.\n\nFor each value of `i` from 0 to 79, the first inner loop iterates over `j` from 0 to 79, scaling `C[i][j]` by `beta` if `j` is within `i`. The level of parallelism for this loop is automatically set by `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`.\n\nThe second inner loop set, executed for each `i`, iterates over `k` from 0 to 59 and `j` from 0 to 79. For each `j` within `i`, `C[i][j]` is updated by the value `alpha * A[i][k] * A[j][k]`. The level of parallelism and other parameters for these loops are automatically determined, as specified by `#pragma ACCEL PIPELINE auto{__PIPE__L2}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` for the `k` loop, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}` for the `j` loop inside the `k` loop.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_syrk(double alpha,double beta,double C[80][80],double A[80][60])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANS = 'N'\n//UPLO  = 'L'\n// =>  Form  C := alpha*A*A**T + beta*C.\n//A is NxM\n//C is NxN\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 80; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 80; j++) {\n      if (j <= i) {\n        C[i][j] *= beta;\n      }\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 60; k++) {\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 80; j++) {\n        if (j <= i) {\n          C[i][j] += alpha * A[i][k] * A[j][k];\n        }\n      }\n    }\n  }\n}\n"}
