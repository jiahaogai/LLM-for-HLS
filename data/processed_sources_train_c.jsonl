{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, ensure that all loops are executed sequentially without pipeline optimizations, maintain the loops in their original size without tiling, and keep loop iterations running serially except for the third nested loop within the second and third major loop blocks\u2014where the iterations should be parallelized with factors of 10 and 50, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a matrix multiplication kernel that treats the function as a hardware kernel for acceleration. Disable automatic pipelining for the top-level loops but allow for a flattened pipelining of the second-level loop in the first matrix multiplication (E:=A*B). Preserve the original loop structure without tiling and ensure all loop iterations run sequentially without parallelism for all loops except the innermost loop in the second matrix multiplication (F:=C*D), where you double its parallelism factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware kernel while disabling automatic pipelining, ensuring that all loops run without tiling or parallel processing except for certain sub-loops within 'F := C*D' and 'G := E*F', which should perform limited parallel computation with factors of 10 and 25, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function `kernel_3mm` as a hardware acceleration kernel, and within the three nested loops for the matrix multiplications `E = A*B`, `F = C*D`, and `G = E*F`, disable automatic pipelining, prevent tiling of the loops, and for the innermost loops, configure parallel processing with a single iteration for the first multiplication, 16 iterations for the second, and 50 iterations for the third.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel function that adheres to the following constraints: define the function as a hardware acceleration kernel, completely disable automatic pipelining for all loops, prevent subdivision of loops into smaller chunks by fixing tiling factors to 1, and enforce sequential execution of iterations within loops by setting parallel factors to 1 for all loops except the final innermost loop, where the parallel factor should be set to 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function `kernel_3mm` as a hardware acceleration kernel. Prevent any automatic pipelining, tiling, or parallelism enhancements for the top-level loops calculating matrix products E=A*B, F=C*D, and G=E*F. Allow natural flattening pipelining for the loop filling the matrix E, and apply a specific 2x parallelism optimization only to the loop updating matrix F and a 16x parallelism optimization for the loop calculating the product for matrix G. Retain all loops in their original form without any division into smaller portions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function titled 'kernel_3mm' which performs three matrix multiplications on differently sized 2D arrays E, F, and G where E is the product of A and B, F is the product of C and D, and G is the product of E and F. Auto-flatten the entire loop structure for E but don't pipeline the loops for E, F, and G. Do not tile or parallelize the loops implicitly, but explicitly express E's innermost loop not to be parallelized while G's innermost loop should be explicitly parallel with a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a High-Level Synthesis (HLS) code for a 3-matrix multiplication kernel function, treating the function as a hardware acceleration kernel, and specify the following criteria using HLS pragmas for each loop nest:\n1. Disable automatic loop pipelining.\n2. Do not split loops into smaller, tiled chunks.\n3. Execute loop iterations sequentially rather than in parallel, except in the innermost loops of the 'C*D' and 'E*F' multiplications, where a parallelization factor of 10 should be applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create HLS C code for a hardware kernel function that performs three matrix multiplications (E=A*B, F=C*D, G=E*F) with specific matrix dimensions, where pragmas disable pipelining, set tiling and parallel factors to 1, except for the innermost loops of the second and third multiplications, which you should set to parallel factors of 10 and 4, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS code that treats a function as a hardware kernel for matrix multiplications without pipelining, tiling, or parallelizing the outer two loops, and employs a parallel factor of 16 in the innermost loops for the second and third matrix multiplication operations. Use auto-range FPGA directives to achieve this, ensuring that the pipelining, tiling, and parallelism factors are automatically set to 'off', '1', and '1' respectively for the outer loops, and '16' for the specified innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Direct the HLS tool to handle three matrix multiplications as separate, unoptimized hardware kernels by specifying: \n\n1. The entire function `kernel_3mm` should be compiled as a hardware kernel.\n2. Within each kernel operation (E=A*B, F=C*D, G=E*F), the outer two loops (over `i` and `j`) should not be pipelined and should be executed sequentially in their original unrolled form.\n3. The innermost loop (over `k`), in the case of G=E*F only, should be attempted to be parallelized with 32 computations simultaneously if possible, while for E=A*B and F=C*D it should be kept as a single computation per iteration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function to perform three matrix-matrix multiplications sequentially. For each matrix multiplication, use the following directives:\n\n1. For the outermost loop of each multiplication, do not pipeline the loop and do not tile or parallelize the iterations, leaving them to execute sequentially.\n2. For the innermost loop of the first multiplication, do not pipeline or tile, but allow automatic determination of parallelization factor.\n3. For the innermost loop of the second multiplication, do not pipeline, tile, or parallelize, thereby enforcing sequential execution.\n4. For the innermost loop of the third multiplication, do not pipeline or tile, but explicitly parallelize with a factor of 8.\n\nFor the first multiplication's nested loops, do flatten and pipeline the outer loop automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration component. For each of the three main code blocks within this function that perform matrix multiplication, adhere to the original sequential processing of the outer loops and prevent any loop pipelining. Within the innermost loops, execute iterations serially except for the innermost loop of the last code block, which should process 50 iterations in parallel. Ensure to define the loop bounds according to the matrix sizes provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named `kernel_3mm` that performs three matrix multiplications (`E=A*B`, `F=C*D`, and `G=E*F`) for matrices with predefined dimensions using iteration loops. Define the function to be a hardware kernel for acceleration. Ensure that the outer loops for these multiplications are not automatically pipelined, and are executed in their original form without tiling and in serial fashion. For the inner loops, specifically the second-level loop, flatten the pipeline to optimize loop execution across iterations and execute this loop without tiling and in serial fashion. For the innermost loops, perform the iterations serially unless stated otherwise. For the specific case of the inner loop multiplying the `C` and `D` matrices, double the parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes, direct the code like so:\n\n- Define the function 'kernel_3mm' as a hardware acceleration kernel.\n- For the first level of loops in all three matrix multiplications, apply the 'flatten' directive to the pipeline pragma to allow aggressive loop flattening for potential parallelism, while keeping the tiling and parallel factor to the original loop configuration.\n- For the second level of loops, disable automatic pipelining, and retain the original loop configurations by using a tiling and parallel factor of 1.\n- Inside the inner loops, for the third level, maintain a parallel factor of 1 for the matrix multiplications to calculate matrices E and F, but for the multiplication to calculate matrix G, increase the parallel factor to 4 to suggest parallel execution of four iterations simultaneously.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration target, and specify optimization pragmas for the nested loops used for matrix multiplication within the function `kernel_3mm`. For the first and last matrix multiplication blocks (`E := A*B` and `G := E*F`), do not apply automatic pipelining, and run iterations sequentially without tiling or parallelizing the loops. However, utilize automatic pipelining on the outermost loop of the `E := A*B` block, while keeping the other loops non-pipelined and non-parallelized. For the second block (`F := C*D`), disable all forms of loop optimizations including pipelining, tiling, and parallelization, except for the innermost loop, where you should enable parallelization with an unspecified factor determined by the compiler.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a dedicated hardware acceleration kernel to be synthesized without any automated loop pipelining optimizations, always maintaining the loops in their original, unaltered full iterations. Specifically, ensure that for each nested loop, no parallelization or loop tiling is enacted except for the innermost loops computing updates of F[i][j] and G[i][j], which can be parallelized with a factor of 10 and 32, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a triple matrix multiplication kernel function performing the operations E = A * B, F = C * D, and G = E * F with specific sizes, and apply the following directives to all loops: disable automatic pipelining, prevent tiling (loop chunking), and set parallelism factors explicitly to 1 for most loops, except for the innermost loop of F and G computations where the factors are set to 10 and 2, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a three-matrix multiply kernel, treating the entire function as an acceleration kernel, while ensuring that each nested loop combination executes sequentially without pipelining, tiling, or parallelizing the iterations, except for the innermost loop of the last matrix multiplication, which should perform four iterations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_3mm' function as a hardware acceleration kernel. For the calculation of matrix 'E' based on 'A*B', allow the outermost loop to potentially be pipelined or flattened for parallelism, but prevent pipelining of the inner 'i' loop entirely while preserving its original form and executing its iterations sequentially. In the computation of matrix 'F' from 'C*D', similarly, avoid any pipelining or parallel optimizations for all loops involved, ensuring they run sequentially as written. Finally, for the output matrix 'G' resulting from 'E*F', entirely disable pipelining of the nested loops, but permit parallel execution of the inner 'k' loop with a factor that is automatically determined with a hint for small inner loop sizes (recommendation of 5).", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS code to handle the matrix multiplication of E = A * B, F = C * D, and G = E * F, using the following specifications:\n\n1. Treat the whole function as a dedicated hardware kernel.\n2. Disable automatic pipelining optimizations for all loops at every nesting level.\n3. Do not tile any of the loops; process each loop in its full form.\n4. Execute all loop iterations sequentially and avoid any parallel implementation, except for the innermost loop in the multiplication of F = C * D, where you can allow two iterations to execute in parallel.\n5. Additionally, for the loop responsible for calculating matrix G, apply loop flattening for pipelining at the first-level nested loop only.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Disable all automatic loop optimizations and prefer sequential execution, except for a specific loop which should allow limited parallel operations with a parallel factor of 16, and flatten the nested loops in the final matrix multiplication stage.\n\n- Treat the 'kernel_3mm' function as an independent hardware kernel.\n- For each matrix multiplication stage, do not automatically pipeline any of the loops.\n- Do not tile any of the loops, implying that each loop should be processed as a whole.\n- Do not introduce parallelism in any of the loops by ensuring each has a parallel factor of 1, thus forcing sequential execution. \n- Specifically, for the inner loop of the second matrix multiplication (C*D), allow 16 operations to occur in parallel by setting a parallel factor of 16.\n- For the final matrix multiplication stage (E*F), in addition to the aforementioned directives, ensure the loops are flattened for more efficient grouped execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to define a hardware kernel function `kernel_3mm` that performs three matrix multiplications (E := A*B, F := C*D, and G := E*F) with specific dimensions, where all loops should be processed without pipelining, tiling or parallel execution except for the innermost loop in the computation of matrix G, which should have a parallelism factor of 50. Use the `off` option for pipeline disabling, set tiling factor to 1, and specify parallel factors where required, including an explicit parallelism factor of 1 where default behavior is to be enforced.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS C code with the appropriate pragmas, you would instruct as follows:\n\nDesignate the function 'kernel_3mm' as a hardware acceleration kernel. For the loops within the 'E := A*B', 'F := C*D', and 'G := E*F' sections, do not use automatic pipelining, do not tile the loops, and execute the outer loops sequentially. In the innermost loops for calculating 'E' and 'F', run loop iterations sequentially. For calculating 'F', process the innermost loop with a parallel factor of 10, and for calculating 'G', use a parallel factor of 50 for the innermost loop. Leave all the other loops in their full form without any modifications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function named `kernel_3mm` for performing matrix multiplication on sets of matrices A, B, C, and D, producing output matrices E, F, and G, respectively. In this function, ensure that all loops are executed sequentially without any automatic pipelining, tiling, or parallelization optimizations, except for the innermost loops in the second and third matrix multiplication sections, where parallel factors of 10 and 5 are applied respectively. Use the appropriate #pragma directives to control the pipelining (`PIPELINE`), tiling (`TILE`), and parallel factor (`PARALLEL`) parameters, disabling automatic optimizations by setting their values explicitly as follows: disable pipelining with `off`, and enforce no tiling and no parallel execution with a factor of `1`, except for the specified innermost loops where you should set the parallel factor as specified above.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function `kernel_3mm` that performs matrix multiplication without typical optimization features. Specifically, disable automatic loop pipelining, do not split any loop into smaller tiled loops, and enforce sequential execution of loop iterations. However, selectively parallelize the innermost loops of the F:=C*D and G:=E*F computations with factors of 10 and 8, respectively. Use the corresponding pragma directives with values set to `off` or `1` to inhibit optimizations and specify the parallelization factors where applicable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the described matrix multiplication kernel into hardware acceleration code, treating each function as a separate hardware kernel. For the loop computing E:=A*B, use fully automated pipelining but avoid loop tiling and parallelization optimization. For the loop computing F:=C*D, disable pipelining, use the simplest form of the loop without tiling, and manually set the parallelization factor of the innermost loop to process two iterations concurrently. Lastly, for G:=E*F, again disable pipelining and tiling, but set the innermost loop's parallelization factor to process ten iterations at once to accelerate computation. Use appropriate HLS #pragma directives to reflect these optimizations, maintaining the loop's original upper bounds.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for matrix multiplications of 2D arrays that does not use automatic pipelining, tile factor, or parallel factor optimization for multiplication of `A` by `B` into `E` and `C` by `D` into `F`, but uses a parallel factor of 5 for the multiplication of `E` by `F` into `G`, while considering all multiplied matrices as part of a single hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration kernel. In the first computation block, calculate the matrix product `E = A*B` without pipelining or parallel execution on the outer loops, but allow the innermost loop to execute sequentially. For the second computation block `F = C*D`, maintain the same no-pipelining and sequential execution constraints, but allow the innermost loop to execute two iterations in parallel. In the final computation block `G = E*F`, use the same constraints for the outer loops, and enable 25-fold parallel execution for the innermost loop's iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Implement a hardware-accelerated kernel function for 3-matrix multiplication (E=A*B, F=C*D, G=E*F) using double-precision floating point, where loop pipelining is disabled, loop tiling is not applied, parallelism is at the base level within innermost loops, except for specific loops where the parallelization factors are explicitly set to 16 and 5. Use the appropriate pragmas to prevent pipelining and tiling, set parallelism factors to 1 where not specified, and apply the given parallelism factors of 16 and 5 to the respective inner loops of the F and G matrix calculations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For this task, treat the function as a dedicated hardware kernel. Prevent the pipelining of all loops, ensure the loops process in their entirety without tiling, and that the first and third nested loops execute sequentially. However, within the core computation of the third nested loop in the last part, allow parallel execution of the loop iterations with a factor of 8. Use the following instructions:\n\n1. \"#pragma ACCEL kernel\" - Define the function 'kernel_3mm' as a hardware acceleration kernel.\n\n2. \"#pragma ACCEL PIPELINE off\" - Disable automatic pipelining for each loop.\n\n3. \"#pragma ACCEL TILE FACTOR=1\" - Disable loop tiling by forcing the entirety of the loop to run unmodified.\n\n4. \"#pragma ACCEL PARALLEL FACTOR=1\" - Ensure sequential execution of loop iterations within the first and second main computations ('E := A*B' and 'F := C*D'). \n\n5. \"#pragma ACCEL PARALLEL FACTOR=8\" - Only for the innermost loop of the last main computation ('G := E*F'), make an exception to allow 8 parallel executions of the loop iterations. \n\n6. The \"#pragma ACCEL PIPELINE auto{flatten}\" directive seems incorrect as it contradicts the instruction to prevent pipelining. It should adhere to the directive to prevent pipelining, \"#pragma ACCEL PIPELINE off\", if the intentions were to prevent it across all loops consistently. If this is indeed a special case that was missed in the instructions, then provide the directive as is, otherwise, it should also be set to off.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware acceleration kernel and for each nested loop within it, disable automatic pipelining, process the loop iterations in their original unaltered sequence without tiling or parallelizing, except in the second nested loop of the 'F' matrix calculation where parallel processing should be applied with a factor of 16, and in the second nested loop of the 'G' matrix calculation where parallel processing should be applied with a factor of 10.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function 'kernel_3mm' to perform matrix multiplication without optimization. Disable pipelining for all loops, do not tile any loops, use straightforward sequential execution for the outer loops, but allow partial parallelization for the innermost loops of the second and third matrix multiplications with factors of 2 and 50, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 3-matrix multiplication algorithm where the function 'kernel_3mm' is designated as a hardware acceleration kernel, and configure the loops for matrix multiplication (E=A*B, F=C*D, G=E*F) to perform computations sequentially without any pipelining, tiling, or parallelism optimizations. Specifically, disable automatic pipelining for all loops, set the tiling factor to the default size (no division of the loop), and enforce a parallel factor of 1 to ensure serialized execution for the first two matrix multiplications, while for the last matrix multiplication (G=E*F), increase the parallel factor to 50 only for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a function designated as a hardware kernel to calculate the matrix multiplication of `E = A * B`, `F = C * D`, and `G = E * F`. For all loops: disable automatic pipelining, set the tiling factor to 1 ensuring no loop tiling, and process all loops in serial execution except the innermost loop in the second matrix multiplication `F = C * D`, which should be processed with a parallel factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function for matrix multiplication and explicitly disable pipelining, tiling, and parallel factor optimization for all loops, keeping them in their original sequential form, except for the innermost loop multiplying matrices C and D, where only parallel execution is allowed with a factor of 20. Indicate that the second-level loop of the third matrix multiplication should attempt loop flattening while maintaining all other default sequential behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function named 'kernel_3mm' to compute the product of matrices with explicitly disabled loop optimizations and pipelining, but allow a specific factor of parallelism for some loops. The function should take as its parameters the dimensions ni, nj, nk, nl, and nm, along with double-precision floating-point matrices E[40][50], A[40][60], B[60][50], F[50][70], C[50][80], D[80][70], and G[40][70]. Use the following directives for each loop:\n\n1. Treat 'kernel_3mm' as the hardware acceleration kernel.\n2. For the first matrix multiplication E = A * B:\n   - Avoid any automatic pipelining.\n   - Ensure the loop runs in its complete original form without any tiling.\n   - Execute all iterations sequentially, not in parallel for all outer loops.\n\n3. For the second matrix multiplication F = C * D:\n   - Same as the first multiplication but allow the innermost loop to have a parallelism factor of 16.\n\n4. For the third matrix multiplication G = E * F:\n   - Same as the first two multiplications but allow the innermost loop to have a parallelism factor of 4.\n\nWrite the loops in their corresponding regions as defined, and ensure the appropriate #pragma directives reflect the above settings with 'auto{off}', 'auto{1}', 'auto{16}', and 'auto{4}' for the pipeline, tile, and parallel factors, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following high-level description into HLS pragma directives to control loop optimizations and function acceleration within the code for a 3-matrix multiplication kernel:\n\n1. Designate the function as a hardware module by turning it into a kernel.\n\n2. For each loop nest involved in the matrix multiplications ('E := A*B', 'F := C*D', 'G := E*F'), do not apply automatic pipelining.\n\n3. Within each loop nest, do not split the loops into smaller chunks.\n\n4. Ensure that all iterations within the innermost loops are executed sequentially and not in parallel, except for the innermost loops computing 'F[i][j]' and 'G[i][j]'. For 'F[i][j]', group the iterations to allow 16 concurrent operations, and for 'G[i][j]', group the iterations to allow 32 concurrent operations.\n\nHere is the corresponding pragma instruction for each requirement:\n\n1. `#pragma ACCEL kernel` - Treat the following function as a hardware kernel for acceleration.\n\n2. `#pragma ACCEL PIPELINE auto{off}` - Do not automatically pipeline this loop.\n\n3. `#pragma ACCEL TILE FACTOR=auto{1}` - Process this loop in its original, full form without breaking it into smaller chunks.\n\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` (default case) - Run the iterations of the respective loops one after the other, not in parallel.\n   `#pragma ACCEL PARALLEL FACTOR=auto{16}` (for loop computing 'F[i][j]') - Allow 16 loop iterations to be processed in parallel.\n   `#pragma ACCEL PARALLEL FACTOR=auto{32}` (for loop computing 'G[i][j]') - Allow 32 loop iterations to be processed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for accelerating matrix-matrix multiplication across three operations. Indicate that all loops should not be automatically pipelined, and they should run without tiling or parallel optimization, except for the innermost loop of the third matrix-matrix operation which runs 16 iterations in parallel. Use a #pragma directive to designate the function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function called 'kernel_3mm' that operates on double precision floating-point matrices for matrix multiplication without pipelining any loops, without tiling any loops, and with parallelization level set to 1 for most loops but set to 2 for the inner loop of the F matrix calculation and set to 10 for the inner loop of the G matrix calculation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated C function `kernel_3mm` for matrix multiplication, designed not to auto-pipeline any loops, to proceed with all nested loops without tiling, and to execute iterations serially within each loop except in the final multiplication loop of F, where iterations run in pairs, and in the final multiplication loop of G, where 25 iterations run in parallel. Use the appropriate pragmas to disable auto-pipelining (`PIPELINE \"off\"`), set tiling to process the full loop without chunking (`TILE FACTOR=1`), set parallelism to serial execution (`PARALLEL FACTOR=1`) except in the specified cases of the F and G computation loops where the factors are 2 and 25 respectively. Additionally, designate the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the provided C code embellished with HLS pragmas into a natural language instruction considering the pragma annotations and their specified values:\n\n\"Generate a hardware kernel to perform triple matrix multiplication without using automatic pipelining, explicitly avoiding the decomposition of loops into smaller parallel chunks by specifying a tile factor of one and a parallel factor of one for each loop. Within the kernel, perform three separate matrix multiplications: First, calculate the matrix product of A and B to produce matrix E, then calculate the product of C and D to produce matrix F, both without any parallel optimization. Finally, calculate the product of matrices E and F to produce matrix G, applying a parallel factor of 20 for the middle loop of the second matrix multiplication and a parallel factor of 10 for the middle loop of the third matrix multiplication.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function as a hardware kernel, and for each loop within it, turn off automatic pipelining, set tiling to process the loop without chunking, and set parallel execution to run iterations sequentially, except in the innermost loop of the third computation block where the parallel factor should be set to execute four iterations concurrently. Use the following natural language instruction:\n\n\"Define the function as a hardware acceleration kernel with all loops inside configured to execute without automatic pipelining and loop tiling, sequentially processing iterations one after the other. However, in the last computation block, for the innermost loop, adjust to process four iterations in parallel.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a matrix multiplication kernel using C, targeting a hardware accelerator, with specific optimizations turned off and explicit parallelism settings. Apply the 'kernel' pragma to the entire function, indicating it's a hardware-accelerated kernel. Inside the triple-nested loops for each of the matrix multiplications (E=A*B, F=C*D, G=E*F), turn off pipelining and use the default loop tiling, while setting the parallelism factor explicitly to 1 for most loops, except for innermost loops where the parallelism factor is specified to be 20 and 16 for the second and third matrix multiplications respectively. Ensure these settings by using the appropriate pragma directives with 'auto{off}' for pipelining, 'auto{1}' for tiling, and the specific 'auto' values provided for parallel factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the provided C functions to use as hardware kernels without automatic optimization, keeping the loops in their original full form and executing iterations sequentially with specific parallelism settings for certain loops.\n\n- Treat the function `kernel_3mm` and its contents as hardware acceleration kernels.\n- For all the loops in the function, disable automatic pipelining, do not break loops into smaller chunks by keeping the tile size at its default (effectively 1), and execute the loop iterations sequentially without parallelism.\n- Within the nested loops of the first matrix multiplication `E := A*B`, run all iterations sequentially.\n- In the nested loops of the second matrix multiplication `F := C*D`, the innermost loop (`k` loop) should execute with a parallel factor of 16.\n- During the third matrix multiplication `G := E*F`, the innermost loop (`k` loop) should execute with a parallel factor of 25.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication hardware kernel function called `kernel_3mm`. Inside the function:\n\n1. Define the kernel using the 'Treat the following function as a hardware kernel for acceleration' instruction.\n   \n2. For the first matrix multiplication `E = A * B`, apply:\n   - 'Automatically optimize the outer loop for pipelining, including flattening of nested loops if beneficial' to the outer `i` loop.\n   - 'Process the inner `j` loop in its original, full form without breaking it into smaller chunks and do not automatically pipeline it' for both the loop and the inner `k` summation loop.\n  \n3. For the second matrix multiplication `F = C * D`, do not pipeline the outer `i` loop and the inner `j` loop, and process these loops in their original full form. For the innermost `k` loop, allow the compiler to automatically decide on the parallelism factor.\n\n4. For the third matrix multiplication `G = E * F`, repeat the directives of the second multiplication: do not pipeline any of the loops and process all loops in their full form, keeping the default parallelism for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, but do not employ automatic pipelining or tiling, and perform the outer loops strictly sequentially. However, for the innermost loop calculating the matrix multiplication for F and G, allow 10 and 16 iterations to run in parallel, respectively. Use default parallel factors for other loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel. Perform matrix multiplication of `A` and `B` to compute `E`, `C` and `D` to compute `F`, and then `E` and `F` to compute `G`. Each matrix multiplication sequence should be processed sequentially, and iterations within the innermost loops of matrix multipliers should be parallelized. For the matrix multiplication resulting in `E`, execute the loop iterations without parallelism or pipelining optimizations. For the matrix `F`, parallelize the innermost loop with a default factor. Finally, for `G`, apply a higher level of parallelism to the innermost loop. The outer loops should remain unoptimized, while the intermediate layers of the loops should be set to have no tiling or additional parallelism enforced.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes with the appropriate pragmas, your instruction should be as follows:\n\n\"Define a hardware kernel function named `kernel_3mm` that performs three matrix multiplications with pre-defined matrix dimensions, ensuring that for each nested loop: disable automatic pipelining, do not tile the loop, and initially run iterations sequentially. For the first and second multiplication, leave all loops in their original form, processing iterations one after the other. In the last multiplication, enable manual pipelining for the outermost loop and set parallel factors of 16 for the innermost loop of the second multiplication and 50 for the innermost loop of the third multiplication, to allow these specific loops to process that many operations in parallel.\"\n\nFor the pragmas, you should apply:\n\n- Disable automatic pipelining with `#pragma ACCEL PIPELINE auto{off}`.\n- Prevent loop tiling by using `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Set the initial parallel factor to 1 with `#pragma ACCEL PARALLEL FACTOR=auto{1}`, keeping loops sequential.\n- For the second multiplication\u2019s innermost loop, overwrite the parallelism with `#pragma ACCEL PARALLEL FACTOR=auto{16}`.\n- For the third multiplication\u2019s innermost loop, increase the parallelism with `#pragma ACCEL PARALLEL FACTOR=auto{50}`.\n- Manually pipeline the outermost loop of the last multiplication with `#pragma ACCEL PIPELINE auto{}` (assuming leaving it empty implies manual).\n\n\nRemember, the applied pragmas directly correspond to the behavior specified in the description for each segment of the code. The '{off}' disables the functionality, and the 'auto{1}' maintains the default behavior without optimization. The 'auto{16}' and 'auto{50}' represent specific optimization directives for those loops in the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Implement a matrix multiplication kernel function in HLS C without applying any automatic pipelining, tiling, or parallelization optimization strategies for all loops. Additionally, explicitly control the parallel factor in the third innermost loop of the second and third matrix multiplication stages, using a parallel factor of 20 for the stage F = C * D, and a parallel factor of 25 for the stage G = E * F. Treat this entire function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a matrix multiplication kernel which treats the function as a hardware acceleration kernel. For each matrix multiplication stage ('E := A*B', 'F := C*D', and 'G := E*F'), ensure that the outer loop does not utilize automatic pipelining. The inner loop should be flattened where possible for 'E := A*B' to increase loop nesting, but not for the other stages. All loops should prevent the compiler from tiling or parallelizing the iterations by setting both tile and parallel factors to 1, except for the loop that computes 'G := E*F', where the innermost loop k should have a parallel factor of 5 to allow some degree of parallel execution of iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the `kernel_3mm` function as a hardware acceleration kernel, where the matrix multiplication operations `E=A*B`, `F=C*D`, and `G=E*F` are to be calculated. Apply the following pragma directives:\n\n1. For the first matrix multiplication `E=A*B`, attempt to optimize the outer two loops `i` and `j` with automatic pipelining but do not apply tiling or parallelization optimizations directly (the 'auto{1}' suggests the tool may decide autonomously if these optimizations are beneficial).\n  \n2. For the innermost loop `k` calculating `E[i][j]`, no pipelining is to be applied, and the iterations should be executed sequentially.\n\n3. For the second matrix multiplication `F=C*D`, perform similar directives to the first one, with one exception: allow a level of parallelization in the innermost loop `k` calculating `F[i][j]` that the tool deems appropriate with a hint of 10 units of parallel processing resource.\n\n4. For the third matrix multiplication `G=E*F`, match the parallel configuration of the first matrix multiplication, meaning no specific directives for parallel processing optimization.\n\n5. Ensure that each multiplication initializes the resulting matrix elements to `0.0` before accumulation within the innermost loops.\n\nEach loop and its nested counterparts should not be interrupted by applying any pipelining (`off`), tiling (`1`), or forced parallelism (`1`), except for the innermost loop of the second matrix multiplication, as previously described. The process of these calculations should be performed by iterative sequential accumulation, without advanced pipelining or parallel processing directives unless the 'auto' keyword allows the tool to make certain decisions.\n\nThe short, natural language instruction encapsulating the above points could be:\n\n\"Create a hardware acceleration kernel for three sets of matrix multiplication operations with automatic pipelining where applicable, with conservative tiling and parallel factors, except for the second operation's inner loop, where automatic parallelism is encouraged to enhance performance.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, while explicitly instructing the compiler to disable automatic pipelining, tiling, and parallelization optimization for all nested loops. However, allow limited parallel execution for specific innermost loops, with a factor of 32 for accumulating the multiply-add operations of the matrix F, and a factor of 5 for the matrix G.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel function named `kernel_3mm` which performs three matrix multiplications E=A*B, F=C*D, and G=E*F, with non-parallelized, non-tiled computation in an original sequential order for the outer loops and with flattened parallel computations on the innermost loops. Use the following specific #pragma annotations:\n\n1. Apply the `#pragma ACCEL kernel` attribute to the whole `kernel_3mm` function to mark it for hardware acceleration.\n2. Disable automatic pipelining for outer loops with `#pragma ACCEL PIPELINE auto{off}`.\n3. Prevent tiling of the loops by setting the tile factor to 1 with `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. Ensure the outer loop runs iteratively without parallelization by setting the parallel factor to 1 with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n5. Flatten the pipeline for the second-level loops using `#pragma ACCEL PIPELINE auto{flatten}`.\n6. Parallelize the innermost loop of the third matrix multiplication with a parallel factor of 50 using `#pragma ACCEL PARALLEL FACTOR=auto{50}`.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the three matrix multiplication processes in the given code to hardware acceleration, ensuring loop optimizations are set as follows: \n\n1. For the computation of matrix E (E := A*B), enable automatic pipelining and loop flattening, but disable pipelining for the inner loops to retain the original loop structure.\n  \n2. For the computation of matrix F (F := C*D), disable automatic pipelining for all loops and run a single iteration of the most inner loop in the original sequential order, but allow two simultaneous iterations to execute for the last multiplication loop, preserving the sequential execution order for the loops that calculate the initial E and F matrices' elements.\n\n3. For the computation of G (G := E*F), continue with the pipelining disabled for all loops, but massively parallelize the last multiplication inner loop, launching up to 32 operations concurrently.\n\nIn all cases, do not tile the loops, which means each loop should remain in its full form without being subdivided into smaller, more manageable chunks. Apply these settings using appropriate #pragma directives to mark the hardware acceleration sections, and control pipelining, tiling, and parallelism factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following directives for hardware synthesis of a triple matrix multiplication algorithm as HLS code:\n\n1. For each loop nest, do not employ automatic pipelining.\n2. Do not subdivide any loop into smaller tiled loops.\n3. Execute the iterations of each innermost loop sequentially, except in the final multiplication where 32 parallel copies of the execution are required.\n4. Consider the whole function as a candidate for hardware acceleration.\n\nThe specific #pragma directives to describe the given processes are:\n\n- \"Do not employ automatic pipelining\" corresponds to `#pragma ACCEL PIPELINE auto{off}`\n- \"Do not subdivide any loop into smaller tiled loops\" corresponds to `#pragma ACCEL TILE FACTOR=auto{1}`\n- \"Execute the iterations of each innermost loop sequentially\" corresponds to `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- \"For the final innermost loop in G matrix calculation, 32 parallel copies of the execution are required\" corresponds to `#pragma ACCEL PARALLEL FACTOR=auto{32}`\n- \"Consider the whole function as a candidate for hardware acceleration\" corresponds to `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` to be a hardware acceleration kernel and implement three matrix multiplication blocks within it: E = A * B, F = C * D, and G = E * F. For each multiplication block, the outer two loops should proceed without any hardware optimization, whereas the innermost loop of the second multiplication block should parallelize its iterations by a factor of 16, and for the third block by a factor of 8.\n\nUse the following HLS pragmas to guide the compiler:\n1. To indicate that `kernel_3mm` is an acceleration kernel, place `#pragma ACCEL kernel` before the function definition.\n2. To prevent automatic pipelining of loops, add `#pragma ACCEL PIPELINE off` before each loop.\n3. To execute loops without tiling, use `#pragma ACCEL TILE FACTOR=1` before every loop.\n4. To run the outer loops' iterations sequentially, use `#pragma ACCEL PARALLEL FACTOR=1` before the first and second loops in each block.\n5. To parallelize the innermost loop's iterations in the second multiplication block, add `#pragma ACCEL PARALLEL FACTOR=16` before the loop.\n6. In the third matrix multiplication block, use `#pragma ACCEL PARALLEL FACTOR=8` before the innermost loop to parallelize its iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Construct a hardware-accelerated kernel that executes matrix multiplications for matrices E, F, and G without automatic loop pipelining, without tiling, and with serialized loop iterations for the first two matrix multiplications, while allowing limited parallelism in the last multiplication, defined explicitly with factors 32 and 2 for inner loop iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a matrix multiplication operation treated as a hardware kernel, where each of the three matrix multiplication steps is executed sequentially without pipelining or parallel processing optimizations. Specifically, instruct the compiler not to automatically pipeline any loops, to process all loops without tiling, and to execute iterations sequentially rather than in parallel, with the exception of the innermost loops within the second and third matrix multiplication steps, where the iterations should be parallelized by a factor of 2 and 5 respectively. Use the following pragma directives at the beginning of each loop nest to control the optimizations accordingly: \n\n- For all loops, set \"`#pragma ACCEL PIPELINE auto{off}`\" to prevent loop pipelining,\n- \"`#pragma ACCEL TILE FACTOR=auto{1}`\" to inhibit loop tiling,\n- \"`#pragma ACCEL PARALLEL FACTOR=auto{1}`\" to enforce sequential loop execution,\n  \nHowever, update the \"`#pragma ACCEL PARALLEL FACTOR`\" for the innermost loops in the second and third matrix multiplication steps to \"`#pragma ACCEL PARALLEL FACTOR=auto{2}`\" and \"`#pragma ACCEL PARALLEL FACTOR=auto{5}`\" respectively to enforce limited parallel execution within these selected loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) code for a 3-matrix multiplication kernel (E=A*B, F=C*D, G=E*F), treating the entire function as a hardware acceleration kernel but ensuring that:\n\n1. Each major loop for calculating E, F, and G is executed according to the original order, without pipeline optimization.\n2. Within the major loop for calculating E, flatten pipeline optimization is allowed for the second loop, but the other optimizations are kept to the original loop form with no tiling or parallelization enhancements.\n3. The loop that initializes matrix F is not pipelined, with no optimizations for tiling or parallelization, apart from the innermost loop where pair-wise parallel execution of two iterations is allowed.\n4. Finally, when calculating G, enable the broadest parallelization possible in the inner loop handling pair-wise multiplications and sums, while the outer loops maintain the original execution flow without extra optimizations.\n\nUse `#pragma ACCEL kernel` to mark the function for acceleration, `#pragma ACCEL PIPELINE auto{off}` to disable pipeline optimization, `#pragma ACCEL PIPELINE auto{flatten}` to allow flattening pipeline optimization, `#pragma ACCEL TILE FACTOR=auto{1}` to avoid loop tiling, and varying `#pragma ACCEL PARALLEL FACTOR=auto{value}` to set the desired level of parallel execution within loops, where `value` is the number of parallel iterations desired (1 for serial, 2 for dual iteration parallel execution, and 50 for fully parallelized execution for the last loop).", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel with strict sequential execution and no optimization. Specifically, designate the entire function as a hardware acceleration kernel. For all loops in this kernel, disable automatic pipelining, prohibit loop tiling, and ensure that the loop iterations are executed serially, not in parallel. However, make an exception for the innermost loop during the 'F := C*D' matrix multiplication, where you should allow four iterations to be processed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following matrix multiplication C code into HLS directives that will prevent pipeline optimization, use the loops in their original form without tiling, and specify different parallelization for specific loop iterations, where for the first matrix multiplication only a single iteration is processed at a time, for the second matrix multiplication 32 iterations are executed concurrently, and for the third matrix multiplication 4 iterations are executed in parallel. Also, declare the function as a hardware kernel suitable for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "I will provide a set of instructions to describe how to generate the HLS C code with corresponding `#pragma` directives given above:\n\n1. Begin by treating the function, containing matrix multiplication kernels, as a hardware acceleration kernel: `#pragma ACCEL kernel`\n\n2. Prevent pipelining and parallel execution, and avoid tiling, in the first two matrix multiplication operations (E = A * B, F = C * D) to maintain their original execution order:\n    - Before each of the three nested loops, write the following:\n        - Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n        - Prevent loop tiling optimization: `#pragma ACCEL TILE FACTOR=auto{1}`\n        - Run loop iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n3. In the final matrix multiplication operation (G = E * F), use the same pragmas as above for the outer two loops, but for the innermost loop, allow some parallelism:\n    - Before the two outer loops (i, j loops), use the same pragmas as in step 2 to disable pipelining and tiling optimizations, and to ensure sequential execution.\n    - For the innermost loop (k loop), enable limited parallelism with a parallel factor of 5: `#pragma ACCEL PARALLEL FACTOR=auto{5}`\n\nThese instructions should generate the HLS code with `#pragma` values as displayed in the example code snippet.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration kernel, and within it, for the matrix multiplication operations E=A*B, F=C*D, and G=E*F, execute the loops without automatic pipelining, without loop tiling, and with the outer loops acting sequentially. However, for the innermost 'k' loop in the computation of F, enable parallel processing with up to 32 simultaneous computations; and for the innermost 'k' loop in the computation of G, allow up to 10 simultaneous parallel computations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a dedicated hardware block for acceleration purposes while enforcing each loop contained within the function to be executed sequentially, without utilizing any hardware optimizations such as pipelining, tiling, or parallel execution, except in two specific instances: one where twenty parallel operations are allowed and another where fifty parallel operations are performed within distinct innermost for-loops, respectively corresponding to the factors 20 and 50 for parallel execution. Use automatic selection where applicable and explicitly set factors to 1 where no parallelism or tiling is desired.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel that treats the function as an acceleration hardware kernel, and maintain the natural flow of all loops by disabling automatic pipelining, preserving their original loop form without tiling, and executing iterations sequentially within each loop, while allowing specific parallelism on the innermost loops of the second and third matrix multiplication processes with factors of 4 and 2, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated function named 'kernel_3mm' to calculate the matrix multiplication of three pairs of matrices (A\u00d7B, C\u00d7D, E\u00d7F), where the dimensions are defined by the input parameters `ni, nj, nk, nl, nm`. Within the function, mark it for acceleration without any loop pipelining, without loop tiling (by setting the tile factor to 1), and without parallel processing of loop iterations (by setting the parallel factor to 1). Specifically for the innermost loop of the F matrix calculation, allow ten iterations to be processed in parallel. In the final matrix multiplication of E\u00d7F to produce G, restore parallel processing to one iteration at a time, but flatten the G matrix calculation's loops. Use the appropriate pragmas to achieve this behavior:\n\n1. Use `#pragma ACCEL kernel` to mark the function for hardware acceleration.\n2. Use `#pragma ACCEL PIPELINE \"off\"` to prevent automatic loop pipelining.\n3. Use `#pragma ACCEL TILE FACTOR=1` to keep the loops in their original, full form.\n4. Use `#pragma ACCEL PARALLEL FACTOR=1` for sequential loop iteration processing.\n5. Exceptionally, in the case of the innermost loop calculating the F matrix, use `#pragma ACCEL PARALLEL FACTOR=10` to process ten iterations in parallel.\n6. In the final G matrix computation, use `#pragma ACCEL PIPELINE auto{flatten}` to flatten the nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, you would use the following natural language instruction:\n\n\"Create a hardware kernel function named 'kernel_3mm' with matrix-multiplication operations for matrices E, F, and G, where the dimensions are given by ni, nj, nk, nl, nm corresponding to matrix sizes, and are pre-defined with dimensions 40x50, 40x60, 60x50, 50x70, 50x80, 80x70, and 40x70 respectively. For each matrix multiplication operation (E=A*B, F=C*D, G=E*F), ensure that the loops are not pipelined, not tiled, and run all iterations in sequence. However, for the innermost loop in the F matrix calculation, allow ten iterations to be processed in parallel.\"\n\nIn terms of #pragma use, you will need to:\n\n1. Specify the function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n2. For each loop structure within the three matrix multiplication operations, use the following:\n   - Turn off automatic pipelining with `#pragma ACCEL PIPELINE \"off\"`.\n   - Disable loop tiling by setting the tile factor to 1 using `#pragma ACCEL TILE FACTOR=1`.\n   - Disable parallel execution by running iterations sequentially with `#pragma ACCEL PARALLEL FACTOR=1`.\n3. In the innermost loop of the F matrix calculation only, allow parallel processing of ten iterations with `#pragma ACCEL PARALLEL FACTOR=10`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function that performs three matrix multiplications (E=A*B, F=C*D, G=E*F) with input matrices A, B, C, D and resultant matrices E, F, G, with specific dimensions. For matrix multiplication, initialize the resultant matrix with zeros and then accumulate the product of corresponding elements. Use '#pragma ACCEL kernel' to designate the function as a hardware acceleration kernel. Apply pipeline pragmas to the top-level matrix multiplication loops with automated optimizations where appropriate, while disabling automatic pipelining for nested loops to maintain them in their original form. Disable loop tiling and parallel processing optimizations for all loops by setting the factors to '1'. However, for the innermost loops of the second and third multiplication, apply parallel processing with factors '2' and '4', respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication kernel function, treating the function as a hardware acceleration kernel. Within the function, create three nested loops for each matrix product. Enable automatic pipeline optimization for the first matrix product but do not use it for the subsequent products. For tiling and parallel execution, leave optimization decisions to the HLS tool by setting the factors to 'auto', except for the innermost loop of the second matrix product; explicitly parallelize it with a factor of 32. Do not break any of the loops into smaller chunks.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, instruct as follows:\n\nTreat the matrix multiplication functions within a single function as separate hardware kernels, and within those, for the loops calculating matrices E, F, and G:\n1. Ensure the outer two dimensions are not pipelined or optimized for parallel execution.\n2. Keep the loops in their complete form without tiling or breaking them down.\n3. Execute the innermost loop calculations in a serial manner for E and G, but allow for a parallel factor of 2 for F.\n\nSpecifically, apply the following #pragma directives: \n\n- For the entirety of the function, use '#pragma ACCEL kernel' to indicate it contains kernel functions for acceleration.\n- For the outer loops of all three matrix multiplications (calculating E, F, and G), use '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{1}', and '#pragma ACCEL PARALLEL FACTOR=auto{1}' to execute iterations serially and without additional optimization.\n- Within the inner loop for matrix E calculation, enable loop flattening with '#pragma ACCEL PIPELINE auto{flatten}' but maintain a single iteration and no tiling or parallel optimization with '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n- For the innermost loop of matrix F and G calculation, maintain the serial iteration for the G matrix but allow a parallel factor of 2 in the innermost loop for the F matrix using '#pragma ACCEL PARALLEL FACTOR=auto{2}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Implement a kernel function 'kernel_3mm' for matrix multiplication without hardware optimization where three matrix multiplication operations are computed sequentially. For the computation of each matrix (E=A*B, F=C*D, G=E*F), explicitly prevent automatic pipelining and tiling while ensuring all loops are executed in sequence without parallelism, except the innermost multiplication loops of 'F' and 'G' where the iterations should be parallelized with parallelization factors of '4' and '10' respectively. Use hardware acceleration directives to achieve this:\n- Declare 'kernel_3mm' as a hardware kernel using a pragma directive.\n- Apply pragma directives with 'off' for pipelining and '1' for tiling and parallel factors before each loop level to ensure sequential execution.\n- Modify the inner parallelism of the 'F' and 'G' matrix calculations with parallel factors '4' and '10' respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please treat the following matrix multiplication operations (E=A*B, F=C*D, G=E*F) within a hardware acceleration kernel as sequential processes. In each of the nested loops for these multiplications, ensure the following: do not introduce additional pipelining, do not split the loops into smaller tiled chunks, and do not execute loop iterations in parallel \u2013 except for the innermost loops in the second (#pragma ACCEL PARALLEL FACTOR=auto{20}) and third matrix multiplication (#pragma ACCEL PARALLEL FACTOR=auto{8}), where a level of parallelism is specified, indicating how many iterations should be processed in parallel for these specific loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a function named `kernel_3mm` that performs matrix multiplication of the given matrices. Mark the function for hardware acceleration, suppress automatic pipelining, prevent tiling optimization in its loops, and enforce original sequential execution within these loops. In the third nested loop for matrix `E*F`, allow the innermost iterations to run 4 at a time in parallel, and in the final nested loop for matrix `G`, increase this parallel factor to allow 50 iterations to run concurrently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated function that computes the product of matrix pairs (A*B=E, C*D=F, E*F=G). Ensure the top-level function execution is handled as a hardware kernel. For each matrix multiplication, prevent automatic loop pipelining, do not tile the loops, and ensure the outer two loops execute serially. However, within the nested inner loops of the first and second matrix multiplications, allow the last loop to be flattened for pipelining, and execute the innermost loops serially with a parallelism factor of 1. For the third multiplication, adjust the parallel factor of the innermost loop to 4 for the last loop iteration. Employ the appropriate 'auto{value}' pragma expressions for pipeline, tile, and parallel operations as specified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) code for a matrix multiplication kernel function, `kernel_3mm`, which performs the matrix multiplication operations \\( E = A \\times B \\), \\( F = C \\times D \\), and \\( G = E \\times F \\) without automatically pipelining any of the loops. In each loop nest, the outer loop should be unoptimized, running iterations sequentially without parallel execution or tiling. The second loop should be pipelined but not parallel or tiled, and the innermost loop should process iterations sequentially; except in the calculation of \\( F \\) where the innermost loop should have a parallel factor of 4. Implement all of these adjustments with explicit #pragma directives to reflect these constraints and optimizations in the HLS code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration kernel, and within this function, for each major matrix multiplication operation (`E := A*B`, `F := C*D`, and `G := E*F`), proceed through the nested loops without any automatic pipelining, keep the loops unaltered by not tiling them, and execute the outer two loops' iterations serially. For the innermost loop of the `E := A*B` operation, also run iterations serially; for the innermost loop of the `F := C*D` operation, allow two iterations to run in parallel; and for the `G := E*F` innermost loop, allow eight iterations to run simultaneously. Use the specific #pragma values as follows:\n\n- Treat `kernel_3mm` as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Disable automatic pipelining for each loop: `#pragma ACCEL PIPELINE auto{off}`\n- Prevent the compiler from breaking the loops into smaller chunks (no tiling): `#pragma ACCEL TILE FACTOR=auto{1}`\n- For the outer two loop levels, ensure iterations proceed one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- For the innermost loop in the first matrix multiplication (`E := A*B`), iterate serially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- For the innermost loop in the second matrix multiplication (`F := C*D`), run two iterations in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n- For the innermost loop in the third matrix multiplication (`G := E*F`), run eight iterations in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{8}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS C code for a three-matrix multiplication kernel with matrix dimensions fixed to small sizes (40x60, 60x50, 50x80, and 80x70), where the kernels are treated as hardware acceleration units. Ensure that the main computation loops are not pipelined but remain in their original form, with inner loop parallelism factors set to 1 for the first two matrix multiplications and 4 and 10 respectively for the last multiplication, and with no loop tiling except as explicitly flattened in the AB multiplication.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) code for a kernel function named 'kernel_3mm' that disables pipelining, maintains original loop nest structure without tiling, and sets parallel factors explicitly for nested loops. Specifically, for the multiplication of matrices 'A' and 'B' into 'E', run all loops without any pipelining or tiling, and without parallel execution. For the multiplication of 'C' and 'D' into 'F', proceed similarly but parallelize the innermost loop with a factor of 20. Finally, for the multiplication of 'E' and 'F' into 'G', again maintain single loop execution without pipelining or tiling, but this time parallelize the innermost loop with a factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, and for each of the three matrix multiplication segments within the kernel, sequentially execute the outer loops 'i' and 'j' without any compiler optimization or loop transformation, while the innermost loops should be parallelized with the specified factors: the loop within 'E:=A*B' execution remains sequential, the loop within 'F:=C*D' allows two parallel operations at a time, and the loop within 'G:=E*F' permits up to thirty-two concurrent operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the enclosed function as a target for hardware acceleration and enforce sequential execution without optimizations like pipeline, tiling, or parallel processing for all nested loops. Specifically, disable pipelining, set the tiling factor to the smallest unit, and restrict parallel processing to individual iterations for each loop. Additionally, allow a moderate degree of parallelism, with a factor of 5, when computing the second matrix multiplication.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_3mm' function as a hardware acceleration kernel and disable automatic pipelining, tile partitioning, and parallel processing for all loops except for the innermost loops of the second and third matrix multiplications, where you should allow a parallel factor of 5 and 25 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS (High-Level Synthesis) code provided, use the following comprehensive but concise instruction:\n\n\"Create a kernel function named 'kernel_3mm' to perform three matrix multiplications (E=A*B, F=C*D, G=E*F) with given matrix dimensions. For the main loops and all nested loops within this function, disable automatic pipelining, enforce no tiling, and ensure that loop iterations are executed sequentially (not in parallel). However, for the innermost loop within the F=C*D multiplication, allow a maximum of 20 operations to occur in parallel.\"\n\nSpecific #pragma values to use for each loop are:\n- Disable pipeline: `#pragma ACCEL PIPELINE auto{off}`\n- No tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Sequential execution: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- For the innermost loop in the F=C*D operation, enable parallel execution with 20 operations at once: `#pragma ACCEL PARALLEL FACTOR=auto{20}`\n\nThis instruction leads to a function 'kernel_3mm' where each multiplication segment (for E=A*B, F=C*D and G=E*F) contains loops with pragmas that prevent pipelining, enforce single iteration execution without tiling, except for the specific loop multiplying matrices C and D where up to 20 simultaneous operations are allowed to achieve some concurrency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided C code with HLS specific pragmas, instruct as follows:\n\nDefine a hardware kernel function called `kernel_3mm` that performs matrix multiplication without any pipelining, tiling, or parallelism optimization for the first two operations where matrices 'E' and 'F' are computed, ensuring that the loops inside run in their original, sequential order. However, during the calculation of matrix 'G', allow the innermost loop to utilize some parallelism by a factor of 25 for performance enhancement while still preventing pipelining and tiling optimizations. Specify these preferences using the appropriate HLS pragmas: disable pipelining with '#pragma ACCEL PIPELINE \"off\"', set the tile factor to 1 with '#pragma ACCEL TILE FACTOR=1', and set the parallel factor according to the loop's requirement using '#pragma ACCEL PARALLEL FACTOR=x' where 'x' is the desired factor (1 for no parallelism and higher values for parallel operations).", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a three-matrix multiplication kernel, treating the function as a hardware acceleration kernel without automatic pipelining, with loops running in original sequence (not tiled or parallelized) except for the innermost loops in the second and third matrix multiplication segments where the parallel factors are set to 5 and 32, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for matrix-matrix multiplication and designate all loops and functions as non-pipelined, without tiling or parallel execution, except for the innermost loop in the last multiplication (G := E*F), which should have a parallel factor of 32. Use the following directives:\n\n1. Mark the function as a hardware kernel using `#pragma ACCEL kernel`.\n2. In each outer loop (i.e., loops over `i` and `j`), turn off automatic pipelining with `#pragma ACCEL PIPELINE auto{off}`.\n3. In each outer loop, prevent loop tiling with `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. In each outer loop, disable parallel execution with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n5. Inside the innermost loop in the last multiplication block, enable a parallel factor of 32 with `#pragma ACCEL PARALLEL FACTOR=auto{32}`.\n\nFor loops where \"flatten\" is mentioned, interpret this as an instruction that was canceled out by the subsequent \"off\" pipeline directive and can thus be ignored for implementation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication kernel function named `kernel_3mm` that performs the following operations: E = A * B, followed by F = C * D, and finally G = E * F, with the input matrix dimensions specified by the arguments `ni`, `nj`, `nk`, `nl`, and `nm`. The matrices are stored in 2D arrays of double precision floating-point numbers with maximum dimensions 40 by 80. \n\nDo not automatically pipeline any of the loops, ensure that all loops run without parallelization or tiling optimizations, with the exception that the innermost loop of the F matrix multiplication should be set up to run with a parallelism factor of 2 and the innermost loop of the G matrix multiplication should run with a parallelism factor of 4.\n\nUse the following #pragma directives before the respective loops to achieve this:\n\n1. Before each loop, use `#pragma ACCEL PIPELINE off` to disable automatic loop pipelining.\n2. Use `#pragma ACCEL TILE FACTOR=1` to prevent the compiler from tiling any of the loops.\n3. Set `#pragma ACCEL PARALLEL FACTOR=1` for all loops except for the innermost loops of the F and G matrix computations. For the innermost loop of the F computation, use `#pragma ACCEL PARALLEL FACTOR=2`, and for the innermost loop of G, use `#pragma ACCEL PARALLEL FACTOR=4` to introduce the specified levels of parallelism.\n\nMark the function to be treated as a hardware acceleration kernel with `#pragma ACCEL kernel` at the beginning of the function definition.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the enclosed function as a hardware-accelerated kernel, and within it, configure all loops to execute sequentially without automatic pipelining or tiling optimizations by default. The innermost loops calculating 'F[i][j]' and 'G[i][j]' are exceptions, parallelize these with factors of 5 and 8, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function named 'kernel_3mm' that performs matrix multiplication on different sets of matrices (A, B, C, D, E, F, G) with given dimensions (ni, nj, nk, nl, nm), where the first operation is E := A * B, second is F := C * D, and third is G := E * F. For all loops involved in these computations, disable automatic pipelining, execute the loops without tiling (with a factor of 1, processing each loop in full without subdividing it into smaller chunks), and execute all iterations sequentially (in serial, with a parallel factor of 1, not allowing concurrent execution of loop iterations). Specifically, for the last loop of the second operation (F := C * D), set the parallel factor to 4, allowing four iterations to be processed in parallel. Furthermore, for the innermost loop in the third computation (G := E * F), enable the pipeline but do not specify any factors, allowing the compiler to choose whether to pipeline the loop. Ensure no optimizations alter the loop structure by specifying 'auto{off}' for the pipeline and 'auto{1}' for the tile and parallel factors. For the case where pipelining is enabled, only specify 'auto{flatten}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the function `kernel_3mm` as a dedicated hardware acceleration kernel and to execute every loop in the function sequentially without pipelining, tiling, or parallelization, except for the specific loop iterations inside the matrix multiplications for matrix F and G where parallel factors of 4 and 5 are specified respectively. Use the following instructions:\n\n1. Treat the `kernel_3mm` function as a hardware acceleration kernel.\n2. In the three matrix multiplication sections (computing matrices E, F, and G), ensure all loops are not pipelined, and each loop is executed in full without tiling.\n3. For the outer two loops in each multiplication section, do not allow any parallel execution.\n4. Inside the third nested loop for computing matrix F, allow four loop iterations to be processed in parallel.\n5. Inside the third nested loop for computing matrix G, allow five loop iterations to be processed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration kernel, and within it, sequentially carry out the matrix multiplications for matrices E=A*B, F=C*D, and G=E*F. Configure the accelerator to execute each loop in the following manner:\n\n1. For loop calculating E=A*B:\n    - Avoid automatic loop pipelining.\n    - Prevent the loop from being subdivided into smaller parts.\n    - Execute the outer loop iterations sequentially, without parallelism.\n    - Inside the inner j-loop:\n        - Allow the loop to be flattened for parallelism opportunities, but no automatic pipelining directive.\n        - Preserve the full form of the loop without subdividing it into smaller chunks.\n        - Run the inner j-loop iterations sequentially, not concurrently.\n    - In the deepest k-loop for E calculation, also ensure sequential execution without parallelism.\n\n2. For loop calculating F=C*D:\n    - Do not automatically pipeline this loop.\n    - Do not break the loop into smaller chunks.\n    - Execute iterations sequentially, with no parallelism for the outer loop.\n    - For the inner j-loop, also prevent automatic pipelining, subdivision, and parallelism.\n    - In the innermost k-loop, opt for a manual replication factor of 2 to allow some parallelism.\n\n3. For loop calculating G=E*F:\n    - Do not employ pipelining automatically.\n    - Keep the loop intact and avoid subdividing it.\n    - Execute the outer loop iterations in a serial fashion, with no concurrency.\n    - Inside the j-loop:\n        - Block the loop from automatic pipelining.\n        - Maintain the loop's full form, without breaking it down.\n        - Ensure consecutive iteration execution, eschewing parallelism.\n    - Use a manual replication factor of 8 for the deepest k-loop to introduce a higher degree of parallelism.\n\nEnsure that these configurations are applied using the appropriate `#pragma ACCEL` directives for pipeline, tile, and parallel factors, along with setting their corresponding auto policies to the needed values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code that treats a specified function as a hardware acceleration kernel and implements three matrix multiplication algorithms, ensuring that:\n\n1. For each loop nest related to matrix multiplication, no loop pipelining optimizations are allowed.\n2. The code must maintain each loop's original iteration structure without tiling optimizations.\n3. In each innermost multiplication loop, the first two should not run any iterations in parallel, while the third innermost loop should allow four parallel iterations for the second matrix multiplication and thirty-two for the third.\n\nYou should use the `#pragma ACCEL kernel` to designate the function as a kernel, and for each loop nest, you should specify: `#pragma ACCEL PIPELINE off`, `#pragma ACCEL TILE FACTOR=1`, and `#pragma ACCEL PARALLEL FACTOR=1` to prevent automatic pipelining, tiling, and parallelization for the first two matrix multiplications' innermost loops. For the second matrix multiplication's innermost loop, use `#pragma ACCEL PARALLEL FACTOR=4` and for the third, use `#pragma ACCEL PARALLEL FACTOR=32` to allow specific parallel execution within those loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel from the function `kernel_3mm` that performs matrix multiplications E=A*B, F=C*D, and G=E*F without pipelining or tiling any of the loops and processing loop iterations serially except for the innermost loops of F and G computations which should have a parallel factor of 5. Use the following pragmas: Treat the function as a hardware kernel, disable automatic pipelining, set tiling factor to 1, set parallel factor to 1 generally, but override the innermost parallel factors for F and G computations to 5.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat 'kernel_3mm' as a function for hardware acceleration where each nested loop is executed sequentially without pipelining, tiling, or parallelization, except for the second innermost loops in the 'F' and 'G' matrix calculations, which should run five and two iterations in parallel, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the function `kernel_3mm` as a dedicated hardware acceleration kernel without employing any automatic loop optimization strategies such as pipelining, tiling, or parallelization for all loops, except for one innermost loop where it should be guided to perform parallel execution with a factor of 32 iterations at a time. Specifically, manually configure the relevant #pragma directives for no pipelining, single-tile execution, and strictly sequential processing for all loops, while allowing a parallel factor of 32 for the loop responsible for computing the `F` matrix multiplication only.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication hardware kernel, where each nested loop should not be automatically pipelined, execute iterations in sequence, and have all its iterations processed without breaking them into smaller chunks. The external loops should have these settings explicitly stated, whereas internal loops should inherit the non-pipelining directive, have a single iteration processed per cycle, but in the second matrix multiplication, configure the second innermost loop to attempt processing five iterations of the loop in parallel. Finally, in the last matrix multiplication, enable loop flattening on the second level loop for potential optimization by merging with adjacent loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a kernel function `kernel_3mm` that performs three matrix multiplications (E=A*B, F=C*D, G=E*F) without optimizing the loops for pipelining, tiling, or parallel execution, except for the inner-most loops in the computation of matrices F and G which should process four iterations in parallel. Use the appropriate `#pragma` directives to disable automatic pipelining, set the tile factor to one, and set the parallel factor to one for all loops except the inner-most loops for F and G where the parallel factor should be four.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a matrix multiplication kernel that includes three compute sections for matrix multiplications E := A*B, F := C*D, and G := E*F, using matrices with predefined sizes. Declare the kernel function with necessary pragmas to prevent automatic pipelining, tiling, and parallelization optimizations for loops, except for the second innermost loops in the computation of matrices F and G where the parallel factor is set to 8 and 2 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel, and for each of the three matrix multiplication loops within 'kernel_3mm', disable automatic pipelining, set the tiling to process the loop without partitioning it into smaller blocks, and manage the innermost loop iterations to run consecutively without parallelism, except in the second matrix multiplication where the innermost loop's iterations should run in chunks of five, and in the third matrix multiplication, where they should run in chunks of fifty.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS (High-Level Synthesis) code in C with the prescribed `#pragma` settings, use the following natural language instruction: \n\n\"Create a hardware kernel function to compute the product of matrices without using automatic pipelining, mandatory complete loop execution in original order, and strictly sequential processing of all loop iterations, except for the second inner loop within the 'C*D' and 'E*F' products where parallel factors of 20 and 5 are to be used respectively for optimizations.\" \n\nMake sure to specify the parallel factor in the nested loop responsible for multiplying matrices 'C' and 'D', as well as the loop for multiplying 'E' and 'F', and keep all other loops without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel and for each nested loop structure within this function, turn off automatic pipelining, keep the loop execution whole by disabling tiling with a factor of one, and set iteration execution in series by setting parallel factor to one, except for the second innermost loop in the second major computation block where the parallel factor is increased to five, and the equivalent loop in the third major computation block where the parallel factor is set to fifty.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a High-Level Synthesis (HLS) kernel function named `kernel_3mm` that performs matrix multiplication on three sets of matrices (E=A*B, F=C*D, and G=E*F) with specified dimensions (ni, nj, nk, nl, nm). Disable pipelining for all outer loops, retain the original loop structure by specifying tiling and parallel factors as 1 for these loops, except for certain innermost loops where you parallelize the multiplication with a factor of 4 for the F matrix computation and a factor of 2 for the G matrix computation. Flatten the pipeline within the computation of the E matrix. Do not optimize or unroll the loops manually; let the HLS tool decide on optimization strategies where 'auto' is specified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for an accelerated three-matrix multiplication kernel function, 'kernel_3mm', that takes matrix dimensions and matrices A, B, C, D, E, F, and G as inputs. For all loops:\n\n- Identify the kernel function for hardware acceleration.\n- Do not automatically pipeline any of the loops.\n- Ensure all loops are processed in their whole, original form without any tiling optimization.\n- Run all loop iterations sequentially, not in parallel.\n\nAdditionally:\n\n- For the loops calculating matrix E, flatten the pipeline for the innermost j loop but do not parallelize any iterations. \n- For the C * D = F computation, set the innermost k loop to be parallelizable by a factor of 4.\n- For the final E * F = G computation, set the innermost k loop to be parallelizable by a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function called `kernel_3mm` for matrix-matrix multiplication with the following stipulations: \n\n1. Treat the entire function as a hardware kernel for acceleration.\n2. For all outer and intermediate loops, explicitly avoid the use of automatic pipelining, disable tiling optimizations, and enforce sequential execution of iterations.\n3. Inside the final multiplication loop for calculating matrix `G`, set the parallel execution factor to the maximum possible for the innermost loop only.\n\nIn code, this results in:\n- Using `#pragma ACCEL kernel` to indicate that `kernel_3mm` function is a hardware acceleration kernel.\n- Applying `#pragma ACCEL PIPELINE auto{off}` to prevent the compiler from automatically applying pipelining to all the loops.\n- Utilizing `#pragma ACCEL TILE FACTOR=auto{1}` to avoid tiling optimization.\n- Inserting `#pragma ACCEL PARALLEL FACTOR=auto{1}` to enforce sequential execution in all loops where it is used, except for the final multiplication of `G`, where you set `#pragma ACCEL PARALLEL FACTOR=auto{50}` to maximize parallelism in that specific loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function treating it as a hardware acceleration point, disabling automatic loop pipelining, preventing loops from being divided into smaller chunks, ensuring all loop iterations run sequentially, and maintaining original loop iteration order without parallelism. Also, explicitly enable loop flattening on the second level loop processing `tmp` matrix and enforce parallel computation with manual reduction on `tmp` and `D` matrices without specifying parallelization factors, except for setting a parallel factor of 16 for the reduction in the final loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 3-matrix multiplication kernel where the first matrix multiplication (A*B) is highly parallelized at the outer-loop level with automated pipelining and no tiling or loop unrolling. The second multiplication (C*D) runs without pipelining but allows for minimal parallel execution in inner-most loops with a factor of 2. The final multiplication (E*F) to produce matrix G is neither pipelined nor tiled but allows considerable parallelization within the inner-most loop with a parallel factor of 25. Instruction keywords are as follows: 'auto' for automatic determination by the HLS tool, 'off' for no optimization, and '25' for a specific parallel factor. Set '1' for implicit behavior on tiling and parallelism where optimizations are not applied, and treat the entire function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS code for a hardware kernel function named 'kernel_3mm' with parameters ni, nj, nk, nl, nm and matrices E, A, B, F, C, D, G. For the three matrix multiplication operations E=A*B, F=C*D, and G=E*F:\n\n1. Define the function as a hardware kernel using the \"#pragma ACCEL kernel\" directive.\n2. For each nested loop level, apply the following pragmas:\n   - Disable automatic pipelining using \"#pragma ACCEL PIPELINE auto{off}\".\n   - Prevent tiling by setting the tiling factor to one using \"#pragma ACCEL TILE FACTOR=auto{1}\".\n   - Set parallel execution to be sequential with a factor of one using \"#pragma ACCEL PARALLEL FACTOR=auto{1}\".\n3. In the innermost loop for each matrix multiplication, adjust the parallel factor for the first operation to use the default value, for the second operation to allow twenty iterations to be executed in parallel using \"#pragma ACCEL PARALLEL FACTOR=auto{20}\", and for the third operation to permit fifty parallel iterations using \"#pragma ACCEL PARALLEL FACTOR=auto{50}\".", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function `kernel_3mm` as a hardware acceleration kernel, and within its three primary nested loop blocks, set the following pragmas:\n\n1. For the loop block computing `E := A*B`, disable automatic pipelining entirely, prevent loop tiling and parallelization, but enable loop flattening for just the middle loop.\n\n2. For the loop block computing `F := C*D`, just like the first block, disable automatic pipelining, prevent loop tiling and parallelization, but for the innermost loop, allow parallel execution with a factor of 4.\n\n3. For the loop block computing `G := E*F`, mirror the configurations of the first block, but for the innermost loop, allow parallel execution with a factor of 5.\n\nAdditionally, for all loops, where 'auto' is followed by an option in curly braces, substitute 'auto' with the respective pragma directive, and where the option is 'off' or '1', apply those specific values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function titled 'kernel_3mm' that multiplies matrices A, B, C, and D to produce matrices E, F, and G, while defining loops and applying specific directives to optimize the generated hardware:\n\n1. Declare the function as a hardware acceleration kernel.\n2. In the first matrix multiplication to produce matrix E, ensure the outer loop over 'i' is not pipelined or tiled and is executed sequentially as is. Inside this loop, set the 'j' loop to be flattened without pipelining or tiling, meaning the loop should be fully unrolled if possible, creating a single large loop body. This same process should apply to the 'k' loop nested within the 'j' loop.\n3. Replicate the directives from the steps to multiply matrices C and D to produce matrix F, using identical settings for pipelining, tiling, and parallelization.\n4. For the final matrix multiplication to produce matrix G, disable pipelining and tiling for both the 'i' and 'j' loops, ensuring they run sequentially in their complete form. However, for the 'k' loop nested inside the 'j' loop, set a parallelization factor that attempts to unroll the loop by 50 iterations, allowing them to be processed simultaneously in hardware if possible.\n\nRemember to disable auto-pipelining for all outer loops, use a tiling factor of 1 to avoid subdividing the loops into smaller chunks, and employ a parallelization factor of 1 to enforce sequential iteration, except for the last mentioned case where the factor is set to 50 to exploit parallel computing resources.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, use the following descriptions for each part respectively:\n\n1. Define a function as a hardware acceleration kernel using \"Designate the function 'kernel_3mm' for hardware acceleration\".\n\n2. For matrix multiplication of `E = A * B`, automatically determine the best way to flatten the loop nest and pipeline the outermost loop i.e., loop 'i' with \"Optimize and flatten the loop structure of matrix multiplication E=A*B, while avoiding pipelining of inner loops\".\n\n3. For the assignment of E[i][j], ensure that the computation runs serially with \"Execute the inner loop 'j' over 'E' in serial without optimization or tiling\".\n\n4. For matrix multiplication of `F = C * D`, disable pipelining and parallelism on both 'i' and 'j' loops with \"Avoid any parallel execution and pipelining in the computation of matrix F=C*D\".\n\n5. For the innermost loop 'k' calculating 'F', enable parallel computation with a factor of 2 using \"Allow a mild level of parallel computation, with factor 2, when populating matrix F\".\n\n6. For matrix multiplication of `G = E * F`, again, ensure no pipelining or parallelism on loops 'i' and 'j' with \"Keep the computation of matrix G=E*F fully sequential without any parallelism or pipeline optimization\".\n\n7. Lastly, for the innermost loop 'k' calculating 'G', enable a higher degree of parallel computation with a factor of 16 using \"Apply a high level of parallelism, with factor 16, for the internal accumulation in the calculation of matrix G\".\n\nRemember to properly wrap each code section with the appropriate pragmas based on the given descriptions to match the provided HLS C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with specific pragma directives, provide the following comprehensive but concise natural language instruction:\n\nDefine and optimize a hardware kernel function 'kernel_3mm' for matrix multiplication with three distinct matrix multiplication operations. The first is 'E := A * B' for matrices with dimensions: E(40x50), A(40x60), B(60x50). The second is 'F := C * D' with dimensions: F(50x70), C(50x80), D(80x70). The third is 'G := E * F' with dimensions: G(40x70), E(40x50), F(50x70).\n\nFor each matrix multiplication operation, ensure the following:\n\n1. Disable automatic pipelining of the top-level operation loop, but allow the inner first-level loop to automatically determine whether it should be flattened for pipelining.\n2. Prevent the loop from being tiled by setting the tile factor to 1, thus ensuring it's processed in its full original form.\n3. Run iterations of the top-level loop sequentially without parallel execution by setting the parallel factor to 1.\n4. In the loop that computes 'F', increase the parallel factor of the most inner loop to 4 to allow parallel computations of four iterations at once.\n5. For the final loop that computes 'G', increase the parallel factor of the most inner loop to 25 for parallel processing of 25 iterations simultaneously.\n\nHere are the specific pragma value instructions in natural language corresponding to the provided code segments:\n\nFor all loops besides the innermost loops of 'F' and 'G':\n- Do not automatically pipeline.\n- Do not tile the loop (process fully).\n- Do not parallelize the loop (execute iterations sequentially).\n\nFor the innermost loop of 'F':\n- Increase parallel execution to 4 iterations at once.\n\nFor the innermost loop of 'G':\n- Increase parallel execution to 25 iterations at once.\n\nAssume none of the loops should be automatically pipelined unless explicitly mentioned, and always use the 'auto{ }' feature to assign the values, enclosing the specifics in brackets.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the three matrix multiplication functions within 'kernel_3mm' as individual hardware acceleration kernels, with each nested loop in these functions executed sequentially without pipeline or tiling optimizations, except for the innermost loops of the second and third multiplication functions, where the second function's innermost loop performs eight operations in parallel, and the third function's innermost loop performs thirty-two operations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel, and for each nested loop performing matrix multiplication, use distinct pragma settings. \n\n1. For the multiplication resulting in matrix 'E', disable automatic loop pipelining, maintain the loop's original structure without tiling, and ensure iterations run sequentially. However, within the 'i' and 'j' nested loops for updating 'E', enable flattened loop pipelining but keep the original structure without tiling or parallelizing inner computation over 'k'.\n\n2. For the 'F' matrix calculation, similarly, disable all automatic loop optimizations like pipelining, tiling, and parallel processing outside and inside the 'i' loop. For iterations within the 'j' nested loop, also disable automatic pipelining and tiling, but allow only a factor of 2 parallelism for the 'k' innermost loop iterating over 'C' and 'D'.\n\n3. Lastly, for computing matrix 'G', turn off all forms of automatic pipelining, tiling, and parallel processing both outside and inside the 'i' loop. Inside the 'j' nested loop, keep the pipelining and tiling disabled and maintain a parallelism factor of '50' for the innermost 'k' loop processing the multiplication of 'E' and 'F'.\n\nAcross all loops, whenever automatic optimization is mentioned, it should be explicitly turned off, with the mention of tiling and parallel factors being set to '1' to indicate no modification to the loop structure or iteration execution order, except for the specified cases of parallelism factors of '2' and '50' for certain inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to accelerate the function 'kernel_2mm' as a dedicated hardware kernel, and for the first two nested loops that initialize 'tmp' with results of matrix multiplication, disable automatic pipelining, process the loops in single iterations without tiling and without running in parallel, and allow reductions on 'tmp' without parallelization. In the second pair of nested loops that accumulate matrix multiplication results into 'D', also disable automatic pipelining for the outer loops, stick to single iterations without tiling for both loops, and allow parallelized reductions on 'D' with a parallel factor of 2 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for the 3 matrix multiplication (3mm) kernel, marking the function as a hardware kernel for acceleration. Within the kernel function, execute the first matrix multiplication (E=A*B) with three nested loops for indices i, j, and k without pipelining, tiling, or parallelism optimization\u2014using default unoptimized loop structure. Then perform the second matrix multiplication (F=C*D) with a similar loop structure, but for the innermost loop over index k, apply a parallelism factor of 8 to unroll and parallelize this loop. Lastly, carry out the third matrix multiplication (G=E*F), reusing the loop structure from the first multiplication, and again apply a parallelism factor of 8 for the innermost loop over index k. Use the respective #pragma directives to implement these specifications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, enforce consecutive execution of all loop iterations without utilizing any parallelism or loop transformation techniques such as pipelining or tiling; however, for the second innermost loop within the computation of F, process every 20 iterations concurrently, and for the second innermost loop within the computation of G, process every 4 iterations concurrently.\n\n- Treat 'void kernel_3mm' function as a hardware acceleration kernel.\n- For each loop nest calculating the matrices E, F, and G:\n  - Disallow automatic loop pipelining.\n  - Prevent the loop from being split into smaller tiled chunks.\n  - Execute iterations sequentially, without parallel execution.\n- Specifically for the loop computing each element of matrix F, enable parallel execution of 20 iterations of the second inner loop.\n- Similarly, for the loop computing each element of matrix G, enable parallel execution of 4 iterations of the second inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the 'kernel_3mm' function as a distinct hardware module without any automatic loop optimization or pipelining, keeping all loops in original sequential order, except for parallelizing the innermost loop in the second and third major loop nests with factors of 4 and 16 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the kernel function 'kernel_3mm' for matrix multiplication is treated as a hardware acceleration kernel without automatically applying pipelining, tiling, or parallelization optimizations to any of the loops inside it, except in the third nested loops in the 'F := C*D' and 'G := E*F' computations where the iterations should be parallelized by factors of 5 and 10 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, flatten the pipeline for the first major loop, do not pipeline the subsequent inner loops, and unroll the innermost loops with different factors where specified:\n\n- Treat 'kernel_3mm' function as a hardware kernel for acceleration.\n- For the computation 'E := A*B', enable automatic pipeline flattening for the outermost loop 'i'.\n- For the inner loop 'j', do not automatically pipeline and prevent loop tiling and parallelization by maintaining the loop in its entirety.\n- For the innermost loop 'k' over '60', allow the loop to execute serially without parallelism or tiling.\n- For the computation 'F := C*D', disable automatic pipeline flattening for all loops, maintain the original loop form for all but the innermost loop 'k' over '80', for which you implement a loop unroll with a factor of 8.\n- For the computation 'G := E*F', again, disable automatic pipeline flattening for all loops and maintain the original loop form for all loops, executing each serially and without parallelism or loop tiling.\n\nIn pragma terms, use the following annotations:\n\n- For kernel acceleration: `#pragma ACCEL kernel`\n- For outer loops without pipeline optimization: `#pragma ACCEL PIPELINE auto{off}`\n- For loops with no tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- For non-parallel loops: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Innermost loop 'k' over '80' with unroll factor of 8: `#pragma ACCEL PARALLEL FACTOR=auto{8}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration candidate and execute the matrix multiplication procedures within it using a series of nested loops for the multiplication of matrices A and B, C and D, and E and F, respectively. For all loops, turn off automatic pipelining, do not tile the loops, and execute the outermost and middle loops without parallelism. Within the operation E[i][j] += A[i][k] * B[k][j], do not parallelize. However, explicitly parallelize the loop where F[i][j] += C[i][k] * D[k][j] is computed with a factor of 8, and further parallelize the loop calculating G[i][j] += E[i][k] * F[k][j] with a factor of 25.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' for hardware acceleration. Within this function, disable automatic pipelining for all loops, maintain the original structure of each loop without tiling, and execute all loop iterations sequentially. For the two-dimensional 'tmp' array computation, flatten the loop pipelining to attempt a full loop unrolling if possible. Finally, when updating the 'tmp' and 'D' arrays within their innermost loops, specify no parallel factor for 'tmp', and set a parallel factor of 5 for 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a matrix-matrix multiplication kernel where:\n\n1. The kernel is marked for hardware acceleration.\n2. All loops should be processed without automatic pipelining.\n3. Loops should not be tiled and should be executed in their entirety.\n4. Loops should be processed serially with the following exceptions:\n   - In the second matrix-matrix multiplication, the innermost loop should have a parallel factor of 4.\n   - In the third matrix-matrix multiplication, the innermost loop should have a parallel factor of 8.\n\nFor each loop:\n- Use '#pragma ACCEL PIPELINE off' to turn off automatic loop pipelining.\n- Use '#pragma ACCEL TILE FACTOR=1' to execute loops in their entirety without tiling.\n- Use '#pragma ACCEL PARALLEL FACTOR=1' to ensure serial execution of loop iterations, adjusting the factor only for the specified exceptions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a function treated as a hardware acceleration kernel that computes the matrix multiplication of three pairs of matrices (A*B, C*D, E*F) without using pipelining, tiling, or parallelizing optimizations. Specifically, disable pipelining for the entire function, set the tiling factor to 1 and the parallel factor to 1 for the first-level loops, while for the second-level loops within the matrix multiplications, keep the parallel factor to 1 for 'A*B' and increase it to 2 for 'C*D' and 'E*F' second-level loops to allow minimal parallelization only at the innermost level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and within it, for all nested loops ensure that they execute sequentially without pipelining or parallel processing and without being tiled, except for the innermost loop in the second set of nested loops where tiling with a factor of 8 and parallel execution with a factor of 5 should be applied. Use reduction pragmas on variables 'tmp' and 'D' in their respective innermost loops with the default factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a hardware-accelerated 2mm kernel function that performs a matrix multiplication followed by another matrix multiplication and an accumulation on a pre-existing matrix, with disabling automatic pipelining, fixing the tiling factor to prevent loop nest splitting, and ensuring no parallel execution of outer loops, while both enforcing and optimizing parallel execution and reduction operations in the innermost loops. Specifically, set the innermost parallel factor for the first multiplication's reduction to one and for the second multiplication's reduction to sixteen; also, for the loop that accumulates onto matrix D, enable automatic pipelining and flatten the second loop level. Remember to mark the function as a kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function treating the function as a hardware acceleration kernel. Ensure that the outer two loops for both computational stages are not pipelined, not tiled, and execute iterations serially. Within the first computational stage's innermost loop, perform the accumulation without pipelining buts allow operations to run in parallel with an automatic factor, applying reduction to the 'tmp' array. In the second computational stage, allow the outer loop to be pipelined automatically, tile it with a factor of 8, and run iterations serially, yet within the innermost loop, allow operations to run in parallel for the 'D' array with a factor of 4, again applying reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given code, you should provide instructions that can be interpreted as commands for the HLS tool to handle the loops. Specifically, direct the HLS tool as follows:\n\nTreat the following function as a hardware kernel for acceleration to denote '#pragma ACCEL kernel'. \n\nFor each loop nest within the function, specify to:\n\n1. Not automatically pipeline the loop with the instruction '#pragma ACCEL PIPELINE auto{off}'.\n\n2. Process the loop in its original, full form without breaking it into smaller chunks with the instruction '#pragma ACCEL TILE FACTOR=auto{1}'.\n\n3. Execute the iterations of the loop sequentially, without parallelization, for the first two nestings with '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n\nAdditionally, indicate a specific level of parallelism for the innermost loops in the matrix multiplications:\n\n1. For the inner loop computing 'E[i][j]', retain sequential execution by stating '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n\n2. For the inner loop computing 'F[i][j]', parallelize 20 executions simultaneously with the command '#pragma ACCEL PARALLEL FACTOR=auto{20}'.\n\n3. For the inner loop computing 'G[i][j]', parallelize 32 executions simultaneously with the instruction '#pragma ACCEL PARALLEL FACTOR=auto{32}'.\n\nMake sure that each '#pragma' applies directly to the corresponding loop to provide the expected level of optimization for the high-level synthesis tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel and execute matrix multiplication for matrices E=A*B, F=C*D, and G=E*F, ensuring that all loops run serially without hardware optimization such as pipelining, tiling, or parallelization, except for the third nested loops inside the matrices F and G multiplications where parallelization factors of 8 and 4 are to be used respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function for 2D matrix multiplication followed by addition, named 'kernel_2mm', taking matrix sizes (ni, nj, nk, nl), scale factors (alpha, beta), and matrices (tmp, A, B, C, D) as arguments. Disable automatic pipelining for the entire function. For each top-level and nested loop within the function, undertake the following: preserve the full loop structure without tiling or parallelizing; flatten the pipeline for the first nested loop to combine loops during pipelining; reduce the 'tmp' matrix with no parallelization in the second nested loop of the first main loop; and reduce the 'D' matrix with a parallel factor of 4 in the second nested loop of the second main loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a matrix multiplication kernel function with three operations: E = A * B, F = C * D, and G = E * F. For each operation, do not perform automatic pipelining but do allow automatic decisions on tiling and parallel factors except on the innermost loops of E and G operations where the parallel factor should be explicitly set to 1, and on the innermost loop of the F operation where the parallel factor should be set to 5. Each matrix multiplication should operate on 2D arrays with varying dimensions specified by the input parameters, and initialize the result matrices with zeros before performing the multiplications. Use 'auto' for the pragma factors where automatic decisions are to be allowed. Treat the entire function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the function 'kernel_2mm' for hardware acceleration while minimizing automatic compiler optimizations; disable automatic pipelining for the overall function, treat each loop in its original sequence without tiling or parallelizing the iterations, except for the innermost loop where there is a reduction on 'tmp' and 'D', which should be parallelized with a parallel factor as required by the context of its use. Use flattening to combine nested loops where applicable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the HLS C code provided, instruct the compiler to designate the entire function as a hardware kernel for acceleration, and within each matrix multiplication block, sequentially execute the loop iterations without automatic pipelining, tiling, or parallelization, except in specific inner loops where you set a predetermined parallel factor of 8 for multiplying matrices C and D, and a parallel factor of 5 for multiplying matrices E and F. \n\nUse the following directives:\n\n- '#pragma ACCEL kernel' to mark the function as a hardware kernel.\n- '#pragma ACCEL PIPELINE auto{off}' to disable automatic loop pipelining.\n- '#pragma ACCEL TILE FACTOR=auto{1}' to prevent loop tiling.\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' to enforce sequential execution in most loops.\n- '#pragma ACCEL PARALLEL FACTOR=auto{8}' specifically in the loop where matrix C is multiplied by D.\n- '#pragma ACCEL PARALLEL FACTOR=auto{5}' specifically in the loop where matrix E is multiplied by F.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the following function as a hardware acceleration kernel, and for each nested loop level within the function, avoid pipelining and tiling optimizations, and run the outer loops without parallelization while specifying distinct parallel factors for the innermost loops.\n\nSpecifically:\n- Treat the entire function 'kernel_3mm' as a hardware kernel for acceleration.\n- For the first two nested loops (calculating matrix E), disable pipelining at the first level and enable pipelining with loop flattening at the second level. Use a tiling factor of 1 for both levels and run the outer loop in serial order.\n- Within the innermost loop of the first two nested loops, run iterations serially.\n- For the next two nested loops (calculating matrix F), disable pipelining for both levels with a tiling factor of 1 and serial execution for the outer loop.\n- Specify a parallel factor of 2 for the innermost loop while calculating matrix F.\n- For the last two nested loops (calculating matrix G), follow the same approach of no pipelining, a tiling factor of 1, and serial execution for the outer loop.\n- Apply a parallel factor of 10 for the innermost loop while calculating matrix G.\n\nCorresponding #pragma directives for each loop level are:\n- '#pragma ACCEL kernel' for designating the function as a kernel.\n- '#pragma ACCEL PIPELINE auto{off}' for disabling loop pipelining.\n- '#pragma ACCEL TILE FACTOR=auto{1}' for disabling loop tiling.\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' for serial loop execution with a parallel factor of 1.\n- '#pragma ACCEL PIPELINE auto{flatten}' for enabling pipelining with loop flattening.\n- '#pragma ACCEL PARALLEL FACTOR=auto{2}' for setting a parallel factor of 2.\n- '#pragma ACCEL PARALLEL FACTOR=auto{10}' for setting a parallel factor of 10.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to define `kernel_2mm` as a hardware acceleration kernel and configure the loop optimizations as follows: disable automatic pipelining for the top-level loops of the algorithm; run them in their original, unsplitted form, and execute iterations sequentially. Inside the nested loops where `tmp` is being calculated, use flattened pipelining to optimize loops across nested levels without altering their structure; avoid parallelization and tiling alterations. However, when updating the `D` matrix, enable the default automatic pipelining for the corresponding top-level loop, keep the loop structure intact and sequential, and allow the innermost loop, which combines `tmp` and `C` into `D`, to be parallelized with a factor of 4 to accelerate reduction operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate high-level synthesis (HLS) code for a matrix multiplication accelerator function `kernel_3mm` that performs three matrix multiplications (E=A*B, F=C*D, and G=E*F) with fixed-sized double-precision arrays, ensuring the following characteristics for each loop nest:\n\n1. For the entire function, designate it as a hardware kernel to be accelerated.\n2. For each of the top-level matrix-multiplication loops, disable automatic pipelining.\n3. Process each loop in its entirety without tiling, applying default tiling factors.\n4. Execute each loop iteration sequentially, avoiding parallel processing within the top-level loops.\n5. Inside the first and third matrix multiplication nests, explicitly flatten the second-level loop for pipelining while maintaining the outer loops without pipelining.\n6. For the nested loops performing the core multiplication, maintain the default sequential execution except for the loop calculating F[i][j], where you should parallelize the innermost loop with a factor of 32.\n\nUse appropriate `#pragma ACCEL` directives to achieve the described behavior, setting values to 'off' if pipelining should be disabled, to '1' if tiling and parallelization should occur at default factor levels, and to 'auto{flatten}' to flatten nested loops for pipelining. For the specialized parallelization factor within the F calculation, explicitly specify the factor as 'auto{32}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel, and for every loop within, disable automatic pipelining, prevent the loop from being tiled or altered from its original size, and ensure all iterations run sequentially. However, in the nested inner loops, allow a parallel reduction operation with specific unrolling factors of 10 for the first reduction on `tmp` and 25 for the second reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function `kernel_2mm` as a dedicated hardware acceleration kernel; do not automatically pipeline the first two loops nor parallelize their iterations, and process their iterations in the original order without tiling. However, in the nested loop where `tmp` is being calculated, allow the operations to reduce `tmp` to operate sequentially. Then, pipeline the outer loop while processing the D matrix updating, tile the loop where `D` is updated with a factor of 8, and parallelize the nested loop's operations reducing `D` with a parallelism factor of 4, with sequential execution for reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS code in C for a kernel function named 'kernel_3mm' that performs matrix multiplication on given input matrices without applying automatic pipelining, tiling, or loop parallelism optimizations, except for specific loops where the iterations inside the innermost loop are to be processed in parallel with a designated factor. Use the pragma directives to treat the function as a hardware kernel, set the pipeline optimization to 'off', tiling factors to '1' for all loops to maintain the original loop form, and parallel factors to '1' for all but the specified loops. Inside the loops for multiplying matrices 'C' and 'D' as well as 'E' and 'F', set the parallel factors respectively to '5' and '4' for the innermost loops to enable limited parallel execution of these iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel, and for each loop within the function, set the following constraints: turn off automatic pipelining, do not tile the loop, and execute iterations sequentially. However, flatten the inner loop of the first double-nested loop pair. In the second double-nested loop pair, automatically determine the pipelining strategy for the outer loop and apply a parallelism factor of 8 with reduction for the inner loop. For the first double-nested loop pair, apply a parallelism factor of 1 with reduction for the inner loop, despite its 'auto' directive, implying no parallelism is desired.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following comprehensive yet concise natural language instructions:\n\n1. Designate the `kernel_2mm` function as a hardware kernel intended for acceleration.\n2. For the outer loops iterating over indexes `i` and `j`, disable automatic pipelining and ensure each iteration is not subdivided into parallel computations by specifying both pipeline and parallel factors to be manually defined as 1.\n3. Within the nested loops, for the operations updating the `tmp` array, enforce no automatic pipelining but allow for an automatic decision on the best factor for tiling without specifying a value. Activate parallel execution for the reduction operation on the `tmp` array with the factor set for automatic determination.\n4. In the final set of nested loops updating the `D` array, turn off automatic pipelining for the outer loop, set manual tiling with a factor of 8, and maintain sequential execution for the outer iterations by setting the parallel factor to 1. For the innermost loop's reduction operation on `D`, allow HLS to automatically choose the appropriate parallel factor.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a matrix multiplication kernel that avoids any automatic loop pipelining, tiling, or parallelization, except for one specific multiply-accumulate operation where 8 iterations of the innermost loop run in parallel. For all loops, specify that:\n\n1. The kernel is marked for acceleration.\n2. No loop pipelining is allowed; loops should run sequentially from start to end.\n3. No loop tiling optimizations are permitted; each loop should run in its entirety without subdivision.\n4. Loops should execute serially with no automatic parallel execution, with the single exception where the innermost loop of the second matrix multiplication has a parallel factor of 8 for parallel execution.\n\nFor each pragma, use the following settings:\n\n- For 'kernel', use \"#pragma ACCEL kernel\".\n- To prevent automatic pipelining, use \"#pragma ACCEL PIPELINE auto{off}\".\n- To stop tiling, use \"#pragma ACCEL TILE FACTOR=auto{1}\".\n- To halt automatic parallel execution, use \"#pragma ACCEL PARALLEL FACTOR=auto{1}\", and use \"#pragma ACCEL PARALLEL FACTOR=auto{8}\" for the specific loop where parallelization is desired.\n\nApply these pragmas to three matrix multiplication sections within the function 'kernel_3mm'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, perform the matrix multiplication and accumulation without automatic loop pipelining or tiling, and maintain the original sequential order of loop iterations. Inside the first loop nest, enable the loop to be considered for complete unrolling during optimization. In contrast, inside the second loop nest, allow the outer loop to be tiled with a factor of 4, and set the innermost loop to consider parallelism with a factor of 50, allowing for reduction on the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and within it, execute each loop sequentially without implementing automatic pipelining, tiling, or parallelization. Specifically, disable automatic pipelining for all loops and set the tile and parallel factors to 1, except for the inner loop within the second main loop, where a tile factor of 8 is specified, and a parallel factor of 4 is applied to the loop that accumulates results into 'D'. Use reduction directives for parallel accumulations in 'tmp' and 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function for a 2mm algorithm, mark all the loops to run serially without loop pipelining or tiling, except for the innermost reduction loop in the second loop nest where you apply parallelism with factor 32. Use `auto` with placeholders to indicate automated suggestions for pipeline and tile factors, but specifically set pipeline to 'off', tile factor to '1', and parallel factor to '1' where auto is not used. Use reduction pragmas for accumulating the results into `tmp` and `D` within the parallelized loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function 'kernel_2mm' as a hardware acceleration kernel but disable any automatic pipelining for the two primary nested loops, prevent loop tiling and ensure all iterations run sequentially. During the calculation of the 'tmp' array, allow partial parallelization with a factor of 10 for the innermost loop but with a reduction clause to accumulate results correctly. When subsequently updating the 'D' matrix, allow the outermost loop to be flattened for pipelining but enforce sequential execution for the inner loops, adding a parallel reduction for the innermost loop with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function to perform three matrix-matrix multiplications E=A*B, F=C*D, and G=E*F for predefined sizes using arrays, ensuring: \n\n1. The entire function should be treated as a hardware acceleration kernel.\n2. All the loops should be executed serially without any automatic pipelining optimizations.\n3. The loops should be fully unrolled when processing the innermost computations, except for the F=C*D calculation's innermost loop which should be partially parallelized with a factor of 16, and G=E*F's innermost loop parallelized with a factor of 2.\n4. None of the loops should be tiled; they should run in their original iteration space. \n\nFor each loop and nested loop in the function, you'll need to add specific pragmas before the loop starts:\n\n- For pipeline pragma, use '#pragma ACCEL PIPELINE auto{off}' to prevent automatic loop pipelining optimizations.\n- For tile pragma, use '#pragma ACCEL TILE FACTOR=auto{1}' to avoid loop tiling and process it in its original form.\n- For parallel pragma above the first and second nested loops, use '#pragma ACCEL PARALLEL FACTOR=auto{1}' to unroll the loops fully and prevent them from running in parallel. For the innermost loop in the F=C*D calculation, use '#pragma ACCEL PARALLEL FACTOR=auto{16}', and for G=E*F's innermost loop, use '#pragma ACCEL PARALLEL FACTOR=auto{2}' to specify the desired parallelization factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS (High-Level Synthesis) code in C with the specific pragmas, use the following natural language instruction:\n\n\"Define a function treated as a hardware kernel performing a 2mm (two-matrix multiply) operation, which first calculates the matrix product of `alpha * A * B` and stores the result in `tmp`, then updates matrix `D` with the product of `tmp * C` scaled by `beta` added to the original `D`. During the computations, perform all loops without automatic pipelining, run each top-level loop in its full form, and process the iterations sequentially. However, for the innermost loop calculating `tmp`, enable loop flattening for better performance. Also, when updating `D`, process the loop that scales `D` by `beta` with tiling factor 4, while maintaining sequential processing, and for the innermost loop, incorporate parallelism with significant unrolling where appropriate for resource-usage tradeoffs.\"\n\nTranslated to HLS `#pragma` directives, this would result in:\n- \"#pragma ACCEL kernel\" to indicate the function is meant for hardware acceleration.\n- \"#pragma ACCEL PIPELINE auto{off}\" to disable automatic pipelining for a loop.\n- \"#pragma ACCEL TILE FACTOR=auto{1}\" to maintain the original loop form without tiling.\n- \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" to ensure iterations are executed sequentially.\n- \"#pragma ACCEL PIPELINE auto{flatten}\" to request loop flattening.\n- \"#pragma ACCEL PARALLEL reduction = [variable] FACTOR=auto{[value]}\" for specifying a parallel reduction operation with an explicit factor, where `[variable]` is either `tmp` or `D`, and `[value]` is the degree of unrolling applied, `1` or `25` as appropriate for each loop in the given context.\n\nUse these directives appropriately above the respective loops and at the start of the function to guide the HLS tool during the synthesis process according to the specified behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare a function as a hardware acceleration kernel, then within this function, ensure that the outer loops run sequentially without any automatic compilation optimizations to pipeline, tile, or parallelize them. In the first nested loop, allow the compiler to decide whether to pipeline it or not, but prevent any tiling or parallelization optimizations, accumulating results sequentially for each element in 'tmp'. For the final nested loop, enable pipelining optimizations as appropriate, increase the amount of computation per cycle by a factor of 8 for the intermediate loop, and aggressively parallelize the innermost loop with a high degree of data parallelism, specified by a factor of 50, while performing a reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware kernel for acceleration. For the two outer loops and their respective inner loops over the matrices: prevent automatic optimization pipelines, execute each loop iteration in sequence without dividing them into smaller chunks, and perform them serially without parallelism. Within the inner loops, specifically in the multiplication and accumulation of matrix elements, allow the first inner loop to be completely flattened but executed without parallelism, while the second inner loop should use parallel reduction on the result matrix with as many parallel elements as there are columns in the matrix being reduced.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Designate the function 'kernel_2mm' as a hardware acceleration kernel, and for each nested loop structure within it, turn off automatic pipelining, prevent tiling, enforce sequential execution, but specifically enable parallel reduction for the innermost loops 'k' in both segment calculations with a factor of 10 for the first and 25 for the second, thereby allowing partial sums of 'tmp' and 'D' to be computed in parallel within those loops.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the `kernel_3mm` function as a target for hardware acceleration, where the function calculates the product of matrices A\u00d7B, C\u00d7D, and E\u00d7F, storing the results in matrices E, F, and G, respectively. For the matrix multiplication loops:\n\n1. For the calculation of E (A\u00d7B), automatically determine if flattening nested loops into a single loop can benefit pipeline parallelism for the outermost loop `i`, while preventing pipelining for the nested loop `j`, leaving all parallelism and tiling for these loops to be automatically optimized with a preferred factor of 1.\n\n2. For the calculation of F (C\u00d7D), disable pipelining for all loops `i` and `j` and instruct the tool to optimize parallelism and tiling for these loops with default settings, except for the innermost loop `k`, where the tool should use a parallelism factor of 2.\n\n3. For the calculation of G (E\u00d7F), again, prevent pipelining for all loops `i` and `j`, using an explicit parallelism and tiling factor of 1, while the innermost loop `k` should optimize parallelism with a factor of 2.\n\nBased on the directives, the following #pragma values should be used respectively:\n\n- Treat `kernel_3mm` function as a hardware kernel with `#pragma ACCEL kernel`.\n- Automatically decide on loop pipelining and flattening with `#pragma ACCEL PIPELINE auto` and `{flatten}` for loop `i` in E calculation.\n- Prevent pipelining for loop `j` in E calculation with `#pragma ACCEL PIPELINE auto{off}`.\n- Instruct the compiler to automatically optimize parallelism and tiling for each loop with `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}` for all loops in E, F, and G calculations, except for:\n   - Loop `k` in F and G calculation should have a parallelism factor of 2 with `#pragma ACCEL PARALLEL FACTOR=auto{2}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, you should request the following processes and associated pragma values in natural language:\n\n1. \"Treat the entire `kernel_3mm` function as a hardware acceleration kernel.\"\n2. \"Do not automatically pipeline any of the loops within the `kernel_3mm` function.\"\n3. \"Execute all loops in their entirety, without tiling or breaking them into smaller, parallel processed chunks.\"\n4. \"Make an exception in the second loop nest by processing eight iterations of the innermost loop in parallel.\"\n5. \"Additionally, maintain the loop structure for the last matrix multiplication (`G := E*F`), but allow the compiler freedom to flatten the pipeline if it develops a more efficient implementation.\"\n\nFor the #pragma directives, use the following specific values:\n\n- `#pragma ACCEL kernel` to declare the `kernel_3mm` function as a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` will disable automatic pipelining for all loops where it's applied.\n- `#pragma ACCEL TILE FACTOR=auto{1}` indicates no tiling, preserving the original loop structure.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` indicates that all loop iterations should be processed serially, preserving the original loop structure.\n- `#pragma ACCEL PARALLEL FACTOR=auto{8}` in the second loop nest, suggesting to process eight iterations of the innermost loop in parallel.\n- `#pragma ACCEL PIPELINE auto{flatten}` on the final loop nest, allowing the compiler the option to flatten the loop for potential efficiency gains.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as the target for hardware acceleration. Within this function, disable automatic loop pipelining and preserve the original full structure for the outermost loops, while allowing inner loops to be automatically optimizable. Specifically, in the matrix computation blocks, for the first block where 'tmp' matrix is calculated, carry out each innermost loop iteration in chunks of ten while accumulating values in 'tmp', and let the remaining loops be optimized individually. In the second block where 'D' matrix is updated, tile the second loop into eight parts, process the final loop iterations in chunks of twenty-five with accumulations in 'D', and prevent automatic pipelining optimizations in the outermost loop of this block.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a triple matrix multiplication kernel that treats all loops in their original, non-pipelined, and non-tiled fashion, except for the innermost loops in the calculation of matrices F and G, where you would process 8 and 16 iterations in parallel, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to:\n\n1. Identify the entire function `kernel_3mm` as a hardware acceleration candidate.\n2. Avoid pipelining all loops, thus disabling automatic pipelining optimization.\n3. Do not tile any loops, signifying that loops should not be divided into smaller chunks for improved parallelism or local memory usage.\n4. Process all loop iterations sequentially, ensuring they are run in order and not in parallel, with the exception of the innermost loop calculating `F[i][j]`, which should be allowed to execute two iterations in parallel, and the innermost loop calculating `G[i][j]`, which should be allowed up to 32 parallel iterations.\n5. Flatten the nested loops during the computation of `E[i][j]`, hinting at potential full loop unrolling or other flattening optimizations.\n\nSpecifically, for each directive:\n\n- Use `#pragma ACCEL kernel` to indicate that the function should be compiled as a hardware kernel.\n- Apply `#pragma ACCEL PIPELINE \"off\"` to prevent pipelining of all loops.\n- Apply `#pragma ACCEL TILE FACTOR=1` to each loop to indicate they should not be subdivided.\n- Apply `#pragma ACCEL PARALLEL FACTOR=1` to every loop to enforce serial execution, except:\n  - Use `#pragma ACCEL PARALLEL FACTOR=2` for the innermost loop computing `F[i][j]`.\n  - Use `#pragma ACCEL PARALLEL FACTOR=32` for the innermost loop computing `G[i][j]`.\n- Use `#pragma ACCEL PIPELINE auto{flatten}` to suggest flattening of the loop nest during the computation of `E[i][j]`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To create the provided HLS C code, instruct the HLS tool as follows:\n\n- Define the function 'kernel_3mm' as a hardware kernel for acceleration.\n- In the multiplication of matrices A and B to compute matrix E, disable automatic pipelining, maintain loops in full form without tiling, and execute loop iterations sequentially.\n- For the computation of matrix E, apply limited loop optimizations like flattening to enhance parallelism within the innermost k-loop but ensure all loop iterations run sequentially.\n- While computing matrix F from matrices C and D, use the same strategy as the computation of matrix E: disable all aggressive optimization techniques such as pipelining and parallel execution, and keep the loops in their original, full form.\n- For the final matrix multiplication computing matrix G, apply a parallelization factor of 50 to the innermost k-loop, enabling it to process 50 loop iterations concurrently while having other optimization strategies turned off.\n- Encase the primary matrix multiplication sections within 'scop' and 'endscop' pragmas if necessary to demarcate the region for analysis and optimization.\n\nSpecific #pragma directives to describe in natural language:\n- For '#pragma ACCEL KERNEL', say \"Declare the function as a dedicated acceleration hardware kernel.\"\n- For all locations with '#pragma ACCEL PIPELINE auto{off}', say \"Disable any automatic pipelining for this loop.\"\n- For '#pragma ACCEL PIPELINE auto{flatten}', say \"Allow the loop to be flattened for increased parallelism but without traditional pipelining.\"\n- For every occurrence of '#pragma ACCEL TILE FACTOR=auto{1}', say \"Maintain the loop in its original, full form without creating smaller tiled loops.\"\n- For '#pragma ACCEL PARALLEL FACTOR=auto{1}' appearing multiple times, say \"Execute iterations of this loop one after another, with no parallel execution.\"\n- For the single instance of '#pragma ACCEL PARALLEL FACTOR=auto{50}', say \"Allow fifty iterations of the loop to be processed in parallel.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following matrix multiplication operations into HLS C code with specific pragmas that dictate no automatic pipelining, no loop tiling nor parallelization for loops, except for the specified cases:\n\n1. Declare a function as a hardware kernel.\n2. For the matrix multiplication `E = A * B`, disable automatic pipelining and set both tile and parallel factors to 1.\n3. For the matrix multiplication `F = C * D`, similarly disable automatic pipelining and use the original loop form, but allow a parallel factor of 8 for the innermost loop over variable k.\n4. For the matrix multiplication `G = E * F`, disable automatic pipelining, use the original loop form but enable a high degree of parallelism for the innermost loop over variable k with a parallel factor of 50.\n\nUse the following pragma arguments respectively: \n- To disable pipelining, use `PIPELINE \"off\"`.\n- To prevent loop tiling, use `TILE FACTOR=1`.\n- To run the iterations of the loop sequentially, use `PARALLEL FACTOR=1`, but adjust the factor as indicated for loops with different parallelism requirements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_2mm' as a hardware function that calculates 'D := alpha*A*B*C + beta*D' for given matrices, ensure all loops within do not use pipelining, are not tiled, and do not run iterations in parallel, except within the innermost loops where the 'tmp' array requires a parallel reduction with a factor of 10 and the 'D' array needs a parallel reduction with a factor of 25.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware acceleration kernel; for all loops within this kernel, disable automatic pipelining, and do not automatically partition the loops but execute each in a serial, non-parallel fashion with a single iteration processed at a time. However, within the second main loop, allow for loop pipelining as default, and for the 'j' loop, process it in chunks of 8 iterations, while the innermost loop 'k' will involve reduction on 'D' with 50 parallel executions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a matrix multiplication kernel that processes the multiplication of three matrix pairs (AxB, CxD, ExF), storing the results in matrices E, F, and G respectively. Do not automatically pipeline the first and third matrix multiplication loops but allow the tool to decide the best pipeline strategy for the second matrix multiplication loop; process these loops in their original full form without breaking them into smaller chunks by setting TILE and PARALLEL FACTOR pragmas to 'auto{1}' for all loops except for the innermost loop of the second matrix multiplication where PARALLEL FACTOR is set to 'auto{20}'. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following high-level HLS directives into concise natural language instructions:\n\n1. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n2. For the matrix multiplication sections, do not automatically pipeline the outer two loops, process them without tiling or parallelism, while the third nested loop can be parallelized:\n   - For `E := A*B`, disable pipelining, tiling, and set parallel factor to 1 in all the loops: \n     ```\n     #pragma ACCEL PIPELINE auto{off}\n     #pragma ACCEL TILE FACTOR=auto{1}\n     #pragma ACCEL PARALLEL FACTOR=auto{1}\n     ```\n   - For `F := C*D`, disable pipelining and tiling for the outer two loops, but allow full loop flattening for the second nested loop, and set parallel factor to 1 for all loops:\n     ```\n     #pragma ACCEL PIPELINE auto{off}\n     #pragma ACCEL TILE FACTOR=auto{1}\n     #pragma ACCEL PARALLEL FACTOR=auto{1}\n     #pragma ACCEL PIPELINE auto{flatten}\n     ```\n   - For `G := E*F`, disable pipelining and tiling for the outer two loops, allow the pipeline directive to decide for the second loop, and manually set parallel factor to 50 for the third nested loop:\n     ```\n     #pragma ACCEL PIPELINE auto{off}\n     #pragma ACCEL TILE FACTOR=auto{1}\n     #pragma ACCEL PARALLEL FACTOR=auto{1}\n     #pragma ACCEL PIPELINE auto{}\n     #pragma ACCEL PARALLEL FACTOR=auto{50}\n     ```", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel and rigorously maintain the original loop structure without any pipelining, tiling, or parallel execution optimizations, except for a specific reduction operation within the innermost loops where you should enable a minimal level of parallelism by allowing two simultaneous operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel for a 2D matrix multiplication operation, ensuring that each loop operates sequentially without automatic optimization or parallelism enhancements, except for the second innermost loop, where the tile factor is set to 8 and the final innermost loop where parallel computations are allowed with a parallel factor of 50 and a reduction on the output matrix 'D'.\n\n- Mark the entire function as a hardware acceleration kernel.\n- Inhibit automatic pipelining for all loops.\n- Keep all loops in their original, full form by specifying a tiling factor of 1, except for the second innermost loop where you set the tiling factor to 8.\n- Keep all loops running iteratively one after the other with a parallel factor of 1, except for the final innermost loop where you set the parallel factor to 50 and note that the loop reduction applies to the matrix 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function performing three matrix multiplications (E=A*B, F=C*D, G=E*F) where each nested loop is unoptimized by disabling automatic pipelining, using the original loop iterations without tiling or breaking the loops into parallel chunks, except for one loop in the calculation of F and G where the iterations are partially parallelized. Specifically:\n\n- Treat the function `kernel_3mm` as a hardware acceleration kernel.\n- For the computation of matrix E, disable pipelining, set tile factor to 1, which means no tiling, and ensure that the outer and inner loops run serially with parallel factor set to 1.\n- When initializing matrix E, enable loop flattening in pipelining to process the matrix initialization as a single loop where applicable. However, for the computation of E keep the two inner loops running in serial with no tiling or parallelization.\n- For the computation of matrix F, disable pipelining and set tile factor to 1 in the outer loops, but in the innermost loop, enable partial parallel processing with a parallel factor of 80, indicating that 80 operations can be performed in parallel.\n- For the computation of matrix G, disable pipelining for the outer loops, keep tile factor at 1, but set a partial parallel factor of 50 for the innermost loop, allowing 50 operations to occur in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated kernel function named 'kernel_2mm' that performs a matrix multiplication and addition calculation without automatic loop pipelining, ensuring the loops maintain their full structure and execute sequentially. Specifically, disable automatic pipelining for the top-level loops, maintain the full structure by not tiling with a factor other than 1, and ensure the iterations of the top-level loops are not parallelized. For the nested loop responsible for the first matrix multiplication, permit parallel execution with a specified factor of 16 for reduction on 'tmp'. For the nested loop that performs element-wise multiplication and addition, allow the compiler to insert pipelining automatically with loop flattening enabled, a tiling factor of 80, and a parallel factor of 1; however, do not parallelize the innermost reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function `kernel_3mm` that computes the product of matrices E=A*B, F=C*D, and G=E*F. Each multiplication step should not use automatic pipelining, should be processed sequentially without parallelization, and without tiling. However, for the innermost loops of the F and G computations, set the parallelization factors to 8 and 10 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware acceleration kernel, and disable automatic pipelining for all loops. Keep all loops in their original full iteration space by setting the tile factor to 1, and enforce sequential execution of loop iterations by setting the parallel factor to 1. Specifically:\n\n1. For the loop updating the `tmp` array, enable pipelining but prevent loop flattening.\n2. For the innermost loop calculating the `tmp` array, allow for reductions on `tmp` with parallel factor set to its default behavior, which executes sequentially due to the parallel factor being set to 1.\n3. For the innermost loop updating the `D` array, allow for reductions on `D` and set a parallelization factor of 25 to enable parallel execution of this innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the `kernel_2mm` function as the focal point for hardware acceleration. Throughout the function, preserve the structure of all loops by avoiding any automatic loop pipelining, tiling, or parallelism optimizations. Explicitly control the parallel reduction of the `tmp` and `D` arrays within nested loops by allowing parallelism within the innermost loop that computes `tmp[i][j]`, using up to 16 parallel processing elements, and maintaining sequential execution for the innermost loop that updates `D[i][j]`. For the second-level nested loop where `D[i][j]` is updated, enable loop flattening to potentially combine nested loops for greater efficiency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` for hardware acceleration and set the following pragma directives for optimization: \n\n1. Mark the function as a hardware kernel.\n2. For the outermost loops, use the default pipeline strategy but ensure they are not automatically pipelined or partitioned into smaller chunks, and execute iterations serially.\n3. For the first inner loop calculating `tmp`, automatically apply pipeline optimization, do not tile the loop, and execute iterations serially with a reduction applied to `tmp` using 32 parallel factor.\n4. For the second inner loop updating `D`, allow the tool to decide the best pipeline strategy, explicitly tile the loop with a factor of 80, and execute iterations serially with a reduction applied to `D` without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel function named 'kernel_2mm' for 2D matrix multiply and addition operations, where the inputs are integer dimensions ni, nj, nk, nl, and double-precision floating-point scalars alpha, beta, and four 2D matrix arrays tmp, A, B, C, D of corresponding sizes. Configure the following HLS pragmas to control optimization:\n\n1. Declare the function as an acceleration kernel.\n2. For the top-level multiplication and addition loops, do not apply automatic pipelining.\n3. For these loops, also prevent the tiling optimization and do not parallelize iterations automatically.\n4. Inside the first nested loop, flatten the pipelining to improve loop-nesting handling but still do not tile or parallelize automatically.\n5. Apply a reduction to the intermediate variable 'tmp' within the third nested loop without parallelizing its iterations.\n6. For the second set of nested loops, enable the default pipeline setting for the outer loop while again preventing tiling and automatic parallelization.\n7. For the innermost loop, use a parallelization factor of 32 while applying a reduction on the 'D' matrix, enhancing parallel processing capability.\n\nEnsure you use the 'auto' keyword and explicitly turn off specific features or set them to '1' as required, indicating no parallelism, tiling, or automatic pipelining where applicable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) kernel function named 'kernel_2mm' that computes a 2-matrix multiplication with scaling factors applied to the result and the secondary product, designed for hardware acceleration but with all automatic optimizations disabled for the loops. The nested loops should be executed strictly in their original sequential order, with no loop iterations running in parallel or being pipelined, except for one inner loop where some parallelism is allowed with a factor of 5 for partial sums. Make sure to explicitly disable loop pipelining and tiling and set the parallelization factor to 1 for every loop except the specified one. Apply a pragma for parallel reduction with a factor of 5 for the first product accumulation and with a factor of 1 for the second accumulation, ensuring that these are the only parallelized sections of the code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. For all loops in this function, disable automatic pipelining, ensure they're processed in their entire original form by preventing tiling, and execute iterations sequentially without parallelization. However, within the nested loops performing the multiplications, enable a specific level of parallelization\u2014use a parallel factor of 5 for the alpha multiplication accumulation on `tmp` and a parallel factor of 10 for the beta operation accumulation on `D`, also enabling reductions on `tmp` and `D` respectively during these operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. Ensure that the outermost loops (`i` loop in both main for-loop blocks) are not pipelined, are unsegmented, and execute sequentially without parallelism. For the first inner `j` loop, apply automatic pipelining, tiling with a factor of 1, and sequential execution with parallelism disabled. For the nested `k` loop inside the first `j` loop, enable parallel reduction on the `tmp` array with a factor of 10. For the second inner `j` loop in the second main for-loop block, disable pipelining, tile with a factor of 1, and keep sequential execution. For the nested `k` loop inside the second `j` loop, enable parallel reduction on the `D` array with a factor of 25.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function 'kernel_2mm' as a hardware kernel. Within the function, first specify not to automatically pipeline the entire matrix multiplication and addition operations. Preserve the original structure of the outermost loop without decomposition. Ensure iterations of the outermost loop are executed sequentially. For the first nested loop, re-enable automatic pipelining, while still preserving the original loop structure and sequential execution of iterations. In the innermost loop, perform reductions on the 'tmp' array in a highly parallel manner, with a parallelism factor up to 32. For the second set of nested loops, restart with automatic pipelining off, preserve the original structure of the first nested loop and execute its iterations sequentially. In the innermost loop, perform reductions on the 'D' array with a moderate level of parallelism, up to a parallelism factor of 10.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function 'kernel_2mm' as a hardware-accelerated kernel, disable automatic pipelining for all loops, maintain the original loop structure without tiling, and ensure the loops run sequentially. Within the first double-nested loop, allow partial parallel computation for the inner loop with a reduction on 'tmp' and a parallelism factor of 5. For the second nested loop, permit pipelining for the outer 'i' loop, and enable partial parallel computation with a reduction on 'D' for the inner loop with a parallelism factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and within it, implement a matrix multiplication and addition algorithm that comprises two main stages with specific loop optimizations:\n\n1. For the matrix multiplication `tmp = alpha * A * B`, set global loop optimizations to run without automatic pipelining. For the first loop iterating over `i`, locally override the loop to allow pipelining, but avoid tiling and parallel processing. Inside this loop for each `j`, initialize `tmp[i][j]` to zero. Then, perform the actual multiplication in a loop over `k`, which should be parallelized with a factor that attempts to optimize to 10 concurrent operations and enable reduction on `tmp`.\n\n2. For the matrix addition `D = beta * D + tmp * C`, preserve the global non-pipelined, non-tiled, and non-parallel behavior. Again, for the first loop iterating over `i`, and the nested loop over `j`, enable local pipelining while avoiding any tiling or parallelization. In the innermost loop, scale `D[i][j]` by `beta`, then accumulate the results of `tmp * C` into `D[i][j]`, applying parallelism with a factor that optimizes to 25 concurrent operations and enable reduction on `D`.\n\nThroughout the code, the `auto` keyword paired with `{off}` or specific factor values indicates that the compiler should make automatic decisions for pipelining, tiling, and parallelization based on the provided constraints rather than its default heuristics. Use this to fine-tune performance for the specific architecture targeted by the HLS tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the following function as a hardware kernel for acceleration, and in the first main loop refrain from applying automatic pipelining, use no tiling, and ensure the loop iterations are not executed in parallel. Within the nested loop, enable automatic pipelining while still preventing tiling and parallel execution. Perform parallel reduction for the innermost multiplication loop, optimizing it for a group of 32 concurrent processes. Similarly, for the second main loop, prevent automatic pipelining but allow for loop flattening, again with no tiling or parallel execution in the outer loops, and use only sequential execution without parallel reduction for the final multiplication loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and for each nested loop within this function, do the following: disable automatic pipelining, prevent the loop from being tiled or split into smaller chunks, and do not execute loop iterations in parallel. However, within the innermost loop of both the 'tmp' and 'D' matrix calculations, perform parallel reduction with a factor that allows 5 parallel iterations for 'tmp' updates and 25 parallel iterations for 'D' updates.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, ensuring that all the loops within are executed sequentially without automatic pipelining. For the innermost loops, enable explicit pipelining with an unspecified factor, carry out the multiplications in a parallel fashion with a specified reduction target and a suggested parallelism factor of 32 for the first loop nest, and 10 for the second loop nest. Use a default tiling factor of 1 for all loops to maintain their original loop boundaries. Specifically, disable automatic pipeline optimization for the top-level loops, while allowing the compiler to choose the best pipeline strategy for the inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the function `kernel_2mm` by treating it as a hardware kernel, and apply optimization strategies as follows:\n\n1. For the entire function, disable automatic pipelining.\n2. Do not tile or parallelize the outermost loops over the `i` index, ensuring they run in their full, original form sequentially.\n3. Within the first nested `j` loop, allow the compiler to determine pipelining, but restrict tiling and parallelism to ensure the loop runs fully and sequentially.\n4. For the innermost loop where `k` iterates for calculating `tmp`, enable parallel execution with a reduction on the `tmp` array, setting the parallel factor to 16 but still allowing the compiler to choose whether to pipeline.\n5. In the second main loop section for updating `D`, automatically determine pipelining with a hint to flatten nested loops, tile the inner `j` loop with a factor of 80, and restrict parallelism to sequential execution. \n6. For the final innermost loop where `k` iterates for updating `D`, use a reduction on `D` with a sequential execution, not parallelizing this loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following code into natural language HLS pragmas: \n\nDeclare the `kernel_2mm` function as a hardware acceleration kernel. \n\nWithin the function:\n1. Turn off automatic pipelining for the outer loops iterating over dimension `i`.\n2. Prevent the division of the outer loops into smaller sub-loops.\n3. Execute the outer loop iterations sequentially, without parallelism.\n\nFor the inner loop iterating over dimension `k` within the first major loop block:\n1. Enable partial parallel computation with an automatic factor suggestion, allowing for some elements of `tmp` to be computed simultaneously, while ensuring the correctness of the sum operation using the reduction strategy for array `tmp`.\n\nFor the nested loop iterating over dimension `j` within the second major loop block, apply the following:\n1. Flatten loop iterations where possible for the j-loop iterations, enhancing the potential for loop pipelining.\n2. Maintain default behaviors for tiling and parallel execution.\n\nFinally, within the innermost k-loop of the second major loop block:\n1. Ensure that iterations are run sequentially, and disable parallel execution for the computation of the `D` matrix with no reduction strategy applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS-accelerated function called `kernel_2mm` for matrix multiplication and addition while preserving the original loop structures without automatic pipelining, with all loops processed serially, except for the innermost multiplication loops where parallelization with factor 5 for the first loop and factor 1 for the second should be applied to enable a reduction operation on the `tmp` and `D` arrays respectively. Use the following directives:\n\n1. Identify the function `kernel_2mm` as a hardware acceleration kernel.\n2. Ensure all loops are processed without pipelining by setting the pipeline directive to 'off'.\n3. Keep each loop in its original form without tiling by setting the tile factor to 1.\n4. Run all loops serially, maintaining the original loop iteration order without parallel execution by setting the parallel factor to 1.\n5. For the first inner loop that accumulates into the `tmp` array, enable parallel execution with a factor of 5 while allowing for a reduction on `tmp`.\n6. For the second inner loop that accumulates into the `D` array, allow parallel execution with a factor of 1, enabling reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel, while inside it, instruct the compiler as follows: do not pipeline any loops, only execute loops in their entire extent without tiling, and execute all loop iterations sequentially. However, within the first nested loop, allow parallel execution of the third nested loop with a parallelization factor of 16 only during reduction operations on 'tmp'. Then in the second main loop, proceed without pipeline optimizations but flatten the inner parallelism, set a tiling factor corresponding to the outer loop's entire iteration space for the first nested loop, and maintain sequential execution within the second nested loop without parallelization during reduction operations on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, use the following comprehensive but concise natural language instructions:\n\n- Define the function 'kernel_2mm' as a hardware acceleration kernel.\n- Disable automatic pipelining optimization for the entire computation.\n- Perform all matrix operations at a loop level without tiling or parallelizing.\n- Hold the outer loops of the 'tmp' and 'D' matrices calculation in their full, sequential form without subdividing or running iterations in parallel.\n- Allow the computation of 'tmp' matrix elements to be parallelized, but without any partitioning - each element is computed one after the other, including the reduction on 'tmp', and specify the parallel factor to be 1.\n- For the second matrix multiplication, re-enable automatic pipelining for iteration over 'D', but restrict loop tiling for 'D' matrix computation to a factor of 8 and also restrict parallel execution to a factor of 5 for the reduction step on 'D' without further parallelization for outer loops.\n\nTranslate these instructions into HLS pragmas as follows:\n\n- Use `#pragma ACCEL kernel` before the function declaration to indicate the function should be treated as a hardware acceleration kernel.\n- Apply `#pragma ACCEL PIPELINE auto{off}` to indicate that no automatic loop pipelining should be performed for the loops that follow.\n- Use `#pragma ACCEL TILE FACTOR=auto{1}` for no loop tiling optimization.\n- Apply `#pragma ACCEL PARALLEL FACTOR=auto{1}` to indicate that no automatic parallel execution optimization should be applied.\n- For the reduction on the 'tmp' and 'D' matrices, apply `#pragma ACCEL PARALLEL reduction = variable_name FACTOR=auto{specific_value}` to capture the parallel factor desired, `1` for 'tmp' and `5` for 'D'.\n\nThese natural language instructions and their corresponding pragmas should guide HLS in treating the loops and functions according to your specified performance and optimization preferences.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to mark the `kernel_2mm` function for hardware acceleration without pipelining the outer loops or modifying the loop structure, use a parallel factor of 14 only during the alpha accumulation in 'tmp' and execute all other loop iterations in series, while permitting the inner loop of the beta accumulation in 'D' to run sequentially without any reduction or pipelining optimizations. Specifically, you would need to:\n\n1. Treat the `kernel_2mm` function as a hardware kernel.\n2. Inhibit automatic pipelining for the outer loops iterating over 'i' and 'j'.\n3. Keep the original single iteration form for the outer loops by using a tiling factor of 1.\n4. Run outer loops' iterations consecutively, ensuring no parallel execution, by using a parallel factor of 1.\n5. For the 'k' loop that accumulates alpha into 'tmp', enable parallelism with a factor of 14, allowing for reduction.\n6. For the 'k' loop that accumulates into 'D', ensure that no pipelining, reduction, or parallel optimizations are applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function named 'kernel_2mm' that disables automatic pipelining, prevents tiling, and enforces serial processing of the outer loops (i and j loops) without parallelization. Additionally, specify a reduction-style parallelization of inner loops (k loops) with explicit parallel factors of 5 for the first nested loop and 2 for the second nested loop to allow some degree of parallel computation on these loops while iterating over matrix multiplication and addition.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function named 'kernel_2mm' as a hardware acceleration kernel engaging in matrix computations without pipelining the main loops. Within the function, carry out the multiplication and addition of matrices 'A', 'B', 'C', and 'D' ensuring that the innermost loop in the first set of nested loops runs with a parallelism factor of 10, applying a reduction operation on the 'tmp' matrix. In the second set of nested loops, the innermost loop should run with a parallelism factor of 25, employing a reduction operation on the 'D' matrix, while avoiding pipelining and maintaining the original loop structure for all loops without any loop tiling optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) function `kernel_2mm` that accelerates matrix multiplication and addition by defining it as a hardware kernel, without using automatic pipelining for the loops. Avoid tiling, loop unrolling, and parallel execution by setting their factors to 1. Use parallel reduction with a customized factor of 7 in the first nested loop where `tmp` is updated, and allow flattening of the pipeline in the second nested loop where `D` is updated. Specify the reduction variable and use the default parallel factor in the innermost loops for sum accumulation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function named 'kernel_2mm' that performs a series of matrix operations without any loop pipelining, parallelism, or tiling optimizations, except for a specific parallel reduction in the nested loops with a parallelism factor of 5 for updating the 'tmp' matrix and a parallelism factor of 50 for updating the 'D' matrix. Use pragmas to disable loop pipelining, set both the tiling and the parallelization factors to 1 for all loops, and apply respective parallel reduction factors where indicated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and for all loops within, disable automatic pipelining, prevent tiling and parallelization optimizations by setting their factors to 1, which implies executing iterations sequentially and keeping the loops intact. However, for the innermost k-loop in the first main loop, enable a parallelism factor of 5 with a reduction on the 'tmp' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel with directives to treat it as a hardware acceleration kernel, but inhibit automatic pipelining and tiling optimizations for the entire function and each loop within it. Additionally, set a specific factor for parallel execution in the innermost loops: use a factor of 5 for the multiplication accumulation on the 'tmp' array, and a factor of 8 for the addition accumulation on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis kernel function called `kernel_3mm` which performs three matrix multiplications (E=A*B, F=C*D, and G=E*F), specifying the following configurations for each involved loop nest and inner operation:\n\n1. Encapsulate the entire function as a hardware kernel for acceleration purposes.\n2. For the top-level loops over the matrices E, F, and G, explicitly prevent any automatic pipelining optimizations.\n3. Ensure the loop tiling is performed over the loops with factors set to the original loop bounds, effectively avoiding loop tiling optimizations.\n4. Allow the execution of the top-level loops to be strictly sequential by disabling parallel execution with a factor defaulting to a sequential execution mode.\n5. For the inner loops responsible for accumulating matrix product results, use the following settings:\n   - For E=A*B, allow automatic pipelining with a flattened loop structure for the second-level loop and disable parallelism for the innermost loop.\n   - For F=C*D, set the parallelism factor of the innermost loop to 4, allowing four iterations to be computed in parallel.\n   - For G=E*F, set the parallelism factor of the innermost loop to 16 to enable parallel computation of sixteen iterations.\n\nEnsure that the three main sections are clearly demarcated through either comments or code structure to indicate the distinct matrix multiplication operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given HLS code, your directive should be:\n\n\"Designate the `kernel_2mm` function as a dedicated hardware acceleration kernel. For the first major nested loop computing the matrix multiplication of A and B with alpha scaling, disable automatic pipelining, leave loop tiles in their original size, and ensure loop iterations run serially with no parallelization. Within the innermost loop, perform a reduction on `tmp` but maintain a serial execution order. For the second major nested loop, apply the same directives as the first, but allow automatic pipelining for the outer loop. In the innermost loop for updating matrix D, allow a degree of parallel execution such that 25 iterations can potentially run simultaneously, in the context of performing a reduction on `D`.\"\n\nConvert this directive into comments and specific #pragma values:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni, int nj, int nk, int nl, double alpha, double beta, double tmp[40][50], double A[40][70], double B[70][50], double C[50][80], double D[40][80]) {\n  int i, j, k;\n\n  /* Disable automatic pipelining for this loop nest, retain the original loop tile size, and ensure serial execution of loop iterations */\n  #pragma ACCEL PIPELINE auto{off}\n  #pragma ACCEL TILE FACTOR=auto{1}\n  #pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n\n    /* Enable loop flattening for better pipelining but still keep the tiles and parallelism at original state */\n    #pragma ACCEL PIPELINE auto{flatten}\n    #pragma ACCEL TILE FACTOR=auto{1}\n    #pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n\n      /* Perform reduction on tmp with serialized execution */\n      #pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n\n  /* Disable automatic pipelining for this loop nest, retain the original loop tile size, and ensure serial execution of loop iterations */\n  #pragma ACCEL PIPELINE auto{off}\n  #pragma ACCEL TILE FACTOR=auto{1}\n  #pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n\n    /* Allow automatic pipelining for the outer loop */\n    #pragma ACCEL PIPELINE auto{}\n    #pragma ACCEL TILE FACTOR=auto{1}\n    #pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n\n      /* Enable parallel execution for up to 25 iterations with a reduction on D */\n      #pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n}\n```\n\nMake sure to adjust the `#pragma` directives as needed so that they follow the described behavior of the loops accordingly, while the `auto{}` is kept as placeholders for an automatic determination done by the HLS tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function for 3 matrix multiplication tasks ('E := A*B', 'F := C*D', and 'G := E*F'), each involving double-precision floating-point numbers and matrix sizes: 'E[40][50]', 'A[40][60]', 'B[60][50]', 'F[50][70]', 'C[50][80]', 'D[80][70]', and 'G[40][70]'. Ensure that:\n\n1. The kernel function should be treated for hardware acceleration.\n2. For each of the three computation blocks within the kernel, disable automatic loop pipelining.\n3. For each loop in all three computation blocks, avoid loop tiling and loop parallelization optimizations that could split the loops or run iterations concurrently, except:\n   - In the 'F := C*D' block, allow the innermost 'k' loop to partially run iterations (up to 5) concurrently.\n   - In the 'G := E*F' block, allow the innermost 'k' loop to run iterations (up to 16) concurrently.\n\nApply these specific constraints using appropriate #pragma statements with 'auto' where the HLS tool determines implementation details within specified bounds.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use the high-level synthesis pragmas as follows to control the behavior of each loop and function within the kernel_2mm function: Mark the 'kernel_2mm' function as a hardware acceleration kernel, disable automatic pipelining, processing the loops in their original unchunked forms, and executing the iterations sequentially without parallelization, except perform parallel reduction in specific inner loops with factors of 5 and 4 for the first and second reduction loops respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Define a hardware-accelerated kernel function named 'kernel_2mm' that performs a matrix multiplication and addition on given matrices 'A', 'B', 'C', and 'D' with scaling factors 'alpha' and 'beta', storing the intermediate result in 'tmp'. For the first major loop, disable automatic pipelining, tiling, and parallelism to run it sequentially. Inside its nested loop, allow the default pipelining while preventing any loop chunking or parallelism, and apply parallel reduction with a factor of 16 on the innermost loop. For the second major loop, turn off automatic pipelining and tiling, but flatten the nested loop pipeline, tile it with a complete output matrix 'D' width, and disable parallelism while enforcing sequential execution for the innermost loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel function (`kernel_3mm`) that performs the operations E = A * B, F = C * D, and G = E * F on fixed-size double-precision floating-point matrices, where matrix dimensions are restricted to sizes 40x50, 50x60, and 50x70 respectively. Mark the function as an acceleration kernel. Within the nested loop structure for matrix multiplication, ensure that:\n\n1. For all loops, disable automatic pipelining, do not divide loops into smaller chunks (set tiling factors to 1), and run iterations sequentially without parallelism (set parallelism factors to 1).\n2. Specifically for the innermost loop calculating E, flat parallelism is applied, although the outer loops still follow the previous constraints.\n3. Within the middle loop for calculating F, increase the parallelism factor to 2 for the innermost loop.\n4. For the innermost loop calculating G, use a parallelism factor of 5 to enable further parallel execution of loop iterations.\n5. Include the appropriate scope pragmas to signal the start and end of the region of interest for optimization. \n\nRemember to adjust all pragmas with the respective values and placement as stated above.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the provided function into a hardware kernel that does not automatically apply any form of optimization, including pipelining, tiling, or parallel processing for the primary loop structures. Explicitly enable a parallel reduction on the first innermost loop with a factor of 7 and on the second innermost loop with a factor of 16, focusing optimizations on the 'tmp' and 'D' arrays respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for accelerating a kernel designated for matrix-matrix multiplication and addition, using the following high-level synthesis pragmas:\n\n1. Treat the entire function 'kernel_2mm' as a hardware acceleration kernel, without any specific directives to improve or modify its functionality (`#pragma ACCEL kernel`).\n\n2. Inside the function, there are two main loops: \n   \n   a. In the first major loop, which computes `tmp = alpha * A * B`, ensure that the loop is NOT pipelined, retains its original form without tiling, and executes serially (`#pragma ACCEL PIPELINE off`, `#pragma ACCEL TILE FACTOR=1`, `#pragma ACCEL PARALLEL FACTOR=1`). \n   \n   b. Within the innermost loop of this first major loop, enable pipelining for improved throughput but prevent any loop unrolling or tiling so that computations are serial (`#pragma ACCEL PIPELINE flatten`, `#pragma ACCEL TILE FACTOR=1`, and `#pragma ACCEL PARALLEL FACTOR=1`). Also, provide a directive to parallelize with reductions on the 'tmp' array without any loop unrolling (`#pragma ACCEL PARALLEL reduction=tmp FACTOR=1`).\n\n3. In the second major loop, which calculates `D = beta * D + tmp * C`, specify the same settings as the first major loop for no pipelining, no tiling, and serial execution at the outer loop level (`#pragma ACCEL PIPELINE off`, `#pragma ACCEL TILE FACTOR=1`, `#pragma ACCEL PARALLEL FACTOR=1`).\n\n   a. Within the nested loop, as in the top-level loop, do not pipeline, and employ a tiling factor of 8 for the 'j' loop iterations to improve performance without impacting the original loop's full form (`#pragma ACCEL TILE FACTOR=8`).\n\n   b. For the final inner loop which performs the addition, allow for parallel execution with the implementation optimizing for a large degree of parallelism factor (50) with reductions focused on the 'D' array (`#pragma ACCEL PARALLEL reduction=D FACTOR=50`).\n\nRemember to keep the transformations for each loop individually specified, as described above, to match the required HLS C code directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the function `kernel_2mm` with the following directives:\n\n1. Treat the function `kernel_2mm` as a hardware kernel for acceleration.\n\n2. For the entire function scope, do not automatically pipeline any loops.\n\n3. For all the nested loops within the function, ensure they maintain their original boundaries by not tiling and running in serial order by setting both the pipeline and parallelization factor to execute iterations sequentially.\n\n4. However, for the innermost `k` loop when computing `tmp`, enable parallelism with a specified factor that allows concurrent execution of loop iterations, with a reduction on the `tmp` array to allow for accumulation of results.\n\n5. For the second major loop block when updating `D`, allow the outer `i` loop to be flattened by the pipeline pragma to optimize the loop nest structure, but keep the subsequent loops within it serial by again not tiling and performing iterations sequentially.\n\n6. Inside this second loop block, keep the innermost `k` loop, which involves the updating of `D`, without parallelism by ensuring only one iteration happens at a time.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided above, use the following comprehensive, yet concise instruction:\n\nDesignate the function `kernel_2mm` for hardware acceleration, and apply different parallelization configurations for various nested loops within it. In the top-level loops, prevent automatic pipelining but permit the compiler to decide on the other parallelization and tiling aspects. In the first nested level, set the loop to automatically decide on pipelining and parallelism, but enforce no tiling. For the innermost loop where `tmp` is being reduced, apply a parallelism factor that permits concurrency up to 10 iterations. In the second set of nested loops, automatically define pipelining, and apply a tiling factor of 8 for the intermediate loop while allowing iterations to run sequentially. Finally, for the innermost reduction loop affecting `D`, apply a parallelism factor that allows up to 25 concurrent iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel without pipelining, tiling, or parallelism optimizations for its nested loops but allow limited parallelism with a factor of 5 in the alpha calculation loop and a factor of 2 in the beta accumulation loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following C function as a hardware acceleration kernel, and within it:\n\n1. Disable automatic pipelining for the two primary nested loops, which means you want the iterations to be executed in sequence without any overlap.\n2. Avoid dividing these two primary nested loops into smaller chunks, implying you treat each as a single, undivided task.\n3. Execute the iterations of these primary nested loops sequentially, rather than in parallel.\n4. However, for the innermost loops, where matrix multiplications occur, do the following:\n   - For the matrix multiplication that computes the temporary matrix (tmp), employ parallel execution with up to 32 simultaneous operations, allowing partial sums to be combined at the end.\n   - For the matrix multiplication that updates matrix D, allow for parallel execution with up to 10 simultaneous operations, also permitting the reduction of partial sums.\n\nRemember to ensure that, while most loops use the original loop settings with factors set to one, the specified inner loops have specific parallel factors to exploit hardware parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a three-matrix multiply kernel with the following characteristics:\n\n1. Designate the function as a hardware acceleration kernel.\n2. Do not automatically pipeline the outer loops 'i' for each of the three matrix multiplication sections but flatten pipelining for the second layer loop 'j' for the first matrix multiplication section.\n3. Ensure that the loop iterations proceed serially by setting the tile factor to 1 and the parallel factor to 1 for every loop, except:\n   - for the innermost loop 'k' of the second and third matrix multiplication sections, parallelize the processing with a factor of 4.\n4. Perform the computation adhering to the original order without splitting the loops into smaller chunks.\n5. First calculate \\( E = A \\times B \\) for matrices with dimensions E[40][50], A[40][60], and B[60][50].\n6. Then compute \\( F = C \\times D \\) for dimensions F[50][70], C[50][80], and D[80][70].\n7. Lastly, obtain \\( G = E \\times F \\) for dimensions G[40][70], utilizing the previously computed E and F matrices.\n\nUse the appropriate pragma values to enforce these behaviors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function that treats the entire function as a hardware acceleration kernel and maintains the original loop structures and dependencies. Pipelining, tiling, and parallelization should be explicitly disabled for outer loops, maintaining a sequential execution with no optimizations. In the innermost loops of the two main computational blocks, enable parallelization with a given factor but without pipelining or tiling: use a factor of 7 for reducing the 'tmp' array and a factor of 2 for reducing the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, disable automatic pipelining, and prevent loop tiling and parallelization, but for the innermost loops where multiplication and addition are taking place, set a specific parallelization factor of 7 for the first nested loop and 32 for the second nested loop to allow for some concurrent execution. Also, ensure that the partial sums in the variables 'tmp' and 'D' are handled correctly through the use of a reduction clause.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel function named 'kernel_2mm' that computes the matrix multiplication of `A`, `B`, and `C`, then scales and adds the result to matrix `D`. Do not apply automatic pipelining or tiling to any of the four nested loops in the function, ensuring that all iterations are executed sequentially. However, you should enable parallel reduction with a specific factor of 7 for the `k` loop that computes `tmp[i][j]`, and factor 16 for the `k` loop that computes `D[i][j]`. For all other `#pragma` directives, disable the automatic feature by specifying 'off' or '1' for pipelining, tiling, and parallelization respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_2mm' as a hardware acceleration kernel, and for all loops within this kernel, prevent pipeline optimization, ensure they run serially without tiling or parallelization, except for the two innermost loops where one should apply a parallelization factor of 7 for operations involving 'tmp' and a factor of 32 for operations involving 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel that computes the matrix expression `D := alpha*A*B*C + beta*D`. For each nested loop level within this function, ensure that the loop is not automatically pipelined, that each loop runs in its original, full iteration sequence without tiling, and that no parallelization is applied. However, specifically for the innermost loop that accumulates values for the `tmp` array, employ a reduction strategy with parallelization factor 7, while for the innermost loop updating the `D` array, use a parallelization factor of 8 for the reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS codes with the following optimizations and pragmas:\n1. Designate the entire function `kernel_2mm` as a hardware acceleration kernel.\n2. For the three outer loops iterating over `i`, disable automatic pipelining, ensure that they are executed in their original, complete form without tiling, and that their iterations are run sequentially, not in parallel.\n3. Within the first `j` loop, calculate `tmp[i][j]` without pipelining, with no tiling and sequentially, but for the inner `k` loop, where `tmp[i][j]` is calculated, enable parallelism with the ability to accommodate up to 10 simultaneous operations, and enable reduction on `tmp`.\n4. In the second `j` loop, where `D[i][j]` is updated, flatten the pipelining, execute without tiling, and sequentially, but for the `k` loop within it, ensure the iterations are processed sequentially and enable reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) kernel function named `kernel_2mm` that performs a 2-matrix multiply operation with parameters `ni`, `nj`, `nk`, `nl`, `alpha`, `beta`, and the matrices `A`, `B`, `C`, `D`, and a temporary matrix `tmp`. Ensure that:\n- The kernel function is designated for hardware acceleration. Indicate this by disabling loop pipelining globally within the function.\n- Each loop should not be pipelined, tiled, or parallelized beyond its original sequential form.\n- In the first nested loop, only allow a limited degree of parallelism with a factor of 5 for the reduction operation on the `tmp` matrix.\n- In the second nested loop, enable parallelism with a factor of 32 for the reduction operation on the `D` matrix, while keeping the pipeline setting on its default state for the innermost loop.\n\nUse the specific `#pragma` directives to achieve the above behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel that should compute the matrix multiplication of `alpha*A*B` plus `beta*D`, without applying automatic optimizations such as pipelining, tiling, and parallel processing to the loops. Specifically, ensure that the first two nested loops calculate the product `alpha*A*B` and accumulate the result in `tmp`, with an attempted parallelized factor of 7 on the innermost loop, while the latter two nested loops adjust `D` by scaling with `beta` and then adding the product of `tmp*C`, with an attempted parallelized factor of 10 on the corresponding innermost loop. Use the parallel directive to enable specific vectorization on the reduction operation in the accumulation of `tmp` and `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS code for a 2mm kernel function with the following specific directives: \n\n1. Define the function as a hardware acceleration kernel.\n2. For the main loops i and j, disable pipelining, prevent the loops from being tiled and ensure their iterations are executed sequentially.\n3. Within the first nested loop, allow parallel execution for the reduction operation on the tmp array with a high factor of parallelism but do not specify the exact number.\n4. For the second main loop, enable automatic pipelining but flatten the nested loops into a single loop level, tile the innermost loop with a factor equivalent to the size of array C's second dimension, and ensure the iterations of the outer loop run sequentially.\n5. In the final nested loop, allow parallel execution for the reduction operation on the D array without specifying the level of parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a high-level synthesis C code that defines a hardware kernel function for a 2mm computation (matrix multiply and add) without automatic pipelining optimizations, ensuring that loop iterations are always processed sequentially without tiling or parallelization. Specifically:\n\n1. Use `#pragma ACCEL kernel` to designate the function as a hardware acceleration kernel.\n\n2. Inside the kernel, apply the following pragmas to the outer loops (`i`-loops) and their direct nested loops (`j`-loops):\n   - Use `#pragma ACCEL PIPELINE off` to prevent automatic loop pipelining.\n   - Use `#pragma ACCEL TILE FACTOR=1` to execute loops without tiling.\n   - Use `#pragma ACCEL PARALLEL FACTOR=1` to ensure loop iterations are processed one after the other.\n\n3. In the innermost `k`-loop of the first nested loops, use `#pragma ACCEL PARALLEL reduction=tmp FACTOR=7` to enable limited parallelism with a factor of 7 while performing reduction on the `tmp` array.\n\n4. In the innermost `k`-loop of the second nested loops, apply `#pragma ACCEL PARALLEL reduction=D FACTOR=1` to process iterations sequentially with reduction operations on the array `D`.\n\n5. The code should sequentially calculate the product of matrices `A`, `B`, and `C`, scaled by `alpha`, followed by adding a scaled version of matrix `D` by `beta`, and store the result in `D` according to the 2mm algorithm.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function that performs matrix multiplication followed by a matrix addition. The function 'kernel_2mm' should have integer parameters 'ni', 'nj', 'nk', and 'nl' for matrix dimensions, and 'double' type parameters 'alpha' and 'beta' for scalar multiplication. Additionally, provide 'tmp', 'A', 'B', 'C', and 'D' as 2D arrays for computational storage with specified dimensions. Annotate the function with a kernel pragma to indicate hardware acceleration. In the first nested loop, disable the automatic pipelining, tiling, and parallel processing pragmas. Inside the innermost loop, include a pragma that allows partial parallel operations with reduction support for 'tmp' array using a factor that results in 14 parallel operations. Configure these automatic pragmas to use default settings except where explicitly disabled or factor values are stated. Then, in the second nested loop set, disable automatic pipelining but utilize the 'flatten' option, tile, and parallel pragmas, applying the standard behavior. Finally, for the reduction of the 'D' array, ensure that the iterations are processed sequentially.\n\nBe sure to frame the code within the appropriate 'scop' and 'endscop' pragmas to define the computational scope.\n\n(Note: When indicating 'auto' in the provided code, it should be replaced with default behavior unless specified otherwise. The 'off' and '1' values after 'auto' enforce specific desired behavior as mentioned.)", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's the comprehensive but concise natural language instruction:\n\nDefine a hardware-accelerated kernel function 'kernel_2mm' that disables automatic pipelining, tiling, and parallelization for looping constructs while handling matrix multiplication and addition for arrays A, B, C, and D, using predefined array sizes. For the innermost loops that accumulate values into 'tmp' and 'D' matrices, enable parallel computation with a reduction on 'tmp' using a factor of 5 and on 'D' using a factor of 16, but do not employ pipelining or tiling for these loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function called `kernel_2mm` to perform a two-matrix multiplication series with scaling on matrices `A`, `B`, `C`, and `D` along with scaling factors `alpha` and `beta`. Implement the matrix calculations without any pipelining, tiling, or parallel processing optimizations for the outer loops over the first two dimensions. For the innermost loop of the first matrix multiplication, enable parallel execution with a factor allowing for up to 32 simultaneous operations for the reduction on `tmp` matrix, but keep it without tiling or pipelining. In the second matrix multiplication, flatten the loop nest structure when processing the `D` matrix, apply tiling with a factor that matches the `D` matrix's second dimension, and permit no parallelism for the innermost loop reduction on the `D` matrix. Do not automatically infer any #pragma values; specify them as provided, directly reflecting the desired computation structure and optimization strategies.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as one not to be automatically pipelined, tiled, or parallelized beyond single iteration processing, but apply parallelization with a factor of 7 for the reduction of matrix 'tmp' in the first nested loop, and parallel reduction on matrix 'D' in the second nested loop using the default factor.\n\nHere are the #pragma descriptions:\n\n- `#pragma ACCEL kernel`: Designate the 'kernel_2mm' function as a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}`: Inhibit automatic pipelining of each loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}`: Prevent the loop from being tiled, meaning it should be run in full without splitting.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Execute iterations sequentially rather than in parallel for all loops, except where specified differently.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}`: In the loop where this is specified, allow parallel execution with a factor of 7 specifically for the reduction operation on matrix 'tmp'.\n- `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`: In the loop where this is specified, allow parallel execution for the reduction operation on matrix 'D' with the default parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a dedicated hardware acceleration kernel, ensuring that neither of the two primary loops within it are automatically pipelined. For both loops, each iteration should be executed sequentially without parallelism or partitioning of the loop iterations. Within the first nested loop, allow parallel accumulation using all available resources for the product of 'alpha', 'A', and 'B' over 'k', and within the second nested loop, perform a linear sequential update of 'D' by scaling with 'beta' and adding the product of 'tmp' and 'C', with no automatic optimization for parallel reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three matrix multiplication kernel, which sequentially performs the matrix multiplication operations \\( E = A \\times B \\), \\( F = C \\times D \\), and \\( G = E \\times F \\), with dimensions specified by the provided parameters. Implement the following optimizations:\n\n1. Designate the entire function as a hardware acceleration kernel.\n2. For all loops within the function:\n   - Disable automatic pipelining.\n   - Do not tile the loops.\n   - Run all loop iterations in series without introducing parallel execution.\n3. Deviate from the aforementioned optimizations for specific loops:\n   - For the innermost loop in the \\( F = C \\times D \\) operation, allow the compiler to perform parallel execution of 8 iterations at a time.\n   - For the innermost loop in the \\( G = E \\times F \\) operation, specify parallel execution of 50 iterations at a time.\n\nEnsure that you use the appropriate #pragma directives to implement the specified optimizations, such as \"#pragma ACCEL kernel\" to identify the function as a hardware kernel, and \"#pragma ACCEL PIPELINE off\", \"#pragma ACCEL TILE FACTOR=1\", and \"#pragma ACCEL PARALLEL FACTOR=1\" to control loop pipelining, tiling, and parallelization, respectively, with the exception of the two innermost loops mentioned, where you should set the PARALLEL FACTOR to the specified values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in natural language, use the following instruction:\n\n\"Define a hardware kernel function for 2-matrix multiplication (2mm) algorithm using double-precision floating-point, with the following characteristics: for all outer and inner loops, turn off automatic pipelining, maintain the original loop structure without tiling, and execute iterations sequentially without parallelism. Additionally, in the first matrix product computation, enable parallel reduction of the 'tmp' array with a factor of 5, and in the second matrix product computation, enable parallel reduction of the 'D' array with a factor of 8.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel that performs matrix multiplication without pipelining or parallel execution optimizations, but applies a specific parallel reduction to certain loops. For the outer loops (i and j), disable automatic pipelining, do not tile the loops, and ensure the iterations run sequentially. In the initial 'tmp' calculation loop, enable a parallel reduction with a factor of 5, while in the 'D' calculation loop, enable a parallel reduction with a factor of 25. Use a pragma to mark the function as a hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat a function as a dedicated hardware acceleration kernel that computes `D = alpha*A*B*C + beta*D`. In this function:\n\n1. Disable automatic pipelining for each loop.\n2. Prevent the tool from tiling any loop by setting the tiling factor to 1.\n3. Ensure all loop iterations are processed serially by setting the parallel factor to 1.\n\nFor the innermost loop of the first double-nested loop structure, enable parallel processing with a reduction on the `tmp` array and a specified parallel factor of 7.\n\nSimilarly, for the innermost loop of the second double-nested loop structure, enable parallel processing with a reduction on the `D` array and a specified parallel factor of 5.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel that disables automatic pipelining, processes loops in their original complete form without subdivision, executes iterations sequentially rather than in parallel, and treats the entire function as a hardware acceleration kernel. Specifically, for the first pair of nested loops that calculate the 'tmp' matrix, set pragmas to avoid pipelining, to not subdivide the loop (use the original loop size), and to run iterations sequentially. However, internally parallelize the reduction operation without specifying a factor, ensuring that all iterations that contribute to a single 'tmp' element execute sequentially. For the second pair of nested loops that update matrix 'D', disable automatic pipelining for the outer loop, subdivide the inner loop by a factor of 8 while iterating sequentially, and parallelize the reduction operation on 'D' with a large automatic factor to combine elements from 'tmp' and 'C', allowing multiple iterations calculating 'D[i][j]' to be in flight simultaneously.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following high-level computation into specialized hardware instructions by marking the top-level function as a hardware acceleration kernel. For all loops inside this function, do not perform automatic pipelining, tile loops with a singular tile, and run all iterations sequentially. However, specifically for the first innermost loop calculating `tmp`, allow a parallel factor of 7 with a reduction on the `tmp` array, and for the second innermost loop updating `D`, allow a parallel factor of 8 with a reduction on the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` for hardware acceleration, ensuring all loops within are executed without pipelining, with no loop tiling, and no parallel execution of iterations. However, within the first two nested loops, enable automatic pipelining while keeping loop tiling and parallel execution unchanged. Within each innermost loop, apply a parallel reduction with a custom factor on specific arrays. For the last two nested loops, only the outermost loop's pipelining should be flattened while maintaining the previously stated configurations for the remaining pragmas. Use the following directives:\n\n1. Treat the `kernel_2mm` function as a hardware kernel suitable for acceleration.\n2. Prevent automatic pipelining of the two outer loops by setting the pipeline optimization directive to 'off'.\n3. Ensure the outer loops run without loop tiling by providing a loop tile factor of 1.\n4. Ensure iterations of the outer loops are not run in parallel by setting parallel factor to 1.\n5. Within the first two nested loops, apply automatic pipelining without specifying a factor, allowing the compiler's heuristics to determine the optimal settings.\n6. Inside the innermost loop of the first two nested loops, enable parallel execution with a reduction on `tmp` and set the parallel factor to 2.\n7. In the second pair of loops, apply the 'flatten' attribute to the outer loop's pipelining to process the loop in its original form without hierarchy.\n8. In the innermost loop of the second set of nested loops, enable parallel execution with a reduction on `D` but without changing the default parallel factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be synthesized as a hardware acceleration kernel, with all the loops inside unoptimized for parallel execution by disabling automatic pipelining, and maintaining each loop in its entire, unsegmented form without tiling. Furthermore, selectively optimize the innermost loops of 'tmp' and 'D' array computations by allowing parallel execution with limited factors of parallelism, specifically a factor of 5, ensuring reduction operations are managed correctly for the 'tmp' and 'D' arrays. \n\nHere's the instruction for each of the specific pragma values used:\n\n- Treat 'kernel_2mm' as a hardware acceleration kernel with `#pragma ACCEL kernel`.\n- Disable automatic loop pipelining for all loops using `#pragma ACCEL PIPELINE auto{off}`.\n- Prevent loop tiling by maintaining the original loop size with `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Run all loop iterations sequentially, not in parallel, with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the innermost loops updating 'tmp' and 'D' arrays, perform parallel computations with a specific parallel factor of 5 and handle reductions correctly using `#pragma ACCEL PARALLEL reduction = <array_name> FACTOR=auto{5}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, ensure all outer and inner loops inside this kernel function are not automatically pipelined, do not tile these loops, and execute all iterations in sequence. However, within the first inner loop, parallelize the reduction operation on 'tmp' with a factor of 8, and within the second inner loop, parallelize the reduction operation on 'D' with a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the provided HLS code for a 2mm kernel, direct the HLS tool with the following instructions:\n\n1. Mark the entire `kernel_2mm` function as a hardware acceleration kernel.\n2. Do not automatically pipeline any of the loops; maintain their original structure without applying any pipeline optimization.\n3. Retain the full structure of each loop without tiling or breaking them into smaller chunks.\n4. Execute all loop iterations sequentially, with no parallelization optimizations.\n\nFor each loop, apply the following specifics:\n\n- For the outermost 'i' and 'j' loops in the first nested block, and the outermost 'i' loop in the second nested block:\n  - Use '#pragma ACCEL PIPELINE \"off\"' to prevent automatic pipelining.\n  - Use '#pragma ACCEL TILE FACTOR=1' to avoid loop tiling.\n  - Use '#pragma ACCEL PARALLEL FACTOR=1' to disallow parallel loop iteration execution.\n\n- Within the innermost 'k' loop of the first block that computes `tmp[i][j]`:\n  - Employ '#pragma ACCEL PARALLEL reduction=tmp FACTOR=7' to specify that parallel computation for loop iterations is allowed with a factor of 7, specifically focusing on reducing the `tmp` array.\n\n- In the innermost 'k' loop of the second block that updates `D[i][j]`:\n  - Choose '#pragma ACCEL PARALLEL reduction=D FACTOR=2' to perform parallel reduction on the `D` array with a factor of 2.\n\nEnsure these #pragma directives are applied in the manner shown in the code to control the behavior of the HLS tool during the hardware synthesis process.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for a 2mm computation without automatically pipelining the top level but do auto-pipeline inner loops, and set the tiling and parallel factors for specific loop levels as follows: limit the main loops to no tiling or parallelization, tile the second loop in the second major computational block with a factor of 8, allow the innermost loop of the first block to be processed in parallel groups of 8 while reducing into a temporary array, and let the innermost loop of the second block proceed in parallel groups of 25 with a reduction into the output array D.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function with the following attributes: the entire function should be accelerated as a hardware kernel; within the function, loop pipelining should be disabled for the outermost loops and enabled for the inner loops without specifying the pipeline initiation interval; no tiling for the outer loops should be set with a tile factor of 1, but one inner loop should use a tile factor of 8; for parallelism, the outer loops should only allow a single iteration at a time, while certain reduction loops should explicitly use factors of 7 and 25 respectively for parallel processing of partial results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a targeted hardware acceleration module, and within this function, prevent automatic loop pipelining, maintain the original loop structure without tiling, force loops to run sequentially, apply reductions directly to temporary arrays 'tmp' and 'D' in sequential order, and flatten nested loop iterations for potential compiler optimizations. Use the directives '#pragma ACCEL kernel' to mark the function, '#pragma ACCEL PIPELINE \"off\"' and '#pragma ACCEL PIPELINE auto{flatten}' to control pipeline behavior, '#pragma ACCEL TILE FACTOR=1' to disable loop tiling, '#pragma ACCEL PARALLEL FACTOR=1' for sequential execution, and '#pragma ACCEL PARALLEL reduction' to manage reductions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is a concise natural language instruction encompassing the directives for HLS code generation:\n\n\"Define a hardware kernel for acceleration that performs a 2mm (two-matrix-multiply) operation on matrices A, B, C, and D with scaling factors alpha and beta. Ensure that the outermost i and j loops are not automatically pipelined, tiled, or parallelized. Inside the first j loop, allow for the aggregation of results in the tmp array with a parallelism factor of 5, but outside this loop, run iterations sequentially. In the second j loop, iterations can be pipelined by default, but without additional tiling or parallelizing beyond the pragma specifying parallel reduction on the D array, again with a parallelism factor of 5.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function as a hardware acceleration kernel, disable automatic loop pipelining, run the first loop iterations sequentially without tiling or parallelizing, and allow the innermost loop of the first section to parallelize with a factor of 8 on the reduction variable 'tmp'. For the second loop, enable pipelining and tile with a factor of 8, but keep iterations sequential. Finally, let the innermost loop of the second section parallelize with a factor of 25 on the reduction variable 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate this routine as a dedicated hardware function without automated pipelining, where both outer loops and their inner loops are executed without parallelism or tiling, and ensure that the first inner loop is fully unrolled while the second inner parallel reduction loop uses a parallelism factor of two.\n\n- For the hardware kernel designation, use: Treat the following function as a hardware kernel for acceleration.\n- To prevent automatic pipelining of loops, use: Don't automatically pipeline this loop.\n- To execute loops in their original, full form without breaking them into smaller chunks, use: Process this loop in its original, full form without breaking it into smaller chunks.\n- To run the iterations of a loop one after the other, not in parallel, use: Run the iterations of this loop one after the other, not in parallel.\n- To fully unroll the loop, use: Completely unroll this loop for the most direct hardware translation.\n- To apply a specific level of parallelism to a loop, say: Apply a parallelism factor of two to this loop for concurrent execution of iterations where possible.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with appropriate pragmas, use this comprehensive, concise natural language instruction:\n\n\"Declare a hardware acceleration kernel function named 'kernel_2mm' that performs a matrix multiplication followed by a matrix addition, where the two matrix operations '(alpha * A * B)' and '(beta * D)' are computed sequentially. For each loop within this kernel, disable automatic pipelining, prevent tiling optimization by processing the loops in their entirety, and execute iterations sequentially without parallelism. Specifically, for the loop that computes temporary results 'tmp', allow for parallel reduction with a factor of 7, and for the loop that sums up the final matrix 'D', allow for parallel reduction with a factor of 4.\"\n\nTo break down the specific #pragma values according to the instruction:\n\n1. `#pragma ACCEL kernel` - Treat the `kernel_2mm` function as a hardware kernel for acceleration.\n\n2. For all the loops, the following pragmas should be set:\n   - `#pragma ACCEL PIPELINE \"off\"` - Disable automatic pipelining for the loops.\n   - `#pragma ACCEL TILE FACTOR=1` - Process the loops in their full form without tiling optimization.\n   - `#pragma ACCEL PARALLEL FACTOR=1` - Ensure that the loop iterations are executed one after the other, not in parallel.\n\n3. For the specific loop that computes the `tmp` array:\n   - `#pragma ACCEL PARALLEL reduction = tmp FACTOR=7` - Enable parallel reduction on the `tmp` array with a factor of 7, which allows a specific number of iterations to be computed in parallel.\n\n4. For the specific loop that updates the `D` array:\n   - `#pragma ACCEL PARALLEL reduction = D FACTOR=4` - Enable parallel reduction on the `D` array with a factor of 4, which specifies the level of parallelism during the reduction operation.\n\nThese pragmas instruct the HLS tool on how to handle the loops and operations within the kernel function to achieve the desired hardware implementation characteristics.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function named 'kernel_2mm' which performs a matrix multiplication and matrix addition using provided matrices 'A', 'B', 'C', 'D', and scalars 'alpha', 'beta'. Specifically, do not introduce any pipeline, tiling, or parallelization optimizations in the outer loops over 'i' and 'j', nor in the nested loop where 'tmp' is calculated. However, for the calculation of 'tmp', allow a partial degree of parallel execution in the innermost loop over 'k' with a factor of 5, and enable a parallel reduction targeting the 'tmp' matrix to accumulate results efficiently. Similarly, within the loop updating matrix 'D', execute the innermost loop over 'k' with parallel factor 4 and enable parallel reduction for 'D'. Ensure all other potential optimization directives are turned off or set to a factor of 1 to execute the loops sequentially in their entirety without dividing them into smaller portions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the `kernel_2mm` function for hardware acceleration; for the first two nested loops, run them without pipelining, tiling, or parallel optimization, but within the innermost loop on `tmp`, enable parallelism with a reduction on the `tmp` array with a full iteration space. Then, for the last two nested loops, again deactivate pipelining and tiling for the outer loops, but for the inner loop flattening the pipeline when optimizing on `D`, allow parallel execution with reduction on `D` but without chunking the iterations. Use default settings where 'auto' is specified, except where concrete values are provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_2mm' as a hardware acceleration kernel. Disable automatic pipelining for all loop nests within the function, maintain the original structure of every loop by preventing any loop tiling and parallelization. In the first nested loops, perform the computation serially for the outer two loops while allowing the innermost loop to sum results into the 'tmp' array, reducing the output dependency with a parallelism factor of 70. In the second nested loops, tile only the second loop with a factor of 8 while keeping the other loops serial, and allow the innermost loop to sum results into the 'D' array with a parallel reduction, limiting the parallelism to a factor of 5.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. In the main 2D nested loops for initializing `tmp` and calculating `D`, flatten and prevent pipelining the outer loops, ensure they are executed in full without tiling or parallel execution. For the innermost loops that perform the reductions, operate them sequentially and disallow any split or vectorization, but enable reduction optimization for the accumulator variables `tmp` and `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function called 'kernel_2mm' that accelerates matrix multiplications and additions with certain loops manually controlled for pipelining, tiling, and parallel execution. The function should have parameters for matrix dimensions `ni`, `nj`, `nk`, and `nl`, as well as the two scalars `alpha` and `beta`. It should operate on double-precision floating-point matrices `tmp`, `A`, `B`, `C`, and `D`, with `tmp` being a 40x50 matrix, `A` a 40x70, `B` a 70x50, `C` a 50x80, and `D` a 40x80.\n\nIndicate that the function is a hardware kernel. For the first pair of nested loops where the `tmp` matrix is calculated through multiplication of `A` and `B`:\n\n1. Do not automatically pipeline the outermost loop, but allow the compiler to decide pipelining for the intermediate loop.\n2. Keep the loops in their full original form without tiling.\n3. Run the iterations of the outermost and intermediate loops sequentially, without parallelism.\n4. For the innermost loop (multiplying elements of `A` and `B`), enable a parallel reduction on `tmp` with a compiler-determined factor, suggest 10 for optimization.\n\nFor the second pair of nested loops where the `D` matrix is updated:\n\n1. Again, do not automatically pipeline the outermost loop and let the compiler optimize pipelining for the intermediate loop.\n2. Do not tile the loops.\n3. Execute the iterations of the outermost and intermediate loops one after the other.\n4. In the innermost loop, while updating `D` through addition of the `tmp` and `C` products, implement a parallel reduction on `D` with a suggested optimization factor of 25, leaving the exact factor to the compiler's discretion.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Ensure that the specified function is designated as a hardware acceleration kernel. Then, for the entire function execution:\n\n1. Prevent automatic pipelining of the outermost loops.\n2. Keep the outermost loops un-tiled and execute their iterations sequentially.\n3. Flatten the inner loop 'j' to enhance its performance while also executing its iterations sequentially without tiling or parallel optimization.\n4. For the loop iterating over 'k' when calculating 'tmp', perform a sequential reduction on 'tmp' without parallel optimization.\n5. Enable default pipelining for the second outermost loop 'i' involved in calculating 'D'.\n6. Explicitly tile the second innermost loop 'j' by the full size of the 'C' array dimension it iterates over (80).\n7. For the loop iterating over 'k' when accumulating onto 'D', perform a sequential reduction on 'D', tiling this loop to process 50 iterations at a time.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function by marking it for hardware acceleration while deactivating automatic loop pipelining, ensuring loops run in a sequential, unoptimized manner (with tile factor set to 1), but enforcing parallelism with an automatic factor for reductions when multiplying matrix A and B (with a reduction on `tmp`), and multiplication of the resulting matrix with matrix C (with a reduction on `D`), with specific tile factors for the second multiplication loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for a matrix multiplication and addition routine, indicating that no automatic loop optimizations such as pipelining, tiling, or parallel processing should be applied to the outer loops, while manually setting the inner loops to partially unroll with a factor of 7 and 4 for the respective reduction operations. Specifically:\n\n- Declare the function 'kernel_2mm' as a hardware kernel for acceleration.\n- For the first pair of nested loops:\n  - Disable automatic pipelining on the outer loop 'i'.\n  - Do not tile the outer loop 'i'.\n  - Do not parallelize the outer loop 'i'.\n  - Within the inner loop 'j':\n    - Again, disable automatic pipelining.\n    - Do not tile the inner loop 'j'.\n    - Do not parallelize the inner loop 'j'.\n    - For the loop 'k', enable parallel reduction on the 'tmp' array with a partial unroll factor of 7.\n- For the second pair of nested loops:\n  - Disable automatic pipelining on the outer loop 'i'.\n  - Do not tile the outer loop 'i'.\n  - Do not parallelize the outer loop 'i'.\n  - Within the inner loop 'j':\n    - Enable automatic pipelining without specifying an \"off\" parameter.\n    - Do not tile the inner loop 'j'.\n    - Do not parallelize the inner loop 'j'.\n    - For the loop 'k', enable parallel reduction on the 'D' array with a partial unroll factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to:\n- Treat the entire function 'kernel_2mm' as a hardware acceleration kernel.\n- Disable automatic pipelining for the entire function and nested loops, ensuring that the original loop structure is maintained and complete iterations are executed sequentially without being subdivided or parallelized.\n- Specifically for loop nestings related to the computation of 'tmp' matrix, perform parallel operations with a factor of 4 on the innermost loop, but without data pipelining and tiling.\n- In the computation sequence for the 'D' matrix, enable automatic pipelining for the outer loop itself, while specifying a parallel reduction factor of 32 for the associated innermost loop that updates the 'D' matrix, again without loop tiling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware acceleration kernel while disabling automatic pipelining, maintaining the natural loop bounds without tiling or parallelizing, and reducing variables 'tmp' and 'D' within nested loops with specified parallel factors:\n\nSet '#pragma ACCEL kernel' to indicate the function 'kernel_2mm' is to be treated as an acceleration kernel.\n\nUse '#pragma ACCEL PIPELINE auto{off}' to disable automatic loop pipelining for each major loop in the kernel.\n\nApply '#pragma ACCEL TILE FACTOR=auto{1}' to ensure loops are not tiled and maintained in their full original size.\n\nSet '#pragma ACCEL PARALLEL FACTOR=auto{1}' for the outer loops to specify that they should execute sequentially.\n\nFor the innermost loop updating 'tmp', use '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}' to allow parallel execution with a factor of 5 and enable reduction on the 'tmp' array.\n\nSimilarly, for the innermost loop updating 'D', apply '#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}' to allow parallel execution with a factor of 10 and enable reduction on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function named 'kernel_2mm' for a 2-matrix multiplication with data scaling that processes the input matrices in a way that the outermost loops are not automatically pipelined, tiled with a factor of 1, and executed in sequence without parallelization. However, the innermost loops that perform the sum of products, one for calculating the intermediate matrix 'tmp' with alpha scaling and the second for updating matrix 'D' with beta and 'tmp' contribution, are parallelized with a specific reduction to 'tmp' and 'D' respectively, with the former using a full unrolling (factor equals full dimension) and the latter using a factor that divides the dimension into chunks of size 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to convert the specified function into a hardware kernel while preventing any loop pipelining, keeping loops un-tiled and iterations executed sequentially, and maintaining the full form of all loops. Specifically, disable loop pipelining, set loop tiling factors to one, and ensure parallel loop execution is set to one. Additionally, for the innermost loops that perform reductions on arrays 'tmp' and 'D', enable parallel reduction but keep the parallel factor for the 'D' reduction loop at one. Use 'auto' in the pragma directives to allow the HLS tool to automatically decide the best values, but override those decisions with explicit instructions where provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the compiler to optimize the `kernel_2mm` function for hardware acceleration by configuring pragma settings to control loop executions. Disable automatic pipelining and parallelization for all loops, ensure the outer loops process one iteration at a time, but allow specific parallel reductions with a reduction variable and a parallel factor of 70 for the first inner-most `k` loop and a factor of 4 for the second inner-most `k` loop; also, for the `j` loop in the second main block, increase tiling to process 8 iterations of the loop at once.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and manage loop properties as follows:\n\n1. In the first two-level loop nest\u2014where 'tmp' matrix is calculated\u2014do not pipeline either of the loops and preserve their original execution without division into smaller loops. Process the innermost 'k' loop iterations to be able to partially execute 8 at a time in parallel with a reduction on the 'tmp' array to ensure proper accumulation of values.\n\n2. In the second two-level loop nest\u2014where 'D' matrix is updated\u2014do not apply automatic pipelining to the outer 'i' loop but tile the inner 'j' loop with chunks of 8 iterations, and do not process its iterations in parallel. In the deepest 'k' loop, allow partial parallel execution of 25 iterations at a time with a reduction on the 'D' array to ensure proper accumulation of values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel without any automatic optimization for pipelining, tiling, or parallelization of both outer loops, with further directives to aggressively pipeline the inner loops while maintaining default serialization for all parallel constructs and the associativity of reductions on `tmp` and `D` arrays.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Direct the HLS tool to designate the provided function 'kernel_2mm' as a hardware acceleration kernel, keep all loops in their full sequential form without any automatic pipelining optimization, maintain original loop boundaries without tiling, and prevent any parallel execution of loop iterations. Specifically, ensure that the outer loops iterating over indices 'i' and 'j' for computation of 'tmp' and 'D' are not pipelined automatically, have no automatic tiling applied, and run iterations sequentially. Additionally, inside the nested loops where the 'tmp' and 'D' arrays are computed, allow the innermost loops iterating over 'k' to be considered for parallel execution but without any reduction optimization to maintain full sequential processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function 'kernel_2mm' that computes `D := alpha * A * B * C + beta * D` for given matrix dimensions, where the computation for both resulting matrix `tmp` and the final matrix `D` should be carried out sequentially, and intermediate reductions should be done in a non-parallel manner. Specifically, disable automatic pipelining for all the outer loops, and for the reduction loops, enforce a reduction on temporary variables `tmp` and `D` without parallelism. Keep all loops in their complete form without division into smaller chunks by using a tiling factor of one where applicable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as one to be accelerated through hardware synthesis, and for all loops inside this function, specify the following:\n- Do not apply automatic pipelining,\n- Do not partition the loops into sub-loops,\n- Execute loop iterations sequentially rather than in parallel,\n- For the innermost loop in the first double-nested loop structure, define a reduction operation on the 'tmp' array with a custom factor, hinting at potential parallel processing but without specifying the exact factor,\n- For the second double-nested loop, enable automatic pipelining for the outer loop only, tile the middle loop with a factor hinting at parallelized execution but without specifying the exact factor, and specify a reduction operation on the 'D' array in the innermost loop with a factor indicating a degree of parallel processing without dictating the precise value.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function designed to compute the matrix multiplications and additions for the 2mm kernel without applying automatic pipelining, tiling, or parallelization optimizations at the top levels of the nested loops, but do specify a custom parallelization factor of 7 for reductions on the \u2018tmp\u2019 array, and a parallelization factor of 10 for reductions on the \u2018D\u2019 array. \n\n- Mark the function itself as a hardware acceleration kernel.\n- At each nested loop level, explicitly turn off automatic pipelining, keep the tiling to its minimal original loop bounds (factor of 1), and avoid parallelizing loop iterations.\n- Apply the specific parallelization factors to the innermost loops where the 'tmp' and 'D' arrays are being reduced.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the specified function as a hardware kernel to be accelerated. Within that function, for the two main loops, disable automatic pipelining and process each loop in its entire form without tiling or parallel processing of iterations. Apply a parallel reduction for the innermost loops, iterating over the 'k' index, targeting the respective temporary result arrays without specifying a particular factor for parallelism or tiling, effectively using the defaults. Also, disable pipelining for the first main j-loop but not for the outermost i-loops, which allows the compiler to automate the pipelining of those loops if beneficial.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2-matrix multiplication kernel where no loop pipelining is enforced on the outer loops, the innermost loop of the first loop nest accumulates with a reduction pragma allowing for up to 70 parallel operations on 'tmp', and the innermost loop of the second loop nest accumulates with a reduction pragma allowing for up to 5 parallel operations on 'D'. The loop tiling is set to its default size except for the second j-loop, which is tiled in blocks of 8. All other loops should be processed sequentially without parallelization. The kernel function itself should be treated as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, and within this function:\n\n1. Inhibit pipelining optimizations for the outermost loops processing the `tmp` array and the `D` matrix.\n2. Preserve the original full loop structure without tiling for the entire function.\n3. Execute all loops at all nesting levels sequentially, ensuring the iterations are not unrolled or parallelized.\n4. Allow the innermost loop that accumulates into the `tmp` array to use a parallel reduction pragma to optimize the reduction operation, but without applying any factor-based modifications to parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a 2mm kernel function, with specific optimization pragmas for loops and functions as follows:\n\n1. Annotate the function `kernel_2mm` that takes matrix dimensions, scalar values alpha and beta, and matrices A, B, C, D, and tmp, to be treated as a hardware acceleration kernel.\n\n2. For the outer loops iterating over dimension i for both major computation blocks (calculating the `tmp` matrix and updating the `D` matrix), implement the following:\n   - Disable automatic pipelining.\n   - Do not tile the loop (preserve the full loop without chunking).\n   - Run loop iterations sequentially, not in parallel.\n\n3. For the inner loops iterating over dimension j within both major computation blocks, apply the same set of directives as for the outer loops:\n   - Disable automatic pipelining.\n   - Do not tile the loop.\n   - Execute loop iterations sequentially.\n\n4. For the innermost loops iterating over dimension k in the first computation block (calculating `tmp`), enable parallel processing with a reduction operation on the `tmp` array and set an automatic factor suggestion, allowing up to 5 parallel operations.\n\n5. For the innermost loops iterating over dimension k in the second computation block (updating `D`), enable parallel processing with a reduction operation on the `D` array and set an automatic factor suggestion to potentially allow up 32 parallel operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the given C function 'kernel_2mm' into a hardware-accelerated kernel, making sure that:\n\n1. The outer loop iterations execute sequentially, without pipelining.\n2. The outer loops are not tiled, and they execute their iterations in series.\n3. The second-level loop (within the first outer loop) should be flattened for pipelining, yet its iterations shall still run in series without tiling.\n4. Ensure that no parallel execution occurs within the multiplication and the first addition of matrices A, B, and C, and the accumulations should happen on a per-element basis of 'tmp' without parallel optimization.\n5. The iterations of the second outer loop and its nested loop shall also execute in sequence without pipeline optimization, and only the nested loop is tiled, specifically into groups of four.\n6. Lastly, enable the parallel execution only in the final addition involving matrix 'D', with the reduction done on 'D' itself, and significantly increase the parallelization factor, aiming for 32 concurrent calculations if possible.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the entire 'kernel_2mm' function to be synthesized as a hardware kernel for acceleration. For the first two-level loop nest, disable loop pipelining, maintain the original loop structure without tiling, and ensure iterations are not executed in parallel. Within the innermost loop of this nest, allow flattening to exploit potential parallelism but enforce the original loop structure and iteration order without parallel execution, with a special consideration for accumulating values in 'tmp' using reduction. For the next two-level loop nest, once again disable pipelining and maintain the original loop structure, but adjust the parallelism of the outer loop to a factor of 10. In the innermost loop, amplify parallelism to a factor of 50, applying reduction when accumulating into 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a 2mm kernel function that treats the entire function as a hardware acceleration kernel. Within the main loops, prevent the first level loop from being automatically pipelined and ensure it processes in its original form without tiling or running iterations in parallel. In the nested loops, allow the second level loop to be automatically pipelined, tiling and parallel factors to be their default value of 1, with one inner-most loop allowing for 5-way parallel execution with reduction on 'tmp'. After the midpoint of the kernel, disable automatic pipelining again for the third level loop while maintaining default tiling and parallelism values of 1, but explicitly flatten the pipeline in the fourth level loop and use 1-way parallel execution with reduction on 'D' for its inner-most loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the compiler to designate the function 'kernel_2mm' as a hardware acceleration kernel but disable automatic pipelining for all loops inside it. Inside the loops, prevent the automatic transformation that subdivides loops into smaller segments or runs iterations in parallel. For the loop that calculates the matrix multiplication between 'tmp' and 'B', enable parallel computation where the 'tmp' array is computed by up to 7 processing elements. In the final loop, allow up to 50 processing elements to work in parallel when updating the 'D' array with the reduction pattern.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, ensuring that all loops within do not have pipelining, tiling, or parallel execution optimizations applied, except for reduction operations which are handled in serial within the loop. Use default settings to turn off pipelining, set tile factor to 1 for no tiling, and set parallel factor to 1 for serial execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function named 'kernel_2mm' for matrix computation without any automatic pipelining. In the first loop nest, avoid pipelining, tiling, or parallelizing, but allow standard loop processing while ensuring there are no reductions or parallel segmentations within its sub-loops. In the second loop nest, continue to prevent pipelining at the outer loop level, but inside the nest, specify tiling with a factor of 4 for the 'i' loop and parallelize the 'j' loop with a factor of 10, while allowing a reduction with a parallel factor of 50 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C with the specified pragmas, the comprehensive instruction is:\n\n\"Define a hardware-accelerated kernel function named 'kernel_2mm' for matrix multiplication and addition with arrays: 'tmp', 'A', 'B', 'C', and 'D'. For all loops, disable automatic pipelining, retain the loops' natural order by preventing the compiler from tiling or parallelizing them, except for the innermost loops where 'tmp' and 'D' are computed with a parallel factor that accommodates all iterations of 'k'. Additionally, apply a loop flattening directive and a parallel factor of 8 on the 'j' loop when processing the 'D' matrix.\" \n\nThe specific #pragma values to be used are:\n\n1. `#pragma ACCEL kernel` - Treat the following function as a hardware kernel for acceleration.\n2. `#pragma ACCEL PIPELINE auto{off}` - Disable automatic pipelining for the loop it precedes.\n3. `#pragma ACCEL TILE FACTOR=auto{1}` - Do not tile the loop and execute it in its full, original form.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` - Avoid parallel execution and execute loop iterations sequentially for the loops over 'i' and 'j'.\n5. `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}` - Parallelize the loop over 'k' with all iterations for reduction on 'tmp'.\n6. `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` - Even though the pragma indicates parallel reduction on 'D', the parallel factor is set to 1, indicating sequential execution.\n7. `#pragma ACCEL PIPELINE auto{flatten}` - Flatten nested loops within this loop level when processing 'D'.\n8. `#pragma ACCEL TILE FACTOR=auto{8}` - Tile the loop with a factor of 8 to divide it into smaller chunks of work.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the `kernel_2mm` function as a dedicated hardware acceleration kernel that does not automatically apply pipelining, tiling, or parallel processing at the top levels of the loops. For the innermost loop that computes the `tmp` array, utilize parallel reduction with a factor of 4, while for the innermost loop that updates the `D` array, execute the iterations serially without parallel optimizations. Specifically, enunciate the following:\n\n- Identify the `kernel_2mm` function as a candidate for hardware acceleration but do not apply any automatic loop transformation strategies such as pipelining, tiling, or parallel processing to the outer loops by stating: 'Do not pipeline, tile, or parallelize the top-level loops automatically'.\n- Within the nested loops that populate the `tmp` array, for the inner reduction loop over `k`, state: 'Apply parallel reduction on the `tmp` array with a factor of 4, but do not automatically apply any transformations to the outer loops'.\n- For the nested loops that update the `D` array, insist: 'Prevent automatic pipelining on the outer loops but allow loop flattening for the inner j-loop, while processing the innermost loop serially with no parallel optimizations'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function utilizing matrix operations with specific optimization constraints using these instructions:\n\n1. Identify the function as a hardware acceleration kernel.\n2. For the entire function, disable automatic pipeline optimization, process the loops in their full, unaltered form, and run loop iterations in sequence rather than in parallel.\n3. Within the first doubly nested loop initializing the 'tmp' matrix, perform parallel reduction on the innermost loop with a factor of 16, allowing multiple iterations to execute simultaneously for improved efficiency where the 'tmp' variable is involved.\n4. Allow the second doubly nested loop altering the 'D' matrix to auto-pipeline the outer loop, keeping the loop structure as is, but execute the middle loop's iterations with a parallelism factor of 10.\n5. In the same loop, for the innermost loop with the reduction operation on 'D', keep the iterations sequential.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the entire `kernel_2mm` function as a hardware kernel for acceleration, but avoid automatically pipelining its loops. Within this kernel, keep both outer loops in their complete forms without any alteration and run their iterations sequentially. For the first outer loop dedicated to matrix multiplication, flatten the pipeline of its inner loop, but do not alter the loop bounds or parallelize its iterations, and additionally, perform a parallel reduction operation on the `tmp` matrix without factorization. For the second outer loop dedicated to updating matrix `D`, leave the pipeline setting for its inner loop as default, ensure no loop bound alterations or iterations are parallelized, apart from a parallel reduction operation on matrix `D` with a factorization of 10 for its innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, use the following instruction:\n\n\"Create an HLS hardware kernel function 'kernel_2mm' for calculating the matrix multiplication and addition of `2mm` (Double Matrix Multiply) with parameters `ni`, `nj`, `nk`, `nl`, `alpha`, `beta`, and matrix arrays `tmp`, `A`, `B`, `C`, `D`. In this function, ensure that:\n\n1. The outermost loops for each operation block run sequentially without any compiler optimizations, such as loop unrolling, pipelining, or tiling.\n2. The inner loop for computing `tmp[i][j]` uses a parallel reduction with a factor that allows some degree of parallel processing, specifically set it to process 10 iterations in parallel.\n3. Similarly, for the inner loop updating `D[i][j]`, employ a parallel reduction with a higher parallel processing factor, set to handle 25 iterations concurrently.\n\nImplement these behaviors with the appropriate #pragma directives. Disable pipelining for all outer loops with '#pragma ACCEL PIPELINE auto{off}', and set tiling and parallel factors for the outer loops to 1 with '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}'. For the inner loops, use '#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{specific_factor}' with 'specific_factor' replaced by 10 or 25 depending on the loop variable being reduced (`tmp` or `D`). Finally, encapsulate the entire function as an acceleration kernel with '#pragma ACCEL kernel' directive.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with appropriate pragmas, you should issue the following comprehensive but concise natural language instruction:\n\nDefine a function named `kernel_2mm` that will be optimized for hardware acceleration, treating it as a hardware kernel. Within the function, perform a matrix multiplication followed by a matrix addition (`D := alpha*A*B + beta*D`). The input arrays should be of fixed sizes with the dimensions provided as function arguments, namely `ni`, `nj`, `nk`, `nl`, `alpha`, `beta`, `tmp`, `A`, `B`, `C`, and `D`. Insert the following pragma directives:\n\n1. For all outer loops, disable automatic pipelining, ensure that the loops are processed without tiling (i.e., no loop partitioning), and execute iterations sequentially without parallelizing.\n2. For the first loop nest responsible for the temporary matrix computation (`tmp[i][j] += alpha * A[i][k] * B[k][j]`):\n   - At the innermost level (`k` loop), allow parallel execution with automatic factor determination but enforce a reduction operation on the `tmp` array.\n3. For the second loop nest handling the addition of the matrix products into `D` (`D[i][j] += tmp[i][k] * C[k][j]`):\n   - Automatically flatten the pipeline at the `i` loop if possible, apply tiling with an automatic factor to the `j` loop, and allow sequential iteration at the innermost loop (`k` loop), marking `D` array for reduction with the default setting for parallel factors.\n\nEncapsulate all transformations applied within `#pragma scop` and `#pragma endscop` directives to indicate the region of the code being optimized. Use the special pragma identifiers `PIPELINE`, `TILE`, `PARALLEL`, and `reduction` where required. For all `auto{}` assignments, let the HLS tool automatically determine the best values rather than specifying them explicitly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function called 'kernel_2mm' that treats the entire function as a hardware kernel and computes the matrix multiplication and addition according to the formula D := alpha*A*B*C + beta*D. Inside the function, you will have two nested loop blocks. \n\nFor the first nested loop block:\n- Do not automatically pipeline the outermost loop.\n- Do not tile or break the outermost loop into smaller chunks, and do not run its iterations in parallel.\n- Inside the outermost loop, automatically determine the appropriate pipelining strategy for the second loop.\n- Again, do not tile the second loop or run its iterations in parallel.\n- For the innermost loop of the first block, use parallelism with a reduction operation on 'tmp' but process only one iteration at a time.\n\nFor the second nested loop block:\n- Again, do not automatically pipeline, tile, or parallelize the outermost loop.\n- Within the outermost loop, automatically determine the pipelining strategy but flatten any loop nestings within this level to a single level of loops.\n- Do not tile the second loop, but run up to ten iterations of it in parallel.\n- Use parallelism with a reduction operation on 'D' for the innermost loop and process only one iteration at a time.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, disable automatic loop pipelining in the first set of nested loops, execute the first set of nested loops without loop tiling and in sequential order, and within the innermost loop of the first set, enable a flattened pipeline structure while also performing a parallel reduction on 'tmp' with sequential execution. Then in the second set of nested loops, allow automatic pipelining, set tiling on the second loop with a factor of 80, maintain sequential execution, and within the innermost loop of the second set, perform a parallel reduction on 'D' with sequential execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS C code for a 2mm kernel function that treats the function as a hardware acceleration kernel, computes the 2mm algorithm `D := alpha*A*B*C + beta*D` with nested loops without automatic pipelining, without tiling, and without parallel optimization, except for the innermost loops where the reduction is to be applied to temporal arrays 'tmp' and 'D' using a parallelization factor of 32 and 1 respectively, while also flattening the pipeline in the second loop nest. Use specific #pragma instructions to control the pipelining, tiling, and parallelization factors accordingly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated 2-matrix multiplication function labeled as a kernel, where the outer loops do not use automatic pipelining, the loops should not be tiled or parallelized beyond their natural iteration, and innermost loops should employ parallel reduction without changing their fundamental sequential nature. Specifically:\n\n1. Define a function called `kernel_2mm` which multiplies and adds matrices, mark it for hardware acceleration.\n2. In the body of the function, disable automatic pipelining for all outer loops while keeping them unfragmented and strictly sequential.\n3. For the two inner loops calculating `tmp` matrix, allow full loop flattening for pipelining but prevent any other form of parallelism or tiling.\n4. Inside the innermost loop of `tmp` calculation, use parallel reduction without altering the original sequence of loop iterations.\n5. Repeat a similar configuration for the loops updating matrix `D`, with no pipelining specified for the outer loop and default pipelining for the inner loop, still maintaining the original sequential processing order.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be synthesized as a hardware acceleration kernel. Within this function:\n- Disable automatic pipelining for the outermost loops.\n- Preserve the original loop structure without tiling for the outermost loops.\n- Execute the outermost loops in a sequential order, without parallelism.\n\nFor the first nested loop handling the calculation of 'tmp', enable pipelining with automatic detection of the optimal pipeline depth, maintain the original loop structure without tiling, and run the iterations sequentially, without parallelism.\n\nFor the innermost loop that computes the 'tmp' array, use parallelization with automatic factor detection, ensure the reductions on 'tmp' are handled correctly.\n\nFor the second set of nested loops updating the 'D' array, enable pipelining with the option to flatten nested loops if beneficial, select a tiling factor of 80 for the intermediate loop, and again, execute iterations sequentially with no parallel factor.\n\nLastly, for the innermost loop updating the 'D' array, apply parallelism with automatic factor detection, ensuring that reductions on 'D' are handled correctly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, then for the first set of nested loops, prevent pipelining, tiling, and parallel execution of the loops, but allow a parallel reduction with a factor of 16 in the innermost loop. In the subsequent set of loops, inhibit pipelining for the top-level loop, but allow the second loop to be flattened and tiled with a factor of 8, and enable executing iterations in parallel with a factor of 10, with the innermost loop's parallel reduction disallowed.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication kernel where matrix E is the product of matrices A and B, F is the product of matrices C and D, and G is the product of matrices E and F. For all loops, disable automatic pipelining, keep loops in their original form without tiling or partitioning, and ensure inner loops are not parallelized except for the loop calculating elements of matrix E, which should have its parallel factor set to 2. Additionally, for the matrices F and G calculations, flatten the pipelining of the j-loops. Use the appropriate pragmas to declare the function as a hardware acceleration kernel and to control the loop behaviors as specified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following 2mm kernel function into a hardware-accelerated kernel and apply specific HLS pragmas as follows:\n\n1. Mark the function as a hardware acceleration kernel.\n2. For the outermost loop iterations over the `tmp` array, flatten the nested loops into a single level of hierarchy and apply the default pipeline optimization strategy.\n3. Within the nested loops calculating the `tmp` array, prevent any automatic pipeline optimization, perform the loop iterations in their original order without any tiling, and run them sequentially without parallelism.\n4. For the parallel innermost loop accumulating into the `tmp` array, apply a reduction strategy and execute the iterations sequentially without parallelism.\n5. Repeat steps 2 to 4 for the outermost loop iterations over the `D` array, ensuring no pipeline optimization, default tiling, and sequential execution within the nested loops, and apply a reduction strategy for the parallel innermost loop accumulating into the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware kernel for acceleration. For the outer loops iterating over 'i' and 'j', do not auto-pipeline, and maintain the original loop structure and sequence by specifying the tiling and parallelism factors to the smallest manageable units. However, for the loop flattening case in the 'i'-'j' nested loops, enable aggressive loop flattening optimization to combine the loops into a single loop when beneficial. Next, inside the 'k' loop for calculating 'tmp', utilize data reduction on 'tmp' without parallelizing the loop. Lastly, for the final nested 'i'-'j' loops, allow automatic optimization decisions for pipelining while keeping the original 'i' loop structure, but for the 'j' loop, apply a moderate level of parallelism (divide it into chunks of 10) and a high degree of parallelism inside the 'k' loop for updating 'D' by specifying an aggressive parallelism factor (process 50 iterations in parallel), again with data reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel and disable automatic loop pipelining, while maintaining the natural ordering and singular execution of each loop's iterations. Specifically, disable pipelining and prevent loop tiling for all loops, and set parallel execution to the singular, sequential factor for the outer and middle loops, while allowing a parallel factor of 2 for innermost loops where the matrix multiplication accumulation occurs.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with appropriate pragmas, use the following comprehensive yet very concise natural language instruction:\n\n\"Treat the function 'kernel_2mm' as a hardware acceleration kernel. Inside the function, for the two main computation loops, initially avoid automatic pipelining, tiling, and parallelization optimizations for the entire loops. Then, within the first loop, automatically enable pipelining. For the innermost loop of the first computation, enable parallel execution with a reduction on the 'tmp' array with a factor of four. In the second main computation loop, allow automatic pipelining but with loop flattening, maintaining no tiling and parallelization optimizations on the entire loop. Within its innermost loop, keep parallel execution with a reduction on the 'D' array with the default factor.\"\n\nSpecific pragma details for each loop are as follows:\n\n1. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n2. Don't automatically pipeline the first and second main computation loops: `#pragma ACCEL PIPELINE auto{off}`\n3. Process the first and second main computation loops in their original, full form without breaking them into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Run the iterations of the first and second main computation loops one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n5. For the first nested loop within the first main computation loop:\n   - Automatically decide whether to pipeline: `#pragma ACCEL PIPELINE auto{}`\n6. In the first innermost loop, enable parallel execution with a reduction on 'tmp' with a factor of four: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}`\n7. For the second main computation loop, enable pipelining with loop flattening optimization: `#pragma ACCEL PIPELINE auto{flatten}`\n8. In the second innermost loop, execute iterations one after another with a reduction on 'D': `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert a given matrix multiplication and addition algorithm into hardware description language, specifying the following directives for optimization: \n\n1. Define the entire function as a hardware acceleration region.\n2. For each of the three major matrix multiplication operations:\n   - Disable automatic loop pipelining.\n   - Preserve the original loop iteration space without tiling.\n   - Execute loop iterations sequentially, not in parallel.\n3. For the innermost loop of the first operation, enable two-way parallelism for loop iterations.\n4. Allow the compilation tool to automatically determine the pipelining strategy for the inner loop of the second operation.\n5. For the second operation, execute all other loops sequentially, preserving the original loop form.\n6. In the third operation, flatten the nested loop structure for the pipelining directive.\n7. In the innermost loop of the third operation, maintain the sequential execution of loop iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel function, where:\n\n1. The function itself is designated as a hardware acceleration kernel.\n2. Within the function, for each major computation section (i.e., computing `E = A * B`, `F = C * D`, and `G = E * F`):\n   - Disable pipeline optimization for the outer two loops by specifying pipeline off.\n   - Avoid loop tiling and parallel processing optimizations by using a tiling factor of 1 and a parallel factor of 1 for the outer two loops.\n\n3. Within each computation section:\n   - For the initialization of the output matrix (`E[i][j] = 0.0`, `F[i][j] = 0.0`, and `G[i][j] = 0.0`), do not apply parallelization to the innermost loop.\n   - For the accumulation computation (`E[i][j] += ...`, `F[i][j] += ...`, and `G[i][j] += ...`), use automatic parallel factor where applicable. Specifically, apply a parallel factor of 1 for the first and last computation sections and use a parallel factor of 32 for the middle section's innermost loop. For the final computation section's innermost loop, apply a parallel factor of 8.\n\nRemember to apply these pragmas immediately before each corresponding loop within the code to control their specific loop optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function named 'kernel_2mm' to accelerate the matrix multiplication and addition process with specific loop optimizations. The function accepts integer dimensions 'ni', 'nj', 'nk', 'nl', and double-precision floating-point scalars 'alpha' and 'beta'. It also takes four two-dimensional arrays 'tmp', 'A', 'B', 'C', and 'D', with the following sizes: 'tmp[40][50]', 'A[40][70]', 'B[70][50]', 'C[50][80]', and 'D[40][80]'. In the first set of nested loops, disable automatic pipelining; prevent tiling and parallelization for the entire loop form; and run another loop within which the 'tmp' array accumulates the product of 'alpha', 'A[i][k]', and 'B[k][j]' in a partially parallel fashion with a parallel factor of eight and a reduction on 'tmp'. In the second set of nested loops, allow automatic pipelining; tile the second nested loop with a factor of eight while preventing parallelization; and execute a reduction operation on 'D' with a parallel factor of twenty-five, accumulating the product of the 'tmp[i][k]' and 'C[k][j]' arrays to 'D[i][j]' after scaling 'D' by 'beta'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS code for a three-matrix multiplication kernel function with specific optimization constraints. Treat the function as a hardware acceleration kernel. For all outer loops, disable automatic pipelining, maintain the original loop structure without tiling, and ensure that iterations run sequentially without parallel execution. For the innermost loops of each of the three matrix multiplication sections, set the parallelization factors to 16, 32, and 50 respectively for the computation of matrices E, F, and G.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication kernel with specific optimization pragmas. Mark the function for hardware acceleration, then optimize the three nested loops for matrix multiplications E=A*B, F=C*D, and G=E*F. For each loop, disable automatic pipelining, maintain the loops' original scale without tiling, and execute iterations sequentially, except for the third loop's innermost computations of E and G, which should have a parallelism factor of 2 and 8, respectively. In the loop for F computation, set the pipelining to be automatically flattened. Use the provided array dimensions for matrices A, B, C, D, E, F, and G.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, and configure all outer loops to execute without pipelining, tiling, or parallelism, manually specifying factors for the innermost loops to enable parallel reduction with different factors for matrix computations.\n\nSpecific instructions for each pragma:\n\n1. Treat `kernel_2mm` as a hardware acceleration kernel: `#pragma ACCEL kernel`.\n2. For all outer loops `i` and `j` in both major computation blocks:\n   - Disable automatic loop pipelining: `#pragma ACCEL PIPELINE auto{off}`.\n   - Keep the loops in full form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`.\n   - Disable parallel execution of iterations: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n3. For inner loop `k` in the first major block, which computes the `tmp` matrix:\n   - Enable parallel reduction on `tmp` with a factor of 7 for efficient utilization of resources in matrix multiplication: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}`.\n4. For the inner loop `k` in the second major block, which updates matrix `D`:\n   - Enable parallel reduction on `D` with a higher factor of 25 for larger granularity in parallel computation: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel that computes matrix multiplications E=A*B, F=C*D, and G=E*F without pipelining any of the loops, without tiling the iteration spaces, and by iterating the outermost loops sequentially. For the innermost loop in the computation of matrix E, set the parallel factor to 2 for potential parallelism, while keeping all other loops in their original form. Specifically, use \"#pragma ACCEL KERNEL\" to identify the function as a kernel, \"#pragma ACCEL PIPELINE auto{off}\" to prevent pipelining of the loops, \"#pragma ACCEL TILE FACTOR=auto{1}\" to avoid loop tiling, \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" in all cases except the inner loop for E where the pragma should read \"#pragma ACCEL PARALLEL FACTOR=auto{2}\".", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a function designed to perform matrix multiplication on the given matrices A, B, C, and D to produce the resultant matrices E, F, and G. Define the function as a hardware acceleration kernel, without applying automatic pipelining, tiling, or parallelization to the main matrix multiplication loops. Instead, apply specific degrees of parallelization to the innermost k-index loops, with factors of 10, 32, and 50 for the multiplications A*B, C*D, and E*F, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a hardware-accelerated kernel function that sequentially computes the matrix multiplications E=A*B, F=C*D, and G=E*F, with explicit dimensions for matrices involved. Disable pipeline optimization for all outer and middle loops, allowing the innermost loops of the first and second matrix multiplications to execute up to 30 and 80 parallel operations, respectively, and keep other innermost loops sequential. Keeping all loops at their original chunk sizes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS codes in C, please apply the following instructions:\n\n1. Declare the function 'kernel_2mm' as a hardware acceleration kernel.\n2. In the first outer loop, disable automatic pipelining, prevent the loop from being tiled, and ensure the loop iterations are run sequentially, not in parallel.\n3. Within the inner loop nested in the first outer loop, enable automatic pipelining, prevent loop tiling, and again ensure iterations are not parallelized.\n4. Apply parallel processing with a specific factor of 8 to the innermost loop, optimizing for reductions on the 'tmp' array.\n5. In the second outer loop, disable automatic pipelining but allow the compiler to potentially flatten the nested loops to optimize performance, prevent loop tiling, and ensure iterations are run one after the other.\n6. Within the inner second loop, facilitate the outer loop to run optimally while still running each iteration sequentially.\n7. Apply parallel processing to the innermost second loop without specifying a factor, optimizing for reductions on the 'D' array.\n\nSpecific '#pragma' directives corresponding to the instructions above:\n\n- '#pragma ACCEL kernel' to treat the function as a hardware kernel.\n- '#pragma ACCEL PIPELINE auto{off}' to prevent automatic pipelining.\n- '#pragma ACCEL TILE FACTOR=auto{1}' to run the loop in its full form without tiling.\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run the loop iterations sequentially.\n- '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}' for the innermost loop of the first outer loop to allow parallel execution with a reduction on 'tmp'.\n- '#pragma ACCEL PIPELINE auto{flatten}' for the second outer loop to allow loop flattening for performance optimization.\n- '#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}' for the innermost loop of the second outer loop to allow parallel execution with a reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given code functions and #pragma directives into a concise natural language instruction:\n\n\"Define a function to accelerate a 2mm matrix-matrix multiplication on hardware using an unoptimized pipeline strategy and serial execution for matrix initialization. Process the second computational loop with low granularity tiling, and highly serialized outer loop but a more parallel inner loop.\" \n\nSpecific #pragma values and their descriptions are:\n\n- Treat the entire 'kernel_2mm' function as a hardware acceleration kernel.\n- For the first loop nest initializing 'tmp', disable automatic loop pipelining, use a tiling factor of 1 ensuring no tiling, and enforce strictly serial loop iterations with a parallel factor of 1.\n- Inside the innermost loop of the 'tmp' initialization, disable loop pipelining and use serial execution but apply a reduction on the 'tmp' array to accumulate the results.\n- For the second loop nest updating 'D', again disable automatic loop pipelining and set a tiling factor of 1 for the outer loop but a tiling factor of 4 for the middle loop, indicating minimal loop unrolling or tiling.\n- Set a parallel factor of 1 for the outer loop, indicating no parallelism, but use a parallel factor of 10 for the middle loop and a high parallel factor of 50 for the inner loop to introduce parallel execution while accumulating the multiply-add results into 'D' with a reduction pragma.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel where each of the nested loops should not be automatically pipelined, with inner loops running sequentially and the original loop structure preserved. Specifically, mark the function as a hardware acceleration kernel without loop transformations and parallelization for the outer and middle loops, while allowing different parallelization factors for the innermost loops: a factor of 20 for the A-B multiplication, 32 for the C-D multiplication, and 50 for the E-F multiplication.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to consider the function as a separate hardware acceleration kernel, and for each nested loop in the series of matrix multiplications disabling optimization and parallelism, such that the outer loops for matrix initialization and the innermost loops for the matrix multiplication computations execute sequentially without pipelining or tiling, while specifying different degrees of parallelism for the innermost loops of each matrix multiplication: 12-way for A*B, 32-way for C*D, and 50-way for E*F. Use \"#pragma ACCEL PIPELINE off\", \"#pragma ACCEL TILE FACTOR=1\", and \"#pragma ACCEL PARALLEL FACTOR=x\" where x is respectively 12, 32, and 50 for the corresponding innermost loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a target for acceleration in hardware, and within it, execute matrix multiplication without pipelining or tiling optimizations for the outer loops, but allow certain levels of parallel execution in the innermost loops: specifically, allow 16-way parallelism when computing the product `E = A * B`, 20-way parallelism when calculating `F = C * D`, and 50-way parallelism for the final product `G = E * F`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function called `kernel_2mm` that performs a matrix multiplication and addition. It takes as inputs dimensions `ni`, `nj`, `nk`, and `nl`, scalars `alpha` and `beta`, and matrices `tmp`, `A`, `B`, `C`, and `D`. Within the function, instruct the compiler to:\n\n- Treat the whole function as a hardware kernel for acceleration.\n- Prevent automatic pipelining of all loops.\n- Disable the tiling optimization by ensuring that each loop is processed without subdividing it into tiles, effectively setting tiling factor to 1.\n- Ensure that all loop operations are performed serially, with a parallel factor of 1 to disable loop unrolling or parallel processing.\n- Specifically for the innermost loop that accumulates `tmp[i][j]`, enable parallel reduction across all 70 iterations with an unspecified factor, allowing the compiler to decide.\n- For the last innermost loop that updates `D[i][j]`, perform parallel reduction and specify that it does allow some parallelism by giving a parallel factor of 2, hinting the compiler to possibly unroll this loop twice for every iteration. Additionally, outside this loop, increase the tiling factor to 8 for the 'j' loop iterating over `D`.\n\nThe process involves a matrix multiplication of `A` and `B`, scaled by `alpha` saved in `tmp`, and a matrix multiplication of `tmp` and `C` added to `D` scaled by `beta`, yielding the final `D` matrix.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, and for each of the nested loops within, perform the following instructions, respectively:\n\n1. For the outermost loops that iterate over indices `i` and `j` before performing matrix multiplications and additions, disable automatic pipelining, retain the loops in their complete unbroken form, and execute iterations sequentially without parallelization.\n\n2. For the innermost loop over index `k` nested within the `i` and `j` loops that calculates the `tmp` matrix, enable full flattening of loop nests for pipelining to enhance performance without breaking it into tiles, run iterations sequentially, and use a parallel reduction on the `tmp` matrix updates with sequential execution.\n\n3. For the next set of loops that iterate over indices `i` and `j` to scale matrix `D` and perform the addition of the `tmp` and `C` matrix product, turn off automatic pipelining, run iterations in original form without breaking them into tiles, and execute the outer loop concurrently with a parallel factor of 10.\n\n4. Finally, for the innermost loop nested within the `i` and `j` loops responsible for updating matrix `D`, enable parallel reduction with a parallel factor of 50 without specifying any pipelining or tiling strategies.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a target for hardware acceleration, and within it, for each nested loop structure that computes the matrix multiplication of the matrices `E = A * B`, `F = C * D`, and `G = E * F`, disable automatic loop pipelining, prevent the loop from being tiled or processed in smaller chunks by setting the tile factor to 1, and maintain the original sequence of loop iterations by setting the parallel factor to 1. Additionally, within the innermost loops that accumulate the products, allow for limited parallelism with factors of 30, 32, and 50 for loops responsible for computing `E`, `F`, and `G` matrices respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a three matrix multiplication kernel function, noting that the kernel itself should be marked for hardware acceleration. Disable automatic pipelining for all loops, ensure all loops are executed in their original form without tiling, and that the iterations of each loop run sequentially except for the innermost loops of each matrix multiplication where you should use parallel factors of 2, 1, and 5 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a dedicated hardware component for acceleration, and in the three matrix multiplication processes within it\u2014namely, \\(E = A \\times B\\), \\(F = C \\times D\\), and \\(G = E \\times F\\)\u2014ensure that the outer and middle loops are not parallelized or pipelined, operating in strict sequential order, while the innermost loops of each multiplication are parallelized with factors of 30, 20, and 50 respectively to introduce concurrency in computations at that level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, use the following comprehensive but concise instruction:\n\nDefine a hardware acceleration kernel function called 'kernel_2mm' for the mathematical operation `D := alpha*A*B*C + beta*D`, where `alpha` and `beta` are scaling factors, and `A`, `B`, `C`, `D`, and `tmp` are matrices with dimensions `ni x nk`, `nk x nj`, `nj x nl`, `ni x nl`, and `ni x nj` restricted to 40x70, 70x50, 50x80, 40x80, and 40x50 respectively. Implement the function with three nested loops:\n\n1. In the first pair of nested loops computing `tmp` matrix product, disable automatic pipelining, prevent the loop from being tiled or parallelized beyond a single iteration per cycle.\n\n2. Within the innermost loop for the `tmp` calculation, apply parallelization with reduction on `tmp` but limit parallel processing to one iteration at a time.\n\n3. In the second pair of nested loops updating matrix `D` with the product of `tmp` and `C`, disable automatic pipeline optimization, maintain the loop's full iteration space without tiling, but flatten the loop's nesting for pipelining, and set the parallel processing of the outer loop to 10 iterations at a time.\n\n4. For the innermost loop updating `D`, parallelize with reduction on `D` limited to one iteration processed at a time.\n\nThe #pragma values to be used are:\n\n- `#pragma ACCEL kernel` to treat the function for hardware acceleration.\n- `#pragma ACCEL PIPELINE \"off\"` to disable automatic pipelining.\n- `#pragma ACCEL TILE FACTOR=1` to maintain the full loop iteration without tiling.\n- `#pragma ACCEL PARALLEL FACTOR=1` to ensure one iteration per cycle without parallelism.\n- `#pragma ACCEL PIPELINE FLATTEN` to flatten the nested loops for pipelining.\n- `#pragma ACCEL PARALLEL FACTOR=10` to indicate parallelism of 10 iterations at a time on the specific loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS compiler to treat the 'kernel_3mm' function as a hardware acceleration unit with the following characteristics: do not apply automatic pipelining to the outer loops of the matrix multiplications; keep each outer loop intact without tiling; execute the outer loop iterations sequentially, but with the innermost loop of the first and third matrix multiplications, allow parallel execution with 30 and 50 iterations processed in parallel, respectively. Additionally, flatten the pipeline of the second outermost loop in the second matrix multiplication.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) function named 'kernel_3mm' that performs matrix multiplication in three stages without automatic pipelining, tiling, or parallelization, except for specific loops where indicated parallel factors should be used. This function should accept integers 'ni', 'nj', 'nk', 'nl', and 'nm', and 2D arrays 'E', 'A', 'B', 'F', 'C', 'D', and 'G'. Inside the function, add hardware acceleration pragmas to:\n\n1. Treat the whole function as an acceleration kernel.\n2. Disable automatic pipelining for all the outer loops processing 'E', 'F', and 'G' matrices.\n3. Disable tiling and automatic parallelization for all the outer and second-level loops.\n4. In the inner loops for calculating 'E[i][j]', set the parallel factor to 15.\n5. In the inner loops for calculating 'F[i][j]', set the parallel factor to 32.\n6. In the inner loops for calculating 'G[i][j]', set the parallel factor to 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function named 'kernel_2mm' that performs a matrix multiply-accumulate operation with alpha and beta scaling, on matrices of fixed size: `tmp[40][50]`, `A[40][70]`, `B[70][50]`, `C[50][80]`, and `D[40][80]`. For the matrix computations, do not leverage automatic pipelining, tiling or parallel processing features for optimization, keeping the loops in their complete, sequential form, except for the second part of the computation where you can experiment with a tile factor of 2 and a parallelization factor of 10 for the `j` loop and a parallelization factor of 50 for the `k` loop without specifying pipelining behavior. Ensure each loop that includes a reduction on `tmp` or `D` matrices uses a pragma for parallel reduction with a default parallelization factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` for hardware acceleration and implement matrix multiplication algorithms for E=A*B, F=C*D, and G=E*F while disabling automatic pipelining, setting tile factors to default loop iterations without tiling, and setting specific parallel factors for the innermost loops: 32 for E's computation, 20 for F's, and 50 for G's.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function named `kernel_3mm` that performs three matrix multiplications (`E = A * B`, `F = C * D`, and `G = E * F`) on fixed-size matrices, and for each nested loop structure, turn off automatic pipelining, set tiling factors to explicitly 1 to keep the loop unbroken, and set parallelism factors to 1 for outer loops while manually setting specific parallelism factors for the innermost loop k to 20 for the first two matrix multiplications and to 50 for the final matrix multiplication.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for matrix multiplication where three separate operations are defined as E := A*B, F := C*D, and G := E*F with specific dimensions. For the first operation, E := A*B, disable automatic pipelining for the outer loop but enable it for the inner loop without tiling or unrolling factors. When computing E[i][j], parallelize the innermost loop with a factor of 32 to exploit data level parallelism. For the second operation, F := C*D, again disable automatic pipelining for the outermost loop, but within the inner loop, enable automatic loop flattening for pipelining, without modifying tiling or unrolling factors, and do not parallelize the innermost loop. Finally, for the operation G := E*F, retain the default pipelining behavior for the inner loop and parallelize the innermost loop with a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided for the function `kernel_3mm`, you would use the following comprehensive but concise natural language instruction:\n\n\"Define a hardware acceleration kernel to carry out matrix multiplication without automatic pipelining, tiling or parallelism enhancements for the outer two loops in each multiplication segment, and specify parallel factors of 32, 80, and 1 for the innermost loops of the three separate matrix multiplication segments, respectively. Explicitly flatten the pipelining for the loop that calculates the final matrix multiplication result.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Design a hardware kernel for matrix multiplication that explicitly handles three separate matrix multiplications (E=A*B, F=C*D, and G=E*F) using a triple nested loop structure. Prevent the outermost loops of each matrix multiplication from being automatically pipelined, but allow for the pipelining of inner loops where suitable. Tile each loop with a factor of one to maintain the original loop bounds. In the accumulation loops for E and G, parallelize the most nested loop with a factor of 32 and 50 respectively to enable simultaneous processing of multiple iterations, whereas for the accumulation loop for F maintain sequential execution with a parallel factor of one. Additionally, apply loop flattening to the pipelining of the inner loop for matrix F=C*D. Apply these directives using the appropriate pragma syntax in the C HLS code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel. Within the function, perform three matrix multiplications without automatically pipelining the loops and without breaking the loops into smaller chunks. Ensure that iterations within the first and third matrix multiplication's innermost loop run in parallel with a parallelization factor matching the middle dimension of the matrices being multiplied, and process the second matrix multiplication in its original loop structure without any parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following HLS C code parameters into natural language instructions:\n\n- Treat the function `kernel_3mm` as a dedicated hardware kernel.\n- For each loop nest inside the `kernel_3mm` function, execute the following directives:\n  - Do not pipeline any of the loops, ensuring they run in their entirety before the next loop iteration starts.\n  - Do not tile any of the loops, so each is performed fully and not reduced to smaller pieces.\n  - Execute the outer and middle loops sequentially, without any parallelism.\n  - For the innermost loops that perform the matrix multiplications `(E = A*B, F = C*D, and G = E*F)`, allow the following parallel factors:\n    - In the `E = A*B` computation, use a parallel factor of 32 for the innermost loop.\n    - In the `F = C*D` computation, use a parallel factor of 20 for the innermost loop.\n    - In the `G = E*F` computation, use a parallel factor of 50 for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a three-matrix multiplication hardware kernel where the first matrix multiplication (E = A * B) processes without any loop pipelining or loop tiling, and each multiplication inside the innermost loop runs all iterations in parallel with a factor of 60. For the second matrix multiplication (F = C * D), disable loop pipelining and tiling, but apply a parallel factor of 10 in the innermost loop. The last matrix multiplication (G = E * F) should have the outermost loop flattened, while keeping loop tiling off and the innermost loop's parallel factor at 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS-optimized C code with three matrix multiplication kernels, ensuring that the outer loops of each kernel are not automatically pipelined and process in their original full form, while the inner loops should be auto-pipelined. The very inner loop of the first kernel should fully parallelize the multiplication for 60 elements, 10 elements are fully parallelized in the second kernel, and no element-level parallelism in the third kernel's innermost loop. Additionally, the first and second kernels must have their intermediary multiplication results initialized to zero before the computations start. All kernels and loops should not be tiled or parallelized beyond the specific instructions. The defined function is to be used as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication kernel where:\n\n1. The function `kernel_3mm` should be treated as a hardware acceleration kernel.\n2. For each matrix multiplication step:\n   - Disable pipelining for the entire operation.\n   - Do not partition the loops into tiles (keep the tiling factor as 1).\n   - Execute loop iterations sequentially (keep parallelism factor as 1).\n\n3. In the calculation of `E = A*B`:\n   - Within the first and second loops, flatten any nested loops to improve data flow.\n   - In the innermost loop for the `k` index, do not introduce parallelism.\n\n4. In the calculation of `F = C*D`:\n   - In the innermost loop for the `k` index, enable parallelism over all 80 iterations.\n\n5. In the calculation of `G = E*F`:\n   - Remove the pragma to prioritize the default behavior for pipelining the first and second loops.\n   - In the innermost loop for the `k` index, enable parallelism over 50 iterations.\n\nThis describes the process represented by the HLS C code including the specific `#pragma` directives, such as:\n- `#pragma ACCEL kernel` to indicate hardware acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` to disable pipelining entirely.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to avoid loop tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to enforce sequential loop execution.\n- In the `E = A*B` computation, `#pragma ACCEL PIPELINE auto{flatten}` within the first two loops to flatten them.\n- In the `F = C*D` computation, `#pragma ACCEL PARALLEL FACTOR=auto{80}` to enable full parallelism in the innermost loop.\n- In the `G = E*F` computation, removal of pipelining pragmas for the first two loops and `#pragma ACCEL PARALLEL FACTOR=auto{50}` to enable parallelism in the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for matrix multiplications without automatic loop pipelining, utilizing whole-loop execution without tiling, and sequential iteration execution. Specifically, for the innermost loop of the first multiplication, use a parallel factor of 32, for the second multiplication, also a factor of 32, and for the third multiplication, a parallel factor of 50. The functions should be treated as hardware kernels.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function as a hardware acceleration kernel, disable automatic pipelining, prevent the subdivision of loops into smaller chunks, and ensure that loop iterations are executed serially. For Matrix E, enable pipelining within the innermost loop and set its parallelization factor to the inner dimension size of the matrix multiplication. For Matrix F and G, likewise flatten the pipeline within their respective innermost loops, but maintain the execution of iterations in sequence without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 3-matrix multiplication kernel with the following characteristics:\n\n1. Declare the function as a hardware acceleration kernel.\n2. For the first matrix multiplication (E = A * B), do not use pipeline optimization for the outer loop, but allow the innermost loop to potentially be parallelized with a maximum factor of 60, keeping the loop structure intact.\n3. For the second matrix multiplication (F = C * D), disable pipeline optimization for both the outer and inner loops, and allow the innermost loop to potentially be parallelized with a maximum factor of 10, keeping the loop structure intact.\n4. For the third matrix multiplication (G = E * F), flatten the pipeline for the outer loop and keep the loop structure intact, without parallelizing the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function 'kernel_2mm' as a hardware acceleration kernel, and within this function:\n\n1. For the first main loop iterating over 'i', do not automatically optimize the loop with pipelining, restrict loop transformations like tiling and unrolling to a factor of 1, and prevent parallel execution of the loop iterations.\n2. Within this loop, enable automatic pipelining optimization for the inner 'j' loop, but again, limit loop tiling and unrolling with a factor of 1 and do not run its iterations in parallel. Additionally, specify that the innermost 'k' loop should perform a reduction on the 'tmp' matrix with no parallel factor optimization.\n3. For the second main loop iterating over 'i', use a flattened pipeline but restrict other loop optimizations to their basic form with a factor of 1, avoiding parallelism.\n4. Within this loop, switch off pipelining for the inner 'j' loop, do not tile or unroll beyond a factor of 1, and do not parallelize its iterations. For the innermost 'k' loop, specify to perform a reduction on the 'D' matrix with no parallel factor optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Develop a function named 'kernel_3mm' as a hardware acceleration kernel that multiplies matrices without any automatic pipelining, tiling, or parallel processing optimizations to preserve the original loop structure. Specifically apply these pragmas: Treat 'kernel_3mm' as a hardware kernel; for the first multiplication ('E = A*B') set the innermost loop to parallelize the multiplication with a factor of 60; for the second multiplication ('F = C*D') set the innermost loop to parallelize the multiplication with a factor of 10; and for the third multiplication ('G = E*F') process the innermost loop sequentially, without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the given function 'kernel_3mm' as a hardware kernel for acceleration. In each of the three primary nested loop constructs within the function:\n\n1. For matrix multiplication 'E = A * B', do not use automatic pipelining, do not tile the loops, and execute the outer two loops sequentially. However, fully unroll the innermost loop involving 'k' by setting the appropriate factor to match the loop iteration count of 60.\n\n2. For matrix multiplication 'F = C * D', again do not use automatic pipelining, do not tile the loops, and execute the outer two loops sequentially. Keep the innermost loop for calculation involving 'k' sequential by specifying a parallel factor of 1.\n\n3. For matrix multiplication 'G = E * F', turn off automatic pipelining for the outer loops but allow pipelining for the innermost loop constructs. Do not tile any of the loops and perform the outer two loops sequentially. Lastly, fully unroll the innermost loop involving 'k' by setting the appropriate factor to the loop iteration count of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function `kernel_3mm` as a hardware kernel, and within it, disable automatic pipelining for all three main loops calculating the matrices `E`, `F`, and `G`. Additionally, ensure these loops are not tiled and are executed iteratively in a single-threaded manner. Specifically, in the loop that calculates `E`, allow parallel processing of the third-level loop with all iterations running concurrently. Similarly, in the loop that calculates `F`, permit parallel processing of the third-level loop with maximum concurrency. In the final segment of the kernel, where `G` is computed, allow the second-level loop to be completely flattened for full parallelism, while keeping the inner-most loop sequential.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is a comprehensive and concise instruction for generating the provided HLS codes:\n\nIdentify the following function `kernel_2mm` as a hardware kernel. For the initial and final matrix multiplication loops, disable automatic pipelining and prevent loop tiling and loop parallelization by explicitly setting these optimizations off or to a factor of 1. In the internal loops, allow the compiler to decide on pipelining. During the first matrix multiplication, prevent any parallel execution and tiling of the outer loop while ensuring the innermost loop executes serially to accumulate values in `tmp` without parallel reduction. In the second matrix multiplication, tile the outermost loop by a factor of 2 and parallelize it by a factor of 10. For the innermost loop, allow a parallel reduction with a high parallelism factor of 50 to accumulate results into `D`.\n\nAnd here is how you would express those directives with #pragma values:\n\n- Treat the function `kernel_2mm` as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n- For the outermost loops in both the initial and final matrix multiplication phases, use `#pragma ACCEL PIPELINE off`, `#pragma ACCEL TILE FACTOR=1`, and `#pragma ACCEL PARALLEL FACTOR=1` to process these loops in their full form sequentially without optimizations.\n- For the inner matrix multiplication loop of accumulating `tmp`, use `#pragma ACCEL PIPELINE auto`, `#pragma ACCEL PARALLEL reduction=tmp FACTOR=1` without loop tiling to enforce the serialized accumulation with no parallel reduction in effect.\n- For the outermost loop in the final matrix multiplication of `D`, automatically optimize pipelining with `#pragma ACCEL PIPELINE auto`, tile the loop by a static factor with `#pragma ACCEL TILE FACTOR=2`, and parallelize it with `#pragma ACCEL PARALLEL FACTOR=10`.\n- For the innermost loop accumulating `D`, allow parallel reduction with `#pragma ACCEL PARALLEL reduction=D FACTOR=50` to exploit a high degree of parallelism in the reduction operation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function called `kernel_3mm` for matrix multiplication that is intended to be compiled into a hardware accelerator kernel. This function takes as input the dimensions `ni`, `nj`, `nk`, `nl`, and `nm`, as well as the matrix operands `A`, `B`, `C`, `D`, and computes the products `E = A*B`, `F = C*D`, and `G = E*F`, storing the results in matrices `E`, `F`, and `G`. Disable automatic pipelining for all loops within the function and set the tile size and parallelism factors to their defaults. For loops computing the matrix products, override the parallelism factor to use 32 for the `E` calculation and 50 for the `G` calculation while computing the multiplication.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-optimized C function called `kernel_2mm` that performs a matrix multiplication and addition of the form D := alpha * A * B * C + beta * D. Define the function with inputs for matrix dimensions (`ni`, `nj`, `nk`, `nl`) and operation parameters (`alpha`, `beta`), alongside four two-dimensional arrays `tmp`, `A`, `B`, `C`, and `D`. The dimensions of these arrays should be `tmp[40][50]`, `A[40][70]`, `B[70][50]`, `C[50][80]`, and `D[40][80]`.\n\nMark `kernel_2mm` as a hardware acceleration kernel. In the two-level nested loop that calculates the intermediate multiplication of `A` and `B`, disable automatic loop pipelining, while within the innermost loop, use pragma to avoid parallel loop execution and disable the division of the loop into smaller chunks. Enable a reduction operation on `tmp` for the parallel pragma with a factor that default to retaining the sequential execution order.\n\nIn the second set of nested loops that accumulates the multiplication of `tmp` and `C` onto `D` after scaling `D` by `beta`, disable automatic loop pipelining for the outermost loop. In the next level, disable pipelining and set a tile factor to split the loop into chunks with a size of 4. For the innermost loop, promote heavy parallelization with a factor favoring parallel execution without specifying the exact value, and use reduction on `D` to harmonize the write operations to the array `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix-multiply kernel, treating the matrices as fixed-size 2D arrays while adhering to the following requirements:\n\n1. Define the function `kernel_3mm` as a hardware kernel for acceleration.\n2. Disable automatic pipelining for each loop level within the kernel.\n3. Preserve the original loop structure without any tiling or partitioning.\n4. Execute the outer loops serially\u2014one iteration after the other.\n5. Parallelize the innermost loops' iterations with a factor of 32 to allow concurrent execution of 32 iterations.\n\nUse appropriate `#pragma ACCEL` annotations to enforce each of these optimizations and restrictions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to designate the function 'kernel_2mm' as a hardware acceleration kernel. Within this function, ensure that all loop structures maintain their original sequential behavior without applying compiler optimizations to pipeline, tile, or parallelize them. Specifically, iterate over the loops sequentially without pipelining, do not partition the loops into smaller blocks, and do not execute iterations in parallel. Exceptions are made for certain loops where the parallelization pragma is applied with a reduction clause for the 'tmp' and 'D' arrays while keeping the parallel factor to 1.\n\nFor the second loop block that modifies the 'D' array, apply loop flattening in the pipelining pragma for the outer 'i' loop and set the tiling factor for the 'j' loop to the whole length of the 'D' array's second dimension, which is 80, allowing that loop to potentially benefit from complete loop unrolling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C with the specified `#pragma` directives, provide the instruction: \"Define a hardware-accelerated kernel function named `kernel_2mm` that performs a matrix multiplication and addition operation without automatically introducing pipeline optimization, tiling, or parallelism to the top-level loops. Specifically, for the first two nested loops, ensure they run sequentially without any optimization, and for the innermost loop of the first two nested loops, enable reduction on `tmp` but without parallelism. Then, for the second set of nested loops, automatically optimize the outermost loop for pipelining, manually flatten the next loop, set a tiling factor of 80 for it, and again ensure the innermost loop's iterations run sequentially while enabling reduction on `D`.\" \n\nHere's a breakdown of what each natural language instruction corresponds with, including the specific `#pragma` values:\n\n- \"Define a hardware-accelerated kernel function named `kernel_2mm`\" corresponds to `#pragma ACCEL kernel`.\n- \"Perform matrix multiplication and addition operation without automatically introducing optimization\" corresponds to:\n  - `#pragma ACCEL PIPELINE auto{off}` for disabling automatic pipeline optimization for the top-level loops.\n  - `#pragma ACCEL TILE FACTOR=auto{1}` for setting tiling factor to `1`, which means no tiling will be applied.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{1}` for ensuring iterations run sequentially by setting the parallel factor to `1`.\n- \"Enable reduction on `tmp`\" corresponds to `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`, allowing reduction operation on `tmp` without parallelism.\n- \"Automatically optimize the outermost loop for pipelining\" corresponds to `#pragma ACCEL PIPELINE auto{}` for the second set of nested loops, suggesting allowing the compiler to decide on pipelining.\n- \"Manually flatten the next loop\" corresponds to `#pragma ACCEL PIPELINE auto{flatten}` for flattening the loop and potentially improving loop execution performance.\n- \"Set a tiling factor of 80\" corresponds to `#pragma ACCEL TILE FACTOR=auto{80}`, which specifies the data should be processed in chunks (tiles) of size 80.\n- \"Enable reduction on `D`\" corresponds to `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`, allowing reduction operation on `D` without parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel for matrix multiplication, ensuring that the top-level loops for each matrix operation (E=A*B, F=C*D, G=E*F) are not automatically pipelined, processed without tiling, and run iteratively rather than in parallel. For the second level loop in the calculation of matrix E and the last level loop in the other matrix calculations, enable automatic pipelining. Use a full parallel factor for the innermost loop when computing E, but restrict parallelism for the innermost loops of F and G calculations to sequential execution, except for fully parallelizing the calculation of the innermost loop in G.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to define a hardware kernel function 'kernel_2mm' that accelerates a matrix multiplication and addition operation using the following directives:\n\n1. Mark the function 'kernel_2mm' to be treated as a hardware kernel for acceleration using \"#pragma ACCEL kernel\".\n2. For the outer two loops that iterate over the dimensions of matrices, disable automatic pipelining, ensure they are not tiled or parallelized by specifying the directives:\n   a. \"#pragma ACCEL PIPELINE auto{off}\" to prevent pipelining of the loop.\n   b. \"#pragma ACCEL TILE FACTOR=auto{1}\" to avoid tiling of the loop.\n   c. \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" to prevent parallelization of the loop.\n3. Within the first nested loops, where temporary matrix 'tmp' accumulates the product of 'alpha', 'A', and 'B', allow automatic pipelining and specify:\n   a. \"#pragma ACCEL PIPELINE auto{}\" to enable pipelining with unspecified default optimization.\n   b. Enable a parallel reduction on the variable 'tmp' with a factor of 7 using \"#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\" for the innermost loop that performs the accumulation.\n4. In the second nested loops, where matrix 'D' is updated using 'beta', enable automatic pipelining with loop flattening and specify:\n   a. \"#pragma ACCEL PIPELINE auto{flatten}\" to flatten nested loops and enable pipelining optimization.\n   b. Use \"#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\" for the innermost loop that performs the update, specifying that parallelization should be done with a factor of 1, which effectively disables it.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, but for each nested loop, specify different optimization strategies:\n\n1. For the outer two loops iterating over variables 'i' and 'j', prevent automatic pipelining, and ensure these loops are executed in the original order without tiling or parallel execution.\n\n2. Inside the loop iterating over 'k' under the 'i' and 'j' loops, enable parallel execution with a reduction operation on 'tmp' with a parallel factor not specified (left to the tool's discretion, but suggested as 10).\n\n3. For the next set of outer loops, where the 'i' loop is followed by the 'j' loop, again prevent automatic pipelining for the 'i' loop but this time, allow tiling for it with the tile factor suggested as 8 while maintaining serial execution.\n\n4. Similarly, within the innermost 'k' loop, enable parallel execution with a reduction operation on 'D' with a parallel factor not specified (left to the tool\u2019s discretion, but suggested as 25).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the High-Level Synthesis (HLS) tool to consider the following function as a dedicated hardware component, and for the top-level 'for' loops and the first loop level inside the matrix multiplication sections, disable automatic pipelining and prevent the loops from being divided into smaller pieces or parallelized. However, enable the tool to make decisions about pipelining for the innermost loops except in sections where iterations are explicitly parallelized over the full dimension of the second matrix in the multiplication.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, use the following comprehensive but concise instruction:\n\n\"Treat the function 'kernel_3mm' as a hardware kernel for acceleration where, within this function, for each of the three main for-loops computing matrix multiplications E=A*B, F=C*D, and G=E*F respectively, ensure that:\n\n1. The outermost loops (looping over i) should not be automatically pipelined or tiled and should process iterations sequentially.\n\n2. The middle loops (looping over j) should also not be automatically pipelined or tiled and should process iterations sequentially.\n\n3. The innermost loops (looping over k) should be handled differently in terms of parallel execution:\n   - For the loop calculating E=A*B, execute a maximum of 32 iterations in parallel.\n   - For the loop calculating F=C*D, execute a maximum of 80 iterations in parallel.\n   - For the loop calculating G=E*F, do not execute iterations in parallel.\n\nAdditionally, when computing G=E*F, the middle loop should 'flatten' any nested loops or conditionals within its scope, essentially linearizing them to possibly allow for better pipelining optimization by the HLS tool.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel. For each of the three loop nests within the function: disable automatic loop pipelining, but manually enable pipelining for the innermost loops. Keep the loop bounds intact without tiling and execute the outermost loops sequentially, while allowing a degree of parallel execution for the innermost loops with factors of 30, 32, and 50 for the E, F, and G matrix computations respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware acceleration kernel where three pairs of matrices are multiplied sequentially. For the first and second matrix pair multiplications, disable automatic pipelining, operate loops without splitting into smaller chunks, and execute iterations sequentially. For the third multiplication, use the same settings except increase parallel processing for the innermost loop by a factor of 16 and the outermost loop by a factor of 2. Specifically, apply '#pragma ACCEL PIPELINE \"off\"', '#pragma ACCEL TILE FACTOR=1', and '#pragma ACCEL PARALLEL FACTOR=1' for the first two multiplications, and for the third multiplication, modify to '#pragma ACCEL PIPELINE \"off\"', '#pragma ACCEL TILE FACTOR=1', '#pragma ACCEL PARALLEL FACTOR=2' for the outer loop, and '#pragma ACCEL PARALLEL FACTOR=16' for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel function that accelerates the hardware computation by avoiding automatic pipeline optimization for the outermost loops, but allowing it to be flattened within the innermost loops. While flattening, let each iteration of the outer loop run sequentially without parallelism, but permit the innermost loop of the final matrix multiplication to run 8 iterations in parallel and the second innermost loop to run 2 iterations in parallel. Ensure that the tiling of loops is done without subdividing them further by setting the tiling factor to 1. Additionally, declare the entire function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate this function 'kernel_3mm' as a hardware acceleration candidate, ensuring that the outer loops for matrix multiplication 'E := A*B', 'F := C*D', and 'G := E*F' operate sequentially without pipelining, tiling, or parallel processing enhancements. Within the matrix multiplication, disable pipelining and tiling for the second-level loops while maintaining a sequential iteration order. For the innermost loop where the actual multiplications occur, allow parallel processing with a grouping of 32 for multiplying matrices 'A*B' and 'C*D', and a grouping of 50 for multiplying matrices 'E*F'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS accelerator kernel for matrix multiplication without pipelining the outer loops, without tiling the loops, and without parallelizing them, but allowing innermost loops' iterations to run two at a time for the first matrix multiplication, sequentially for the second multiplication, and four at a time for the third multiplication. Use `#pragma ACCEL PIPELINE \"off\"`, `#pragma ACCEL TILE FACTOR=1`, and `#pragma ACCEL PARALLEL FACTOR=auto{X}` where X is `1`, `2`, and `4` for the respective levels of parallelism in the matrix multiplications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, you would instruct the code generator as follows:\n\nDeclare a function named 'kernel_2mm' as a hardware acceleration kernel intended for FPGA synthesis, that calculates the matrix multiplication and addition according to the formula D := alpha * A * B * C + beta * D. The function takes integers ni, nj, nk, nl, doubles alpha, beta, and four 2D double arrays named tmp, A, B, C, and D with specified sizes of 40x50, 40x70, 70x50, 50x80, and 40x80 respectively.\n\nFor the outermost D-matrix computation loop, do not automatically pipeline the loop, keep the loop in its full form without tiling, and process the iteration serially rather than in parallel.\n\nFor the inner loop that initializes the temporary matrix tmp, pipeline the loop without specifying the factor, prevent the loop from being tiled, and execute iterations serially.\n\nWithin the innermost loop calculating tmp, perform the accumulation in parallel with a factor of 7 and specify a reduction operation on the temporary variable 'tmp'.\n\nFor the final nested loop matrix calculation updating D, pipeline the loop without specifying the factor, tile the loop with a factor of 8, and again, ensure that iterations are processed serially.\n\nLastly, within the innermost loop updating D, specify a parallel factor of 25 with a reduction on the variable 'D'.\n\nThe #pragma instructions in natural language are:\n\n- Treat 'kernel_2mm' as a hardware acceleration function. (#pragma ACCEL kernel)\n- Don't automatically pipeline the outermost loop, prevent it from being tiled, and process its iterations serially. (#pragma ACCEL PIPELINE auto{off}, #pragma ACCEL TILE FACTOR=auto{1}, #pragma ACCEL PARALLEL FACTOR=auto{1})\n- Allow the pipelining of the inner loop that initializes 'tmp' but do not specify the factor, don't tile it, and make its iterations run serially. (#pragma ACCEL PIPELINE, #pragma ACCEL TILE FACTOR=auto{1}, #pragma ACCEL PARALLEL FACTOR=auto{1})\n- Within the innermost loop for 'tmp', enable parallel execution with a factor of 7 for the reduction operation on 'tmp'. (#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{7})\n- Enable pipelining without a specified factor for the loop updating 'D', tile this loop with a factor of 8, and iterate serially. (#pragma ACCEL PIPELINE, #pragma ACCEL TILE FACTOR=auto{8}, #pragma ACCEL PARALLEL FACTOR=auto{1})\n- In the innermost loop for 'D', set up parallel execution with a factor of 25 for the reduction on 'D'. (#pragma ACCEL PARALLEL reduction=D FACTOR=auto{25})", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code that treats the function `kernel_3mm` as a hardware kernel for acceleration, where matrix multiplication is performed among matrices `A`, `B`, `C`, `D`, and the results are stored in matrices `E`, `F`, and `G`. For each matrix multiplication, the outer two loops should not be automatically pipelined and should keep their original sequential form, the innermost loop should run sequentially, and intermediate results are calculated in flattened pipelining. Additionally, matrix `G` should have a parallel factor of 2 for the second outer loop, whereas all others maintain a parallel factor of 1. For tiling, use the default factor of 1 for all loops to ensure the original, full loop form is used.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to:\n\n1. Designate the `kernel_2mm` function as a hardware-accelerated kernel.\n2. Prevent automatic pipelining across the outer two `for` loops.\n3. Keep the first nested `for` loop in its entirety, and allow the innermost loop to perform eight parallel multiplications with an automatic reduction on the `tmp` array.\n4. In the second nested `for` loop, tile the loop with a factor of 8, while automatically determining the optimal parallelization factor for the innermost loop which should maximize parallel operations on the `D` array up to a factor of 25.\n\nSpecific pragmas and their meanings:\n- `#pragma ACCEL kernel` - Treat the `kernel_2mm` function as a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` - Do not automatically pipeline the subsequent for loops.\n- `#pragma ACCEL TILE FACTOR=auto{1}` - Do not tile the loop; execute it in full each iteration.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` - Execute loop iterations sequentially, not in parallel.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}` - Perform parallel operations on the `tmp` array, automatically determining the factor up to 8 with reduction to accumulate the results.\n- `#pragma ACCEL PIPELINE auto{}` - Allow automatic pipelining optimization decisions for the following loops.\n- `#pragma ACCEL TILE FACTOR=auto{8}` - Tile the second nested loop\u2019s iterations to be processed in chunks of size 8.\n- `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}` - Perform parallel operations on the `D` array, automatically determining the factor up to 25 with reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, the following natural language instruction encapsulates the necessary guidance:\n\n\"Treat the entire function 'kernel_3mm' as a hardware acceleration kernel, and within this function for each of the three nested loop structures computing matrix multiplications, first, do not pipeline the outermost loop, ensuring it executes in the standard sequential fashion without optimization. In the inner loops, leave the default automatic pipelining setting, which allows the compiler to decide on the pipelining strategy. For the middle loop in each structure, indicate that it should not be tiled or parallelized beyond the original loop formulation. Finally, in the innermost loops where the actual multiplication takes places, specify a parallelism factor exactly equal to the number of iterations of the loop, allowing all iterations to be computed in parallel if resources permit.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration kernel, and within it, for all the nested loops, disable automatic pipelining, set tiling to process the loops without division, and set the default parallelization factor to 1, except for the specific inner loops of `F` and `G` calculation where parallelization factors are set to 20 and 32, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware kernel for acceleration, and within it, prevent any automatic optimization for loop pipelining, ensure each loop is executed in its full sequential form without tiling or running iterations in parallel except for the third loop in the second matrix multiplication where you should use a parallelization factor of 20, and in the third matrix multiplication adopt a parallelization factor of 2 for the outer loop and 32 for the inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a three-matrix multiplication kernel treating it as a hardware acceleration function without pipelining, tiling, or parallel optimization for the main operation loops, but allow for automatic decision on the pipelining of the innermost loop of the final matrix multiplication with a specific optimization for doubling the parallel factor in the innermost loop of the first matrix multiplication operation. Use the following detailed instructions:\n\n1. Treat the function `kernel_3mm` as a hardware kernel for acceleration.\n2. For each of the three main matrix multiplications (E=A*B, F=C*D, G=E*F), do not pipeline the computation, process each loop in its original full form without breaking it into smaller chunks, and run the iterations of each loop one after the other, not in parallel.\n3. In the innermost loop of the first matrix multiplication (E=A*B), set the parallel factor to double the default setting.\n4. Allow the compiler to automatically decide whether to pipeline the innermost loop of the final matrix multiplication (G=E*F) and specify 'flatten' for the pipelining option. \n5. For all other innermost loops and all other situations, use the default settings without any optimizations.\n\nTranslate this into HLS pragmas as follows:\n\nFor the `kernel_3mm` function, use `#pragma ACCEL kernel` to specify hardware acceleration.\n\nFor each loop where you should not apply optimizations, use `#pragma ACCEL PIPELINE \"off\"` to prevent pipelining, `#pragma ACCEL TILE FACTOR=1` to avoid tiling, and `#pragma ACCEL PARALLEL FACTOR=1` to disable parallel execution.\n\nFor the specific parallel factor doubling, use `#pragma ACCEL PARALLEL FACTOR=2`.\n\nFor the loop with the automatic pipelining decision and 'flatten' option, use `#pragma ACCEL PIPELINE auto{flatten}`.\n\nNo pragma is needed for default parallel settings, but you can explicitly mention `#pragma ACCEL PARALLEL FACTOR=auto{1}` if clarity is required.\n\nHere are the specific #pragma values described for each part of the code:\n\n1. `#pragma ACCEL kernel` - Treat the following function as a hardware kernel for acceleration.\n2. `#pragma ACCEL PIPELINE auto{off}` - Do not automatically pipeline this loop.\n3. `#pragma ACCEL TILE FACTOR=auto{1}` - Process this loop in its original, full form without breaking it into smaller chunks.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` - Run the iterations of this loop one after the other, not in parallel.\n5. `#pragma ACCEL PARALLEL FACTOR=auto{2}` - Specifically for the innermost loop of E=A*B, double the parallel factor.\n6. `#pragma ACCEL PIPELINE auto{flatten}` - For the innermost loop of G=E*F, allow automatic pipelining with a 'flatten' option.\n\nPlease note that the use of \"auto{}\" in pragma comments is a placeholder to reflect that the decision would be compiling-time dependent; normally, these are not included in actual pragma syntax.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` as a hardware acceleration kernel, and for each loop within this function, do the following: \n\n1. For the outermost loops (the ones iterating over `i`), prevent automatic pipelining, use the original loop form without tiling, and execute iterations sequentially. \n2. For the first inner loop (the one iterating over `j` for calculating `tmp`), flatten the loop during pipelining, do not tile it, and execute iterations sequentially. \n3. Within this inner loop, for the innermost loop (the one iterating over `k` for calculating `tmp`), enable parallel execution with a reduction on `tmp`, but do not increase the factor beyond what is automatically set (factor=auto implies using default parallelism).\n4. For the second inner loop (the one iterating over `j` for updating `D`), again, prevent pipelining, do not tile the loop, and execute iterations sequentially.\n5. Within this loop, for the innermost loop (the one iterating over `k` for updating `D`), enable parallel execution with a reduction on `D` and set the parallel factor to 8 times what is automatically recommended.\n\nRemember, do not enable pipelining or tiling unless specifically stated, and execute outer and second-level inner loop iterations in a sequential manner by default. The reference to \"auto{value}\" means the directive will use the value provided if it's better than the automatic determination by the tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Mark the function `kernel_2mm` as a hardware acceleration kernel, and configure the high-level synthesis (HLS) tool to handle all loops in the following ways:\n\n1. Keep the outermost loops for both the `tmp` calculation and the `D` update fully sequential without any automatic pipelining or tiling optimisations. \n2. For the innermost loop calculating `tmp[i][j]`, allow reduction on `tmp` with a full parallel computation across the k-domain that consumes `alpha`, `A`, and `B`, yet without any specified parallel factor.\n3. For the inner loop updating `D[i][j]`, flatten the pipeline stages, tile the computations with respect to `D`'s secondary dimension (i.e., `j` ranging from 0 to 80), force sequential execution along the k-domain, and allow reduction on `D`, again without setting a specific factor for parallel computation.\n\nSpecific pragma values are as follows:\n\n- For disabling automatic pipelining on the two outermost loops and their immediate nested loops, use `#pragma ACCEL PIPELINE auto{off}`.\n- To prevent automatic tiling across the loops and ensure full loop execution, use `#pragma ACCEL TILE FACTOR=auto{1}`.\n- To maintain sequential loop iteration without parallelism for the specified loops, use `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Apply `#pragma ACCEL PARALLEL reduction =` with the targeted variable (either `tmp` or `D`) for parallel reduction; no specific factor is given, hence `FACTOR=auto{}` is not used for these loops.\n- When updating `D`, instruct the compiler to flatten pipeline stages with `#pragma ACCEL PIPELINE auto{flatten}`, and explicitly tile the computation for the second dimension of `D` with `#pragma ACCEL TILE FACTOR=auto{80}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel by using the following instructions: \n\n1. Declare the function as a hardware kernel for acceleration.\n2. Disable automatic pipelining for the entire 2mm computation process.\n3. Ensure that the main computation loops are executed in their original form without tiling or parallelizing them beyond the default settings.\n4. Enable pipelining for individual nested loops to potentially improve loop throughput but do not change the default tiling or parallel factors.\n5. Perform the matrix multiplication of `A*B` and accumulate the result in `tmp` using a parallel reduction pragma to allow concurrent summation but without specifying how many elements can be processed in parallel.\n6. Subsequently, compute `alpha*A*B + beta*D` by first scaling matrix `D` by `beta`, then performing an element-wise addition of the `tmp` matrix multiplied by matrix `C`, with pipelining enabled but without changing the default tiling. Use the parallel reduction pragma for the innermost loop for adding the products to `D` but process the reductions sequentially.\n\nCorresponding pramga details:\n\n1. Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`.\n2. Don't automatically pipeline the top-level loops: `#pragma ACCEL PIPELINE \"off\"`.\n3. Run the top-level loops without breaking them into smaller chunks: `#pragma ACCEL TILE FACTOR=1` and `#pragma ACCEL PARALLEL FACTOR=1`.\n4. For nested loops, enable pipelining with default settings: `#pragma ACCEL PIPELINE auto{}`. Ensure loops are in original size and iterations run consecutively with `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n5. Enable parallel reductions for accumulating matrix products with unspecified parallelism: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{}`.\n6. For the second matrix operation, pipeline the outer product loop with `#pragma ACCEL PIPELINE auto{flatten}` and specify a preference for tiling in the second dimension with `#pragma ACCEL TILE FACTOR=auto{80}` while keeping the inner reduction sequential: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel with specific optimization directives. Define the kernel function that performs the matrix multiplications E = A * B, F = C * D, and G = E * F with loop bounds based on the provided matrix dimensions. Disable automatic pipelining for each top-level matrix multiplication loop. Within these loops, allow the innermost k-loop to be fully parallelized with a factor corresponding to the full inner dimension of the resulting product matrix (60 for E, 80 for F, and 50 for G, respectively). Ensure that tiling and additional parallelization on the outer loops i and j are suppressed by setting their factors to 1, while also preventing them from being automatically pipelined. Additionally, specify the entire function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function `kernel_3mm` that performs matrix multiplications to calculate E=A*B, F=C*D, and G=E*F without pipelining, tiling, nor parallel processing enhancements for the main matrix multiplication loops, and use auto-pragmas to flatten the innermost multiplications in E=A*B and F=C*D. For G=E*F, use auto-pragmas to turn off pipelining and tiling, set parallel factor to 2 for the outer loop, and parallelize the innermost loop of G=E*F with a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel. Inside this kernel, perform matrix multiplications for E=A*B, F=C*D, and G=E*F. For the first matrix multiplication, handle the loop iterations for variables i, j, and k sequentially without pipelining, tiling, or parallel processing. For the second matrix multiplication, process the i-loop without pipelining or parallel processing but permit loop flattening during pipelining in the j-loop. Again, process without tiling or parallel processing in k-loop. For the final matrix multiplication, turn off pipelining and tiling in the i-loop with a moderate parallel factor of 2, while aggressively parallelizing the k-loop with a factor of 32 when computing G=E*F.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a high-level synthesis (HLS) C code for matrix multiplication and addition algorithms to create three matrices (E, F, G) by carrying out the following operations. Define the function as a hardware kernel for acceleration, and for each operation, configure the loops through the following preferences:\n\n1. Initialize the computation for matrices E, F, and G by disabling automatic pipeline optimization, not allowing the loop to be broken into smaller chunks, and specifying that loop iterations should be executed sequentially.\n2. While computing the matrix E as the product of matrices A and B, enable pipeline optimization for the second nested loop to automatically set pipeline depth, prevent loop tiling, and execute the innermost loop with a maximum parallelism corresponding to the number of elements processed in the k dimension for matrix B.\n3. Compute the matrix F as the product of matrices C and D by again disabling automatic pipeline optimization for the outermost loop, flattening nested loops where feasible, utilizing default tiling, and keeping the innermost loop sequential.\n4. Compute the matrix G as the product of matrices E and F, re-enable pipeline optimization for the second nested loop automatically, maintain the default tiling, and set the innermost loop's parallelism to match the number of elements processed in the k dimension for matrix E.\n\nSpecifically, apply the following #pragma directives:\n- For initializing each matrix computation, use `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the second nested loop while computing matrix E, use `#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{60}`.\n- For the second nested loop while computing matrix F, use `#pragma ACCEL PIPELINE auto{flatten}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the second nested loop while computing matrix G, use `#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{50}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a matrix multiplication kernel, treating the entire function as a hardware acceleration kernel and configuring the pragmas for the nested loops as follows: Disable automatic pipelining, maintain the original loop structure without tiling, and execute loop iterations sequentially. For the first and third matrix multiplications, flatten the pipelining of the outer loops, set the parallelization factor to 1 for all but the second innermost loop of the second matrix multiplication, which should have a parallelization factor of 20, and set the parallelization factor to 2 for the outermost loop of the third matrix multiplication. Make sure to initialize matrix elements to zero before accumulation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function implementing triple matrix multiplication: First, compute the matrix product of A and B, storing the result in E; then, compute the product of C and D, storing the result in F; lastly, compute the product of the previously obtained matrices E and F, storing the final result in G. The dimensions for these matrices are given as ni, nj, nk, nl, and nm. Each of the three main matrix multiplication loops and their nested loops should not be automatically pipelined, should not be tiled to process chunks concurrently, and should not automatically parallelize. However, inside the innermost loop where actual multiplication and accumulation are happening, set the parallelization to process 30, 32, and 50 elements at once respectively for the three matrix multiplications E=A*B, F=C*D, and G=E*F. Ensure that all computational loops operate correctly according to their bounds and that the kernel function is optimized for FPGAs by allocating a dedicated area of the chip for this calculation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the following function as a hardware kernel, ensuring that the outermost loops for each matrix computation block are not pipelined, are not tiled, and run in sequential order. However, enable automatic pipelining with default settings for the second-level nested loops while keeping them non-tiled and also running in sequential order. For the innermost loops, set the parallelization factors to 1 for both the first and second computations, while setting them to 32 and 25 for the third computation's inner loops, corresponding to the multiplication of matrices C and D, and the multiplication of matrices E and F respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code that treats a specified function as a hardware acceleration kernel while ensuring that all loops within the function execute without pipelining, tiling, or parallel optimization, except where explicitly mentioned:\n\n1. Start by defining the function as a hardware kernel for acceleration with `#pragma ACCEL kernel`.\n2. In the first and third computational segments (matrix multiplications of `A*B` and `E*F`), disable loop pipelining, disable tiling by setting the tiling factor to 1, and run the loops serially by setting the parallel factor to 1 for the outer and middle loops.\n3. In the first computational segment, enable parallel execution of the innermost loop with a parallel factor of 32 using `#pragma ACCEL PARALLEL FACTOR=auto{32}`.\n4. In the second computational segment performing `C*D`, set the innermost loop to have no parallel factor, implying serial execution with `#pragma ACCEL PARALLEL FACTOR=auto{1}` and flatten the outer loop's pipeline by using `#pragma ACCEL PIPELINE auto{flatten}`.\n5. In the third computational segment, enable parallel execution of the innermost loop with a parallel factor of 50 using `#pragma ACCEL PARALLEL FACTOR=auto{50}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a matrix multiplication kernel, treating the function as a hardware acceleration kernel, executing the loops without pipelining, without tiling, and without parallelization except for the final nested loop in the 'G := E*F' section which should be partially parallelized with a factor of 32. Specifically, you should:\n\n1. Treat the function 'kernel_3mm' as a hardware acceleration kernel.\n2. For the first multiplication 'E := A*B':\n   - Do not use pipelining for the outer and inner loop processing.\n   - Do not break the loops into smaller chunks (tiling).\n   - Execute iterations of the innermost loop sequentially (no parallel factor).\n3. For the second multiplication 'F := C*D':\n   - Do not use pipelining for the outer loop, and let the inner loop process in its original full form (use flatten pragma for pipelining the inner loop).\n   - Do not tile the loops.\n   - Execute iterations of the innermost loop sequentially (no parallel factor).\n4. For the final multiplication 'G := E*F':\n   - Do not use pipelining or tiling for the outer loop, but pipeline the inner loop without hinting parallelism.\n   - Do not tile the loops.\n   - Parallelize the innermost loop with a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function named 'kernel_3mm' for matrix multiplication of fixed-size 2D arrays, applying the following optimization directives in High-Level Synthesis (HLS):\n\n1. Mark the function as a kernel for hardware acceleration.\n2. For the top-level 'for' loops, disable the automatic pipeline optimization.\n3. For nested 'for' loops calculating 'E', disable automatic pipelining and configure the loops to run without tiling or parallel optimization.\n4. For nested 'for' loops calculating 'F', similarly disable automatic pipelining and keep loop execution without tiling or parallel optimization.\n5. For nested 'for' loops calculating 'G', turn off automatic pipelining, leave tiling unoptimized, and set the parallel factor to 2 for the loop iterating over the 'j' index.\n6. Enable automatic pipeline optimization with flattening for the outer loops where possible.\n\nEnsure that the respective `#pragma` directives are applied to each loop within the function definition as indicated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, the following comprehensive instruction can be articulated:\n\nFor the C function named 'kernel_3mm' designed to perform three matrix-matrix multiplications on fixed-size 2D arrays, designate the function as a hardware kernel suitable for acceleration. Within each matrix multiplication (E=A*B, F=C*D, and G=E*F), proceed as follows: do not automatically pipeline the outermost loops, but rather maintain their full, original loop structure and execute the iterations sequentially without parallelism or tiling optimizations. However, for the nested inner loops, apply automatic pragma decisions to determine if they should be flattened for improved pipelining, while still executing iterations sequentially and preventing the loops from being tiled. Specifically, set parallel processing for the second-most inner loops (initializing E, F, and G matrices to zero) to run iterations in series and use a parallel factor of one, indicating no actual parallel execution. Finally, for the innermost loops performing the element-wise multiplications, utilize a parallel factor of 32 for the G matrix computation, allowing simultaneous processing of 32 iterations, while leaving the other multiplication loops with a default sequence of operation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a three-matrix multiplication kernel function 'kernel_3mm' which carries out the operation `E = A * B`, `F = C * D`, and `G = E * F` on given input matrices. Define the function as a hardware kernel for acceleration. For the top-level loops over each matrix multiplication, disable automatic pipelining, process them without tiling, and ensure they run sequentially by applying a parallel factor of 1. For the inner loop 'j' responsible for initializing the output matrices (E, F, and G), enable flattening for pipeline optimization, use tiling with a factor of 1, and maintain a parallel factor of 1 for sequential computation. For the computation innermost loop 'k', which accumulates the multiplications, set parallel factors to 1 for the first two matrix multiplications (E = A * B and F = C * D). For the final matrix multiplication (G = E * F), specify a parallel factor of 2 for the loop 'j' and a parallel factor of 10 for the loop 'k'. Ensure that the matrix G's outer loop over 'i' also has pipelining turned off and tiling factor set to 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C with specific pragmas, please use the following natural language instruction:\n\n\"Define a hardware kernel function called 'kernel_3mm' that performs matrix multiplication with three pairs of matrices. For the first pair of matrices A and B, the result is stored in matrix E; for the second pair C and D, the result is in F; and for the third pair (resultant matrices E and F), the result is in G. For each of the matrix multiplications, fully disable automatic pipelining, retain the original loop structure exactly as-is without any tiling, and set all loops to run without any parallelism except for the innermost loop of the second matrix multiplication, which should have a parallel factor of 32, and the middle loop of the third matrix multiplication, which should operate with a parallel factor of 2. Additionally, for the outer loops of both the first and third matrix multiplications, use a pragma that explicitly demands the loop to be flattened.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a three-matrix multiplication kernel without pipelining, using single iteration chunks, with the following specifications: The outermost loop of each matrix multiplication should not be pipelined or parallelized, while the first inner loop should be flattened and not parallelized, and the second inner loop should be run serially. The loops for calculating the intermediate matrix F should allow a parallelization factor of 10 for the innermost loop. For the final multiplication resulting in matrix G, use a parallelization factor of 2 for the inner loop assigning values to G. Use the ACCEL directives to denote the kernel function and control loop transformation behaviors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with specified pragmas, use the following comprehensive but concise natural language instruction:\n\n\"Define a hardware kernel function that performs three matrix multiplications consecutively, with the first and second operations iterating without pipelining, tiling, and parallelization; fully serialize the innermost loop for the first operation except for a parallel factor of 16, flatten the pipelining for the middle loop of the second operation with no parallelization in its innermost loop, and in the final operation, enable default pipelining for the middle loop with slight parallelization with a factor of 2 and aggressively parallelize its innermost loop with a factor of 32.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and within this function:\n\n1. For the outermost loop and its immediate nested loops (processing elements 'i' and 'j'), disable loop pipelining to ensure the loops execute sequentially without optimization.\n\n2. Refrain from tiling the loops by setting the tile factor to one, which means process the loop in its entirety without partitioning it into smaller blocks.\n\n3. For the innermost loops within each 'i' and 'j' loop, schedule the loop iterations to run one after another (in series, not in parallel) by setting the parallel factor to one. Additionally, for the reductions on 'tmp' and 'D', setup parallel reduction while still adhering to the requirement of serial execution of iterations.\n\n4. In the second group of nested loops (processing elements 'i' and 'j'), where 'D' is updated, allow flattening the pipelined loops to combine them into a single loop structure. This is the only place in the code where you will differ from the pattern of disabling pipeline optimizations.\n\nEach of these directives corresponds to specific '#pragma ACCEL' statements:\n\n- '#pragma ACCEL kernel' is used to identify the function to be treated as a hardware kernel.\n- '#pragma ACCEL PIPELINE auto{off}' is used to disable automatic loop pipelining.\n- '#pragma ACCEL TILE FACTOR=auto{1}' is used to prevent loop tiling by setting the tile factor to one.\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' is used to prevent loop parallelization by setting the parallel factor to one.\n- '#pragma ACCEL PARALLEL reduction = [variable_name] FACTOR=auto{1}' is used to specify that a parallel reduction occurs on the specified variable, 'tmp' or 'D', while loop iterations execute serially.\n- '#pragma ACCEL PIPELINE auto{flatten}' is used to specify that loop pipelining should be flattened.\n\nPlease note that 'auto{}' is just a placeholder, and in reality, you would not include 'auto' and the curly braces in the actual pragma; it would simply be '#pragma ACCEL PIPELINE off', for instance.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, instruct as follows:\n\n1. Define a function named `kernel_3mm` as a hardware kernel to accelerate the matrix-matrix multiplication of three pairs of matrices, A*B=E, C*D=F, and E*F=G.\n\n2. Within the function, for each of the three matrix-matrix multiplication sections, manage the loops as described:\n   \n   - For the loop that computes matrix E (`E := A*B`):\n     - Enable automatic pipelining but prevent loop flattening.\n     - Do not tile or parallelize the outermost loop.\n     - Prevent pipelining of the second loop (iterations over `j`).\n     - Within the innermost loop (iterations over `k`), do not adjust parallelism.\n\n   - For the loop that computes matrix F (`F := C*D`):\n     - Disable pipelining for the outer two loops (iterations over `i` and `j`).\n     - In the innermost loop (iterations over `k`), do not adjust parallelism.\n\n   - For the loop that computes matrix G (`G := E*F`):\n     - Only pipeline the second nested loop (iterations over `j`) with a parallel factor of 10.\n     - In the innermost loop (iterations over `k`), do not adjust parallelism.\n   \n   Make sure to initialize the result matrices E, F, and G to zero before any matrix multiplication.\n\nUse these instructions to insert the appropriate HLS pragmas:\n- Treat the `kernel_3mm` function as a hardware kernel with `#pragma ACCEL kernel`.\n- For sections requiring automatic pipelining without flattening, use `#pragma ACCEL PIPELINE auto{flatten}` followed by `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Where pipelining needs to be turned off, use `#pragma ACCEL PIPELINE auto{off}` and ensure neither tiling nor parallelism is adjusted by setting their factors to 1.\n- In the last section, specifically increase parallelism of the second loop with `#pragma ACCEL PARALLEL FACTOR=auto{10}` while not adjusting it for other loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with the appropriate pragmas, you should use this comprehensive but very concise instruction:\n\n\"Define a kernel function named 'kernel_3mm' that takes seven arguments: three integer sizes 'ni', 'nj', 'nk', 'nl', 'nm' and four 2D array inputs 'A', 'B', 'C', 'D' for matrix multiplication, along with two 2D array outputs 'E', 'F', and 'G'. For the matrix multiplication of 'E=A*B', 'F=C*D', and 'G=E*F', do not automatically pipeline the top-level loops; ensure the innermost loops are pipelined, but flatten them into a single loop structure. Disable loop tiling by specifying a tile factor of one, and for the 'G=E*F' multiplication, set the parallelism factor to 2 for the 'j' loop and 25 for the 'k' loop to compute multiple iterations in parallel; for all other loops, process the iterations sequentially with a parallelism factor of one.\" \n\nHere's the corresponding #pragma descriptions for clarity:\n- Treat 'kernel_3mm' as a function for hardware acceleration: `#pragma ACCEL kernel`\n- Do not automatically pipeline the top-level loops: `#pragma ACCEL PIPELINE auto{off}`\n- Process the top-level loops in their original form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Process the iterations of the top-level loops one after the other: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Flatten nested loops for the 'i' and 'j' loops inside 'E=A*B' and 'F=C*D' calculations and pipeline them: `#pragma ACCEL PIPELINE auto{flatten}`\n- Use default tiling (no tiling) for inner 'j' loops: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the iterations sequentially for the 'k' loops inside 'E=A*B' and 'F=C*D': `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Set parallel computation with a factor of 2 for the 'j' loop and 25 for the 'k' loop in the 'G=E*F' computation: `#pragma ACCEL PARALLEL FACTOR=auto{2}` and `#pragma ACCEL PARALLEL FACTOR=auto{25}` respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate 'kernel_3mm' as a function for hardware acceleration. For each multiplication block 'E = A * B', 'F = C * D', and 'G = E * F', disable loop pipelining, set tiling to prevent subdivision, and ensure that loop iterations run sequentially. Within the 'E' and 'G' calculations, for the outer 'i' loops, flatten the inner loops for pipelining with no parallelization or tiling, while in the 'F' calculation, the outer 'i' loop defaults to normal pipelining. For the innermost loops, always prevent any parallel loop execution. Specifically, in the 'G = E * F' block, allow for pairs of iterations in the 'j' loop to be executed in parallel. Use appropriate #pragma directives to inform the HLS tool to translate the C code into hardware description language adhering to these constraints.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to interpret the function 'kernel_2mm' as a hardware acceleration kernel, and apply the following pragma directives for loop handling:\n\n1. Do not use automatic pipelining for the outer loops iterating over dimensions 40 and 50, and run their iterations sequentially without parallelization or tiling.\n2. For the first loop nest, completely unroll the innermost loop for the 'k' dimension that accumulates the matrix multiplication of alpha * A * B into 'tmp', also without parallelization, but flatten the outer loops 'i' and 'j' to potentially consolidate them into a single loop level.\n3. In the second loop nest, while iterating over dimensions 40 and 80, again do not use automatic pipelining or parallelization for the outer loops, but tile the second loop 'j' using a factor of 80 to break it down into smaller chunks. Again, do not parallelize the innermost loop for the 'k' dimension that performs the final computation of D := alpha * A * B * C + beta * D, but fully unroll it to maximize computational efficiency.\n\nThese pragmas can be summarized in natural language instructions as follows:\n\n- Treat the 'kernel_2mm' function as a hardware kernel.\n- Disable automatic pipelining and run loops sequentially for the outer loops of both main computation blocks.\n- Flatten the pipeline for the middle loops, where 'flatten' is a command to merge loops, if possible.\n- Unroll the innermost loops completely and ensure no parallel computation or tiling occurs within these loops, except for the second 'j' loop in the second main computation block where tiling is set with a factor of 80.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Implement a three-matrix multiplication kernel, designated as hardware acceleration, where the first two loops should be flat and not pipelined, and the innermost computations within these loops should run sequentially. In the G matrix calculation, allow the j-indexed loop iterations to run in 14-way parallelism, but not the other loops. Use the following pragma directives: for auto-pipelining with flattening, set '#pragma ACCEL PIPELINE auto{flatten}'; for disabling pipeline, use '#pragma ACCEL PIPELINE auto{off}'; for loops that should not be tiled or parallelized beyond single-iteration processing, apply both '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}'. Specifically, in the G matrix computation loop indexed by 'j', use '#pragma ACCEL PARALLEL FACTOR=auto{14}' for controlled parallelism. Keep other loops in their full, original iterations without parallel execution or loop tiling. Indicate beginning and end of scope for optimization with '#pragma scop' and '#pragma endscop' respectively, and mark the function for acceleration with '#pragma ACCEL kernel'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{14}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The instruction to generate the provided HLS C code with the specified #pragma values could be articulated as follows:\n\n\"Create a hardware-accelerated kernel function named 'kernel_3mm' which performs matrix multiplication for three pairs of matrices: E = A * B, F = C * D, and G = E * F. For each of the matrix multiplications within the function, disable automatic pipelining, prevent tiling by maintaining each loop in its original undivided form, and specify the iteration execution schemes. In particular, for the calculation of matrix E, allow a slight degree of two-way parallelism within the innermost loop only. Similarly, disallow parallel execution for the calculation of matrix F. Finally, for the computation of matrix G, enable a higher degree of parallelism, with thirty-two-way parallelism on the innermost loop. Apply no new attributes for the pipelining on the outer loop of matrix G calculation.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a hardware-accelerated 3-matrix multiplication kernel function named `kernel_3mm` with inputs `ni`, `nj`, `nk`, `nl`, `nm`, and matrix parameters `E`, `A`, `B`, `F`, `C`, `D`, `G` resembling the sizes 40x60, 60x50, 50x80, 80x70, and 40x70 respectively. For loop constructs, disable automatic pipelining, enforce that the body of the loops should not be tiled, and ensure all iterations run sequentially. However, within the innermost loops where matrix multiplication occurs, allow for the full parallel execution of operations based on the second matrix dimension. The explicit #pragma instructions you need to use are as follows:\n\n1. Treat the entire function as a dedicated hardware kernel with `#pragma ACCEL kernel`.\n2. To prevent loop pipelining, use `#pragma ACCEL PIPELINE auto{off}` above the first for loop blocking automatic pipelining.\n3. To disable loop tiling, specify `#pragma ACCEL TILE FACTOR=auto{1}` above the first for loop, indicating that the loop shouldn't be broken into tiles.\n4. To enforce non-parallel execution of outer for loop iterations, use `#pragma ACCEL PARALLEL FACTOR=auto{1}` above the first for loop iteration.\n5. Within the innermost loop where matrix multiplications are performed, achieve full parallelism corresponding to the matrix dimension being iterated over: `#pragma ACCEL PARALLEL FACTOR=auto{60}`, `#pragma ACCEL PARALLEL FACTOR=auto{80}`, and `#pragma ACCEL PARALLEL FACTOR=auto{50}` for each of the three multiplication operations respectively. The parallel factor corresponds to the dimension of the second matrix being multiplied.\n\nRemember to start and end the region you wish to accelerate with `#pragma scop` and `#pragma endscop`, if applicable, although these are not included in the provided code example.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel, disable automatic pipelining for all main loops and their nested loops, keep each loop unpartitioned by setting the tile factor to 1, and ensure that the first level nested loops run sequentially by using a parallel factor of 1. Then, for the innermost loop of 'E' matrix multiplication, allow up to 32 iterations to run in parallel, for the 'F' matrix allow up to 20, and lastly, for the 'G' matrix, allow the second level nested loop to run 2 iterations in parallel but parallelize its innermost loop with a parallel factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware kernel for acceleration. For the matrix multiplication operations within this kernel, purposely restrict hardware optimization directives by doing the following: prevent automatic pipelining of all loops, enforce loop execution in the original sequence without tiling or subdividing the loops, and execute all loop iterations in a strictly sequential manner, except for the innermost loop of the 'E := A*B' and the 'G := E*F' computations, where the iterations should be minimally parallelized with a factor of 2 and maximally with a factor of 16, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named `kernel_3mm` designated as a hardware acceleration kernel, which computes the matrix multiplications E=A*B, F=C*D, and G=E*F for the provided matrix dimensions and matrices. For each matrix multiplication, ensure that the overall i-j nested loops do not utilize automatic pipelining, do not tile the loops, and the outer two loops (i and j) are processed sequentially. In the innermost k loop of E=A*B and F=C*D, use parallelism with a factor of 32 while in G=E*F, use a parallel factor of 10 for the innermost k loop and 2 for the j loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code with the following specifications:\n\n1. Treat the function `kernel_3mm` as a hardware acceleration kernel.\n2. For the outer loops that calculate matrix multiplications for E, F, and G, disable automatic pipelining, process each loop iteration in its entirety without tiling, and execute iterations sequentially.\n\n   - Inside the first outer loop for calculating E, enable pipelining with flattening to potentially merge nested loops but maintain the tiling and parallelism settings to process the loop without splitting and sequentially.\n\n   - Inside the second outer loop for calculating F, use the default pipelining strategy, process the loop without tiling, but in the most inner loop, allow parallel processing of 80 iterations, thereby maximizing concurrency.\n\n   - Inside the third outer loop for calculating G, enable pipelining with flattening similarly to E's loop, process the loop without tiling, but set the parallel factor to 2 for the second inner loop to allow slight parallelism, while the most inner loop remains sequential.\n\nIn pragmas:\n\n- Use `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipelining.\n- Use `#pragma ACCEL PIPELINE auto{flatten}` to enable pipelining with the potential to merge nested loops.\n- Use `#pragma ACCEL TILE FACTOR=auto{1}` to prevent loop tiling.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to execute loop iterations sequentially, and `#pragma ACCEL PARALLEL FACTOR=auto{80}` or `#pragma ACCEL PARALLEL FACTOR=auto{2}` to control the parallelism where indicated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize a 2mm kernel function for hardware acceleration where the first level loop is not automatically pipelined, and the inner loops are processed without parallelization or tiling (all with a factor of 1), except for the second-level loop in the second main loop iteration which is both pipelined and tiled to accommodate parallelism across 80 elements. The function operates on matrices with the dimensions given in the arguments, and uses double precision floating-point arithmetic, with specific pragma directives for reduction on the temporary and resulting matrices without automatic optimization, ensuring serial execution of the loop iterations.\n\nTo generate the HLS C code:\n\n1. Designate the function as a hardware kernel using `#pragma ACCEL kernel`.\n2. Inhibit automatic pipelining of the top loop and disable automatic tiling and parallelization by setting the pragma directives `PIPELINE`, `TILE`, and `PARALLEL` to 'off' or a factor of 1, explicitly for the first level loop.\n3. Enable pipelining for the second-level loop within the first main loop iteration without adjusting other parameters.\n4. For the reduction loop within the first main loop, specify a `PARALLEL` pragma directive with a reduction clause on the `tmp` array, maintaining a parallel factor of 1.\n5. Repeat the pragma instructions for the top-level loop for the second main loop, keeping the directives to not automatically pipeline and to process with defaults.\n6. For the second-level loop within the second main loop, set `PIPELINE` to an automatic 'flatten', apply a tiling factor of 80 for `TILE`, and maintain a parallelization factor of 1 for `PARALLEL`.\n7. Use a `PARALLEL` pragma with a reduction clause on the `D` array in the reduction loop of the second main loop iteration, with a parallel factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a matrix multiplication kernel with the following loop optimizations:\n\n1. For the entire kernel_3mm function, create hardware acceleration with the default settings.\n2. Disable automatic pipelining for all three major loop nests calculating matrices E, F, and G.\n3. Process the loops calculating matrices E, F, and G in their full form by disabling automated tiling.\n4. Ensure that iterations within these loop nests run sequentially and are not parallelized automatically.\n5. For the innermost loop of E calculation, explicitly parallelize the operations to match the inner dimension of E's input matrices (A and B) with a parallel factor for the full range.\n6. Within the F and G calculations, let the tool choose the best pipeline strategy for the first two loops but flatten the pipelining in the innermost loop of G calculation.\n7. For the innermost loop of F calculation, set the parallel factor to match the inner dimension of F's input matrices (C and D).\n\nSpecifically, use the following #pragma directives:\n\n- Treat the kernel_3mm function as a hardware acceleration kernel.\n- Use `#pragma ACCEL PIPELINE` with \"off\" to disable automatic pipelining for the major loop nests.\n- Use `#pragma ACCEL TILE FACTOR=1` to avoid loop tiling.\n- Use `#pragma ACCEL PARALLEL FACTOR=1` to prevent automatic parallelization for major loop nests, while within the E and F computation use the specific values 60 and 80, respectively, for the innermost loops.\n- Use `#pragma ACCEL PIPELINE flatten` in the G computation's innermost loop to flatten the pipelining.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the provided C code with HLS pragmas into a natural language instruction as follows:\n\n\"Define a hardware kernel function that performs three matrix-matrix multiplications with the given inputs. For the first multiplication, do not pipeline the outer loop; however, flatten the pipelining in the first inner loop, running iterations in series for both without tiling or parallelization. For the second multiplication, again do not pipeline or tile any of the loops, but fully parallelize the innermost loop to the extent of its bound. For the final multiplication, do not pipeline or tile the outer and first inner loops, double the iterations run in parallel in the first inner loop, and set a parallelization factor of 50 for the innermost loop, effectively splitting it into two parallel chunks.\" \n\nIn the code:\n- Use `#pragma ACCEL PIPELINE auto{off}` to prevent the automatic pipelining of loops.\n- Use `#pragma ACCEL TILE FACTOR=auto{1}` to ensure loops are not tiled.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to run loop iterations serially.\n- Change the parallel factor pragmas to `#pragma ACCEL PARALLEL FACTOR=auto{80}` and `#pragma ACCEL PARALLEL FACTOR=auto{50}` to fully parallelize the innermost loop for the second multiplication and to run two threads for the innermost loop of the last multiplication, respectively.\n- The `#pragma ACCEL PIPELINE auto{flatten}` is used to flatten all nested loops inside the pipelined loop into a single, pipelined loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate 'kernel_2mm' as a hardware acceleration function. Execute the outermost loops without any pipeline optimization and preserve their initial structure by keeping the loop body intact and running iterations sequentially. Within the inner loops, flatten the first pipeline to promote parallelism within its scope while the other pipelines remain unoptimized. For parallel reduction operations on 'tmp' and 'D' arrays, maintain a one-to-one correspondence between iterations and computational elements, except for the last parallel reduction on 'D', which should have a higher degree of parallelism. Apply a tiling factor of two for the second instance of the 'j' loop to divide it into smaller blocks and increase the parallel factor to ten for concurrent execution of its iterations. Finally, massively parallelize the last inner loop by assigning fifty computational elements.\n\nSpecific #pragma instructions:\n\n- Treat 'kernel_2mm' function as a hardware acceleration kernel (#pragma ACCEL kernel).\n- Do not pipeline the first-level 'i' loop (use #pragma ACCEL PIPELINE auto{off}).\n- Keep the first 'i' loop's body together without tiling (use #pragma ACCEL TILE FACTOR=auto{1}).\n- Run the first 'i' loop's iterations sequentially (use #pragma ACCEL PARALLEL FACTOR=auto{1}).\n- Flatten the pipeline in the first instance of the 'j' loop's \"tmp\" calculations for better parallelism (use #pragma ACCEL PIPELINE auto{flatten}).\n- No tiling within the first 'j' loop's \"tmp\" calculations (use #pragma ACCEL TILE FACTOR=auto{1}).\n- Run the 'k' loop's iterations one after another with a single computational element (use #pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}).\n- Do not pipeline the second-level 'i' loop for \"D[i][j]\" updates (use #pragma ACCEL PIPELINE auto{off}).\n- Tile the second instance of the 'j' loop with a factor of two (use #pragma ACCEL TILE FACTOR=auto{2}).\n- Set a parallel factor of ten for the second instance of the 'j' loop (use #pragma ACCEL PARALLEL FACTOR=auto{10}).\n- Heavily parallelize the 'k' loop that updates \"D[i][j]\", using 50 computational elements (use #pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function `kernel_3mm` that computes the product of matrices with specific dimensions. Utilize the following directives: designate the function as a hardware acceleration kernel, avoid pipelining and tiling optimizations in the outer loops, and in the inner loops, selectively disable pipelining while allowing the outer loops to be flattened. For matrix-multiplication, the first multiplication should not parallelize, but the second and third should parallelize with factors of 80 and 50, respectively, for their innermost loops. Let the second-level nested loop of the third multiplication parallelize with a factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to configure the nested loops for matrix multiplication without automatic pipelining, while maintaining the original loop structure and executing iterations sequentially. The outer product loops should have no parallelism, while the innermost loop for the first two matrix multiplications ('E := A*B' and 'F := C*D') should parallelize with a factor of 32, and the last multiplication ('G := E*F') with factors of 2 for the second-level loop and a factor of 10 for the innermost loop. Additionally, the entire function should be designated as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, and within this function, for each matrix multiplication construct (E=A*B, F=C*D, G=E*F), disable the automatic pipelining, omit tiling, and perform the outer two loops of each multiplication sequentially. In the innermost loops, fully parallelize the summation of products for E=A*B using all elements of B, run F=C*D without concurrency, and partially parallelize G=E*F for pairs of elements from E and F with a factor of 50 for E and a factor of 2 for the outer loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For the outer loops iterating over 'i' and for the first instance of the inner loop iterating over 'j', disable automatic pipelining, enforce serial execution without tiling or parallelization by setting the respective pragmas to a factor of 1. During the computation of 'tmp', flatten the pipeline for the second 'i-j' loop and enforce serial execution of the 'k' loop for the reduction operation on 'tmp' with a parallelization factor of 1. In the second major block updating 'D', do not pipeline or tile the loops, while the last inner loop over 'k' updating 'D' should have a parallel reduction with a parallelization factor suggesting, but not necessarily setting it to 10, leaving it to automation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Write a High-Level Synthesis (HLS) C function called `kernel_3mm` which acts as a hardware acceleration kernel. This kernel will compute the matrix multiplications E=A*B, F=C*D, and G=E*F with given dimensions and matrices, where E, A, B, F, C, D, and G are two-dimensional arrays of double-precision floating-point numbers.\n\nFor all loops, do not utilize the automatic pipelining optimization -- instead, execute the loops serially, fully preserving their original sequential nature. Also, ensure that each loop is processed as a single large task, without tiling or splitting it into smaller parallel tasks. Specifically, in the nested loops for calculating E and G, execute their innermost loops with a parallelism factor of 2 and 10 respectively, allowing some degree of simultaneous computation within those loops only, while in the nested loop for calculating F, maintain the original order of execution.\n\nIn HLS pragma syntax, express these conditions using the following:\n- Use `#pragma ACCEL kernel` to indicate that this function is a candidate for acceleration.\n- Use `#pragma ACCEL PIPELINE off` to prevent automatic pipelining optimizations of loops.\n- Use `#pragma ACCEL TILE FACTOR=1` to avoid dividing loops into smaller chunks to be processed in parallel.\n- Use `#pragma ACCEL PARALLEL FACTOR=1` for loops to be processed sequentially, and use `#pragma ACCEL PARALLEL FACTOR=2` and `#pragma ACCEL PARALLEL FACTOR=10` for the specific inner loops that allow some parallel operations within themselves.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the HLS pragma directives into natural language instructions ensuring that specific optimization techniques are applied or disabled for each loop nest within the `kernel_3mm` function. The function multiplies matrices A, B, C, and D to produce matrices E, F, and G. In the first two loop nests, disable automatic pipelining, ensure that loops are not tiled, and that iterations are run serially. However, in the innermost loops of E and F calculation, process 30 and 20 iterations in parallel respectively. Finally, for the G calculation, preserve the original form of the outermost loops, auto pipeline the first nested loop, run two iterations in parallel of the second nested loop, and process 50 iterations in parallel of the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided High-Level Synthesis (HLS) code in C, use the following instruction:\n\n\"Define a kernel function named 'kernel_2mm' that accelerates matrix multiplication with two stages on hardware. The function should take six single-value inputs (ni, nj, nk, nl, alpha, beta) and four 2D array inputs (tmp, A, B, C, and D), with explicit sizes for each array. Within the function, under a 'kernel' pragma to notify hardware acceleration:\n\n1. For the first matrix multiplication stage, disable automatic pipelining and other optimization pragmas for the two outer loops (over indices 'i' and 'j'), effectively leaving these loops unoptimized.\n2. Inside the innermost loop (over index 'k'), apply a parallelization factor of 7 with a reduction on the 'tmp' array. This allows elements of 'tmp' to be computed in parallel, accumulating the results.\n\n3. For the second matrix multiplication stage, perform the same loop configuration for the outer loops as in the first stage, with the pipelining turned off and the loops left at the original granularity.\n4. Inside the innermost loop (over index 'k') of the second stage, apply a parallelization factor of 5 with a reduction on the 'D' array to allow for parallel computation of its elements as well.\n\nIn summary, the two pipelined loops should be suppressed while the other loops should run sequentially, and specify 'off' for the automatic factor of 'PIPELINE' and '1' for the automatic factor of 'TILE' and 'PARALLEL' pragmas. Additionally, adjust parallel factors for reduction operations in the innermost loops to 7 and 5, respectively.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS code for a three-matrix multiplication kernel function, where the first loop E := A*B is entirely serial with no hardware optimization, the second loop F := C*D is pipelined but the innermost loop parallelization is manually set to 5, and the third loop G := E*F is partially parallelized with a parallel factor of 2 for its intermediate loop. Specifically, ensure the outer loops do not pipeline or tile, and the innermost loop of the first two matrices is serial, while the innermost loop of the last multiplication parallelizes the iterations by a factor of 1. Also, for the outer loop of the last multiplication, implement loop flattening to maximize parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS C code for a matrix multiplication kernel function which prevents automatic pipelining, tiling, and parallelization of all loops, except where indicated:\n\n- Declare the function as a hardware acceleration kernel.\n- Initialize each element of matrix `E` to zero, sequentially processing the outer i-j loops without pipelining or parallel execution, and carry out the k loop of E's computation with a doubling of parallel factor.\n- Sequentially compute each element of matrix `F` with outer loops explicitly unflattened and inner loop running without parallelism.\n- Similarly, compute each element of matrix `G`, enabling automatic pipelining for the outer i-j loops and specifying a parallel factor of 32 for the internal k loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to convert the function `kernel_3mm` into a hardware kernel, disable automatic pipelining for the main loops calculating the matrices E, F, and G, process these loops without tiling or further parallel optimization. Then for the innermost loops of E and G, enable parallelism with factors of 60 and 50 respectively, letting them compute multiple iterations concurrently. Specifically, for the innermost loop of matrix E computation, run up to 60 iterations in parallel, and for the innermost loop of matrix G computation, run up to 50 iterations in parallel. For the innermost loop of matrix F, keep the iterations running sequentially without parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code in C, use the following natural language instruction:\n\nDefine a function named `kernel_3mm` that operates as a hardware acceleration kernel, taking six 2D arrays `E`, `A`, `B`, `F`, `C`, `D`, and one result array `G`, with integer size parameters `ni`, `nj`, `nk`, `nl`, and `nm`. Within this function, perform three matrix multiplication operations: the first one between `A` and `B` to compute `E`, the second between `C` and `D` to compute `F`, and the last one between `E` and `F` to compute `G`. Ensure each multiplication loop nest contains three levels of loops: \n\n1. The outer loop iterates over the rows of the resulting matrix.\n2. The middle loop iterates over the columns of the resulting matrix, where each element is initialized to zero.\n3. The inner loop computes the matrix product, summing the product of corresponding elements.\n\nFor each loop nest, configure the pragmas so that the outer and middle loops are not automatically pipelined, are not tiled, and run their iterations in a strictly sequential order without parallelism. For the innermost loop of the first matrix multiplication, allow up to 30 iterations to run in parallel. For the other two matrix multiplications' innermost loops, allow up to 32 iterations to run in parallel.\n\nIn code, this translates to setting specific #pragma directives at the start and before each loop to control the degree of pipeline, tiling, and parallelism with the appropriate #pragma values as follows:\n\n- `#pragma ACCEL kernel` to treat the function as a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipelining for the loop following this pragma.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to indicate not to tile the loop.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to indicate that the iterations of that loop run sequentially.\n- `#pragma ACCEL PARALLEL FACTOR=auto{30}` or `#pragma ACCEL PARALLEL FACTOR=auto{32}` for the innermost loops to allow 30 or 32 iterations to run in parallel, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following HLS directives in C to their respective natural language instructions for generating three matrix-matrix multiplication loops within a hardware-accelerated kernel function:\n\n1. Treat the function 'kernel_3mm' as a specialized hardware kernel for acceleration.\n2. Do not automatically optimize the main loops for pipelining.\n3. Keep the loops in their original complete form without dividing them into smaller parts.\n4. Execute each iteration of the loop sequentially without parallelization.\n\nFor the innermost loop of the first matrix multiplication (calculating matrix E), execute four iterations in parallel.\n\nFor the innermost loop of the second matrix multiplication (calculating matrix F), do not apply any parallelization and flatten the loop nest if possible to potentially optimize memory access.\n\nFor the innermost loop of the third matrix multiplication (calculating matrix G), execute two iterations of the outer loop in parallel and thirty-two iterations of the innermost loop in parallel.\n\nMake sure to cover all loops with appropriate pragmas to indicate that no automatic pipelining, tiling, or straightforward parallelization optimizations should be applied unless explicitly stated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function 'kernel_3mm' for accelerating the computation of the matrix-matrix multiplication of three pairs of matrices (A*B=E, C*D=F, E*F=G) with specific sizes. The pipeline strategy should not be applied automatically to the outer loops but should be flattened for the intermediate loops. Do not tile or parallelize the outer loops, enforce serial execution for the first two pairs' inner loops, and set parallel factors selectively for the last pair, with the innermost loop being highly parallelized. Use the following pragma directives:\n\n1. Treat the entire 'kernel_3mm' function as a hardware acceleration kernel.\n\n2. For the E=A*B computation:\n   - Disable automatic loop pipelining.\n   - Prevent the loop from being tiled.\n   - Do not parallelize the outer loop and set the parallel factor to one for the first and second inner loops.\n\n3. For the F=C*D computation:\n   - Similar to the E matrix computation, disable pipelining, prevent tiling, and enforce serial execution for the loops.\n\n4. For the G=E*F computation:\n   - Disable pipelining for the outer loop, prevent tiling, and set the parallel factor to two for the middle loop.\n   - Highly parallelize the innermost loop with a parallel factor of 50.\n\nEnsure that the pipeline pragma for the middle loops is set to flatten to allow complete unrolling of iterations where beneficial.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated function named `kernel_3mm` to perform three matrix-matrix multiplications (E=A*B, F=C*D, G=E*F) with specific sizes, ensuring that the main function is treated as a hardware acceleration kernel. For the first matrix multiplication loop, prevent the pipeline optimization, do not tile the loop, and ensure iterations occur sequentially without parallelization. For the nested loop calculating values of E, allow automatic loop flattening, but avoid tiling and parallelization. For the second matrix multiplication, apply the same restrictions as the first. In the final matrix multiplication forming G, again prevent pipeline optimizations and tiling at the topmost loop, but allow two iterations to happen in parallel for both the outer and innermost loops. The #pragma directives should reflect these optimizations as follows:\n\n- Treat the `kernel_3mm` function as a hardware acceleration kernel with `#pragma ACCEL kernel`.\n- In the first matrix multiplication (calculating E), use `#pragma ACCEL PIPELINE \"off\"`, `#pragma ACCEL TILE FACTOR=1`, and `#pragma ACCEL PARALLEL FACTOR=1` to prevent pipelining, tiling, and parallelization at the top loop level.\n- For the inner loop calculation of E, allow the loop to be flattened automatically with `#pragma ACCEL PIPELINE auto{flatten}` while keeping tiling and parallelization off using `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Replicate these settings for calculating F, with pipeline optimizations turned off and tiling and parallelization avoided at the top loop level (`#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, `#pragma ACCEL PARALLEL FACTOR=auto{1}`), and for the innermost loop, similar to E, use automatic loop flattening while avoiding tiling and parallelization (`#pragma ACCEL PIPELINE auto{flatten}`, `#pragma ACCEL TILE FACTOR=auto{1}`, `#pragma ACCEL PARALLEL FACTOR=auto{1}`).\n- For the final matrix multiplication forming G, at the top loop level, turn off pipeline optimizations with `#pragma ACCEL PIPELINE auto{off}`, avoid tiling with `#pragma ACCEL TILE FACTOR=auto{1}`, but allow two parallel iterations with `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n- In the nested loop calculating G, allow two parallel iterations with `#pragma ACCEL PARALLEL FACTOR=auto{2}`, while other optimizations are not applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplier kernel with specific loop optimizations. The first two loops should calculate the product of matrices A and B and store in E, without any loop pipelining or tiling, and they should execute in the original sequential order. In the innermost loop of this section, enable full parallelism across the loop iterations for multiplication. Then, similarly calculate the product of matrices C and D and store in F, with the innermost loop fully parallelized. Lastly, calculate the product of E and F, store in G, and for the outermost loop, do not pipeline but flatten the loops. For the innermost loop, allow a parallel factor of two for concurrent execution of two iterations. Use automatic selections for pragma values where indicated by 'auto'. Use 'flatten' only in the pipelining pragma of the final matrix multiplication stage outer loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' for hardware acceleration and apply the following optimizations to its nested loops:\n- For matrix multiplication calculating 'E', disable automatic loop pipelining, maintain original loop iteration structure and prevent parallel loop iteration execution. Within the innermost loop, allow individual operations to occur in series.\n- When computing 'F', disable pipelining for the outer two loops and tile the matrices with a factor of one, signifying no tiling is to occur; however, within the innermost loop, enable the ability to execute 80 iterations concurrently.\n- For the final matrix multiplication yielding 'G', prevent automatic pipelining and keep the loop structure intact. Execute two iterations of the middle loop in parallel and maintain serial order for the innermost loop.\n\nTranslate the above optimizations to HLS #pragma directives as follows:\n- '#pragma ACCEL PIPELINE auto{off}' to prevent loop pipelining.\n- '#pragma ACCEL TILE FACTOR=auto{1}' to maintain the original loop structure without tiling.\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' for serial loop iteration execution, except for the innermost 'F' calculation where '#pragma ACCEL PARALLEL FACTOR=auto{80}' allows parallel execution of 80 iterations.\n- Use '#pragma ACCEL PIPELINE auto{flatten}' on the first loop iterations for 'E' and 'G' if you wish to consider the nested loops as a single pipelined loop without interleaving.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel from a function that performs three matrix multiplications (E=A*B, F=C*D, and G=E*F) with dimensions specified by the input parameters. For each matrix multiplication, execute the outer two loops sequentially without optimization or parallelism, and in the innermost loop of the first two multiplications, allow full parallelization over one matrix dimension. For the final matrix multiplication's innermost loop, execute sequentially.\n\n- Treat the function as a hardware acceleration kernel.\n- Disable automatic pipelining for each of the outer two loops for each matrix multiplication.\n- Prevent the outer two loops from being tiled or split into smaller chunks.\n- For the E and F matrix calculations, fully parallelize the inner loop over the entire second dimension of the A and C matrices, respectively.\n- For the G matrix calculation, keep the original sequential execution order in the innermost loop.\n\nTo elaborate further, here are the pragma statements you should use for each task:\n\n- Treat the following function as a hardware kernel for acceleration:\n  `#pragma ACCEL kernel`\n- Don't automatically pipeline the outer loops:\n  `#pragma ACCEL PIPELINE auto{off}`\n- Don't tile the loops (process them in their original, full form):\n  `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the outer loop iterations one after the other, not in parallel (execute sequentially):\n  `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- For the E and F calculations, allow the k loop to be fully parallelized:\n  `#pragma ACCEL PARALLEL FACTOR=auto{60}` (for E calculation) and\n  `#pragma ACCEL PARALLEL FACTOR=auto{80}` (for F calculation)\n- For the G calculation, disable pipelining for the i loop while still preserving its natural execution order (flatten):\n  `#pragma ACCEL PIPELINE auto{flatten}`\n- For the G calculation, execute the k loop iterations one after the other (sequentially) without parallelizing:\n  `#pragma ACCEL PARALLEL FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration candidate, then process the elements of matrices A, B, C, D, E, F, and G using triple matrix multiplication (E=A*B, F=C*D, G=E*F) with the following constraints: keep the first and second levels of nested loops sequential without enabling pipelining, avoid tiling at the first and second levels, and employ fixed parallelism at the third level of each multiplication\u201432 for the first two multiplications and 50 for the last\u2014except for the G matrix's second loop, where you should reduce the parallelism factor to 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) C function called `kernel_3mm` that accelerates matrix multiplication for three pairs of matrices A*B, C*D, and E*F, where the result of A*B is stored in E, C*D in F, and E*F in G. The function takes in sizes `ni`, `nj`, `nk`, `nl`, and `nm` along with matrix inputs `A`, `B`, `C`, `D`, and outputs `E`, `F`, `G`. Treat the entire function `kernel_3mm` as a hardware kernel.\n\nFor all three nested loops performing the matrix multiplications:\n\n1. Disable loop pipelining.\n2. Prevent loop tiling, ensuring each loop runs in its entirety.\n3. Run loop iterations sequentially, not in parallel.\n\nFor the loop multiplying A and B:\n- Inside the innermost loop, set the number of parallel loop iterations to be equal to the number of columns in B, which is `nk`.\n\nFor the loop multiplying C and D:\n- Inside the innermost loop, set the number of parallel loop iterations to be equal to the number of columns in D, which is `nl`.\n\nFor the loop multiplying E and F:\n- Enable loop pipelining only on the middle loop.\n- Inside the innermost loop, set the number of parallel iterations so that they execute one by one without parallelism.\n\nCorresponding #pragma statements for each step:\n\n1. `#pragma ACCEL PIPELINE auto{off}`\n2. `#pragma ACCEL TILE FACTOR=auto{1}`\n3. `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\nInner loop specific:\n- For A*B multiplication: `#pragma ACCEL PARALLEL FACTOR=auto{60}`\n- For C*D multiplication: `#pragma ACCEL PARALLEL FACTOR=auto{80}`\n- For E*F multiplication:\n  - Middle loop: `#pragma ACCEL PIPELINE auto{flatten}`\n  - Innermost loop: `#pragma ACCEL PARALLEL FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C, the following comprehensive but very concise natural language instruction can be provided:\n\n\"Create a hardware kernel function to compute three matrix multiplications: E=A*B, F=C*D, and G=E*F. For each multiplication operation, do not automatically pipeline the outermost loop. Process the outer and middle loops without tiling or implementing parallelism beyond the default. In the innermost loop of the first multiplication, implement full parallelism according to the size of the B matrix. In the innermost loop of the second multiplication, enable a factor of 5 for parallelism, and in the third multiplication, use a parallelism factor of 2 for both middle and inner loops.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS compiler to designate the function as a hardware kernel, and within the matrix multiplication loops for E=A*B, F=C*D, and G=E*F, set up the following pragmas: disable automatic loop pipelining; set the tiling factor to the original loop size to prevent loop tiling; ensure that the outer and inner loops execute serially, by setting their parallel factors to 1; and specifically in the innermost loops, exploit full parallelism on the k dimension by setting the parallel factor to the size of the respective loop\u201460 for A*B, 80 for C*D, and 50 for E*F.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function to perform three matrix-matrix multiplications (E=A*B, F=C*D, and G=E*F), ensuring that the outer loops are not pipelined or tiled, and run sequentially. Within the E=A*B multiplication, allow tripling of the inner loop's parallelism. In the F=C*D multiplication, enable a parallel factor for the inner loop that matches the size of the dimension being reduced (80), while for G=E*F, set this factor to 8 for the inner loop and double the outer loop's parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` to be synthesized as a hardware kernel. In the matrix multiplication segments within this kernel, set the outer loops to not be automatically pipelined, to keep iterations sequential, and to avoid loop tiling optimization. In the innermost loops of the first and second segments, allow full parallelism across the entire dimension of the inner matrix (60 and 80 respectively) for computation. However, in the final segment, set the parallelism factor to half of the inner matrix's dimension (50), thereby allowing for two iterations to be computed in parallel. Use standard pipelining, no tiling, and sequential iteration execution for all immediate nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use the following directive to configure the matrix multiplication kernel without pipelining or tiling: Declare the function as a hardware acceleration kernel, and for each loop in the matrix multiplications for matrices E, F, and G, disable automatic pipelining, set the tiling factor to 1 to prevent loop tiling, and set the appropriate parallel factor \u2014 use the innermost loop's iteration count as the parallel factor for that loop, except in the final G matrix multiplication, where you set the outermost parallel factor to 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following C code segment into an HLS kernel without any pipeline optimization, keeping loops un-tiled and un-parallelized, except for the 'tmp' and 'D' reduction loops which should be parallelized with a factor of 8 and 1 respectively:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni, int nj, int nk, int nl, double alpha, double beta, double tmp[40][50], double A[40][70], double B[70][50], double C[50][80], double D[40][80]) {\n  int i;\n  int j;\n  int k;\n\n  /* D := alpha*A*B*C + beta*D */\n\n  // Ensure the next loop does not have any pipeline optimization applied\n  #pragma ACCEL PIPELINE off\n\n  // Keep the loop in its full original form, no tiling applied\n  #pragma ACCEL TILE FACTOR=1\n\n  // Do not run the iterations of the loop in parallel\n  #pragma ACCEL PARALLEL FACTOR=1\n  for (i = 0; i < 40; i++) {\n\n    // Likewise, ensure the nested loops are neither pipelined nor parallelized\n    #pragma ACCEL PIPELINE off\n    #pragma ACCEL TILE FACTOR=1\n    #pragma ACCEL PARALLEL FACTOR=1\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n\n      // Parallelize this loop with a factor of 8, keeping it as the only optimization\n      #pragma ACCEL PARALLEL reduction=tmp FACTOR=8\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n\n  // Repeat the off-pipeline, full-loop form and non-parallel pattern for the next segment\n  #pragma ACCEL PIPELINE off\n  #pragma ACCEL TILE FACTOR=1\n  #pragma ACCEL PARALLEL FACTOR=1\n  for (i = 0; i < 40; i++) {\n\n    // Actually, allow loop flattening here for pipeline optimization\n    #pragma ACCEL PIPELINE flatten\n    #pragma ACCEL TILE FACTOR=1\n    #pragma ACCEL PARALLEL FACTOR=1\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n\n      // Keep the parallel factor at 1 for this reduction to maintain sequential order\n      #pragma ACCEL PARALLEL reduction=D FACTOR=1\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function `kernel_3mm` as a hardware acceleration kernel. Inside, prevent automatic pipelining for the three major computation loops calculating matrix products `E = A * B`, `F = C * D`, and `G = E * F`. Ensure that the first two computation loops run without tiling and parallelism, but allow the innermost loop of the `E` matrix multiplication to fully unroll. For the `F` matrix multiplication, also unroll the innermost loop completely. Lastly, in the `G` matrix multiplication, flatten the pipeline for the outer loop, set parallelism of the middle loop to a factor of 2, and do not unroll the innermost loop. Do not modify the loop form or parallelism for the outermost loops other than disabling automatic pipelining.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to consider the provided function as a dedicated hardware acceleration kernel, and then manage three matrix multiplication loops with varying levels of optimization. For the first matrix multiplication, turn off automatic loop pipelining, process the loop in a single chunk, and execute iterations sequentially. In the innermost loop, however, parallelize the calculation fully across its range. For the second matrix multiplication, similarly turn off pipelining, avoid tiling, and enforce sequential execution at the outer loops, but fully parallelize the innermost loop. In the final matrix multiplication, allow automatic pipelining except for the outermost loop, execute the outer loop iterations two at a time in parallel, and for the innermost loop, perform the operations across half of its range in parallel. In code terms, specify the desired behavior with `#pragma ACCEL` directives using appropriate 'PIPELINE', 'TILE', 'PARALLEL FACTOR' values, auto config braces, and 'off' or numbers where applicable.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named 'kernel_3mm' that computes the product of two matrices, disables automatic pipelining for the top-level loops, processes inner loops with the original, full size without tiling except for one specified to fully parallelize over its range, and specifies varying parallel factors for different nested loops, as follows:\n\n- Treat 'kernel_3mm' as a hardware kernel for acceleration.\n- For matrix multiplication E=A*B: \n  - Do not automatically pipeline the outermost i loop. \n  - Do not tile, but process each iteration of the innermost k loop with full parallelism up to the size of the matrix B's first dimension (60).\n- For matrix multiplication F=C*D: \n  - Disable automatic pipelining for both i and j loops.\n  - Use the original loop form without tiling or parallelizing the innermost k loop.\n- For matrix multiplication G=E*F: \n  - Disable automatic pipelining for the outer i loop and process each iteration sequentially.\n  - Double the iterations processed in parallel within the j loop.\n  - Fully parallelize the innermost k loop up to the size of the matrix E's second dimension (50).", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a matrix multiplication kernel without any loop pipelining, keeping the loops in their full form, and treating the loops with the specified parallelism factors.\n\n1. Define the function as a hardware acceleration kernel.\n2. For the first matrix multiplication (E = A * B), disable loop pipelining for both the outer and inner loops, keep the loops in their full form without tiling, and run the innermost loop with a parallelism factor of 8.\n3. For the second matrix multiplication (F = C * D), similarly disable loop pipelining for the outer loop, keep the loops in their full form without tiling, explicitly flatten the pipeline and set the parallelism factor of the innermost loop to 1.\n4. For the third matrix multiplication (G = E * F), disable pipelining for the outer loop again, keep the loops in their full form without tiling, and set the innermost loop's parallelism factor to 32 while setting the middle loop's factor for parallel execution to 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a matrix-matrix multiplication kernel with three separate multiplications\u2014E=A*B, F=C*D, and G=E*F\u2014without using automated pipelining or tiling optimizations, running the outer loops in serial fashion but allowing parallel execution for some inner loops with specified factors. Specifically, apply the following pramga directives:\n\n1. Designate the function as a hardware kernel with `#pragma ACCEL kernel`.\n2. For each outer loop, disable automatic pipelining with `#pragma ACCEL PIPELINE auto{off}`, do not tile the loop with `#pragma ACCEL TILE FACTOR=auto{1}`, and enforce serial loop iteration with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n3. In the multiplication of matrices A and B, parallelize the innermost loop with a factor of 32 using `#pragma ACCEL PARALLEL FACTOR=auto{32}`.\n4. For the multiplication of matrices C and D, flatten the innermost loop with `#pragma ACCEL PIPELINE auto{flatten}` and run it serially (`#pragma ACCEL PARALLEL FACTOR=auto{1}`).\n5. In the multiplication of matrices E and F, allow the second innermost loop to execute two iterations in parallel with `#pragma ACCEL PARALLEL FACTOR=auto{2}` and the innermost loop with a parallel factor of 32 using `#pragma ACCEL PARALLEL FACTOR=auto{32}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a triple matrix multiplication kernel where:\n\n1. The function 'kernel_3mm' is treated as a hardware acceleration kernel.\n2. The outer loops for computing the products of matrices 'A*B', 'C*D', and 'E*F' should not be pipelined.\n3. Within these outer loops, keep the loops in their original form, and run iterations sequentially, not in parallel.\n4. In the innermost loops for 'A*B' matrix multiplication, use default pragmas to calculate elements of matrix 'E'.\n5. For 'C*D' multiplication, allow the innermost loop to parallelize the multiplication to utilize all 80 computing units.\n6. In the 'E*F' calculation, do the innermost multiplication in parallel using half the available computing units for 'C*D', resulting in 50 computing units for parallel processing.\n7. Specify 'flatten' for pipelining in the inner loop of 'A*B' calculation, which collapses nested loops into a single loop for increased parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the entire `kernel_3mm` function as a hardware kernel designated for acceleration, but to execute the main computation loops within the function sequentially, with no automatic pipelining optimization, preserving the full original iteration space without data tiling. Specifically, disable automatic loop pipelining for each loop nest, keep the loop iterations executing in their original order without subdividing them, and only allow the innermost loops to process multiple elements in parallel based on the dimensions of the corresponding matrices involved in the matrix multiplication operations.\n\nFor each loop nest involving the computation of matrix `E`, `F`, and `G`, configure the corresponding #pragma directives as follows:\n\n1. For the `E=A*B` computation:\n   - Disable automatic pipelining for the i and j loops.\n   - Preserve original iteration space for the i and j loops without data tiling.\n   - Process the innermost k loop iterations (dimension 60) in parallel, but execute the outer loops sequentially.\n\n2. For the `F=C*D` computation:\n   - Disable automatic pipelining for the i loop only.\n   - Preserve original iteration space for the i and j loops without data tiling.\n   - Execute the innermost k loop (dimension 80) sequentially without parallel processing.\n\n3. For the `G=E*F` computation:\n   - Disable automatic pipelining for the i loop and execute the j loop sequentially too.\n   - Preserve original iteration space for the i and j loops without data tiling.\n   - Execute the innermost k loop iterations (dimension 50) in twofold parallel processing, while the outer j loop iterations are executed with a parallel factor of two.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a matrix multiplication kernel without applying any automated optimizations such as pipelining or tiling, while treating the function as a hardware kernel for acceleration. Specifically, disable automatic pipelining for every loop level, utilize the original loop iteration space without tiling, and run the outermost iterations sequentially. However, for the innermost loop of the first two matrix multiplications, allow an automatic degree of parallelism to utilize all elements of the second matrix's row or the third matrix's column respectively, and for the final multiplication, specify a parallelism factor of 2 for the outer loop and allow automatic parallelism for accumulating the product over the complete row of matrix E.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to consider the function as a hardware acceleration kernel, do not automatically pipeline the primary loops, and keep the top-level loops unprocessed without tiling or parallelization. However, manually enable pipelining for the first nested loops in each matrix multiplication section. Additionally, explicitly define the factor of loop parallelism for the innermost loops to match the number of iterations over k\u201460 for the first matrix multiplication, 80 for the second, and gradually increase parallelism to a factor of 2 for the outer loop and 50 for the innermost loop in the third matrix multiplication sequence.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the described HLS C code, provide the following instruction:\n\n\"Define a hardware acceleration kernel function called 'kernel_3mm' that performs matrix multiplication for three sets of matrices (A*B, C*D, and E*F) without enabling automatic pipelining, tiling, or parallel factor optimizations for the top-level matrix multiplication loops. Specifically, do not pipeline or tile any of the loops and run each iteration sequentially in the two outer loops. However, for the innermost multiplication loops, manually set a parallel factor to allow for 32-way parallel processing in the A*B and C*D computations, but for the E*F computation use a 4-way parallel processing at the outer loop and an 8-way parallel processing at the inner loop. These parallel factors should speed up the inner computations. Ensure you only apply these settings within their relevant loop nests using the appropriate pragma directives.\" \n\nThis instruction outlines the creation of the hardware acceleration function and the configuration of optimization pragmas that control the behavior of pipelining, tiling, and parallel processing within the function loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for matrix multiplication where three matrix multiplication operations are performed sequentially: first, matrix A multiplies matrix B to produce matrix E; then matrix C multiplies matrix D to produce matrix F; and finally, matrix E multiplies matrix F to produce matrix G. \n\nFor each matrix multiplication, initialize the product matrix to zero before performing the actual multiplication. Treat the entire function as a hardware kernel for acceleration. Do not pipeline the outer loop of each matrix multiplication to maintain its original structure, and prevent the loop from being divided into smaller chunks. \n\nWithin the outer loop, do pipeline the nested loops automatically, except for the last multiplication where you should fully unroll the innermost loop while keeping the first loop level parallel factor at 2 for partial parallelization. Set all other parallel factors to the maximum to fully unroll corresponding loops when it is beneficial.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function 'kernel_3mm' that performs three matrix multiplications (E=A*B, F=C*D, G=E*F) on matrices with predefined dimensions, ensuring that the outer loops for each multiplication are not pipelined, unrolled or tiled, while allowing different degrees of parallelism for innermost loops: 32-way for the first and second multiplications, and 10-way for the third multiplication's innermost loop with a 4-way parallelized second inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, but ensure that all loops within the function are not automatically pipelined. Instead, execute each loop iteration sequentially without subdivision or parallel processing, except for specific inner loop levels where parallelism is partially allowed. For the matrix multiplication of E and F, allow minimal parallelism with a factor of 3 and 8 respectively, while for the multiplication of C and D, significantly increase parallelism with a factor of 80. Use the following directives for each loop as marked in the code:\n\n1. \"`#pragma ACCEL kernel`\" at the beginning of the function to define it as a hardware acceleration kernel.\n2. \"`#pragma ACCEL PIPELINE off`\" before each loop to prevent automatic loop pipelining.\n3. \"`#pragma ACCEL TILE FACTOR=1`\" before each loop to execute the loop in its original form without tiling.\n4. \"`#pragma ACCEL PARALLEL FACTOR=1`\" before most loops to perform iterations sequentially.\n5. For the first matrix multiplication's innermost loop (E=A*B), allow a small level of parallelism with \"`#pragma ACCEL PARALLEL FACTOR=3`\".\n6. For the second matrix multiplication's innermost loop (F=C*D), increase the parallelism significantly with \"`#pragma ACCEL PARALLEL FACTOR=80`\".\n7. For the final matrix multiplication's innermost loop (G=E*F), use a moderate level of parallelism with \"`#pragma ACCEL PARALLEL FACTOR=8`\".\n\nAdditionally, the auto modifier with braces (`{}`) is used to manually override the pragma's default behavior, such as in \"`auto{off}`\" and \"`auto{1}`\", indicating specific values for pipeline, tile, and parallel factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Produce a code for a matrix multiplication kernel that treats the entire function as a hardware acceleration kernel, for the loops within the multiplication sections ensure that they are not automatically pipelined, are executed without tiling, and have the outer two levels of loops run sequentially with the exception of the innermost loop of the third multiplication section which runs 4 iterations in parallel. The first two multiplication sections should have their innermost loop running all iterations in parallel, but with the outer loops still running sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration kernel, prevent the automatic pipelining of matrix multiplications, maintain the loops' original sequential order without dividing them into smaller parallel chunks, but allow for some limited parallelism within certain nested loops.\n\nSpecifically:\n\n- Treat `kernel_3mm` as a hardware acceleration kernel.\n- For the outermost loops of all three matrix multiplications (E := A * B, F := C * D, G := E * F):\n  - Turn off automatic loop pipelining.\n  - Process these loops entirely in their original form, without tiling.\n  - Run each loop iteration sequentially, not in parallel.\n\n- In the middle loop of the first matrix multiplication (E := A * B):\n  - Allow for automatic loop pipelining.\n  - Process the loop in its original form without tiling.\n  - Run each loop iteration sequentially, not in parallel.\n\n- In the innermost loop of the second matrix multiplication (F := C * D):\n  - Enable limited parallelism, allowing up to 32 operations to be executed in parallel.\n\n- In the middle loop of the third matrix multiplication (G := E * F):\n  - Turn off loop pipelining.\n  - Process the loop in its original form without tiling.\n  - Enable limited parallelism, allowing up to 2 operations to be executed in parallel.\n\n- In the innermost loop of the third matrix multiplication (G := E * F):\n  - Allow for significant parallelism, with up to 25 operations running concurrently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code in C, with the correct #pragma directives, instruct the code generator to:\n\n\"Define a function called 'kernel_3mm' that will act as a hardware acceleration kernel, operating on the matrices 'E', 'A', 'B', 'F', 'C', 'D', and 'G' with dimensions given by the provided sizes ni, nj, nk, nl, nm. For each matrix multiplication operation involved in computing 'E = A * B', 'F = C * D', and 'G = E * F', disable automatic pipelining, set tile factors to 1 making the compiler not break the loops into smaller chunks, and enforce parallelism execution to occur at the original loop sequential order, except within the innermost loops where you will set a specific parallel factor of 30 for 'A * B', 32 for 'C * D', and 50 for 'E * F'. Additionally, for outer loops of 'G = E * F', allow the compiler to decide whether to apply automatic pipelining and set a parallel factor of 2 for improved resource utilization or reduced resource usage, though careful to not cause data dependencies that block parallel operation.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware kernel, process all top-level loops respecting their original, sequential order, and allow the innermost loops to exploit parallelism with specific intensity. Specifically:\n\n1. Mark `kernel_2mm` as a hardware kernel to accelerate, translating it into a hardware description.\n2. Within the first two levels of nested loops for initialization of `tmp`, allow the compiler to decide pipeline optimization, maintain the loop forms without subdivision, and execute iterations sequentially.\n3. For the innermost loop calculating `tmp`, enable parallel execution with a factor hint of 16 and apply a reduction clause for the `tmp` array.\n4. In the following nested loops updating `D`, permit the compiler to choose whether to flatten pipeline stages within the first level, keep the loop without tiling, and run the second loop with a parallel factor hint of 10.\n5. For the innermost loop updating `D`, perform updates sequentially and enforce a reduction on the `D` array without parallel enhancements.\n\nIn pragma terms, this translates to:\n- Treat `kernel_2mm` as a hardware kernel (`#pragma ACCEL kernel`).\n- For the first set of nested loops, prevent automatic loop pipelining (`#pragma ACCEL PIPELINE auto{off}`), disable loop tiling (`#pragma ACCEL TILE FACTOR=auto{1}`), and maintain sequential execution within each loop (`#pragma ACCEL PARALLEL FACTOR=auto{1}`).\n- Inside the loop for calculating `tmp`, enable some parallel execution with a factor of 16 while applying a reduction on the `tmp` array (`#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}`).\n- In the second set of nested loops, allow the compiler to decide pipeline strategy while maintaining tiling disabled and parallel execution with a factor of 10 inside the first level (`#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL TILE FACTOR=auto{1}`, `#pragma ACCEL PARALLEL FACTOR=auto{10}`).\n- Finally, enforce sequential execution in the innermost loop while enabling a reduction on the `D` array (`#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function named `kernel_3mm` for matrix multiplication of different dimensions, disable automatic pipeline optimization for all outer loops and do not tile them, ensuring these run in their entire form without splitting, with all iterations executed sequentially. Within the nested loops for initializing matrices E and F, disable all forms of parallelism and pipelining as well. However, in the computation of E, enable parallel execution across the maximum extent of the innermost loop k (B's column dimension). Similarly, in the computation of F, fully parallelize the innermost loop k (D's column dimension). In contrast, in the last segment for calculating G, opt to flatten the j-loop for improved pipelining, while allowing a parallel execution with a factor of 2, and run the innermost loop k sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a three-matrix multiplication kernel that does the following: Designate the function as a hardware acceleration kernel, prevent automatic pipelining, ensure loops aren't split into tiles, and process loop iterations sequentially. Specifically, disable pipelining the outermost loops and apply a flattened pipeline to the second level of loops. For the final matrix multiplication, use some parallelism with a factor of 5 in the second-level loop and a factor of 4 in the innermost loop without specifying the pipeline strategy in the outermost loop for this segment.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function 'kernel_2mm' to perform a modified 2-matrix multiplication with double precision floating-point arithmetic on input matrices A, B, C, and D of fixed sizes, scaling the result of A*B by alpha and adding it to the scaled original matrix D by beta. Prevent any automatic pipelining optimization in the outermost loops. Within the inner loops, flatten the first inner loop's pipeline, but keep the tiling and parallelism factors as default. For the second inner loop, set parallel execution with a specific factor for computing the reduction of tmp, while keeping the tiling factor as the default. For the final innermost loop, activate parallel execution with an aggressive factor for computing the reduction of D, which significantly increases the level of parallelism compared to the outer loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Direct the HLS tool to:\n- Consider the `kernel_3mm` function as a candidate for hardware acceleration.\n- In the first major loop, compute the matrix multiplication of E = A * B without enabling automatic loop pipelining, tiling, or parallel processing enhancements.\n- Inside the innermost loop where E is calculated, fully unroll the loop by processing all iterations in parallel, subject to resource availability.\n- In the second major loop, calculate the matrix multiplication of F = C * D without automatic pipelining but allow the tool to decide the best way to flatten pipelining for potential optimization.\n- In the innermost loop where F is calculated, run the iterations sequentially without parallelization.\n- In the third major loop, compute the matrix multiplication of G = E * F without automatic pipelining or tiling, setting the parallel factor to run two iterations concurrently if resources permit.\n- Finally, in the innermost loop where G is calculated, unroll the loop to process 50 iterations in parallel, consonant with hardware resource constraints.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a 3-matrix multiplication kernel function that defines hardware acceleration for three resultant matrices E, F, and G, where E results from multiplying matrices A and B, F from C and D, and G from E and F. For the multiplication loops that result in E and F, pipeline optimizations should be turned off, each loop should not be tiled or parallelized beyond the basic level. However, for calculating E specifically, enable full parallelization for the innermost summation loop. For F, also keep the innermost loop in the default sequential operation. In the final multiplication generating G, activate loop flattening for the outer loop and set a moderate level of parallelization while leaving the summation loop sequential. Apply these instructions:\n\n1. Mark the function as a hardware kernel for acceleration.\n2. For each matrix multiplication (E=F, F=C*D, G=E*F):\n   - Disable automatic pipeline optimizations for the main loops.\n   - Set the loop tiling factor to 1, indicating no loop tiling should occur.\n   - Set parallel factor to 1 for all main loops to ensure sequential execution.\n3. Within the loop for calculating E, set a parallel factor that takes advantage of the highest possible parallelization for the summation loop.\n4. For the outer loop that calculates G, apply loop flattening, and set a parallel factor of 4 to allow some parallel execution, while the summation loop remains sequential.\n\nSpecify these settings using the proper `#pragma ACCEL` annotations before the corresponding loops in the C code with `\"auto{value}\"` where `value` corresponds to the desired behavior, like `\"off\"` for pipeline disabling or the respective numerical factor for tiling or parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS-accelerated kernel function `kernel_3mm` performing 3 matrix-multiplication operations (`E=A*B`, `F=C*D`, `G=E*F`) with specified matrix dimensions, ensuring the following conditions:\n\n1. The entire function should be treated as a hardware acceleration kernel.\n2. For the computation of matrix `E`, disable automatic pipelining of the outermost loop, execute it in its original full form, and process iterations sequentially.\n3. Within that loop, flatten the next inner loop pipeline, also with no tiling and sequential processing of iterations.\n4. For the innermost loop that calculates the `E[i][j]` elements, enforce sequential iteration processing.\n5. Replicate this loop behavior structure for the computation of matrix `F`.\n6. When computing matrix `G`, disable pipelining for the outermost loop, again use a full form execution and process iterations for the `i` loop in pairs (parallelize with a factor of 2).\n7. Within the innermost loop that calculates the `G[i][j]` elements, distribute the iterations into groups of four (parallelize with a factor of 4).\n\nSpecifically, use the following pragma directives:\n- `#pragma ACCEL kernel`: Treat the `kernel_3mm` function as a hardware kernel.\n- `#pragma ACCEL PIPELINE auto{off}`: Deactivate automatic pipelining for loop execution.\n- `#pragma ACCEL PIPELINE auto{flatten}`: Flatten the pipeline for the respective loop, enhancing parallelism potential within a single iteration space.\n- `#pragma ACCEL TILE FACTOR=auto{1}`: Avoid loop tiling and keep loops in full original size.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`: For sequential iteration execution within a loop.\n- `#pragma ACCEL PARALLEL FACTOR=auto{2}`: Parallelize iterations with a factor of 2 for the loop that calculates the `G[i][j]` elements.\n- `#pragma ACCEL PARALLEL FACTOR=auto{4}`: Parallelize iterations with a factor of 4 for the innermost loop when computing matrix `G`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Initiate a hardware accelerated kernel function `kernel_2mm` for a matrix-multiplication and addition operation with given parameters. Ensure the outermost loop is not automatically pipelined, maintain its original form, and execute iterations sequentially. Within the first nested loop, enable full loop flattening, keep it in its original tiled form, and permit parallel execution of two iterations where the innermost loop accumulates result without parallelisation. In the second major loop block, prevent pipelining, tile the loop with a factor of 8, and process 5 iterations in parallel with the innermost loop drastically parallelised by a factor of 50 to handle reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration candidate, and within it for the three matrix multiplication sections:\n\n1. Inhibit pipelining and perform the 'E := A*B' loop iterations in a standard serial order without any loop transformations.\n2. For the 'F := C*D' section, maintain the original loop form but allow innermost parallel execution with a factor matching the dimension of 'D[k][j]'.\n3. Lastly, for 'G := E*F', maintain the natural loop form, allow automatic pipelining of the outermost loop, and permit the second inner loop to process two iterations in parallel.\n\nFor each loop nest, ensure that:\n- Pipelining is either turned off or allowed to be determined by the compiler's auto directive.\n- Loop tiling factors are set to 1 to preserve the original loop bounds.\n- Parallelization factors match the specific number provided or are left to be determined by compiler auto directives.\n\nSpecifically, use the following #pragma instructions:\n- `#pragma ACCEL kernel` to indicate 'kernel_3mm' function as an acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` to prevent automatic pipelining.\n- `#pragma ACCEL PIPELINE auto{flatten}` to enable flattening of the pipeline when beneficial.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to keep loop bounds unaltered.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` for serial execution of loop iterations.\n- `#pragma ACCEL PARALLEL FACTOR=auto{80}` to execute 80 loop iterations in parallel for the 'C*D' computation.\n- `#pragma ACCEL PARALLEL FACTOR=auto{2}` to execute two loop iterations in parallel for the 'E*F' computation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function that performs three matrix-matrix multiplications sequentially. Specifically, disable automatic pipelining for the top-level loops of each multiplication, forcing them to run in the default unoptimized manner. For the second level of loops inside each multiplication, enable pipelining without specifying optimization parameters but restrict tiling and parallelism to their minimum, essentially keeping the loop structures intact. Inside the innermost loop calculating `E`, fully parallelize across all elements of `B`, while inside the innermost loop calculating `F`, use a parallelism factor of 16 for elements of `D`, and finally, for computation of `G`, proceed with minimal parallelism but flatten the pipeline of the enclosing loop to improve the design's resource usage.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the function 'kernel_3mm' as a hardware kernel without pipelining, tiling, or parallelizing any of the loops within the kernel, with the exceptions of fully parallelizing the innermost loops 'k' in the first and third computation blocks, and flattening the pipeline for the 'j' loop in the second computation block.\n\nSpecifically:\n\n- Declare 'kernel_3mm' as a kernel for acceleration.\n- For the first computation block calculating 'E := A*B':\n  - Disable pipelining for the 'i' loop.\n  - Process the 'i' loop without tiling.\n  - Run iterations of the 'i' loop sequentially (without parallel execution).\n  - Inside the 'i' loop, also disable pipelining for the 'j' loop.\n  - Process the 'j' loop without tiling.\n  - Run iterations of the 'j' loop sequentially.\n  - Fully parallelize the 'k' loop (factor 60) within the 'j' loop to compute each element of 'E'.\n\n- For the second computation block calculating 'F := C*D':\n  - Disable pipelining for the 'i' loop.\n  - Process the 'i' loop without tiling.\n  - Run iterations of the 'i' loop sequentially.\n  - Flatten the pipelining for the 'j' loop within the 'i' loop.\n  - Process the 'j' loop without tiling.\n  - Run iterations of the 'j' loop sequentially.\n  - Do not parallelize the 'k' loop (factor 1) while calculating 'F'.\n\n- For the third computation block calculating 'G := E*F':\n  - Disable pipelining for the 'i' loop.\n  - Process the 'i' loop without tiling.\n  - Run iterations of the 'i' loop with a parallel factor of 2.\n  - Enable pipelining for the 'j' loop within the 'i' loop by setting the auto directive to its default behavior (but without specifying \"off\").\n  - Process the 'j' loop without tiling.\n  - Fully parallelize the 'k' loop (factor 50) within the 'j' loop to compute each element of 'G'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS codes for a three-matrix multiplication kernel function that treats the function as a hardware acceleration kernel, with the outer loops of each matrix multiplication not pipelined, and every loop iteration processed in a non-parallel, unchunked manner. However, automatically determine if loop flattening should be applied to the first matrix multiplication and apply loop flattening and a parallel factor of 4 to the third matrix multiplication's inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the entire `kernel_2mm` function as a hardware acceleration kernel, disable automatic pipelining for the outermost loops of calculating `tmp` and `D`, maintain the original loop structures for the entirety of the function by setting the tile and parallel factors to their most basic configurations (factor=1), except specifically double the parallelization factor for the innermost `j` loop within the `tmp` calculation and quintuple it within the `D` calculation, while also allowing for the reduction optimization across the innermost `k` loops, and finally, flatten the pipelining of the `D` matrix multiplication to aggressively optimize its parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS accelerated kernel function called `kernel_3mm` that carries out a series of matrix multiplications for matrices E=A*B, F=C*D, and G=E*F, with sizes specified by the parameters `ni`, `nj`, `nk`, `nl`, `nm`. For the outer loops calculating E, F, and G matrices, disable automatic pipelining, ensure these loops run in their full form without tiling, and execute iterations sequentially. For the second loop in E matrix computation, enable the flattening of the loop hierarchy for pipelining, while keeping loop tiling disabled and iterations running sequentially. Specifically, for k loops computing E and G matrices, keep the parallel factor to execute iterations sequentially, and for the k loop calculating the F matrix, set the parallel factor to execute two iterations in parallel. Use the following #pragma directives: \n\n- Treat `kernel_3mm` as a hardware kernel with `#pragma ACCEL kernel`.\n- Disable loop pipelining with `#pragma ACCEL PIPELINE auto{off}`.\n- Ensure that loops run in their original form without tiling with `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Ensure that loop iterations run sequentially with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Enable loop flattening for pipelining with `#pragma ACCEL PIPELINE auto{flatten}`.\n- For increased parallelism in the F matrix calculation, use `#pragma ACCEL PARALLEL FACTOR=auto{2}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the `kernel_3mm` function without automatically optimizing the top-level loops, so they remain as full loops, and process the iterations successively without tiling or parallel execution enhancements. Inside the nested loops for matrix multiplication, leave the outer loops unoptimized, but allow for automatic decisions on optimizations for the innermost loops. Specifically:\n\n1. Treat the `kernel_3mm` function as a hardware kernel to be accelerated.\n2. For the top-level loop sections performing matrix multiplications ('E := A*B', 'F := C*D', 'G := E*F'), do not apply automatic pipelining (#pragma ACCEL PIPELINE \"off\"), do not tile the loops (#pragma ACCEL TILE FACTOR=auto{1}), and run the iterations successively (#pragma ACCEL PARALLEL FACTOR=auto{1}).\n3. Within the first multiplication 'E := A*B':\n   - At the second level of nesting, do not pipeline the loop, do not tile, and run iterations successively.\n   - At the third level of nesting (innermost loop calculating the dot product), allow automatic decision on parallelization factor.\n4. Apply the same settings for the second multiplication 'F := C*D'.\n5. For the third multiplication 'G := E*F':\n   - At the second level of nesting, do not pipeline the loop, do not tile, and apply a parallelization factor of 5.\n   - At the third level of nesting, allow automatic decision on parallelization factor.\n\nPlease note that automatic decisions for tiling and parallelization are context and implementation specific. Hence, the 'auto' keyword in '#pragma ACCEL PIPELINE auto{flatten}' implies the tool should decide when to flatten loop nests for pipelining. Similarly, 'auto{1}' means the tool decides whether to apply a parallelization or tiling factor of 1 or to choose an automatically determined factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a C function for high-level synthesis that treats the entire function as a hardware kernel and explicitly disables automatic pipelining for most processing loops, but selectively enables different parallel factors or keeps them in fully serial mode. Specifically, for the 'E := A*B' calculation, prevent pipelining and parallelism for the outer loops, but allow a parallel processing factor of 12 for the innermost loop. For the 'F := C*D' calculation, allow the default pipelining for the outer loop while preventing any pipelining for the inner loop and set a parallel processing factor of 5 for the innermost loop. For the final 'G := E*F' calculation, flatten the pipelining of the outer loop, strictly serialize the middle loop, and allow the parallel processing factor of 4 for the outer loop while keeping the innermost loop serially processed. Remember to initialize output matrix elements to zero before accumulation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS compiler to treat the entire function `kernel_3mm` as a standalone hardware acceleration kernel. Within this function, disable automatic pipelining for the two outer loops computing matrix multiplication for `E := A*B` and `F := C*D`, and set their tiling and parallel factors to their minimal values, ensuring these loops run sequentially without optimizations. However, explicitly parallelize the innermost loop of the `E` matrix calculations across the full width of the `B` matrix (60). Then, repeat the sequential operation for the `F` matrix computation outer loops, but parallelize its innermost loop over the complete width of the `D` matrix (80). Finally, for computing `G := E*F`, parallelize the execution of the outer loop's iterations four at a time, maintain a flat structure for this outer loop by applying pipeline flattening, and run the innermost loop sequentially without parallelization.\n\nSpecific pragma instructions:\n\n1. Treat the `kernel_3mm` function as a hardware kernel: `#pragma ACCEL kernel`\n2. For matrix `E`:\n   - Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n   - Set tiling factor to 1: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Set parallel factor to 1: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n   - Parallelize the innermost loop over the full width of matrix `B`: `#pragma ACCEL PARALLEL FACTOR=auto{60}`\n3. For matrix `F`:\n   - Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n   - Set tiling factor to 1: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Set parallel factor to 1: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n   - Parallelize the innermost loop over the full width of matrix `D`: `#pragma ACCEL PARALLEL FACTOR=auto{80}`\n4. For matrix `G`:\n   - Disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n   - Flatten the pipeline: `#pragma ACCEL PIPELINE auto{flatten}`\n   - Set tiling factor to 1: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Parallelize the outer loop in chunks of four: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n   - Run the innermost loop without parallelism: `#pragma ACCEL PARALLEL FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration kernel and perform three matrix multiplications, `E=A*B`, `F=C*D`, and `G=E*F`. For the outer loops of each multiplication, disable automatic pipelining, do not tile the loops, and execute iterations serially. In the innermost loops for calculating `E` and `F`, enable full parallelization over the `k` index with the complete dimension of the second matrix (`60` for `E` and `80` for `F`). In computing `G`, partially parallelize the innermost loop with a factor of `4` for the outer loop and fully parallelize the calculation over the `k` index with a factor equivalent to the size of `E`'s second dimension (`50`). \n\nUse these specific #pragma values respectively:\n\n- Treat the function as a hardware acceleration kernel: `#pragma ACCEL kernel`\n- Disable automatic pipelining for outer loops of `E`, `F`, and `G`: `#pragma ACCEL PIPELINE auto{off}`\n- Prevent tiling of all loops: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Execute outer loop iterations serially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Full parallelization over `k` index for `E` and `F`: `#pragma ACCEL PARALLEL FACTOR=auto{60}` for `E` and `#pragma ACCEL PARALLEL FACTOR=auto{80}` for `F`\n- Partial parallelization of outer loop and full parallelization of inner loop for `G`: `#pragma ACCEL PARALLEL FACTOR=auto{4}` for the outer loop and `#pragma ACCEL PARALLEL FACTOR=auto{50}` for the inner loop", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive but concise instruction to generate the provided HLS codes written in C, including the described pragma directives:\n\n\"Define a hardware kernel function 'kernel_3mm' for matrix-matrix multiplications with three matrix pairs A*B=E, C*D=F, and E*F=G, using square matrices of provided dimensions, without automatic pipelining for the top-level matrix multiplication loops. For the innermost k loops, enable parallelism by running 60, 16, and a single iteration concurrently for each respective matrix multiplication while keeping the outer loops sequential. Flatten the pipeline structure of the G matrix computation's i loop to ensure all its stages are combined. For all loops, avoid loop tiling and ensure that loops run sequentially unless specified otherwise.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function named 'kernel_3mm' for performing matrix multiplication with specific dimensions, where the loops for computing matrix E=A*B, F=C*D and G=E*F should not be automatically pipelined. The computation within the outermost loops should be kept in its entirety, with no tiling or parallel execution enhancements. However, in the innermost computation of matrix E and F, flatten the loops with no specific directives for parallel processing or loop tiling. When computing matrix G, explicitly set the parallel execution factor of the second loop to 7 while keeping all other pragma settings to prevent modification of pipelining, tiling, and parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following C function into high-level synthesis (HLS) code with directives to optimize for hardware acceleration:\n\n1. Designate the function `kernel_2mm` as a hardware acceleration kernel.\n2. In the outermost loop block, do not pipeline the loop; execute the loop iterations sequentially and do not partition the loop into smaller chunks or run iterations in parallel.\n3. In the inner loop where the `tmp` array is calculated:\n    - Allow the HLS tool to automatically decide if it should pipeline the loop.\n    - Do not partition the loop but use a parallel factor of 2 to allow some degree of parallel execution for the loop iterations.\n    - Apply a parallel reduction to the innermost loop with a parallel factor of 32 for the `tmp` array calculation to increase its computational parallelism.\n4. In the outer loop block for the `D` array calculation:\n    - Auto-decide on the pipelining of the outer loop.\n    - Set a tiling factor of 1 for the loop manually, ensuring it is processed as one complete entity.\n    - Do not set any parallel factor for the outermost loop iterations, implying sequential execution.\n5. For the inner loop updating the `D` array:\n    - Prevent automatic pipelining of the loop.\n    - Manually set a tiling factor of 8 to break down the loop into smaller chunks of size 8.\n    - Apply a parallel factor of 5 to enable concurrent execution of loop iterations.\n    - Finally, apply a parallel reduction to the innermost loop with a parallel factor of 16 for the `D` array calculation to enhance parallelism.\n\nUse appropriate #pragma directives before the loop constructs to reflect these instructions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the `kernel_2mm` function as a hardware acceleration kernel. For the first level `i` loop, do not pipeline the iterations, execute them in their original form without tiling, and run the iterations sequentially. Inside the nested `j` loop, also do not pipeline and do not use tiling, but allow two iterations to be executed in parallel. For the deepest nested `k` loop, perform parallel operations with a reduction on `tmp` without limiting the factor. For the second block starting with the `i` loop, automatically determine if pipelining is beneficial, do not break the loop into smaller chunks, and execute iterations sequentially. Within the nested `j` loop, flatten the loop iterations for pipelining, tile with a factor of 8, and allow five iterations to run in parallel. Finally, in the innermost `k` loop, only allow sequential execution with a reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) code in C for a kernel function named `kernel_3mm` which performs matrix multiplications for three pairs of matrices: A*B = E, C*D = F, and E*F = G. Ensure that the function accepts dimensions ni, nj, nk, nl, nm, and matrix arrays E, A, B, F, C, D, G as its parameters. For all outer and middle loops, disable automatic pipelining, setting the TILE and PARALLEL pragmas to a factor of 1 to prevent the loop from being split or executed in parallel. However, for the innermost loop that calculates the matrix multiplication, allow a PARALLEL pragma factor of 32 for E and F, and use different factors of 5 and 10 respectively for the G calculation's middle and inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS C code for a 3-matrix multiplication kernel where the computation of matrix products E=A*B, F=C*D, and G=E*F is performed in a non-pipelined, non-tiled, and fully serial manner except the last product G=E*F, where the outer loop runs with limited parallelism of factor 5 and the innermost loop with parallelism of factor 8. Disallow automatic pipeline optimizations, maintain the original full loop form for the outer and middle loops, and ensure the inner loops are not processed in parallel for the first two products E and F. The last outer loop for G's computation should also prevent pipelining, and its inner loops should have specified parallel factors. Treat the entire function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with specific pragma instructions, use the following natural language instruction:\n\n\"Designate the enclosed function as a dedicated hardware acceleration kernel. For the main matrix multiplication loops calculate the result without pipelining, tiling, or altering the loop parallelism, but compute the innermost loop of the first multiplication (E=A*B) with full parallelism over the 'k' index. For the second multiplication (F=C*D), allow flattening of loops during pipelining, and keep the loop in its original form without altering the parallelism or tiling. For the final multiplication (G=E*F), employ double parallelism for the 'j' index loop and full parallelism over the 'k' index. Maintain the original structure for the rest of the loops without optimization directives.\"\n\nTo translate this into code with correct #pragma values:\n\n1. `#pragma ACCEL kernel` indicates that the function is a hardware acceleration kernel.\n2. `#pragma ACCEL PIPELINE auto{off}` specifies not to pipeline the corresponding loop.\n3. `#pragma ACCEL TILE FACTOR=auto{1}` ensures the loop is not divided into smaller chunks (no tiling).\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` indicates that the loop iterations should run one after the other.\n5. `#pragma ACCEL PARALLEL FACTOR=auto{60}` (or the maximum number of iterations for the 'k' loop in the first multiplication) enables full parallel processing of the 'k' index.\n6. `#pragma ACCEL PIPELINE auto{flatten}` for the second multiplication's 'i' loop allows loop flattening when pipelining.\n7. `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the 'j' loop in the final multiplication specifies that iterations should be processed in pairs.\n8. `#pragma ACCEL PARALLEL FACTOR=auto{50}` for the 'k' loop in the final multiplication sets the loop to run with full parallelism over the 'k' index.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware acceleration kernel, and for each of the three nested loop computations within the function (`E := A*B`, `F := C*D`, and `G := E*F`), apply the following directives:\n\n1. For the computation `E := A*B`:\n    - Disable automatic pipelining for the outermost loop iterating over variable `i`.\n    - Preserve the full structure of the outermost loop without tiling.\n    - Execute iterations of the outermost loop sequentially, without parallelism.\n    - Within the loop over variable `j`, enable full loop flattening for pipelining optimization.\n    - For the innermost loop iterating over variable `k`, disable automatic tiling and parallelism, ensuring sequential execution for this loop level.\n\n2. For the computation `F := C*D`:\n    - Disable automatic pipelining for the outermost loop iterating over variable `i`.\n    - Preserve the full structure of the outermost loop without tiling.\n    - Execute iterations of the outermost loop sequentially, without parallelism.\n    - Enable automatic pipelining for the middle loop over variable `j`, without specifying the exact pipelining strategy.\n    - For the innermost loop iterating over variable `k`, disable tiling and allow maximum exploitation of parallelism by parallelizing all 80 iterations.\n\n3. For the computation `G := E*F`:\n    - Disable automatic pipelining for both the outermost loop iterating over variable `i` and the middle loop over variable `j`.\n    - Preserve the full structure of both loops without tiling.\n    - For the outer loop, execute iterations in pairs, allowing two iterations to be processed in parallel.\n    - For the innermost loop iterating over variable `k`, aim for a high level of parallelism by processing 50 iterations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function to be synthesized into a dedicated hardware accelerator, calculate the matrix multiplications E=A*B, F=C*D, and G=E*F sequentially. For each matrix multiplication, disable automatic pipelining for the outermost loop. In the first and second matrix multiplications, keep each nested loop in its full original form without any tiling or parallelization, except flatten the innermost loops for pipelining. In the third matrix multiplication, process seven iterations of the second loop in parallel, and within the innermost loop, allow ten simultaneous operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. For the first outer loop `i`, prevent automatic pipelining, and keep the loop unaltered without tiling or parallelizing its iterations. For the inner loop `j`, within the first outer loop, enable loop flattening for pipelining, but prevent the loop from being split into smaller chunks and set half of the possible parallelism without specifying an exact factor. For the innermost loop `k` within the loop `j`, allow reductions on the `tmp` array, but run the iterations one after the other without any parallelism.\n\nFor the second outer loop `i`, automatically determine if pipelining is beneficial, and again, do not tile or parallelize its iterations. Within this loop, disable pipelining for the first level inner loop `j`, do not tile it, and explicitly set a parallel factor greater than the default, without specifying the exact value. For the innermost loop `k`, permit reductions on array `D` and set a high degree of parallelism without providing a precise factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C, you should provide the following comprehensive but concise natural language instruction:\n\nDefine a hardware kernel function `kernel_3mm` to perform matrix multiplication for three different matrix pairs `(A, B)`, `(C, D)`, and `(E, F)`, disabling automatic pipelining, tiling, and parallelization for the outer two loops of each multiplication, but setting a specific parallel factor for the innermost loops. Specifically, the first innermost loop has a parallel factor of 32, the second retains the default behavior with no specified factor, and the third has a staggered parallel factor, first 4 then 10. Use the following #pragma annotations:\n\n1. Treat the `kernel_3mm` function as a hardware kernel with `#pragma ACCEL kernel`.\n2. For each matrix multiplication step, turn off automatic pipeline optimization with `#pragma ACCEL PIPELINE auto{off}` at the outermost loops to ensure that the loop runs sequentially without any pipelining.\n3. Prevent loop tiling by setting the tile factor to 1 with `#pragma ACCEL TILE FACTOR=auto{1}` on the outermost loops to keep the original loop bounds.\n4. Ensure the outermost loops run sequentially by specifying `#pragma ACCEL PARALLEL FACTOR=auto{1}`, to not perform any parallel execution of loop iterations.\n5. Specify a parallel factor of 32 for the first innermost loop with `#pragma ACCEL PARALLEL FACTOR=auto{32}`, to allow the concurrent execution of 32 iterations.\n6. Leave the second innermost loop to execute with default behavior, indicated by the omission of the parallel factor `#pragma ACCEL PARALLEL FACTOR=auto{}` or its plain version without `auto{}`.\n7. Apply a parallel factor of 4 for the first part and a parallel factor of 10 for the second part of the third outermost loop with `#pragma ACCEL PARALLEL FACTOR=auto{4}` and `#pragma ACCEL PARALLEL FACTOR=auto{10}` respectively, to allow different levels of concurrency in different parts of the calculation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the following function as a hardware kernel for acceleration. Within the function, refrain from automatically pipelining the outermost loop and process it in its original, full form without breaking it into smaller chunks, running the iterations one after the other, not in parallel. For the inner loop, enable automatic pipelining, maintain the loop in its full form without tiling, and allow a parallelization factor of two. Within this loop, use a parallel reduction on an array with a full parallelization factor to accumulate results. In the second outermost loop, automatically pipeline the loop but do not unroll or flatten it, tile this loop with a factor of eight, and permit a parallelization factor of five. For the inner loop in this section, apply a parallel reduction on an array without parallelizing iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a function `kernel_3mm` that computes three matrix multiplications E=A*B, F=C*D, and G=E*F, where A, B, C, D, E, F, and G are two-dimensional arrays with given dimensions. Make sure to treat the function as a hardware kernel for acceleration. For the matrix multiplications, avoid automatic pipelining, do not tile the loops, execute all iterations sequentially, and flatten the loops when appropriate. Specifically, use a parallel factor of 8 for the innermost loop of the F=C*D calculation, and use a parallel factor of 2 in the j loop of the G=E*F calculation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, then for the matrix multiplications within the function, set the loop optimizations as follows:\n\n1. For the first matrix multiplication resulting in matrix `E`, disable pipelining for the outer loops but flatten the second loop's pipeline. Do not tile or parallelize any of these loops beyond a single iteration, effectively running them sequentially.\n\n2. For the second matrix multiplication resulting in matrix `F`, again disable pipelining for the outer loops, but automatically enable it for the second loop without a specific argument. Only the innermost loop should be parallelized with 32 simultaneous iterations, while other loops remain un-tiled and not parallel beyond a single iteration.\n\n3. For the third matrix multiplication producing matrix `G`, turn off pipelining for all loops and prevent tiling. For the middle loop, allow five simultaneous iterations, but keep the innermost and outermost loops at just one.\n\nThe specific pragma directives to use for each loop optimization scenario mentioned above are:\n\n- `#pragma ACCEL kernel` for designating the function as an acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` for disabling pipelining.\n- `#pragma ACCEL PIPELINE auto{flatten}` for enabling pipelining with flattening.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to avoid tiling loops.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to prevent parallelism beyond a single iteration.\n- `#pragma ACCEL PARALLEL FACTOR=auto{32}` to allow 32-fold parallelism for the specified loop.\n- `#pragma ACCEL PARALLEL FACTOR=auto{5}` to allow five-fold parallelism for the specified loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS code, instruct the HLS tool as follows:\n\n- Mark the 'kernel_3mm' function for acceleration, indicating that it is to be synthesized into a dedicated hardware module.\n- In all the loops within the 'kernel_3mm' function, do not use automatic pipelining optimization.\n- Ensure that all the loops retain their original iteration space by disallowing tiling optimization.\n- Execute all loop iterations sequentially without applying parallelism across iterations.\n- For the innermost loops where matrix elements are multiplied and accumulated:\n  - Specifically for the loop iterating over the variable 'k' while computing E[i][j] and F[i][j], override the default parallel processing by enforcing some level of parallelism with the factor set to 32.\n  - For the computation of G[i][j], adjust the parallelism factor to 5 for the loop over 'j' and to 10 for the innermost loop over 'k'.\n\nCorrespondingly, indicate the pragma settings to the HLS tool as follows:\n\n- Use `#pragma ACCEL kernel` to designate the function as a hardware kernel.\n- Use `#pragma ACCEL PIPELINE auto{off}` to prevent automatic pipelining of loops.\n- Use `#pragma ACCEL TILE FACTOR=auto{1}` to maintain the original loop iteration space without tiling.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to indicate that loop iterations should be executed sequentially.\n- Modify the parallel factor for certain loops with `#pragma ACCEL PARALLEL FACTOR=auto{<factor>}`, where `<factor>` is the desired level of loop iteration parallelism, specifically 32 for the multiplication accumulation loops in the computation of E and F, and variable factors of 5 and 10 for the loops in the G computation.\n\nApply these directives immediately before the loop constructs to which they relate in the HLS C code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For a function 'kernel_3mm' that performs 3 matrix-matrix multiplications `(E=A*B, F=C*D, G=E*F)` where matrices have sizes `E[40][50]`, `A[40][60]`, `B[60][50]`, `F[50][70]`, `C[50][80]`, `D[80][70]`, and `G[40][70]`, treat the function as a hardware acceleration kernel, and for each multiplication disable automatic pipelining, do not tile the loops (keep them in full form), and initially set the parallel factors to process loop iterations serially. However, in the first multiplication (E=A*B) within the innermost loop (over 'k'), set the parallel factor to fully unroll the loop with a factor equal to the loop's trip count (60), in the second multiplication (F=C*D) set the parallel factor for the innermost loop to partially unroll with a factor of 5, and in the third multiplication (G=E*F) set the parallel factor for the inner loops (both over 'j' and 'k') to unroll with a factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For the first loop level in the function, perform the loop in sequence with no pipelining, and use the original full loop iteration without tiling or parallelism. In the nested loop inside, allow the first sub-level loop to automatically decide when to pipeline, but ensure the loop is processed as a single block with no tiling, and permit two iterations to be processed in parallel. Within the innermost loop of the first nested loop, reduce 'tmp' array as needed while forcing a sequential run through the iterations. For the second loop level, automatically configure pipelining, without tiling, and with eight parallel iterations. In the nested loop of the second loop level, reduce the 'D' array as needed, again with sequential iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the given 2D matrix-multiply and matrix-add operation function into a dedicated hardware kernel, with specific directives to fine-tune parallelism and loop optimizations during hardware synthesis:\n\n1. Declare the function as a hardware acceleration kernel.\n2. Prevent the function's outermost two loops from being automatically pipelined.\n3. Preserve the original loop structures without tiling for all loops, ensuring they are not split into smaller chunks.\n4. Execute the outermost two loops sequentially, with no parallel execution of iterations.\n5. In the second level loop nested within the first outermost loop, enable loop-flattening optimization for increased parallelism.\n6. Perform the computation of the `tmp` array with only the second level nested loop allowing a doubling of parallelism, while other loops maintain sequential iteration. Apply reduction to safely enable parallel updates to the `tmp` array elements.\n7. For the second main loop sequence that updates the `D` array, enable automatic pipelining without specifying an off or flatten attribute.\n8. Increase the parallel factor significantly in the third-level nested loop during the `D` array update, potentially by a factor of 50, and again use reduction for parallel updates to the `D` matrix.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel and prevent its loops from being automatically pipelined. For the second layer loops within the 'E := A*B' and 'G := E*F' sections, avoid pipelining, automatic tiling, or parallelization. However, allow five- and two-fold parallel execution in the third layer loops of 'E := A*B' and 'G := E*F', respectively. In the 'F := C*D' section, do not pipeline the first loop, proceed without any automatic tiling, and permit a parallel factor of one. For its nested loop, enable automatic pipelining with a parallel factor of eight in the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated kernel function named 'kernel_3mm' implementing three matrix multiplications (E = A * B, F = C * D, G = E * F) with the following constraints:\n\n1. For all loops, disable automatic pipelining.\n2. Retain each loop's original structure without tiling.\n3. Execute the outer loops serially (not in parallel).\n4. For second-level loops, enable pipelining but flatten the loop hierarchy to enhance dataflow.\n5. For the third-level loops in the first two matrix multiplications, disable parallel loop execution.\n6. For the third-level loop in the third matrix multiplication (G = E * F), set the parallel factor to '7' to allow parallel processing of seven iterations.\n7. Treat the entire function as a single computation kernel for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function `kernel_3mm` with parameters `ni`, `nj`, `nk`, `nl`, `nm`, and matrix arrays `E`, `A`, `B`, `F`, `C`, `D`, and `G`. Automatically optimize loop pipelining and tiling, but disallow parallelism and pipelining within the main computation loops of matrices E, F, and G. Specifically:\n\n1. Annotate the function as a hardware kernel for acceleration.\n2. For the matrix multiplication E = A * B, enable automatic loop flattening and pipelining but prevent pipelining within the first nested loop, and do not alter loop tiling or parallelism settings; use default values.\n3. For matrix multiplication F = C * D, disable any automatic pipelining optimization for both main and nested loops, and keep default tiling and parallelism settings.\n4. For the matrix multiplication G = E * F, maintain default pipelining, tiling, and parallelism in the main loop, use the default for the pipelining and tiling, but set a custom parallelism factor of 7 for the first nested loop and default value for the second nested loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a candidate for hardware acceleration. Inside this function, maintain the original execution sequence for the top-level loop without pipelining or parallelizing it and without division into smaller blocks. Flatten the nested loop's pipelining to amalgamate iterations when possible, allowing two of its iterations to run in parallel. For the innermost loop, enable parallel execution without reduction on partial results. Then, for the subsequent top-level loop, enable automatic pipelining optimization. Partition the loop blocking for the nested loop into pairs and allow eight parallel iterations. For its innermost loop, implement a parallel reduction on its target array with a high parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication kernel without pipelining optimizations but with selected parallelization for the nested loops. Specifically:\n\n1. Declare the function as a hardware kernel for acceleration.\n2. For each matrix multiplication section, prevent automatic pipelining for the outermost loops, enforce no tiling optimisation, and execute the loop iterations serially without introducing any parallelism.\n3. Within the nested loops, maintain the original loop form for the intermediate loops without any tiling optimization, and execute these iterations serially.\n4. For the innermost loops that perform element-wise multiplication and accumulation, set the parallel processing of the loop according to specific iteration factors. This includes:\n   - A factor of 1 for the first and second matrix multiplication sections (E = A*B and F = C*D) indicating sequential execution, meaning there's no loop unrolling or parallel processing introduced.\n   - A factor of 7 for the outer loop and a factor of 16 for the inner loop in the third matrix multiplication section (G = E*F), indicating parallel execution with a specific number of compute resources allocated for those loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a dedicated hardware acceleration kernel. When processing the first loop, ensure that it runs serially without optimization transformations like pipelining. Leave each loop iteration intact without tiling or partitioning. In the nested loop for initializing 'tmp', apply full loop flattening to allow for simultaneous iteration computation but limit loop tiling to a single iteration per tile. Allow for two iterations of the loop to be processed in parallel. Use parallel reduction on 'tmp' with a single iteration per compute unit. In the following sequences of loops for updating 'D', enable automatic pipelining and loop tiling but process the loops serially, without parallelism. In the innermost loop for updating 'D', pipeline eight iterations of the j-loop in parallel and assign fifty compute units for parallel reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a matrix multiplication kernel that prohibits automatic pipelining and parallelism enhancements for the top-level loops, while specifying parallelism factors for nested loops. Set all tile factors to preserve the original loop structure. Specifically, disable pipelining in the outer loop executions, maintain the loop structure without tiling, proceed with serial execution in the first nesting level, allow a parallel factor of 2 for the second level of the 'E=A*B' loop, enforce pipeline flattening in the 'F=C*D' computation while keeping single-threaded execution for the innermost loop, and enable increasing levels of parallelism with factors of 2 and 4 at different nesting levels for the 'G=E*F' computation. Treat the entire function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes to implement a matrix-matrix multiplication hardware kernel with specific optimization directives: Disable automatic pipelining for the outermost loops, keep the loops in their full form without tiling, ensure the outer loop iterations run sequentially, and for specific nested loops, flatten or parallelize with given factors. Use '#pragma ACCEL kernel' before the function to indicate it should be treated as a hardware kernel. Set all loops to run sequentially except for the second nested loop within the 'G := E*F' computation, which should have a parallel factor of 7, and the innermost loop within this same computation, which should have a parallel factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel function, disabling pipelining for top-level loops, preserving the full tiled structure of loops without splitting into smaller chunks, executing top-level loops sequentially, and treating the function as an acceleration kernel. For the matrix E computation, keep all loops sequential and unoptimized. For matrix F calculation, flatten the inner j-loop while maintaining others without optimization. During the matrix G computation, set the parallelization factor to 7 for the middle j-loop and to 16 for the innermost k-loop, keeping others unoptimized.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a triple matrix multiplication kernel with the following constraints: Disable automatic loop pipelining throughout, use the original loop form without chunking, process loop iterations sequentially, mark the function as a hardware kernel, and specifically in the last nested loop, allow seven iterations to run in parallel and then in the innermost loop allow for twenty-five parallel operations. Use the following #pragma directives: '#pragma ACCEL KERNEL' to designate the function as a hardware kernel, '#pragma ACCEL PIPELINE \"off\"' to disable pipelining, '#pragma ACCEL TILE FACTOR=1' to prevent loop chunking, '#pragma ACCEL PARALLEL FACTOR=1' to ensure sequential execution except for the last two nested loops where '#pragma ACCEL PARALLEL FACTOR=auto{7}' and '#pragma ACCEL PARALLEL FACTOR=auto{25}' are used respectively to parallelize specific portions of the computation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code that specifies treating a function as a hardware kernel for acceleration with no automatic pipelining for the matrix multiplication loops, which should process each matrix operation in its entirety without tiling or running iterations in parallel. For the last loop nest that computes matrix `G`, configure limited parallelism with a FACTOR of 7 for the 'j' loop and more aggressive parallelism with a FACTOR of 32 for the innermost 'k' loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Develop a 2-D matrix multiplication accelerated kernel named 'kernel_2mm' for an HLS tool, with specific control over loop transformation and hardware parallelism. Define the function to accept dimensions 'ni', 'nj', 'nk', and 'nl', with multiplier scalars 'alpha' and 'beta', and matrices 'tmp', 'A', 'B', 'C', and 'D' as parameters. Disable automatic pipelining for the entire function. For the first set of nested loops that calculate 'tmp = alpha * A * B,' keep the outer loop without pipelining or tiling, and process its iterations serially. For the innermost 'k' loop handling the 'A*B' product, enable pipelining without loop flattening, let the loop run in its entirety without tiling, and only allow a maximum of two iterations to execute in parallel with a reduction operation on 'tmp'. For the second set of nested loops that update 'D = beta * D + tmp * C', again avoid any pipelining or tiling for the outer loop and perform iterations serially. For the 'j' loop which handles 'D' updating, allow full automatic pipelining, prevent tiling, and enforce maximum parallel execution of eight iterations. Within the deepest 'k' loop concerning 'tmp * C' addition, perform a reduction on 'D' with an automatically determined maximum amount of parallelism capped at fifty.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel; within that function, ensure that the main loops operate sequentially and are not subdivided or executed in parallel beyond a single iteration granularity. Specifically, in the first major loop, allow dynamic pipelining decisions to be made for the inner loop, with exactly two iterations running concurrently, and apply a parallel reduction with the maximum possible parallelism for the innermost loop. In the second major loop, instruct the compiler to consider the inner loop as a single, fully unrolled entity, enforce a strict sequential execution for the intermediate loop with a parallelism factor of eight, and the innermost loop should execute sequentially without any parallel reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Accelerate the given `kernel_3mm` function as a hardware kernel, while keeping the top-level loops unaltered by disabling automatic pipelining, maintaining their original order with tiling and parallel factors set to singleton. In the innermost loops, automate parallelism where applicable and explicitly specify a parallel factor of 7 for the G matrix computation, and a parallel factor of 5 for its inner dimension. Flatten pipelining in the second-level loops where each element of E, F, and G matrices is computed.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel. Within this function, perform a series of matrix multiplications between arrays A, B, C, D to yield results in arrays E, F, G, ensuring that none of the three main for-loops are automatically pipelined. Each of these loops should execute iterations sequentially, not in parallel, and without being broken into smaller chunks. Specifically:\n\n1. In the loop calculating E = A * B, allow the innermost loop that accumulates into E[i][j] to run in parallel with a factor of 1.\n2. In the loop calculating F = C * D, similarly, let the innermost loop that accumulates into F[i][j] operate sequentially, with only one instance running at a time.\n3. In the loop calculating G = E * F, do not pipeline the outer two loops, but allow the innermost loop, where G[i][j] accumulates, to operate with up to 5 parallel instances, and within that, allow the nested loop to run with up to 16 parallel instances.\n\nMake sure to flatten the inner loops where possible to optimize loop iterations for potential loop-carried dependencies, but within a single layer of loop nesting only, maintaining the sequential execution for the overall loop bodies.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code that defines a hardware-accelerated kernel function `kernel_2mm` with the following features: Set the function for hardware acceleration, disable automatic pipelining, do not tile or parallelize the outermost loops, but set the innermost loop of the first computation block to automatically determine the parallel factor and conservatively to run iterations one after the other without breaking the loop. In the reduction loop of the first block, allow for up to 70 iterations to be executed in parallel as part of a reduction operation. For the second computation block, similarly disable automatic pipelining, and do not tile the loop, but set the parallel factor of the second loop to 8 and allow the innermost reduction loop to execute up to 50 operations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS code in C, you should instruct the code generator with the following natural language directions:\n\n- Designate the 'kernel_2mm' function as a hardware acceleration kernel.\n- For the first loop 'i', do not automatically apply pipeline optimization. Keep the loop intact without subdividing it and execute iterations sequentially.\n- Inside the first loop 'i', for the nested loop 'j', fully unroll the loop for parallel processing and flatten it for optimization purposes.\n- Within the same scope, at the innermost loop 'k', apply parallelization to the reductions of the 'tmp' array, but do not modify loop partitioning or factor.\n- For the second 'i' loop for updating matrix 'D', apply the same directives as the first 'i' loop: no automatic pipelining, no subdivision, and sequential execution.\n- Within this second set of loops, for the nested loop 'j', increase the parallel factor to 8 to allow eight iterations to be processed concurrently.\n- Finally, in the innermost loop 'k', apply parallelization to the reductions of the 'D' array, with no custom partitioning or factor adjustments.\n\nIn terms of #pragma values, convey the following settings:\n\n- Use '#pragma ACCEL kernel' to mark the function for hardware acceleration.\n- Specify '#pragma ACCEL PIPELINE \"auto{off}\"' for loops where pipelining should be switched off.\n- Indicate '#pragma ACCEL TILE FACTOR=\"auto{1}\"' to maintain original loop chunk sizes.\n- Dictate '#pragma ACCEL PARALLEL FACTOR=\"auto{1}\"' for sequential processing and '#pragma ACCEL PARALLEL FACTOR=\"auto{2}\"' for parallel processing of two concurrent iterations, and '#pragma ACCEL PARALLEL FACTOR=\"auto{8}\"' to process eight at the same time.\n- For loops where full unrolling is intended, use '#pragma ACCEL PIPELINE \"auto{flatten}\"'.\n- When applying parallelization specifically to array reductions, use '#pragma ACCEL PARALLEL reduction=array_name'.\n\nEnsure the corresponding pragmas are placed directly before the loops to which they apply.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function called `kernel_3mm` that performs a series of matrix multiplications on input matrices `A`, `B`, `C`, and `D` to produce output matrices `E`, `F`, and `G`. The function parameters define the dimensions (ni, nj, nk, nl, nm) and the matrices themselves. Within the function, follow three main matrix multiplication steps: first, compute `E` as the product of `A` and `B`; second, compute `F` as the product of `C` and `D`; and third, compute `G` as the product of `E` and `F`. Avoid automatic loop pipelining, tiling, and parallelization for the matrix multiplication loops by setting the respective #pragma values to ensure that each loop is processed in full without being split into smaller blocks or executed in parallel. Specifically, configure the pipeline, tile, and parallel pragmas to be turned \"off\", with tiles and parallel factors set to \"1\". However, within the innermost loop of each multiplication step, partially unroll the loop by setting #pragma ACCEL PARALLEL FACTOR to \"30\" for the computation of `E`, to \"20\" for `F`, and to \"50\" for `G` to accelerate the summation in the dot product calculation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) configuration that defines a hardware kernel without automatic pipelining or tiling optimization, where loop iterations are executed sequentially, but with custom parallelization factors for certain loops. Specifically:\n\n1. Declare the function `kernel_3mm` as a hardware kernel to be accelerated.\n2. Prevent the compiler from automatically applying pipeline optimization to the top-level loops (processing matrices E, F, and G).\n3. Instruct the compiler not to split the loops into smaller chunks (tiling) by setting the tiling factor to 1.\n4. Execute loop iterations one after the other without parallelism (parallel factor of 1) for the outer loops.\n5. For the first nested loop processing matrix E and the loop for matrix F, enforce sequential execution but with the possibility of complete loop flattening for pipelining by using 'auto{flatten}' with the pipeline directive.\n6. For the construction of matrix G, retain the top-level loop's configuration of no pipelining and no tiling, but set a custom parallel factor of 7 for the outer loop and a parallel factor of 16 for the most inner loop that multiplies matrices E and F.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, but disable automatic pipelining for the main computation loops, ensuring they execute sequentially in their entirety. Within the innermost loops for matrices E and F, allow limited parallelism for the multiplications by factors of 8 and 32, and a higher degree of parallelism by a factor of 50 for updating matrix G. Maintain loop bounds as they are without tiling. Here are the specific instructions:\n\n1. Treat the entire function 'kernel_3mm' as an acceleration kernel.\n2. For all the outer and second-level loops, disable automatic pipelining and avoid splitting the loops into smaller parallel chunks, ensuring that iterations run sequentially without any form of automatic parallelization.\n3. Enable a parallelization factor of 8 for the third-level loop that computes matrix 'E', a factor of 32 for the loop computing matrix 'F', and a factor of 50 for the last loop computing matrix 'G'.\n4. Leave the loop bounds untouched by setting the tiling factor to 1 for all loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a dedicated hardware acceleration kernel. Within this function, do the following: \n\n1. When iterating over the first dimension of size 40, turn off automatic pipelining and do not tile or parallelize the outermost loop. \n2. In the nested loop iterating over the second dimension of size 50, also turn off automatic pipelining. Then, set tiling to a single iteration per tile and allow two iterations to run in parallel. \n3. In the innermost loop, perform a reduction operation on the temporary array, enabling thirty-two simultaneous operations for the calculations involving 'alpha', 'A', and 'B'.\n\nIn the second main loop block:\n\n1. Enable automatic pipelining for the loop iterating over the first dimension of size 40. Do not tile or parallelize this loop.\n2. Within the loop iterating over the third dimension of size 80, turn off automatic pipelining. Set the TILE FACTOR to eight iterations per tile, and permit five iterations to run concurrently in parallel.\n3. In the innermost loop for updating 'D', perform another reduction operation, setting the parallel factor to allow sixteen concurrent operations for the calculations involving 'tmp' and 'C'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, use the following concise instructions for the respective #pragma annotations:\n\n1. For the entire function, declare it a hardware acceleration kernel with: \"Designate the function as a hardware kernel to be accelerated.\"\n2. For the loop that computes E = A * B, disable pipeline optimization and execute the loop iterations sequentially with: \"Do not apply pipelining, tiling, or parallelism optimizations to the outer loops that calculate matrix E.\"\n3. Inside the innermost loop of matrix E computation, allow the innermost loop to parallelize with a factor that matches the inner dimension of matrices A and B with: \"Parallelize the innermost loop of matrix E computation with a factor equal to the number of columns in matrix A (or rows in B).\"\n4. For the loop that computes F = C * D, repeat the same instructions as step 2 with the substitution of matrix F for E.\n5. Within the innermost loop of matrix F computation, parallelize the loop with a factor that matches the inner dimension of matrices C and D with: \"Parallelize the innermost loop of matrix F computation with a factor equal to the number of columns in matrix C (or rows in D).\"\n6. For the loop that computes G = E * F, apply similar instructions as in step 2 with the necessary changes for matrix G.\n7. When computing G, parallelize the second outer loop with a factor of 4 using: \"Parallelize the second outer loop when calculating matrix G with a factor of 4.\"\n8. For the innermost loop in matrix G computation, set up parallelism that coincides with the number of columns in E (or the number of rows in F) with: \"Parallelize the innermost loop of matrix G computation with a factor equal to the number of columns in matrix E (or rows in F).\"\n\nRemember to replace the placeholders 'auto{off}', 'auto{1}', 'auto{60}', 'auto{80}', and 'auto{50}' with the appropriate #pragma values to match the specific parallelism factors and directives as per the instructions above.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to:\n\n1. Utilize the following function as a dedicated hardware acceleration kernel.\n2. For each major matrix multiplication step (E = A*B, F = C*D, G = E*F), execute the outer two loops sequentially, without any loop transformation optimizations such as tiling or pipelining. Hence, disable automatic pipelining, set the tiling factor to 1, and ensure the outer loops run their iterations serially with a parallel factor of 1.\n3. For the innermost loop of the first two multiplication steps, permit the tool to parallelize computation across all elements involved in the dot-product operation (for E = A*B, parallelize across all 'k' iterations up to 60; for F = C*D, parallelize across all 'k' iterations up to 80).\n4. In the last step (G = E*F), allow the tool to partially unroll the second innermost loop with a factor of 5, while the innermost loop should remain without parallelization and other associated optimizations. Also, enable loop flattening for the outermost loop, but ensure no automatic pipelining is applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare a function `kernel_3mm` to be accelerated for hardware implementation, which calculates the product of matrices `A*B` into `E`, `C*D` into `F`, and `E*F` into `G`. For each matrix multiplication, disable automatic pipelining for the outer loops, and ensure that every level of loop is unrolled or processed in its entirety (`TILE FACTOR=1` and `PARALLEL FACTOR=1`) except:\n\n1. For the innermost loop of `A*B`, fully parallelize the computation across all iterations (`PARALLEL FACTOR=60`).\n2. For the innermost loop of `C*D`, fully parallelize the computation across all iterations (`PARALLEL FACTOR=80`).\n3. For the middle loop of `E*F`, do not fully unroll but instead use a partial parallel factor (`PARALLEL FACTOR=5`).\n4. Disable the nested loop flattening in the outerloop when calculating `E*F`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a C function named 'kernel_3mm' designed as a hardware acceleration kernel, which computes the product of matrix pairs (A*B = E), (C*D = F), and (E*F = G) with specified dimensions. For all outer loops of these matrix multiplications, disable automatic pipelining, ensure they execute without tiling or parallelization optimization, and preserve their original, full loop structure. For the middle loops, where elements are being accumulated, enable pipelining with flattening to aggressively optimize these loops but without tiling or parallelization. For the innermost loops, allow parallel execution of the iterations with no pipelining or tiling in the first two matrix multiplications, and with a parallel factor of 1 and 5 respectively for the two innermost loops of the last matrix multiplication. \n\nHere are the respective #pragma values:\n\n1. Treat the following function as a hardware kernel for acceleration:\n   `#pragma ACCEL kernel`\n\n2. Don't automatically pipeline this loop:\n   `#pragma ACCEL PIPELINE auto{off}`\n\n3. Process this loop in its original, full form without breaking it into smaller chunks:\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. Run the iterations of this loop one after the other, not in parallel:\n   `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n5. Aggressively optimize this loop by flattening and enabling pipelining, but don't break it into chunks or run iterations in parallel:\n   Inside the first loop inside 'E:=A*B' and 'F:=C*D':\n   `#pragma ACCEL PIPELINE auto{flatten}`\n\n6. For the final loop in 'G:=E*F', allow parallel execution of iterations with a specific parallel factor where applicable:\n   For the innermost loop within 'E:=A*B' and 'F:=C*D':\n   `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n   For the second innermost loop within 'G:=E*F':\n   `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n   For the innermost loop within 'G:=E*F':\n   `#pragma ACCEL PARALLEL FACTOR=auto{5}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 3-matrix multiplication kernel function, treating the entire function as a hardware acceleration kernel. For each of the three double-nested loops calculating the matrix products (E=A*B, F=C*D, G=E*F), turn off automatic pipelining, do not tile the loops, and run the outer two loops sequentially. However, fully parallelize the innermost loop of the first matrix product with a factor equal to the inner dimension (60), do the same for the second matrix product's inner loop with a factor of 80, and for the final matrix product use a parallelization factor of 50 for its inner loop while having the second to last loop run with a parallelization factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel for a 2-matrix multiplication operation with the following attributes: Do not utilize automatic pipelining for the kernel's outer loops, instead preserve them in their full, original iteration sequence and ensure their iterations are executed serially. In the primary nested loops, allow the compiler to automatically decide the pipeline strategy, but still retain the entire body without tiling. For the innermost loop, process two iterations simultaneously without parallelizing the reduction operation's accumulation of the `tmp` and `D` arrays; use a parallel factor of one for reductions to ensure sequential execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the given function as a hardware acceleration kernel, and control the optimization of loops within it as follows: Disable automatic pipelining, inhibit the loop from being split into smaller chunks (use a tiling factor of 1), and ensure the loops execute iterations sequentially (parallel factor of 1) for all the base-level outer loops. Inside the first nested loop, enable full loop flattening while still maintaining a tiling factor and parallel factor of 1. For the loops calculating F and G, use a parallel factor of 4 for the innermost loop. Specifically, in the calculation of G, increase the parallel factor of the second nested loop to 7.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware acceleration kernel, ensuring top-level loops run sequentially without pipelining, but allowing the second loop level to be flattened and parallelized with four concurrent iterations. At the deepest loop level, perform reductions on the 'tmp' array in a sequential manner and parallelize the final nested loop with 32 iterations running in parallel, but only for the reduction operation on the 'D' array. Use automatic pragma values where applicable, except for explicitly turning off pipelining at the top level and fixing the tile factor value to one at every loop level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the provided HLS C code with specific pragmas, you can describe it using the following natural language instruction:\n\n\"Create a hardware kernel that performs three matrix multiplications: the first is A times B producing E, the second is C times D producing F, and the third is E times F producing G. In the first multiplication, disable the automatic pipeline and do not tile or parallelize the outer two loops, but fully parallelize the innermost loop with a factor equal to the loop iteration count. In the second multiplication, similarly, disable the automatic pipeline for the outer loops but fully parallelize the innermost loop with a factor corresponding to its iteration count. For the third multiplication, disable pipeline optimization for all loops, set the tile factor to prevent loop tiling, parallelize the middle loop with a factor of 4, and fully parallelize the inner loop with a factor equal to its iteration count.\" \n\nHowever, note that the 'auto{}' and 'auto{value}' syntax used after pragma directives are not standard. The expected standard pragma directive in HLS would normally be written without 'auto' and would look like `#pragma HLS PIPELINE off`, `#pragma HLS TILE factor=1`, and `#pragma HLS PARALLEL factor=N`. The 'auto{off}' and 'auto{1}' might be an artifact of an automated tool, or represent custom directives for a specific proprietary HLS tool. Please ensure to follow the syntax and usage as per the rules of the specific High-Level Synthesis tool being used.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the provided C code for three matrix multiplications into high-level synthesis (HLS) with specific directives: \n\n1. Mark the function as an accelerator kernel.\n2. In matrix multiplication E = A * B, fully preserve the outer loops but do not automatically pipeline them. Initialize all elements of E to zero. For the innermost loop, parallelize with a factor of 12.\n3. For multiplication F = C * D, also fully preserve the outer loops without automatic pipelining. Initialize all elements of F to zero. In the innermost loop, set a higher degree of parallelism with a factor of 32.\n4. For the final product G = E * F, follow similar outer loop directives as before but restrict parallelism in the second loop to a factor of 7. In the last loop, massively parallelize the computation with a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_2mm' function as a hardware acceleration kernel, and for the first level 'for' loops in the kernel, use a pipeline with no optimization, keep the loop whole without tiling, and process iterations sequentially. However, flatten the innermost loop of the first block to fine-tune the pipeline, use the default tiling factor for it, and parallelize its iterations two at a time. For the reduction step inside this innermost loop, handle loop iterations sequentially without parallelism. For the outer loop in the second processing block, apply default pipelining, no tiling, and sequential iteration processing, and for its first inner 'for' loop, disable pipelining, tile the loop into chunks of eight, and process five iterations in parallel. Finalize the second block by allowing fifty iterations of the innermost loop to be processed in parallel for the reduction operation.\n\nSpecific #pragma instructions:\n- To disable automatic pipelining for certain loops, use '#pragma ACCEL PIPELINE \"off\"'.\n- To keep loops intact without tiling, set the tile factor to one with '#pragma ACCEL TILE FACTOR=auto{1}'.\n- To ensure iterations run sequentially (not in parallel), set the parallel factor to one with '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n- For the first loop within the first block that needs a flattened pipeline, use '#pragma ACCEL PIPELINE auto{flatten}'.\n- The inner loop of the second block should have its tile factor set to eight with '#pragma ACCEL TILE FACTOR=auto{8}' and a parallel factor of five with '#pragma ACCEL PARALLEL FACTOR=auto{5}'.\n- For the reduction operations, apply a parallel factor of fifty with '#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{50}', where 'variable' is the name of the variable being reduced ('tmp' and 'D').", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS C code with various #pragma directives for a kernel function named 'kernel_3mm' that performs three matrix-matrix multiplications (E=A*B, F=C*D, and G=E*F), use the following comprehensive but concise natural language instructions:\n\n- Declare `kernel_3mm` as a hardware acceleration kernel.\n- For the multiplication of A and B, as well as C and D, disable automatic pipelining, do not tile the loops, and execute the outer loops serially. In the innermost loop of multiplying A and B, enable manual parallelization with a factor of 3, whereas in multiplying C and D, set the parallel factor to 8.\n- In the final multiplication of E and F to compute G, disable automatic pipelining for the two outer loops but not specified for the third loop, prevent tiling across all loops, set a parallel factor of 5 for the middle loop, and for the innermost loop, use a parallelization factor of 8.\n\nSpecific #pragma instructions to include:\n\n- For non-pipelining and serial execution of outer loops, use `#pragma ACCEL PIPELINE auto{off}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}` respectively, ensuring the loop runs without any parallelization or pipelining optimizations.\n- Inhibit loop tiling by using `#pragma ACCEL TILE FACTOR=auto{1}`, signaling the compiler not to split loops into smaller chunks.\n- To manually specify parallel factors for inner loops, utilize `#pragma ACCEL PARALLEL FACTOR=auto{3}` for the E matrix computation, `#pragma ACCEL PARALLEL FACTOR=auto{8}` for the F matrix, and for the final product G, use `#pragma ACCEL PARALLEL FACTOR=auto{5}` for the middle loop and `#pragma ACCEL PARALLEL FACTOR=auto{8}` for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, do not apply automatic loop pipelining to the top-level loops, keep them running sequentially without partitioning into stages, and without executing iterations in parallel. Within the first nested loop, allow for flattening the pipeline without explicit parallelization or tiling configuration. For the innermost loop of the first nested block, permit automatic parallel reduction on `tmp` with the default setting. For the second nested loop block, maintain the same top-level sequential execution without pipeline optimization and process the innermost loop with parallel reduction on `D` with a parallel factor equivalent to the size of the C array's inner dimension.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code where the function `kernel_2mm` computes a matrix-matrix multiplication and addition while treating the function as a hardware acceleration kernel. For the first and third loops, ensure they are executed with no pipeline optimization, without tiling, and with sequential execution of iterations. For the nested loops, disable the pipeline optimization, maintain the original loop form without tiling, and for the second loop allow a parallel execution factor of 2. For the most inner loops, perform parallel reduction on the respective output array (`tmp` and `D`) with a parallel execution factor of 1. For the second occurrence of the i-j loop nest, pipeline the outer loop and set a parallel factor of 80 for the inner loop iteration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for matrix multiplication and ensure the main matrix multiplication loops are not pipelined, but still use a single iteration tiling and run in serial. Specifically, do not pipeline the loop for calculating matrix 'E', and for the innermost loop of 'E', run 15 calculations in parallel. Similarly, do not pipeline the calculation of matrix 'F', but allow 32 parallel operations in its innermost loop. Finally, for matrix 'G', pipeline only the middle loop, run 7 operations in parallel in the middle loop, and allow 50 operations to run in parallel in the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_3mm` as a hardware acceleration kernel that performs matrix multiplication on three pairs of matrices (A*B=E, C*D=F, E*F=G) with specific dimensions. For each of the three major nested loop constructs:\n\n1. The outermost multiplication loops for computing E, F, and G should not be pipelined and should execute iterations sequentially without breaking them into smaller fragments or tiling them.\n2. The intermediate loops initializing matrix E and F should be pipelined allowing the compiler discretion for pipelining, executed in full without tiling, and iterate sequentially without parallel execution between iterations.\n3. For the innermost multiplication loops computing E and F, allow the compiler to automatically parallelize the multiplication with an unbounded parallel factor while for G, restrict the parallel factor to two for the intermediate loop and to fifty for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel function called 'kernel_2mm' that performs a matrix multiplication and addition algorithm, without automatically optimizing the loops inside. Specifically, configure the following: \n\n1. Treat 'kernel_2mm' function as a hardware acceleration kernel.\n2. For the outermost 'i' loop of the 40 iterations, disable loop pipelining optimization, set tiling factor to 1, and perform iterations serially.\n3. For the nested 'j' loop of the 50 iterations inside the first outer 'i' loop, also disable pipelining, set tiling factor to 1, but allow two iterations to run in parallel.\n4. For the inner 'k' loop of 70 iterations inside the first 'j' loop, enable parallel reduction on the 'tmp' array, with the maximum allowed parallelization factor for the hardware.\n5. For the second outer 'i' loop of 40 iterations, again disable pipelining, set tiling factor to 1, and perform iterations serially.\n6. Within the nested 'j' loop of 80 iterations inside the second outer 'i' loop, disable pipelining, set tiling factor to 1, and allow eight iterations to run in parallel.\n7. For the inner 'k' loop of 50 iterations inside the second 'j' loop, enable parallel reduction on the 'D' array, with the maximum allowed parallelization factor for the hardware.\n\nThe code skeleton should include the initialization of a 2D temporary array 'tmp', the matrix multiplication of 'alpha * A * B', and the addition of 'beta * D' with the matrix multiplication of 'tmp * C', to update the 'D' array. Each pragma should be specified right before the loop it is intended to optimize or configure.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the following function as a hardware kernel for acceleration, and within it perform matrix multiplications without pipelining the outermost loops, while keeping the inner loops in their original, full sequence. For the multiplication of E(i,j) items, flatten the pipeline pragma for efficient hardware implementation without tiling or parallelizing the loop; for the computation of F(i,j) items, parallelize the innermost loop with a factor of 4; and for the computation of G(i,j) items, use a parallel factor of 7 in the second loop and a much higher parallel factor of 25 for the innermost loop, enabling greater parallel execution within hardware constraints.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a three-matrix multiplication kernel implementing the operations E = A * B, F = C * D, and G = E * F, with specific loop optimization directives as follows:\n\n1. Disable automatic pipelining for all major loops.\n2. Do not tile any of the loops.\n3. Execute all loops serially, without parallel execution, except for specified cases.\n4. Treat the entire function as a hardware acceleration kernel.\n5. For the internal loop that computes E[i][j], parallelize loop iterations with a factor of 12.\n6. For the internal loop that computes F[i][j], parallelize loop iterations with a factor of 5.\n7. Flatten the most nested loop in the G = E * F operation, and set the parallel factor for the intermediate loop to 7.\n\nIn your code, ensure to place each #pragma directive immediately before the loop it is intended to optimize, following the structure provided in the example code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C codes for a three-matrix multiplication kernel function avoiding automatic pipelining of major loops, setting the tiling factor to 1 to keep loop chunks unpartitioned, and specifying different parallel factors for the innermost loops to control the degree of parallelism. The kernel function handles matrix multiplication operations for three pairs of matrices (A*B, C*D, and E*F) with predefined dimensions stored in 2D arrays. Use appropriate pragmas before loops to disable automatic pipelining, set the tile factor to 1 for outer loops, allow full sequential processing, and assign specific parallel factors to inner loops ranging from 7 to 50, depending on the loop, as indicated in the provided code segments.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive but concise natural language description of the code and pragma directives:\n\n\"Define a hardware accelerated kernel function called 'kernel_2mm' which performs two matrix-matrix multiplications with scaling factors alpha and beta, using matrices 'A', 'B', 'C', and 'D' and a temporary matrix 'tmp'. Within this function, prevent automatic pipelining optimization on the outer loops iterating over the first and third matrix dimension. Ensure the initial multiplication loop nest processes each iteration in sequence without tiling or parallelization, but allow the innermost loop to compute partial sums of the 'tmp' array in parallel with up to 70 streams. In the subsequent loop nest, where 'D' is updated with the result and scaled by beta, flatten the loop for pipelining while iterating over the columns of 'D'. Then, parallelize the innermost loop for updating 'D' with 5 concurrent threads but do not use data reduction or parallelism while multiplying the 'tmp' and 'C' matrices.\"\n\nSpecific pragma directives and their descriptions:\n\n- `#pragma ACCEL kernel`: Indicates that the following function is to be treated as a kernel for hardware acceleration.\n- `#pragma ACCEL PIPELINE auto{off}`: Disables automatic loop pipelining for the following loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}`: Specifies that the loop should not be tiled and should be processed in its original form.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Specifies that the iterations of the loop are to be processed sequentially.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}`: Allows up to 70 concurrent operations in the reduction of the 'tmp' array within the innermost loop.\n- `#pragma ACCEL PIPELINE auto{flatten}`: Enables loop pipelining with loop flattening to optimize performance for the following loop.\n- `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`: Specifies no parallelism for the reduction on the 'D' array, ensuring it runs sequentially.\n\nNote: 'auto{}' would typically allow the compiler to determine the appropriate optimization, but with additional parameters inside {}, such as 'auto{off}', 'auto{1}', 'auto{2}', etc., it overrides the automatic behavior with specified values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Design the hardware acceleration of a three-matrix multiplication (3mm) algorithm in HLS C, where the matrices (E, F, G, A, B, C, D) have fixed dimensions. Prevent pipeline optimization for the main multiplication loops. Maintain the original loop structure without tiling, and execute loop iterations sequentially. Internally, for the computation of E, allow the innermost loop to parallelize up to 10 iterations concurrently. For F, allow up to 32 parallel iterations. Lastly, for G, perform tiling with a factor of 7 in the j-loop and parallelize up to 50 iterations in the k-loop. Declare the function as a kernel for hardware acceleration. Use automatic pragma values except where explicit factor values are provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given function `kernel_2mm`, implement hardware acceleration with the following specifications:\n\n- Declare the function as a hardware kernel for acceleration.\n- Keep the outermost loop (`i` loop) running sequentially without automatic pipelining optimization.\n- Do not tile or parallelize the outermost loop, ensuring it executes in its entirety for each iteration.\n- Within the first nested loop (`j` loop), apply automatic pipelining without specific constraints.\n- Do not tile the first nested loop, but set its inner parallel loop (`k` loop) to have a parallelism factor of 2 and enable a parallel reduction on `tmp`.\n- Apply automatic pipelining optimization to the second outer loop (`i` loop).\n- For the second nested loop (`j` loop), disable automatic pipelining and tile it without breaking it into smaller chunks.\n- Set a high parallelism factor of 8 for this nested loop and enable a parallel reduction on `D` with an internal loop (`k` loop) constraint to parallelize on the full range of iterations.\n\nIn directive terms:\n- Treat the function as a hardware kernel (`#pragma ACCEL kernel`).\n- For the first `i` loop, use `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the first `j` loop inside the first `i` loop, use `#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{2}`, with `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}` for the inner `k` loop.\n- For the second `i` loop, use `#pragma ACCEL PIPELINE auto{}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the `j` loop inside the second `i` loop, use `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{8}`, with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}` for the inner `k` loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function named `kernel_3mm` to perform three matrix multiplications (`E=A*B`, `F=C*D`, and `G=E*F`). In this function, first turn off automatic pipeline optimization for the outermost loops of each multiplication. Keep the loops in their complete form without tiling or splitting for parallel execution by setting their tiling and parallel factors to one. Then, for the innermost loops, apply maximal flattening to the `j` loop and specify a parallel factor of 1 for the `k` loop in the first two matrix multiplications. For the final matrix multiplication, maintain the defaults for the `j` loop's pipeline pragma, set the tiling factor to 1, use a parallel factor of 7 for the `j` loop, and set the parallel factor to 25 for the `k` loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following natural language instruction:\n\nDefine a function as a hardware acceleration kernel where matrix multiplication and addition operations are performed on rectangular arrays, without automatic pipelining or tiling optimizations. In the first main loop, where a temporary matrix is calculated, treat the most inner loop with a high degree of parallelism to optimize the reduction operation. In the second main loop, where the final matrix D is updated, apply a similar level of parallelism to optimize the reduction while keeping the other loops running sequentially without pipelining or tiling optimizations. Specifically, disable automatic pipelining for all outer loops, and set the TILE and PARALLEL factors to their original values, except for the inner loops performing reduction operations, where the parallelism should be maximized up to sixteen-fold.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 3-matrix multiplication kernel that disables automatic pipelining at the outermost layer of each multiplication process while ensuring no tiling occurs, and enforces sequential execution of iterations. At the innermost loop of the first and third multiplication, allow some level of parallelism by setting parallel execution to 16 and 50 respectively, and even more parallelism at a factor of 32 for the second multiplication. The matrix sizes should correspond respectively to the set dimensions: E[40][50], A[40][60], B[60][50], F[50][70], C[50][80], D[80][70], G[40][70]. Also, ensure the entire function is treated as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' for hardware acceleration using the HLS `#pragma ACCEL kernel`. Disable automatic pipelining for all outer loops, maintaining their original loop execution without tiling or parallelization. Within the first two-level nested loops, manually enable pipelining without specifying parameters but keep the remainder of the loop settings default, involving no loop tiling and no parallel execution for both outer and inner loops, to execute iterations as they occur without concurrency. Set specific parallelization factors for the innermost loop of the first nested loop set to 30 and the second to 5. In the final loop nest, manually pipeline the second-level loop, maintain its iterations as sequential, and set the parallelization factor for the innermost loops to 7 and 5 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and configure loop optimization pragmas as follows: do not pipeline any of the loops, maintain the outer loops in a single, non-parallelized iteration, while allowing the inner second-level loop 'j' in the first main loop block to execute 32 iterations in parallel but without pipelining. Then, in the second main loop block, pipeline the outer loop 'i' employing flattening to linearize any loop nest inside of it, while keeping the rest of the loops within this block non-pipelined and non-parallelized. The reduction clause should be applied to parallel pragma within innermost loops of both loop blocks, with 'tmp' and 'D' arrays as the reduction variables, and the 'j' loop in the first block and 'k' loop in the second block both parallelized with a factor of 32 and 1, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` for hardware acceleration but disable automatic pipelining for the entirety of its execution. Process each loop in its original form without tiling or parallelizing, except within the most inner loops where specific parallel factors are applied to enhance performance.\n\n- Treat `kernel_3mm` as a hardware acceleration kernel.\n- For each top-level loop calculating `E`, `F`, and `G`, disable automatic pipelining, tiling, and parallelizing.\n- In the `E` calculation, do not automatically pipeline the outer `j` loop but allow the inner `k` loop to have a parallel factor of 3.\n- In the `F` calculation, disable automatic pipelining for both `i` and `j` loops and set a parallel factor of 8 for the inner `k` loop.\n- For the `G` calculation, automatically pipeline the `i` loop iteration but set a parallel factor of 5 for the `j` loop and a factor of 8 for the inner `k` loop.\n\nHere are the respective #pragma values as per the instructions:\n\n- `#pragma ACCEL kernel` to mark the function for acceleration.\n- All top-level loops (`i` loops for `E`, `F`, and `G` calculations) use:\n  - `#pragma ACCEL PIPELINE auto{off}` to prevent automatic pipelining.\n  - `#pragma ACCEL TILE FACTOR=auto{1}` to prevent tiling the loops.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{1}` to prevent parallel execution of iterations.\n- For the `j` loop in the `E` calculation, use the same pragmas as above to maintain the loop structure and disable automatic optimizations.\n- In the `k` loop computing `E`, use:\n  - `#pragma ACCEL PARALLEL FACTOR=auto{3}` to parallelize the loop with a specified factor.\n- For the `F` calculation, the `i` and `j` loops should have the same pragmas as the top-level `i` loops to disable automatic optimizations.\n- In the `k` loop computing `F`, use:\n  - `#pragma ACCEL PARALLEL FACTOR=auto{8}` to set a higher parallel factor for this loop.\n- For the `G` calculation, allow pipelining of the `i` loop iterations with the following pragma:\n  - `#pragma ACCEL PIPELINE auto{}` with no specific instruction to let it pipeline automatically.\n- For the `j` loop in the `G` calculation, do not change looping structure but use:\n  - `#pragma ACCEL PARALLEL FACTOR=auto{5}` to parallelize loop execution with the specified factor.\n- In the `k` loop computing `G`, use:\n  - `#pragma ACCEL PARALLEL FACTOR=auto{8}` same as for the `F` calculation for parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function by following these directives: \n\n1. Treat 'kernel_2mm' as a hardware-accelerated function.\n   \n2. Within 'kernel_2mm', for the first-level loops (loops over 'i'), disable automatic pipelining, do not tile the loops, and do not parallelize them.\n\n3. Within 'kernel_2mm', for the second-level loops (loops over 'j'), turn on automatic pipelining, tile the loops with a factor of 1, and set parallelization to a factor of 4.\n\n4. For the third-level loop (loop over 'k') computing 'tmp', enable parallelization with a reduction on 'tmp' with a factor of 16.\n\n5. For the second-level loops (loops over 'j') updating 'D', enable automatic pipelining, do not tile the loop, and do not parallelize it.\n\n6. For the third-level loop (loop over 'k') updating 'D', enable parallelization with a reduction on 'D' with a factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a hardware kernel for acceleration, and for each of the three major sections involving matrix multiplication `(E := A*B), (F := C*D), (G := E*F)`, apply the following directives to the corresponding loops:\n\n1. For the computation of matrix `E`, disable automatic pipelining, enforce no loop tiling (each loop is to remain whole), and no parallel execution (single iteration at a time) for the outermost loop. Within the innermost loop that computes the dot product, allow the computations to run sequentially.\n\n2. For the computation of matrix `F`, disable automatic pipelining for the outermost loop, and similarly prohibit loop tiling and parallel factor adjustment. However, within the innermost loop that computes the dot product of matrices `C` and `D`, enable the computation to be split into 4 parallel execution paths.\n\n3. For the computation of matrix `G`, disable automatic pipelining and loop tiling for the outermost loop, but within the second loop, permit each iteration to be split into two parallel execution paths. The innermost loop remains sequential, with no parallel execution.\n\nAdditionally, ensure that the accumulation of sums for `E`, `F`, and `G` are set to `0` at the beginning of the respective inner loops.\n\nIn code, the specific `#pragma` directives used for these descriptions would be:\n\n- Use `#pragma ACCEL kernel` to declare the function as a hardware kernel.\n- Use `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipeline inference in loop headers.\n- Use `#pragma ACCEL TILE FACTOR=auto{1}` to avoid splitting the loops into multiple chunks.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to ensure single iteration execution at a time within a loop.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{4}` for the specific loop in matrix `F` computation for parallelizing the innermost loop into 4 parallel execution paths.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the specific loop in matrix `G` computation to enable 2 parallel paths of execution. \n\nNote that for the `PIPELINE` directive, `auto{flatten}` is used in some inner loops to indicate the flattening of nested loops into a single pipeline.\n\nFor matrix multiplication loops:\n- Initialize the accumulator (e.g., `E[i][j]`, `F[i][j]`, `G[i][j]`) to `0.0` before the innermost loop starts its computation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code for the kernel function `kernel_3mm` performing matrix multiplications, request the following: \n\n1. Designate the function for acceleration using HLS pragmas.\n2. For all outer loops (`i` and `i`, `j` loops across the three matrix multiplication sections):\n    - Disable loop pipelining.\n    - Set tiling factors to not tile the loops (i.e., use the whole loop).\n    - Set parallel factors to prevent parallel execution of loop iterations.\n3. For the innermost loop (`k` loop) in the first matrix multiplication (`E := A*B`), allow the compiler to decide how many iterations to execute in parallel; default to 30 if auto-selection is unavailable.\n4. For the innermost loop in the second matrix multiplication (`F := C*D`), parallelize the iterations entirely irrespective of the parallel factor provided.\n5. For the innermost loop in the third matrix multiplication (`G := E*F`), process iterations sequentially, in series.\n6. Additionally, in the third matrix multiplication's `i` loop, enable complete loop flattening if loop pipelining is turned off.\n\nSpecifically mention each `#pragma` directive followed by the choice of the option. For example, to indicate not tiling a loop, say 'Do not tile the loop,' which corresponds to `#pragma ACCEL TILE FACTOR=auto{1}`. If the `#pragma` directive has an `auto` option, explain it should default to the value within curly braces if auto-selection is disabled. To describe the directive for treating a function as a hardware acceleration kernel, use 'Designate the entire function for hardware acceleration.'", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, the overarching instruction you require is as follows:\n\n\"Create a kernel function that performs a two-matrix multiplication (2mm) operation with the matrices A, B, and C to update matrix D. The operation should calculate D as D equals the product of alpha times A times B times C plus beta times D. The size specifications for the matrices should be 40x70 for A, 70x50 for B, 50x80 for C, and 40x80 for D. Additionally, include intermediate storage as a temporary matrix tmp sized 40x50. Ensure that all loops in the function are not automatically pipelined, do not tile the loops, and run the iterations of each loop sequentially one after another\u2014meaning no iteration-level parallelism should be applied. However, within the innermost loops that accumulate the values for tmp and D, apply parallel reduction with a factor of 7 for tmp and 25 for D respectively.\"\n\nSpecifically, for each `#pragma` value, the descriptions would be:\n\n- `#pragma ACCEL kernel`: Treat the following function as a hardware kernel for acceleration.\n  \n- `#pragma ACCEL PIPELINE auto{off}`: Do not automatically pipeline this loop. It should run in its entirety for each iteration without pipelining optimization.\n\n- `#pragma ACCEL TILE FACTOR=auto{1}`: Process this loop in its original, full form without breaking it into smaller chunks, which means no tiling should be applied.\n\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Run the iterations of this loop one after the other, not in parallel.\n\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}`: Apply a parallel reduction to this loop for the tmp matrix with a factor of 7, allowing for some iterations to be combined together in parallel for faster reduction.\n\n- `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}`: Apply a parallel reduction to this loop for the D matrix with a factor of 25 to accelerate the computation through parallelism.\n\nThis detailed instruction will produce the desired HLS code with the necessary pragma directives to control the loop optimizations for a 2mm operation in a manner suitable for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Within the function, for the top-level loop, prevent any automatic pipelining optimization. Inside the first loop 'i', for the 'j' sub-loop, automatically determine the pipelining strategy and set parallel processing factor to 2. When computing the intermediate 'tmp' array, use a reduction with parallel factor 70.\n\nFor the second top-level 'i' loop, enable automatic pipelining with a flattened hierarchy. In the 'j' sub-loop for updating the 'D' matrix, set parallel processing factor to 5 and apply reduction with a factor of 1 for the innermost 'k' loop. Ensure that all loops execute in their full original form without tiling optimization by setting the tile factor to 1 wherever the 'auto' is followed by an open curly brace and 1 in curly braces.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_3mm` as a target for hardware acceleration, and within this function, for the calculation of matrices E, F, and G, proceed as follows:\n\n1. In calculating matrix `E`, disable automatic pipelining of the outer loop, and ensure that both the outer and inner loops operate without tiling and in a non-parallel fashion, processing iterations sequentially. In the innermost k-loop, where elements are accumulated, allow the iterations to run sequentially without parallelization.\n\n2. For matrix `F`, apply the same approach as matrix `E`: Turn off automatic pipelining and disable tiling and parallel processing for all loops, ensuring sequential execution.\n\n3. When computing matrix `G`, turn off automatic pipelining at the outermost i-loop, preserve the loop's full form without tiling, and set the inner j-loop parallel factor to 7 to allow some degree of parallel processing. Within the innermost k-loop, set a parallel factor of 8 to permit eight iterations to be processed concurrently. \n\nIn code:\n\n- Use `#pragma ACCEL kernel` to mark the function for hardware acceleration.\n\n- For each loop nest calculating matrices `E`, `F`, and `G`:\n  - Use `#pragma ACCEL PIPELINE auto{off}` directly before the outer loop to disable automatic loop pipelining.\n  - Use `#pragma ACCEL TILE FACTOR=auto{1}` before each loop to prevent tiling.\n  - Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` before each loop, except for the innermost loops of `G`, to ensure sequential processing.\n  - Specifically for `G`, set `#pragma ACCEL PARALLEL FACTOR=auto{7}` before the j-loop, and `#pragma ACCEL PARALLEL FACTOR=auto{8}` before the k-loop for limited parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For the first 'i' loop, disable automatic pipelining, enforce no loop tiling, and run iterations sequentially. Inside this loop, enable loop flattening when pipelining the 'j' loop, avoid loop tiling, and run two iterations of 'j' in parallel. For the innermost 'k' loop under 'j', allow parallel execution with a single iteration at a time and enable reduction on the 'tmp' array.\n\nFor the second 'i' loop, again disable automatic pipelining, enforce no loop tiling, and run iterations sequentially. Within this loop, disable automatic pipelining for 'j', avoid loop tiling, and run eight 'j' iterations simultaneously. In the 'k' loop nested within 'j', allow a parallel reduction operation on the 'D' array with up to fifty concurrent iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following computational kernel into an HLS code with specific directives to manage its pipelining, tiling, and loop parallelization:\n\n1. Designate the entire function as a hardware acceleration kernel.\n2. Inhibit loop pipelining optimization for each of the three major matrix multiplication sections (E=A*B, F=C*D, G=E*F).\n3. Specify that loops should not be tiled, meaning each of them should be executed in its entirety without segmentation.\n4. Enforce sequential execution for the most outer loops of each matrix multiplication section.\n5. Allow parallel execution within the innermost loop of the first matrix multiplication (E=A*B) with a factor of 6, in the second one (F=C*D) with a factor of 8, and in the third one (G=E*F) with a factor of 5, both for the loop itself and its nested innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS accelerated kernel function named `kernel_3mm` that performs three matrix-matrix multiplications on the input matrices `A`, `B`, `C`, `D`, and outputs the results to matrices `E`, `F`, `G` respectively, with restrictions on optimization:\n\n1. For the entire function `kernel_3mm`, designate it as a hardware acceleration kernel.\n2. Within the computation of matrix `E`, avoid automatic pipelining for the outermost loop through the `i` index; instead, retain the original loop without pipelining.\n3. Use tiling with a factor of 1, which maintains the full loop structure without dividing it into tiles, for the loop through the `i` index when computing matrix `E`.\n4. Execute the loop through the `i` index serially, without parallelism, when computing matrix `E`.\n5. For the innermost loop through the `j` index in the computation of matrix `E`, automatically flatten the nested loops if possible, allowing for inner loop optimization.\n6. For the innermost loop through the `k` index in the computation of matrix `E`, allow only one iteration to run at a time to maintain serial execution.\n\nRepeat the same pragmas for computing matrix `F`, but allow the innermost loop through the `k` index to be parallelized with 16 iterations able to be processed concurrently, rather than just one.\n\nFor the computation of matrix `G`, apply the same configuration as the computation of matrix `E`, but allow 2 iterations of the innermost loop through the `j` index to be processed in parallel.\n\nThe respective pragma directives will look like this:\n\nFor the whole function:\n- `#pragma ACCEL kernel` to accelerate the function as a whole.\n\nFor matrix `E` computation:\n- `#pragma ACCEL PIPELINE auto{off}` to avoid automatic pipelining of the outermost loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to prevent tiling of the loop.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to avoid parallelizing the loop.\n- `#pragma ACCEL PIPELINE auto{flatten}` to flatten nested loops in the innermost loop, optimizing its execution.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` for serial execution of the loop through `k`.\n\nFor matrix `F` computation:\n- Use the same pragmas as for matrix `E` but with `#pragma ACCEL PARALLEL FACTOR=auto{16}` for the `k` loop, allowing up to 16 concurrent iterations.\n\nFor matrix `G` computation:\n- Use the same pragmas as for matrix `E` but with `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the `j` loop, allowing 2 concurrent iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Enable hardware acceleration for the function 'kernel_2mm' which performs matrix multiplication and addition. In the first pair of nested loops, deactivate automatic pipelining, use single-tile execution, and run the outer loop without parallelism, but allow the inner loop to run two iterations in parallel. Within the innermost loop, enable parallel reduction across all iterations for the 'tmp' array. In the second pair of nested loops, switch on default pipelining, run outer loop without pipelining but flattened for better optimization, enforce single-tile process with the entire loop being processed as a whole, and parallelize the outermost loop with 80 iterations in parallel. Within the innermost loop, apply parallel reduction with only serial execution for updating the 'D' matrix. The relevant #pragma values are: kernel, PIPELINE \"off\", TILE FACTOR=1, PARALLEL FACTOR=1, PARALLEL FACTOR=2, parallel reduction with dynamic FACTOR, and PIPELINE with flatten option.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) code for a hardware-accelerated 2mm kernel function with the following specifications: \n\n- Declare the function as a hardware acceleration kernel. \n- In the first main loop, prevent pipelining, do not tile the loop, and execute all loop iterations sequentially.\n- Within the nested loop, again prevent pipelining and tile the loop with a factor of 1, but allow four parallel loop iterations.\n- In the third loop, addressing the 'tmp' calculation, allow two iterations to execute in parallel with enabled reduction on 'tmp'.\n- For the second main loop, which updates matrix 'D', switch off automatic pipelining, tile this loop with a factor of 2, and maintain sequential execution order.\n- Lastly, in the nested loop, use a reduction on 'D' and set the parallel factor to 25 to allow 25 iterations to run concurrently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware-accelerated 2mm kernel function with specific optimization directives for loops. Apply the following optimization strategies:\n\n1. Mark the function as a hardware accelerator kernel.\n2. Inhibit automatic pipelining of the outermost loops.\n3. Do not tile the outer loops, keeping their iterations whole.\n4. Run the outer loops without parallelism, forcing sequential iteration.\n5. Enable automatic pipelining for the second-level loop that initializes `tmp`, adjusting it as needed.\n6. Force the second-level loop that initializes `tmp` to run without tiling.\n7. For this same loop, enable a parallel factor of 4.\n8. Apply parallelism with a factor of 1 to the innermost loop inside the loop that initializes `tmp`, enabling reduction on `tmp` to avoid dependency issues.\n9. For the loop updating `D`, disable automatic pipelining but allow the compiler to decide if flattening is beneficial.\n10. Keep iterations of the second-level loop that updates `D` whole, without tiling.\n11. Employ a high degree of parallelism with a factor of 10 in the same loop.\n12. Within the innermost loop updating `D`, use parallelism with a factor of 1 and enable reduction on `D` to maintain data coherence.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Designate the function as a hardware acceleration kernel, and within that, for each two-dimensional matrix multiplication within the three nested loops (E=A*B, F=C*D, G=E*F), disable automatic pipelining for the outer loops. Set the tile factor to 1 to avoid loop tiling. Require that the innermost loop of the E=A*B and F=C*D computations run sequentially, while it is partially parallelized with a factor of 30 for E=A*B and 5 for F=C*D, allowing some concurrent computations. For the G=E*F computation, allow partial parallelism in both the middle and the innermost loops with parallel factors of 7 and 5 respectively, thus permitting more concurrency than in the previous computations.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate the HLS C code that provides hardware acceleration for a 2mm kernel, where matrix operations are performed serially without pipelining or parallelization for the main loop iterations, and the nested loops have varying degrees of parallelism and pipelining optimizations. Use pragma directives to accomplish the following:\n\n1. Define the function as a hardware acceleration kernel.\n2. Disable automatic loop pipelining for the outermost loops.\n3. Prevent the outermost loops from being tiled or parallelized.\n4. Enable loop pipelining with default settings for the first nested loop and keep it from being tiled, while allowing four iterations of the innermost loop of the first nested loop to be processed in parallel.\n5. Perform a reduction on the 'tmp' array with a factor of 2 to enable parallel accumulation of results in the innermost loop of the first nested loop.\n6. For the second nested loop, use the default loop flattening for pipelining.\n7. Perform a straightforward reduction using the 'D' array without parallelization in the innermost loop of the second nested loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For the first level loop, disable automatic pipelining, maintain the original loop structure without tiling, and execute iterations sequentially without parallelization. Inside this loop, enable automatic pipelining for the second level loop, do not break apart the loop, but allow four iterations to run in parallel, and for the reduction operation on 'tmp', parallelize with a factor of ten. For the subsequent loop outside the parallel region, again disable automatic pipelining, maintain the loop structure, and enforce sequential execution. Within this loop, flatten the iterations when pipelining, tile the loop with a factor of two, and allow 16 iterations to run in parallel, while for the reduction on 'D', do not parallelize.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function named 'kernel_3mm' with parameters `ni`, `nj`, `nk`, `nl`, and `nm` representing matrix dimensions, and matrices `E`, `A`, `B`, `F`, `C`, `D`, and `G`. Disable automatic pipelining and ensure the loops are not subdivided by setting tiling to the original loop size, and that iterations are executed one after another by specifying a parallel factor of one for all loops except the last nested loop within the third matrix multiplication where a parallel factor of seven for the outer loop and two for the inner loop are used. Additionally, nest three separate matrix multiplication operations within the kernel and apply loop flattening only to the inner loops of the first two matrix multiplications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel with the function `kernel_2mm` for matrix computations with no automatic pipelining, and no automatic loop tiling or parallelization. For the first loop nest, enable pipelining but do not change the tiling or parallelization. Perform reduction on the variable `tmp` with parallel factor 1. For the second loop nest, disallow automatic pipelining, flatten the pipeline, and use the maximum parallel factor for the second loop. Perform reduction on the variable `D` with parallel factor 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix multiplication kernel that avoids automatic pipelining of the top-level loops but allows for inner loop optimizations. Mark the function as a hardware kernel and manually manage the optimization for the computation of E=A*B, F=C*D, and G=E*F matrices as follows: \n\n1. In the multiplication of A and B, as well as C and D, disable automatic pipelining for the two outer loops. However, perform the k loop in parallel with a factor of 16 and 32, respectively, for improved parallelism.\n\n2. In the computation of G, disable pipelining for the i loop, and parallelize the j loop's iterations with a factor of 7. Within the innermost k loop, use a high degree of parallelism with a factor of 50.\n\n3. For the loops where automatic pipelining is prevented, allow the inner loops to be pipelined by not specifying 'off' and use a tiling factor of 1 to ensure loops are executed in their full original form without being broken down.\n\nAppropriately apply '#pragma ACCEL PIPELINE', '#pragma ACCEL TILE', and '#pragma ACCEL PARALLEL' directives to achieve the described behavior, letting the HLS tool choose the optimal settings when the 'auto' keyword is present without an explicit factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code to perform a three-matrix multiplication where the first operation is A*B=E, the second is C*D=F, and the final is E*F=G, while ensuring:\n\n1. Treat the function 'kernel_3mm' as a dedicated hardware kernel for acceleration.\n2. For the outermost computational loops for each matrix multiplication (E, F, G calculations), do not apply automatic pipelining or any optimizations that alter the loop control flow \u2013 allowing the loops to run in their original sequential order.\n3. Within these outermost loops, for the innermost loops that initialize matrix E and F to zeros, ensure that the pipelining is set to automatic while preventing any tiling or parallel execution changes.\n4. For the multiplication accumulation loops (A*B, C*D, E*F), enforce the following:\n    - For E accumulation (A*B), parallelize the k loop iterations (innermost) with a parallelization factor of 60.\n    - For F accumulation (C*D), parallelize the k loop iterations with a parallelization factor of 16.\n    - For G accumulation (E*F), pipeline the i-j nested loops while flattening their execution and set a parallelization factor of 4 for the j loop, without parallelizing k loop (keep a factor of 1).\n\nSpecific pragma instructions to be used are:\n- '#pragma ACCEL kernel' to mark the function as a hardware acceleration kernel.\n- '#pragma ACCEL PIPELINE \"off\"' to avoid automatic pipelining of a loop.\n- '#pragma ACCEL TILE FACTOR=1' to prevent tiling optimizations.\n- '#pragma ACCEL PARALLEL FACTOR=1' to avoid parallel execution for the corresponding loop, with the exception that:\n   - for E[i][j] accumulation, use '#pragma ACCEL PARALLEL FACTOR=60',\n   - for F[i][j] accumulation, use '#pragma ACCEL PARALLEL FACTOR=16',\n   - and for G[i][j] accumulation within the innermost (k) loop, use '#pragma ACCEL PARALLEL FACTOR=1'.\n- For G[i][j] accumulation within the i-j loop, use '#pragma ACCEL PIPELINE auto{flatten}' to enable pipelining with loop flattening and a parallelization factor of 4 for the j loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 3-matrix multiplication kernel where:\n1. The main function is designated as a hardware kernel for acceleration.\n2. For the 'E=A*B' computation:\n   - Disable automatic pipelining for the outermost loop.\n   - Keep the first inner loop in its original, full form without tiling or parallelizing.\n   - Flatten the pipelining of the second inner loop and execute iterations serially.\n3. For the 'F=C*D' computation:\n   - Similarly to 'E=A*B', do not automatically pipeline, tile, or parallelize the first level of loops and complete all iterations consecutively.\n   - Then, flatten pipelining for the inner loop without parallel execution.\n4. For the 'G=E*F' computation:\n   - Turn off automatic pipelining for both loops.\n   - On the first level of the loop, do not tile but run 7 iterations in parallel.\n   - Within the inner loop, process 50 iterations in parallel without pipelining.\n\nDenote these settings by using appropriate `#pragma ACCEL` directives before each loop, specifying 'off' to disable features and numerically indicating factors for tiling and parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel for a 2mm computation, where:\n- The top-level loops for processing the matrices should not be pipelined, and should run in their complete form without being split into smaller parts, executing iterations sequentially.\n- For the first set of nested loops, prevent pipelining and tiling for the outer loop, but parallelize the inner loop with a parallelization factor of 4, and further, within this inner loop, parallelize the reduction of 'tmp' with a parallelization factor of 32.\n- For the second set of nested loops, flatten the pipelining of the outer loop, keep it un-tiled, and run the inner parallel reduction loop for 'D' sequentially, without parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, instruct the code generation system as follows:\n\nFor the function to be considered a hardware acceleration kernel without any automated loop pipelining, apply '#pragma ACCEL kernel' at the beginning of the function and use '#pragma ACCEL PIPELINE auto{off}' before the main for-loops for each matrix multiplication section.\n\nEnsure that the execution of all loops proceeds in a strictly sequential manner, without tiling or parallelization optimizations, by specifying '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}' before each nested loop.\n\nThen, within the innermost loops of the matrix multiplications, specify different levels of loop unrolling to allow some degrees of parallel execution: use '#pragma ACCEL PARALLEL FACTOR=auto{32}' for loops calculating matrices E and F, enabling 32 simultaneous iterations for the k-loop, and apply varying parallel factors for the innermost loops of the G matrix multiplication step, with '#pragma ACCEL PARALLEL FACTOR=auto{7}' outside the j-loop and '#pragma ACCEL PARALLEL FACTOR=auto{5}' within the k-loop for a tailored level of parallelism for these specific loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the `kernel_2mm` function as a target for hardware acceleration. Within this function, structure the first matrix multiplication loop such that the outer loop is not pipelined or parallelized and operates without tiling, while the intermediate loop has optional pipelining, no tiling, and limited parallelism with a factor of 4, and the innermost loop allows pipelining with a parallel reduction on the `tmp` matrix with a parallel factor of 2. In the second matrix multiplication loop, again ensure the outer loop is not pipelined, tiled or parallelized, while the intermediate loop should have optional pipelining, moderate tiling with a factor of 2, and default parallelism, and the innermost loop should be capable of parallel reduction on matrix `D` with a high parallel factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. In the first loop nest, prevent pipelining, don't partition the iteration space, and execute iterations serially. Within the second-level loop, enable loop flattening, maintain a single chunk of iterations, and execute every two iterations in parallel up to a factor of 2. Reduce the `tmp` array in serial order within the innermost loop.\n\nIn the second loop nest, enable automatic pipelining, do not tile, and run iterations one by one. Within the nested loop, disable pipelining, tile every two iterations, and parallelize every five iterations up to a factor of 5. Reduce the array `D` in the inner loop, considering a parallel factor of 32.\n\nSpecifically, use the following pragma directives:\n\n1. Treat `kernel_2mm` as a hardware kernel: `#pragma ACCEL kernel`\n2. Prevent pipelining in the first `i` loop: `#pragma ACCEL PIPELINE auto{off}`\n3. Use the default tiling factor and prevent splitting the `i` loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Run `i` loop iterations in series: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n5. Enable loop flattening in the second-level `j` loop: `#pragma ACCEL PIPELINE auto{flatten}`\n6. Maintain a single chunk in the `j` loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n7. Execute every pair of `j` loop iterations: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n8. Run the `k` loop with a serial reduction of the `tmp` array: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`\n\nMove to the second loop nest:\n\n9. Enable automatic pipelining for the `i` loop: `#pragma ACCEL PIPELINE auto{}`\n10. Do not tile the `i` loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n11. Serial execution for `i` loop iterations: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n12. Disable pipelining for the `j` loop: `#pragma ACCEL PIPELINE auto{off}`\n13. Tile the `j` loop every two iterations: `#pragma ACCEL TILE FACTOR=auto{2}`\n14. Parallelize the `j` loop every five iterations: `#pragma ACCEL PARALLEL FACTOR=auto{5}`\n15. Use a parallel reduction on the `D` array within the `k` loop, with a factor of 32: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function 'kernel_3mm' to perform three matrix-matrix multiplications (E=A*B, F=C*D, and G=E*F) without automatic loop pipelining, tiling or parallel processing optimizations for the outer loops. For the innermost loop of the first multiplication, run the loop iterations in parallel with a factor corresponding to the iteration count. For the innermost loop of the second multiplication, use the maximum possible parallelization factor. For the G matrix calculation, flatten loop pipelining only for the outermost 'i' loop and parallelize the 'j' loop with a factor of seven, but process the innermost 'k' loop sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following matrix multiplication and addition operations into specialized hardware instructions for FPGA acceleration without automatic parallel pipeline optimization, but allow parallel execution with specified factors:\n\n1. Prepare the function 'kernel_2mm' to be compiled as a hardware kernel for dedicated acceleration.\n2. For the first top-level loop (iteration over 'i'), disable auto-pipelining, and execute the loop iterations in sequence with no tiling or parallelization optimizations applied.\n3. Within the nested loop (iteration over 'j'), again disable auto-pipelining, ensure each iteration is fully executed before the next begins (no tiling), and set the innermost loop (iteration over 'k') to allow for parallel execution of four loop iterations simultaneously with a reduction operation on 'tmp'.\n4. For the second top-level loop (iteration over 'i'), turn off auto-pipelining but enable loop flattening optimizations within the nested loop over 'j'. Execute both of these loops without any tiling or parallelization optimizations applied.\n5. Inside the innermost loop (iteration over 'k'), allow for parallel execution of each single loop iteration without reduction since each computation is independent.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a triple matrix multiplication kernel, labeling the main function as a hardware kernel, turning off automatic pipelining, keeping all loops in their original full form without tiling, and specifying parallelization factors for certain loops. Specifically, disable pipelining for the outer loops of each matrix multiplication, allow a parallelization factor of 3 for the innermost loop of the first matrix multiplication, a factor of 5 for the innermost loops of the second and third matrix multiplications, and set a factor of 7 for the parallel loop in the compute of the final multiplication result.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{3}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a function named `kernel_2mm` that computes the matrix operation `D := alpha*A*B*C + beta*D`, where `A`, `B`, `C`, `D` are matrices with dimensions `(40, 70)`, `(70, 50)`, `(50, 80)`, `(40, 80)` respectively, `tmp` is a `(40, 50)` temporary matrix, and `alpha`, `beta` are scalar values. The function should be marked as a hardware kernel for acceleration.\n\nFor the outer two loops iterating over 40 and 50 elements, disable automatic pipelining, enforce no tiling and no parallelization (default single iteration at a time). In the first nested `k` loop for computing `tmp`, use parallel reduction on `tmp` with all 70 iterations to potentially be run in parallel.\n\nFor the subsequent outer two loops iterating over 40 and 80 elements, enable automatic pipelining for the first `i` loop, enforce no tiling, and allow the inner `j` loop to have 8 iterations running in parallel. In the final nested `k` loop for updating `D`, use parallel reduction on `D` and set to potentially run all 50 iterations in parallel.\n\nSpecific #pragma directives in the stated order:\n\n1. Treat the following function as a hardware kernel for acceleration:\n`#pragma ACCEL kernel`\n\n2. For both `i` loops and the first `j` loop, disable automatic pipelining:\n`#pragma ACCEL PIPELINE auto{off}`\n\n3. For both `i` loops and both `j` loops, prohibit loop tiling to avoid breaking them into smaller chunks:\n`#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. For the first `i` loop, enforce sequential execution. For the first `j` loop and the second `i` loop, execute 1 and 1 iterations in parallel respectively (essentially sequential execution):\n`#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n5. For the second `j` loop, parallelize with 8 iterations able to run concurrently:\n`#pragma ACCEL PARALLEL FACTOR=auto{8}`\n\n6. In the first `k` loop for the temporary matrix `tmp`, enable parallel reduction with the potential to execute all 70 iterations in parallel:\n`#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{70}`\n\n7. Re-enable automatic pipelining for the second outer `i` loop:\n`#pragma ACCEL PIPELINE auto{}`\n\n8. In the final `k` loop for matrix `D`, enable parallel reduction with the possibility of running all 50 iterations in parallel:\n`#pragma ACCEL PARALLEL reduction=D FACTOR=auto{50}`\n\nThis instruction creates the provided code with specific #pragma values to control the loop execution strategy, optimizing it for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' for acceleration and prevent the loops from being automatically pipelined. The first nested loop should not be broken into tiles nor processed in parallel, but allow for a parallelization factor of 4 within the innermost loop of the first major loop with a reduction operation applied to 'tmp', using up to a factor of 10 parallel accumulations. For the second major loop, permit the automatic flattening of the pipeline hierarchy, set a tiling factor of 2, and use a parallelization factor of 16 for the corresponding loop nest, but process the innermost loop of the second major loop sequentially with a reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to recognize a function as a hardware acceleration candidate, and configure how loops within this function should behave during code transformation. Specifically, disable automatic pipelining for the outermost loops of three matrix multiplication processes, but allow the innermost loop in the first multiplication to be completely unrolled. For the second multiplication's innermost loop, perform unrolling by a factor of four. In the final multiplication, set the second loop to unroll seven iterations concurrently, and again unroll the innermost loop by a factor of four. Ensure that all loops are not tiled, meaning they should execute in their full, original dimensions without being divided into smaller blocks for processing.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a three-matrix multiplication kernel without applying any automatic pipelining to the outer loops, and use parallelization with the maximum allowed factor for the innermost loops. Specifically, set the outer loops to not be auto-pipelined, not tiled, and to run iterations serially. For the matrix E calculation, parallelize the innermost loop with a factor that unrolls the loop completely. In the matrix F calculation, flatten the pipeline for the j-loop and run the k-loop serially. Finally, for matrix G, parallelize the last loop of the i-loop with a factor of 7 and the innermost loop with a factor of 5. Treat the entire function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code that treats the function `kernel_2mm` as a hardware acceleration kernel, processing the matrix multiplication and addition as indicated in the function. Disable automatic pipelining for all loops and process the outermost loops without breaking them into smaller chunks or parallelizing them. Within the first nested loop block, set the parallel factor to 4, and for the loop inside it, reduce the `tmp` matrix with a parallel factor of 14. In the second nested loop block, flatten the pipelining, tile with a factor of 2, set the parallel factor to 16, and within its nested loop, use a parallel reduction on the `D` matrix without additional parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the function \"kernel_2mm\" as an acceleration kernel, apply no pipelining to the top-level loops, use the smallest tiling and parallel factors for processing while forcing the inner loops to use specific tiling and parallel factors as well as reductions for accumulating results, through the following steps:\n\n1. Identify \"kernel_2mm\" as a function for hardware acceleration.\n2. Do not pipeline the outer loops 'i' iterating over dimensions 40 and 50 respectively.\n3. Both outer 'i' loops should be processed entirely without partitioning into smaller chunks or parallelizing their iterations.\n4. In the first inner 'j' loop, pipeline each iteration automatically while keeping the loop in its full form and executing two iterations in parallel.\n5. Enable parallel reduction for the innermost 'k' loop, accumulating into the 'tmp' array with an unspecified degree of parallelism, implying using as many resources as needed.\n6. For the second 'i' loop, automatically infer the pipelining strategy and flatten nested loops to maximize throughput.\n7. Tile the second 'j' loop with a factor of 8, parallelizing 8 iterations at a time.\n8. Do not parallelize the innermost 'k' loop for reduction into 'D', execute it serially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be synthesized into a hardware acceleration kernel, and apply specific optimization directives to various loops within the function as follows:\n\n1. In the first pair of nested loops where the 'tmp' array is computed:\n    - Do not automatically pipeline the outer loop that iterates over the 'i' index.\n    - Do not tile (split) the outer 'i' loop into smaller chunks.\n    - Execute the iterations of the outer 'i' loop sequentially, not in parallel.\n    - Within the inner 'j' loop, also prevent automatic pipelining.\n    - Do not tile the inner 'j' loop.\n    - Execute every fourth iteration of the inner 'j' loop in parallel.\n    - For the k-loop inside the 'j' loop, allow exactly two iterations to be processed in parallel and specify that any partial sums for 'tmp[i][j]' are to be reduced at the end of the parallel section.\n\n2. In the second pair of nested loops where the 'D' array is updated:\n    - Suppress automatic pipelining for the outer loop iterating over the 'i' index.\n    - Do not tile the outer 'i' loop.\n    - Do not parallelize the outer 'i' loop, insisting on sequential execution.\n    - On the contrary, enable automatic pipelining for the inner 'j' loop.\n    - Do not break the inner 'j' loop into smaller chunks.\n    - Make sure the inner 'j' loop iterations are to be run in sequence, not in parallel.\n    - For the k-loop within the 'j' loop, parallelize 25 iterations at a time and enable reduction on 'D[i][j]' to accumulate the results of the parallel computation. \n\nEach pragma should be specified with a directive and an 'auto' value followed by the specific setting within curly braces, as already demonstrated in the given code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, but avoid pipelining, tiling, or parallelizing the outer two loops and keep the iterations sequential. In the first inner loop associated with `tmp` initialization, do not pipeline or tile, but allow four iterations to run in parallel. In the computation of `tmp`, enable a reduction operation with ten parallel elements. In the second inner loop for updating `D`, enable pipelining (automatically configured), no tiling, and parallel execution of sixteen iterations. For final computation involving `D`, allow reduction with two elements processed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2D matrix multiplication kernel that sequentially executes the outer loops without pipelining, processes the first inner loop sequentially and the second inner loop with a parallel factor of 4, prohibits automatic tiling and pipelining optimizations for all loops, and handles the last nested loop with a parallel factor of 16 while flattening the immediately surrounding loop to enable pipelining. Apply parallel reduction with a factor of 2 for the accumulation of `tmp` array and with a factor of 1 for `D` array updates. The function should be treated as a hardware kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, disable automatic pipelining, maintain original loop structures without tiling or parallelization, except for the innermost loops which apply parallel reductions with specified factors to efficiently compute a matrix-matrix multiplication followed by a scaled matrix addition.\n\nSpecifically, use the following directives:\n\n- Apply `#pragma ACCEL kernel` to mark 'kernel_2mm' function for hardware acceleration.\n- Disable pipelining throughout the outer loops with `#pragma ACCEL PIPELINE off`.\n- Prevent outer loops from being broken into smaller chunks with `#pragma ACCEL TILE FACTOR=1`.\n- Specify that the outer loops should execute sequentially with `#pragma ACCEL PARALLEL FACTOR=1`.\n- Within the first nested loop computing `tmp[i][j]`, still bypass pipelining but allow the innermost 'k' loop to perform parallel reduction on `tmp` with a factor of 7 using `#pragma ACCEL PARALLEL reduction=tmp FACTOR=7`.\n- Maintain standard pipelining without any modifications for the second nested loop with `#pragma ACCEL PIPELINE`.\n- Execute the 'j' loop for updating `D[i][j]` with a flattened pipeline structure using `#pragma ACCEL PIPELINE flatten` and parallelize its innermost 'k' loop with a parallelization factor of 8 using `#pragma ACCEL PARALLEL FACTOR=8`, while the innermost parallel reduction on `D` remains non-parallelized with `#pragma ACCEL PARALLEL reduction=D FACTOR=1`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Write an HLS C code for a 3-matrix multiplication kernel where the first multiplication result `E` is between matrices `A` and `B`, the second multiplication result `F` is between matrices `C` and `D`, and the third multiplication result `G` is between `E` and `F`. For each multiplication, disable automatic pipelining for the outermost loop and process the loop without tiling or running the iterations in parallel, effectively leaving the loop completely unoptimized. Inside the second level loop, enable pipelining without specific directives, meaning it should be automatically optimized by the compiler. When computing the `E` matrix, fully parallelize the innermost loop as the second dimension of `B` and `A` is fully unrolled with a factor of 60. When computing the `F` matrix, parallelize the innermost loop with a parallel factor of 16, which indicates partial unrolling based on the hardware resources. For the final multiplication resulting in `G`, flatten the second level loops and parallelize the innermost loop with a factor of 7, a partial unrolling matching the last dimension of `F`. Leave the final innermost loop for computing `G` without special parallelization, indicating no unrolling.\n\nEnsure you identify the kernel function for hardware acceleration with the appropriate directive. Use 'auto' for pragmas where the actual factor or setting is not specified, but provide the specific values where indicated, such as 60, 16, 1, and 7, for the respective parallel factors.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Disable automatic pipelining for the outermost loops but leave it enabled for nested loops, unless specified otherwise. Prevent any loop tiling optimizations and execute outermost loops in order without parallelization. In the first double nested loop, run the innermost loop with a parallel factor of 4, but do not enable reduction parallelism. Within the second double nested loop, flatten the nested loops into a single loop level for pipelining purposes and run the innermost loop with a parallel factor of 10, while still avoiding reduction parallelism for it.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following High-Level Synthesis (HLS) directives into a single, comprehensive instruction for generating appropriate C code with specific pragma values:\n\n- Treat the function as a hardware acceleration kernel.\n- For the top-level loop in the first loop nest, prevent automatic pipelining, execute the loop iterations entirely before moving to the next iteration, and do not run iterations in parallel.\n- For the second-level loop in this nest, again inhibit pipelining, but for concurrent execution, allow up to four iterations to proceed in parallel.\n- For the innermost loop, enable a parallel reduction operation on the `tmp` array with up to sixteen operations happening concurrently.\n- In the second loop nest, specify the same pipelining and tiling preferences as the first top-level loop, but allow up to two iterations to proceed in parallel for the second-level loop.\n- Finally, for the inner loop of the second nest, allow aggressive parallelization of up to fifty iterations to contribute to the concurrent reduction operation on the `D` array.\n\nPlease ensure all specified pragma values are correctly incorporated into the HLS codes effectively to meet the desired constraints and optimizations for each loop and operation as intended.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a three-matrix multiplication kernel with specific pragmas that dictate the following constraints: \n\n1. The outer function should be marked for acceleration but without any internal loop being automatically pipelined.\n2. All loops should be processed in their entirety without tiling or running iterations in parallel, except:\n3. In the first matrix multiplication (E = A*B), the innermost loop should fully unroll to exploit parallelism inherent in the matrix multiplication across all elements of 'B'.\n4. In the second matrix multiplication (F = C*D), the first inner loop should be flattened (potentially merging loops for more efficient pipelining), but the innermost loop should not attempt any parallelism.\n5. In the final matrix multiplication (G = E*F), the second inner loop should be parallelized with a factor of 7, and the innermost loop should be parallelized with a factor of 16 to efficiently utilize hardware resources.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code to accelerate matrix multiplication across three nested loops while disabling automatic pipelining, restricting loop transformation by tiling with a factor of 1, and executing loop iterations serially except explicitly parallelized. Specifically, use the following directives: Treat the outer function 'kernel_3mm' as a hardware acceleration kernel, do not pipeline the outer two levels, but for the innermost loops, run two iterations in parallel for the first multiplication, run them serially for the second, and for the third, run five and eight iterations in parallel for the second and first inner loops respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to define a hardware kernel function 'kernel_3mm' that performs three matrix-matrix multiplications (E=A*B, F=C*D, and G=E*F) for fixed-size input matrices with dimensions as mentioned in the argument list. The first multiplication loop (E=A*B) should not be pipelined and should execute iterations in their original sequence without any tiling or parallelism, except for the innermost loop that has a parallelism factor of 6. The second multiplication loop (F=C*D) should have an inner loop pipelined by default, but the outer loops should not be pipelined or tiled, with the innermost loop's parallelism factor being 20. Finally, the third multiplication loop (G=E*F) should be set to run without any pipelining or tiling, except for the middle loop that has a parallelism factor of 8. Use the appropriate #pragma statements to control the pipelining, tiling, and parallelism of the corresponding loops in the function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for the matrix multiplication of three pairs of matrices, ensuring that each loop runs sequentially without pipelining, does not get tiled, and executes in a single thread, except for specific inner loops where you should maximize parallel processing up to the available hardware resources considering the dimensions of the matrices involved \u2013 particularly for the innermost loops where you process the full extent of the first and second pairs of matrices, and a parallel factor of 16 for processing the product of those results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a matrix multiplication kernel function named 'kernel_3mm' that performs three matrix multiplications (E = A * B, F = C * D, and G = E * F) using double-precision floating point numbers. Design the code to optimize for a hardware accelerator with the following specifications:\n\n1. The kernel function should be marked for acceleration.\n2. The outer loops for the matrix multiplications should not be automatically pipelined, remain unfolded, and run serially.\n3. The innermost loops should be automatically pipelined without specifying any constraints on the optimization process.\n4. For the multiplication resulting in matrix E, allow up to 12 operations in parallel within the innermost loop.\n5. For the multiplication resulting in matrix F, allow up to 32 operations in parallel within the innermost loop.\n6. For the multiplication resulting in matrix G, the outer loop should have a parallelism factor of 7 and the innermost loop should allow up to 50 parallel operations.\n\nEnsure that the loops are fully unrolled where parallelism factors are specified and also specify that the tiles for the loops should not be broken down into smaller chunks, thus having a tile factor of 1. Use automatic settings for unspecified #pragma values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the 'kernel_2mm' function as a hardware acceleration kernel, and ensure that the outer loops run sequentially while the inner loops leverage different types of parallel optimizations. Specifically, do not apply automatic pipelining to the outer loops; keep their iterations running one after another and process them without tiling, meaning each loop is treated as a whole. For the first inner loop, apply pipelining without specifying a strategy but restrict its parallelization to a single dimension, allowing for a factor of 4 concurrent executions. Enable aggressive parallelization across 14 iterations with a reduction on the 'tmp' array for the deepest loop in the first block. For the second block's inner loop, use loop flattening to combine loops, double the size of the execution chunks with tiling, and allow 16-fold parallel execution. Lastly, enforce sequential execution by not parallelizing the innermost loop in the second block.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following comprehensive but concise natural language instruction for the process of code execution and pragma values:\n\n\"Define a hardware kernel function that performs matrix multiplications on fixed-sized arrays without applying any loop pipelining, tiling, or parallel optimizations on the top-level loops. However, for the innermost loop of the first matrix multiplication, fully unroll the loop. In the second matrix multiplication, unroll the innermost loop by a factor of 16. For the final matrix multiplication, partially parallelize the middle loop by a factor of 8 and the innermost loop by a factor of 2.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as hardware kernel, prevent task pipelining in the outer loops, maintain original loop structure without tiling, enforce sequential execution for the top-level loops, but allow a specific level of parallelism within the nested loops, with special attention to perform reductions on temporary arrays using a specified number of processing elements.\n\n- Use '#pragma ACCEL kernel' to specify the function `kernel_2mm` as a hardware acceleration kernel.\n- Use '#pragma ACCEL PIPELINE \"off\"' to prevent pipelining of the first and second outer for-loops.\n- Use '#pragma ACCEL TILE FACTOR=1' to keep the original loop structure without tiling for all for-loops.\n- Use '#pragma ACCEL PARALLEL FACTOR=1' to enforce sequential loop execution without parallelism for the first outer for-loop and the second outer for-loop.\n- Use '#pragma ACCEL PIPELINE \"flatten\"' to fully flatten the nested loop iterations into a single pipeline stage for the first inner for-loop.\n- Use '#pragma ACCEL PARALLEL FACTOR=2' to enable parallel processing of two iterations at a time inside the second for-loop within the first major loop block.\n- Use '#pragma ACCEL PARALLEL reduction = tmp FACTOR=1' to perform the reduction operation on the `tmp` array with no parallelism (sequentially) in the third nested for-loop within the first major loop block.\n- Use '#pragma ACCEL PIPELINE auto{}' (with no off) to enable automatic pipeline optimization for the second major loop block.\n- Use '#pragma ACCEL PARALLEL FACTOR=8' to allow eight iterations to be processed in parallel in the second inner for-loop within the second major loop block.\n- Use '#pragma ACCEL PARALLEL reduction = D FACTOR=50' to perform reduction on the `D` array with a high degree of parallelism, using fifty processing elements, in the third nested for-loop within the second major loop block.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to define a hardware kernel that performs three separate matrix-matrix multiplications (E=A*B, F=C*D, G=E*F) with specific directives applied. For the E=A*B computation, ensure the outer loop does not pipeline, processes without tiling, and executes iterations serially; but fully parallelize the innermost loop across its iterations. During the F=C*D computation, flatten the pipelining of the second loop but keep other loops serial. Lastly, for G=E*F, have seven iterations of the second loop running in parallel and within it, eight iterations of the innermost loop executed concurrently, while the rest of the loops should not be pipelined or tiled. Use the following directive format, replacing 'auto{X}' with the specific value: '#pragma ACCEL DIRECTIVE_TYPE FACTOR=X'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code in C to implement a matrix multiplication kernel without pipelining or tiling optimizations and minimal parallelization except for specific loops. Label the function as a hardware kernel for acceleration. Specify no pipelining with '#pragma ACCEL PIPELINE auto{off}', use the default tile factor with '#pragma ACCEL TILE FACTOR=auto{1}', and set minimal parallelization with '#pragma ACCEL PARALLEL FACTOR=auto{1}' for all loops except for the innermost loops which should have a parallel factor of 60, 16, and 32 for the first, second, and third multiplication sections respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel, providing the following directives:\n\n1. Designate the `kernel_2mm` function itself as a hardware kernel that can be targeted for acceleration.\n2. For the top-level loops (those that iterate over 'i'), avoid automatic pipelining, maintain the loop's full iteration without tiling, and enforce sequential execution of iterations.\n3. Within the nested loop that initializes the 'tmp' matrix, disable automatic pipelining for the 'j' loop, prevent any loop tiling, and parallelize the innermost 'k' loop with a predetermined optimal factor, also include a directive to perform reduction on 'tmp' across the iterations.\n4. For the final matrix multiplication loop (updating matrix 'D'), fully flatten the nested 'j' loop so it's treated as a single loop by the compiler, apply a tiling factor that produces an optimal balance between parallelism and resource usage for the 'j' loop, and execute the innermost 'k' loop with aggressive parallelism. Avoid any kind of parallelism reduction for the innermost loop to ensure independent iterations run concurrently without any conflicts.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` to be a hardware kernel and implement the matrix multiplication and addition operations with varying degrees of optimization as follows:\n\n1. Disable loop pipelining for the outer loops of both matrix operations.\n2. Preserve the original loop organization by setting the tiling factor to 1 for each loop.\n3. Avoid parallel execution of iterations in the outer loops by setting the parallel factor to 1.\n4. Allow for pipelining within the second loop of the first matrix operation; however, it should not be aggressively optimized \u2013 use the default setting.\n5. Set the parallel reduction factor to 16 for the third loop in the first matrix operation to accumulate the results, employing 16-fold parallelism.\n6. Pipeline the second loop of the second matrix operation, and allow the pragma to decide the best way to flatten the loop structure for potentially improved performance \u2013 use 'flatten'.\n7. Exploit 16-fold parallelism in the second loop of the second matrix operation by setting the parallel factor to 16.\n8. In the innermost loops where the actual accumulation takes place (for both matrix operations), the parallel reduction factor should remain at the default of 1 to ensure correct results.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Concisely instruct the HLS tool to generate code for a 3-matrix multiplication kernel with all loops fully serialized, disabling automatic pipelining and tiling, yet allowing specified levels of parallelism in the innermost k-loops with a parallel factor of 60 for the A*B multiplication, a factor of 16 for the C*D multiplication, and a factor of 50 for the E*F multiplication.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware kernel, running the first level loop without pipelining or tiling and with no parallelism. Inside this, pipeline the second level loop, keep it unchunked with tiling set to 1, and allow for partial parallel processing with a factor of 4. In the deepest level, enable parallel reduction on 'tmp' with a high factor of 16. For the third level loop, disable pipelining, use a moderate tiling factor of 2, and a high parallel factor of 10. Finally, enable parallel reduction on 'D' with the maximum specified factor of 50 in the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a high-level synthesis (HLS) function `kernel_3mm` as a hardware accelerator to perform three matrix multiplications, where each of the nested loops for the multiplications is neither pipelined, tiled, nor parallelized, except for specific inner loops. In the innermost loop of the first multiplication (E=A*B), set the parallel factor to the size of the inner dimension (60). For the second multiplication (F=C*D), allow automatic pipeline optimization for the middle loop and set the innermost loop's parallel factor to the size of the inner dimension (80). In the final multiplication (G=E*F), allow loop flattening optimization for the middle loop, and set its parallel factor to 7, while keeping the innermost loop not parallelized.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function `kernel_3mm` to perform three matrix multiplications (E = A * B, F = C * D, G = E * F) on provided matrices with specific dimensions. For the E=A*B multiplication, turn off automatic pipelining, do not tile the loops, and execute loop iterations serially for the outer loops, while fully parallelizing the innermost loop with a parallelization factor of 60. For the F=C*D multiplication, again turn off automatic pipelining for the outer loops but flatten the pipeline of the inner loop, run all loops without tiling, and keep the innermost loop serial. Lastly, for the G=E*F multiplication, disable automatic pipelining and tiling for the outer loops, parallelize the middle loop with a factor of 8, and the innermost loop with a factor of 10.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the described HLS codes, provide the following instruction encompassing the three distinct matrix multiplication processes: \"Create a hardware kernel function that performs three separate matrix multiplications (E=A*B, F=C*D, and G=E*F) with varying degrees of parallelism, and ensure that the outer loops of each multiplication are not automatically pipelined nor tiled, but manually manage the pipeline and parallel properties of the inner loops with specific configurations.\"\n\nFor each matrix multiplication section in the function, use the following values for the respective pragmas:\n\n1. For the \"E=A*B\" multiplication:\n    - Outer loop 'i':\n        - Disable automatic pipelining using `#pragma ACCEL PIPELINE auto{off}`\n        - Set tiling factor to 1, effectively not tiling, using `#pragma ACCEL TILE FACTOR=auto{1}`\n        - Run the iterations serially using `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n    - Inner loop 'j':\n        - Enable pipelining without specifying a factor using `#pragma ACCEL PIPELINE auto{}`\n        - As with the outer loop, use `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n    - Innermost loop 'k':\n        - Fully parallelize the loop by setting the parallel factor to the loop limit using `#pragma ACCEL PARALLEL FACTOR=auto{60}`\n\n2. For the \"F=C*D\" multiplication:\n    - Outer loop 'i':\n        - Same pragmas as in the \"E=A*B\" section with automatic pipeline turned off.\n    - Inner loop 'j':\n        - Enable pipelining and allow the tool to choose whether to flatten the loop using `#pragma ACCEL PIPELINE auto{flatten}`\n        - Avoid tiling and parallelism on this level with `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n    - Innermost loop 'k':\n        - Run the loop iterations serially with `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n        \n3. For the \"G=E*F\" multiplication:\n    - Outer loop 'i':\n        - Disable pipelining, avoid tiling, and set the parallel factor as in the previous sections.\n    - Inner loop 'j':\n        - Set parallel factor to a portion of the loop limit, allowing for some parallelism using `#pragma ACCEL PARALLEL FACTOR=auto{7}`\n    - Innermost loop 'k':\n        - Increase parallelism further by setting the parallel factor closer to the loop limit using `#pragma ACCEL PARALLEL FACTOR=auto{16}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function called 'kernel_3mm' that performs matrix multiplication operations on three sets of matrices without any loop pipelining, maintaining the original single-threaded loop execution order. Specifically, for the loops iterating over matrix E and F multiplication, ensure all computations are performed sequentially without dividing them into smaller chunks. However, for the innermost loop multiplying matrices A and B, allow the loop iterations that calculate individual elements of matrix E to run in parallel up to 60 instances. Similarly, allow up to 16 parallel instances for computing elements of matrix F, but for the final multiplication resulting in matrix G, reduce the parallelism to 8 and 10 respectively for the two nested loops. Use 'off' to disable automatic loop pipelining, use '1' for the tile factor to prevent loop tiling, and specify the parallel factors as '60', '16', '8', and '10' for relevant loops to control the level of parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function named 'kernel_3mm' that performs matrix multiplication on 2D arrays representing matrices E=A*B, F=C*D, and G=E*F. For the E=A*B computation, do not pipeline or tile the outer two loops and process the innermost loop using 60-way parallelism. For the F=C*D computation, similarly do not pipeline or tile the outer two loops, but use 16-way parallelism for the innermost loop. For the final G=E*F computation, apply 8-way parallelism to the second outermost loop and 4-way parallelism to the inner loop, with no pipelining or tiling optimizations enabled for all loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware kernel for acceleration. For the first matrix multiplication 'E = A * B', automatically decide the best pipeline strategy while keeping the iterations unaltered and serial within each nested loop. Do not pipeline the second multiplication 'F = C * D', run its outer loop iterations five at a time in parallel while the inner loops remain serial. Similarly, do not pipeline the third multiplication 'G = E * F' and keep all of its loops running in their original serial form.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_2mm' as a dedicated hardware accelerator block, set all global pipeline and tiling optimizations off but individually optimize each nested loop: in the first 'i' loop, enable automatic pipelining, in the 'j' loop, enable automatic pipelining, tile by a factor of 1 and parallelize by a factor of 4, with a parallel reduction for 'tmp' at a factor of 8; in the second 'i' loop, flatten the pipeline, tile by a factor of 2 and parallelize by a factor of 16, with no parallelization in the reduction loop for 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes for a matrix multiplication and accumulation function, named 'kernel_2mm', by applying hardware acceleration strategies with the following specifics:\n\n1. Designate 'kernel_2mm' itself as a hardware function for acceleration.\n2. For the outer 'i' loop that iterates 40 times, disable automatic pipelining, maintain the loop in its original form, and execute iterations serially.\n3. Within the nested 'j' loop that iterates 50 times, turn off pipelining, keep the original loop structure, but allow limited parallel execution with a factor of 4.\n4. In the innermost 'k' loop iterating 70 times, which calculates the temporary matrix results, use parallel execution with a factor of 7 and specify 'tmp' matrix for reduction.\n5. For the second 'i' loop, also iterating 40 times for the final computation, disable automatic pipelining, but flatten the structure of the nested 'j' loop that iterates 80 times.\n6. In this nested 'j' loop, tile it into chunks with a factor of 2, and ramp up the parallel execution factor significantly to 16.\n7. Finally, in the inner 'k' loop within this second section, allow execution of iterations in series, with a focus on the 'D' matrix for reduction.\n\nKeep other settings in their default state, unless explicitly mentioned otherwise.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a matrix multiplication kernel with specific pragmas to control parallelism and pipelining, using the following guidelines:\n\n1. Define the kernel function for FPGA acceleration.\n2. Disable automatic pipelining for the top-level operations within the matrix multiplications.\n3. Preserve the original loop structure without any tiling for all the loops.\n4. Run the top-level loop iterations sequentially, without parallel execution.\n5. For the first matrix multiplication (E = A * B), calculate each element of E without any change in the loop nesting but allow the innermost loop to run 60 iterations in parallel.\n6. For the second matrix multiplication (F = C * D), maintain the loop structure and flatten the pipeline for the workload of each i.\n7. For the third matrix multiplication (G = E * F), use a factor of 8 for parallel processing of the outer j loop, and a factor of 16 for the innermost parallel k loop.\n\nNote: Where you see 'auto{value}', replace it with the corresponding value or instruction you provided earlier.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes with the following specifications: designate the function as a hardware acceleration kernel, handle the first level of nested loops without automatic pipelining, without tiling, and in sequential execution. For the second level of nested loops, allow automatic pipelining, use a minimal tiling factor, and double parallel execution. For the innermost loops, execute 32 operations in parallel. In the second major loop block, flatten the loops for pipelining but maintain other settings consistent with the first loop block.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code for a matrix multiplication kernel without any loop pipelining, using natural iteration order without tiling, while treating each nested loop as independent, running the innermost loops of the first and second matrix multiplication blocks with parallel factors of 60 and 80 respectively, and the innermost loop of the third matrix multiplication block with no parallelism, except for the middle loop which should use a parallel factor of 8 and be fully flattened.\n\n- Treat the function as a hardware acceleration kernel.\n- For the first matrix multiplication block: do not pipeline any of the loops, do not tile the loops, and run the innermost loop with a parallel factor of 60.\n- For the second matrix multiplication block: do not pipeline any of the loops, do not tile the loops, and run the innermost loop with a parallel factor of 80.\n- For the third matrix multiplication block: do not pipeline the outer and innermost loops, fully flatten the middle loop using pipelining, do not tile any of the loops, set a parallel factor of 8 for the middle loop, and use no parallelism for the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a dedicated hardware acceleration kernel. For all outer loops responsible for matrix computations, disable loop pipelining to preserve the original loop structure and run loop iterations in sequence without using parallel hardware resources. Specifically, in the loop that populates matrix E, execute the innermost loop, which iterates 60 times, in parallel using all available hardware resources. In the second matrix multiplication that creates matrix F, allow parallel execution of the innermost loop iterating 80 times with a parallelization factor of 16. Finally, for the third matrix computation that results in matrix G, configure the first level of nested loops to process eight iterations in parallel and the innermost loop to run with a parallelization factor of five.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS C code, use the following comprehensive instruction:\n\n\"Define a kernel function named 'kernel_2mm' for matrix multiplication and accumulation, where the matrix multiplication is between alpha times A times B and the accumulation is adding it to beta times D. Apply the following constraints: Disable automatic pipelining for the outer loops, allow the innermost loops to be flattened for pipelining when multiplying A and B, enforce no parallelization or tiling on the main loops or the loop multiplying A and B, and for the loop accumulating the result into D, tile the inner j-loop by a factor of 8 and apply a parallelization factor of 16 for the reduction on D, while also keeping the default pipelining behavior for the accumulation loop only.\"\n\nAnd implement this through these specific #pragma directives:\n\n1. `#pragma ACCEL kernel` at the beginning to indicate that the function 'kernel_2mm' should be treated as a hardware kernel for acceleration.\n\n2. `#pragma ACCEL PIPELINE auto{off}` before the first two major for-loops to disable automatic pipelining for both matrix multiplication and accumulation loops.\n\n3. `#pragma ACCEL PIPELINE auto{flatten}` before the inner j-loop of the matrix multiplication to suggest that this loop should be flattened for pipelining, speeding up the execution.\n\n4. For all main loops and the multiplication loop, set `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}` to prevent tiling and parallel execution, maintaining the original loop form.\n\n5. For the accumulation loop, maintain the default pipelining behavior with `#pragma ACCEL PIPELINE auto{}`.\n\n6. `#pragma ACCEL TILE FACTOR=auto{8}` before the inner j-loop of the accumulation loop to tile this loop by a factor of 8.\n\n7. `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}` before the inner k-loop of the accumulation to enable parallel execution with a factor of 16 specifically for the reduction operation on the array D.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a three-matrix multiplication kernel that does not employ pipeline optimization for any of the loops, computes each matrix product sequentially, and restricts the transformation of loops to their pre-set chunk sizes without automatic tiling or parallelization. Specifically, disable automatic pipelining, set the tiling factor to '1' to avoid loop tiling, and enforce a parallel factor of '1' to maintain sequential loop execution. Apply these settings for all the nested loops in the computation of the products of matrices A and B into E, C and D into F, and finally, E and F into G. Additionally, treat the entire function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a C function named `kernel_3mm` designated as a hardware kernel that computes the product of two matrix pairs and assigns them to two distinct matrices, with three nested loops for each product computation. Apply pipelining optimization only at the first level loop for matrix `E`, without partitioning into smaller loops or parallelizing loop iterations. Ensure that the second level loop for `E` is not pipelined or parallelized but runs in full sequence. When computing matrix `F`, disable pipelining and set the parallel factor of the second level loop to 10; the other loops should neither be pipelined nor parallelized. Finally, for matrix `G`, compute the product without any pipelining or parallelization optimizations for all loop levels. Use \"auto\" where relevant to allow the directive to determine the appropriate setting, and specify \"off\" where pipelining should be disabled.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following HLS codes, use this comprehensive but very concise instruction:\n\n\"Define a hardware kernel function 'kernel_3mm' that performs matrix multiplication for three pairs of matrices (A*B=E, C*D=F, E*F=G) without pipelining or tiling any loop, and enforce serial execution for all loops except certain innermost loops where parallelism with full and reduced factors is specified.\"\n\nThis describes the intended function ('kernel_3mm') and the #pragma values:\n\n- Treat the 'kernel_3mm' function as a hardware kernel: `#pragma ACCEL kernel`.\n- Prevent automatic pipelining of loops: `#pragma ACCEL PIPELINE auto{off}`.\n- Maintain each loop in its original, unaltered structure without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Enforce serial execution of the loops without parallelism, except where noted: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- In the first pair of matrices (A*B=E), fully unroll the innermost loop for maximum parallelism: `#pragma ACCEL PARALLEL FACTOR=auto{60}`.\n- In the second pair of matrices (C*D=F), partially unroll the innermost loop with a factor of 16 for partial parallelism: `#pragma ACCEL PARALLEL FACTOR=auto{16}`.\n- In the third pair of matrices (E*F=G), partially unroll one of the outer loops with a factor of 8 for partial parallelism: `#pragma ACCEL PARALLEL FACTOR=auto{8}`, while keeping the innermost loop serial: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For the outer loops associated with indices 'i' and 'j', both in the first and second main loop nests, maintain their original, sequential iteration order without any parallel or pipelined execution optimizations. However, explicitly parallelize the innermost loops with indices 'k' - for the first loop nest, use a high degree of parallelism with a factor of 32 for summing up the matrix multiplications into 'tmp', and for the second loop nest achieve a more moderate parallelism with a factor of 2 for summing into 'D'. Additionally, apply pipelining to the middle loop of the first loop nest without specifying the factor, allowing the tool to opt for automatic factor determination, but prevent the same in the middle loop of the second loop nest.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel function named 'kernel_2mm' that performs a matrix multiplication followed by a matrix addition. Inside the function, initially disable automatic pipelining, tiling, and parallel processing for all loops. Within the first nested loop block, enable automatic pipelining for iterating over elements of array 'tmp', maintain full loop nest form, and parallelize the innermost loop with a default factor that synthesizes a parallel implementation for the accumulation without specifying the factor number, utilizing a reduction clause for array 'tmp'. In the second nested loop block, perform similar configurations to the first block, except set the parallel factor of the innermost loop reduction on array 'D' to the default value synthesizing a parallel implementation without specifying the factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function `kernel_3mm` designed for hardware acceleration that calculates the product of matrix A and B into E, the product of matrix C and D into F, and then the product of matrix E and F into G. For the outer loops of each matrix multiplication, disable automatic pipelining, use the whole loops without tiling, and run iterations sequentially. For the first inner loop initializing matrix E, enable default pipelining, tile by factor 1, and parallelize the second inner loop with a factor of 32. The initialization loop for matrix F should pipeline with flattening optimizations, tile by factor 1, and run iterations in sequence. Finally, for the initialization loop of matrix G, enable default pipelining, use the original loop without tiling, and parallelize the innermost loop with a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function that processes matrix-matrix multiplication and accumulation with specific optimization directives:\n\n1. Designate the entire function as a hardware kernel for acceleration.\n2. For the outermost loops (`i` loops), disable automatic pipelining, maintain the original loop structure without tiling, and execute iterations sequentially without parallelization.\n3. Within the first nested loop (`j` loop for `tmp` computation), enable default pipelining optimization, process the loop in its full form without tiling it, and partially parallelize the loop with a factor of 4.\n4. For the innermost loop within the first nested loop (`k` loop for `tmp` computation), enable parallel execution with a factor of 2 and specify a reduction on `tmp`.\n5. Similarly, for the second outermost loop (`i` loop for `D` computation), disable pipelining, maintain the loop in its full form without tiling, and execute iterations sequentially.\n6. For the first nested loop within the second outermost loop (`j` loop for `D` computation), apply pipelining with loop flattening, do not tile the loop and significantly parallelize the loop with a factor of 16.\n7. Lastly, for the innermost loop within the second nested loop (`k` loop for `D` computation), execute iterations sequentially with no parallelization, enabling a reduction on `D`.\n\nUse the specified pragma directives for each loop to control the pipelining, tiling, parallelization, and reduction behavior as directed above.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware kernel that will be accelerated. Within this function, for the first two-layer loop nest, turn off automatic pipelining for the outer loop, ensure the inner loops are processed in their original, full form without tiling or parallelizing the outer loop. However, enable full automatic pipelining for the innermost loop, manually tile the inner loop with a factor of 1, and set the parallel factor to 4 while performing a reduction on the 'tmp' array with a parallel factor of 16. For the second two-layer loop nest, disable automatic pipelining for the outer loop, and within the inner loop, allow automatic pipelining with loop flattening, manually tile it with a factor of 2, and parallelize it with a factor of 16, with the innermost loop dedicated to reducing the 'D' array without further parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated 2mm kernel function that operates on matrices without automatic pipelining, tiling, or parallel processing optimization for the outer loops, while specifically allowing some parallel processing optimization for the inner loops. For the first set of nested loops: disable automatic pipelining, keep the loop body as a single chunk without tiling for the outer loop, run iterations sequentially (not in parallel) for the outer loop, and permit a factor of 4-way parallelism for the middle loop with a factor of 2-way parallelism for the innermost loop, also enabling reduction on the 'tmp' matrix. For the second set of nested loops: again disable automatic pipelining and do not tile the outer loop, but enable automatic pipeline optimization for the inner loop, apply a tiling factor of 2 for the middle loop, perform iterations sequentially for outer loops, and enable a factor of 32-way parallelism with reduction on the 'D' matrix in the innermost loop. Use '#pragma ACCEL kernel' to denote the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code:\n\nDeclare a hardware acceleration kernel function for matrix multiplication with explicitly named input matrices and output matrices, specifying the dimensions as arguments. Disable automatic pipelining for all major loops to keep the original loop form. Within the first two sets of nested loops that calculate matrix E, disable pipelining and tiling, run the innermost loop in parallel with a factor that utilizes all elements of the B matrix (factor of 60). For the nested loops calculating matrix F, also disable pipelining and tiling but parallelize the innermost loop with a reduced factor (factor of 16) to ensure resource constraints are met. Finally, for the nested loops calculating matrix G, disable pipelining and tiling at the outer loops, but set the parallel factor for the two innermost loops to partially parallelize computation with a factor of 8 to balance resource usage and performance.\n\nSpecific #pragma values to be used are:\n\n1. Treat the entire function as a hardware acceleration kernel: `#pragma ACCEL kernel`\n2. Disable automatic loop pipelining: `#pragma ACCEL PIPELINE auto{off}`\n3. Process loops without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Run outer loops sequentially without parallelism: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n5. Parallelize the innermost loop for computing E with a full factor: `#pragma ACCEL PARALLEL FACTOR=auto{60}`\n6. Parallelize the innermost loop for computing F with a factor of 16: `#pragma ACCEL PARALLEL FACTOR=auto{16}`\n7. Parallelize the two inner levels of loops for computing G with a balancing factor of 8: `#pragma ACCEL PARALLEL FACTOR=auto{8}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function 'kernel_2mm' that functions as a hardware kernel for acceleration, operates on matrices 'A', 'B', 'C', 'D', and 'tmp' with dimensions and coefficients provided as arguments. Ensure that for the outer two loops, one calculating 'tmp = alpha * A * B' and the other updating 'D = beta * D + tmp * C', you disable automatic pipelining, use the entire loop without tiling, and process iterations sequentially. Inside the first nested loop, enable loop flattening for pipelining, fix tiling to factor 1, and set parallel processing to a factor of 4. Within the innermost loop, activate parallel reduction on 'tmp' but without parallel processing factor tuning. For the second outer loop's nested loop, disable pipelining, use no tiling, and set parallel factor to 20. Lastly, within its innermost loop, activate parallel reduction on 'D' with a parallel factor set explicitly to 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to define a hardware kernel function `kernel_3mm` that executes matrix multiplications on fixed-size 2D arrays without loop pipelining at the top level. Within each top-level matrix multiplication, the outer `i` and `j` loops should not be tiled or parallelized, while the innermost `k` loop should run with the specified parallel factor: 60 for the `A*B`, 80 for the `C*D`, and retain the default behavior for `E*F`. Moreover, in the last `E*F` multiplication, flatten the pipelining of the `i` loop and restrict parallel execution of the `j` loop to 5 iterations at a time. For each section of the function, adjust the pragma directives as follows:\n\n1. Treat the function as a hardware kernel for acceleration.\n2. For matrix multiplication `E := A*B`, prevent the `i` loop from being pipelined, ensure the `j` loop is not tiled, and run the `k` loop with a parallel factor of 60.\n3. For matrix multiplication `F := C*D`, prevent the `i` loop from being pipelined, ensure the `j` loop is not tiled, and run the `k` loop with a parallel factor of 80.\n4. For matrix multiplication `G := E*F`, disable pipelining for the `i` loop, set the `j` loop to parallelize with a factor of 5, and do not apply any special pragma optimizations to the `k` loop. Additionally, flatten the pipeline for the `i` loop.\n\nHere are the specific pragmas to use:\n\n- Use `#pragma ACCEL kernel` to declare the function as a hardware kernel.\n- For all the top-level `i` and `j` loops inside the respective matrix multiplications, use:\n  - `#pragma ACCEL PIPELINE \"off\"` to prevent pipelining of the loop\n  - `#pragma ACCEL TILE FACTOR=1` to discourage loop tiling\n  - `#pragma ACCEL PARALLEL FACTOR=1` to prevent parallelizing\n- For the `k` loop in the `A*B` multiplication, use `#pragma ACCEL PARALLEL FACTOR=60`.\n- For the `k` loop in the `C*D` multiplication, use `#pragma ACCEL PARALLEL FACTOR=80`.\n- For the `i` loop in the `E*F` multiplication, use `#pragma ACCEL PIPELINE \"flatten\"` to enable flattening of the pipeline.\n- For the `j` loop in the `E*F` multiplication, use `#pragma ACCEL PARALLEL FACTOR=5`.\n- Do not apply any special pragmas to the `k` loop in the `E*F` multiplication; let it run with the default behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a three-matrix multiply function where each of the three for-loop nests processes the multiplication in a sequential and non-pipelined manner. The innermost loops in the first two nests should have parallelism set to the width of the second matrix operand, while the second nest's innermost loop should be fully sequential. In the third nest, set the loop parallelism for the outer loop to 8 while keeping the innermost acts sequentially. The function should be considered as a hardware kernel. This can be reflected using the following #pragma directives:\n\n1. `#pragma ACCEL kernel` to specify that the following function is to be considered a hardware kernel for acceleration.\n2. `#pragma ACCEL PIPELINE \"off\"` to prevent automatic pipelining of loops.\n3. `#pragma ACCEL TILE FACTOR=1` to maintain the original loop structure without tiling.\n4. `#pragma ACCEL PARALLEL FACTOR=1` to ensure loops run in a fully sequential manner, one iteration after the other.\n5. Adjust the `#pragma ACCEL PARALLEL FACTOR` to match the width of the second matrix operand for the innermost loops of the first two matrix multiplication nests and set it to 8 for the outer loop of the third nest while keeping the innermost loop of the third nest sequential.\n6. For the third matrix multiplication nest, `#pragma ACCEL PIPELINE auto{flatten}` to flatten the nested loops into a single level of hierarchy to potentially improve the design's performance or area utilization.\n\nApply these pragmas appropriately within the given C code to control the behavior of the HLS tool according to the instructions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel, disable automatic pipelining for the entire function, and ensure the first level loops iterate sequentially in their entirety. In the second inner loop, still inhibit pipelining but allow four-way parallel execution of iterations. Apply a reduction optimization on the variable `tmp` with two-way parallel execution in the innermost loop of the first section. In the second section, enable pipelining optimization with flattening transformation for the first inner loop while allowing twenty-way parallel execution. Finally, perform the reduction on `D` with sequential execution in the innermost loop of the second section.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, perform the matrix multiplication and addition without automatic pipelining, tiling, or parallelization at the topmost level to maintain the original loop structure. Within the nested loops, utilize automatic pipelining where applicable, set the loop tiling factor to one to respect the original loop boundaries, and designate different factors of parallelism \u2014 specifically, parallelize the innermost loop with a reduction on 'tmp' by a factor of 32 and on 'D' by a factor of 2, while the loop iterating over 'j' should be parallelized by a factor of 4.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware kernel intended for acceleration by applying the appropriate pragma. Inside the function, initially prevent loop pipelining, loop tiling, and parallel loop processing by setting their pragma directives to turn off auto-optimization and use the default behavior for the outer loops, which iterate over `i`.\n\nFor the inner loop iterating over `j`, apply pragmas to flatten the pipeline structure, keep the loop in its full form without tiling, and allow for some degree of parallelism with a parallelization factor of 4.\n\nEnable a parallel reduction for the computation that sums into `tmp[i][j]`, turning off automatic factor selection and allowing for strictly sequential execution\u2014a parallel factor of 1.\n\nRepeat the process for the subsequent loop nest that updates matrix `D`, again disabling loop pipelining and tiling while setting the parallel factor to 4 in the middle loop. Apply a pragma for a parallel reduction in the innermost loop with a factor of 50 to accumulate into `D[i][j]`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, ensuring that in the first main loop, the loop should not be pipelined or tiled to keep it intact, and each iteration should run sequentially. Within the nested loop, enable full loop flattening, and allow four iterations of this nested loop to be processed in parallel, but without dividing the loop. Inside the innermost loop, allow the reduction operation on `tmp` to happen without any parallelism. For the second main loop, do not auto pipeline the main loop but allow it to remain untiled, and run its iterations sequentially. Within its nested loop, allow pipelining to be automatically determined, and let four iterations of this nested loop run in parallel without tiling. In the innermost loop, allow for a reduction operation on `D` with up to fifty parallel operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a two-matrix multiplication kernel function `kernel_2mm` that takes sizes `ni`, `nj`, `nk`, `nl`, and matrices `A`, `B`, `C`, `D` along with scalars `alpha` and `beta`. The function computes `D = alpha*A*B + beta*D`, storing the intermediate result of `A*B` in `tmp`. Apply the following constraints:\n\n1. Treat the entire function as a hardware acceleration kernel.\n2. For the outermost loop of each matrix multiplication, disable automatic pipelining, prevent the loop from being partitioned into smaller blocks, and execute iterations sequentially.\n3. For the inner loop where `tmp[i][j]` is computed, again disable automatic pipelining, and set parallelism for this loop to four times the base level but prevent it from being tiled into smaller blocks.\n4. Enable parallel reduction on `tmp` with twice the minimal parallelism in the innermost loop where elements of `A` and `B` are multiplied.\n5. For the second matrix multiplication loop computing `D[i][j]`, flatten the pipelining, allowing loops to be merged for more efficient hardware usage, double the base tiling factor, but keep the outermost loop's iterations sequential.\n6. Finally, set single-level parallelism for the reduction phase on `D` in the innermost loop of the second matrix multiplication where `tmp` and `C` are multiplied, without tiling or increasing the parallelism.\n\nIn structured HLS pragma directives:\n\n- To mark the function as a hardware acceleration kernel, use `#pragma ACCEL kernel`.\n- Use `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipelining.\n- Use `#pragma ACCEL TILE FACTOR=auto{1}` to avoid tiling the loop into smaller chunks.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to execute iterations sequentially.\n- Increase parallelism with `#pragma ACCEL PARALLEL FACTOR=auto{4}` for the desired loop.\n- Apply parallel reduction with `#pragma ACCEL PARALLEL reduction` and specify the matrix variable and factor.\n- Use `#pragma ACCEL PIPELINE auto{flatten}` to enable loop flattening.\n\nApply these pragmas immediately before the corresponding `for` loops they are intended to control within the function body.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel, and within it manually control the optimization of loops for matrix multiplication operations E=A*B, F=C*D, and G=E*F as follows:\n\nFor the outermost loop in each matrix multiplication, disable automatic pipelining, don't tile the loop beyond single iteration chunks, and prevent any parallelization beyond sequential execution. \n\nWithin the second-level loop for calculating E and G, enable automatic pipelining, maintain loop tiling at the granularity of single iterations, and parallelize two iterations at a time.\n\nFor the innermost loop calculating E, allow for parallelizing 32 iterations simultaneously to increase computation throughput.\n\nWhen calculating F in the second-level loop, enable pipelining with automatic flattening to merge nested loops where possible, maintain loop tiling at single iteration chunks, and parallelize four iterations at a time. However, keep the innermost loop's parallel factor to one, resulting in sequential execution. \n\nLastly, for the inner G calculation within the nested loop, allow simultaneous processing of 32 iterations for improved computational efficiency.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration kernel, disable automatic pipelining, prevent loop tiling, and enforce sequential loop iteration for the outermost loop of the matrix multiplications E = A * B, F = C * D, and G = E * F. Within the nested loops for computing E and G, enable pipelining with automatic factor detection, permit tiling with a single iteration per tile, and set the parallelism factor to 2 for the middle loop and to 32 for the innermost loop, intensifying parallel processing of the multiplications. For computing F, allow loop flattening during pipelining, and enforce a parallelism factor of 2 for the middle loop while running the innermost loop sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code that represents a matrix multiplication and addition routine optimized for acceleration hardware. Define the function \u2018kernel_2mm\u2019 as a hardware kernel for FPGA acceleration. Within the function, execute the primary loop that computes the temporary matrix product without any pipelining, tiling, or parallelization acceleration strategies. For the inner double-nested loops that calculate the temporary matrix and multiply-add to the result matrix, enable pipelining while keeping tiling at the highest granularity (i.e., unaltered processing order) and specifically parallelize the second loop with a factor of 4 iterations and the deepest loop with a factor of 50 iterations for the multiply-add to the result matrix. Additionally, perform a parallel reduction for the temporary matrix variable in the intermediate multiplication and for the result matrix 'D' in the final computation loop, without parallelizing other parts of these loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to implement a 2mm kernel function as a hardware acceleration kernel that computes the equation `D = alpha * A * B * C + beta * D` with specific matrix dimensions. Ensure that the outer loops for operational stages do not utilize hardware pipelining or parallel processing, and are thus not split into smaller chunks. However, within these outer loops, enable flattening based pewn pipeline optimization for the nested loops, allowing four iterations of the inner loops to run in parallel. Lastly, apply reduction to accumulate results for variable `tmp` and `D` in their respective loops, without parallel processing of the reduction variable. Use the following pragmas with their specified values:\n\n1. Treat the entire function as a hardware acceleration kernel: `#pragma ACCEL kernel`\n2. For the first and second outer loops (`i`-loops), do the following:\n   - Disable automatic loop pipelining: `#pragma ACCEL PIPELINE auto{off}`\n   - Process each loop in its entirety without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Run iterations sequentially without parallelism: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n3. For the first nested loops (`j`-loops), apply the following:\n   - Flatten the loop for pipelining optimization: `#pragma ACCEL PIPELINE auto{flatten}`\n   - Ensure the inner loop is not split into chunks (no tiling): `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Allow four iterations of the innermost loop (`k`-loops) to execute in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n   - Use reduction on `tmp` without parallel execution: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`\n4. For the second nested loops (`j`-loops) that update matrix `D`, replicate the same `#pragma` settings as used for the `tmp` update, including a parallel factor of four and reduction for `D`: \n   - Enable flattening pipeline optimization: `#pragma ACCEL PIPELINE auto{flatten}`\n   - No tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Four-way parallel execution: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n   - Use reduction on `D` without parallel execution: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`\n\nThis will result in HLS C code configured for efficient matrix-matrix multiplication while adhering to the specified loop transformation directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 3-matrix multiplication kernel function that accelerates the computation of \\( E = A \\times B \\), \\( F = C \\times D \\), and \\( G = E \\times F \\), with specific sizes for matrices \\( A \\), \\( B \\), \\( C \\), \\( D \\), \\( E \\), \\( F \\), and \\( G \\). The loops for matrix multiplications should not be pipelined, should not be tiled, and all outer and middle loops should run sequentially; however, the innermost loops should be parallelized with factors of 60, 16, and 25 respectively for the three computations. The entire function should be treated as a hardware kernel for acceleration purposes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel, then configure the main time-stepping loop and its nested loops to bypass automatic optimization features. Specifically, prevent the compiler from pipelining, tiling, or parallelizing any of the loops by applying 'off' to pipeline pragmas and setting a parallelization and tiling factor of '1' for all but the last row sweep's backward traversal loop, where the parallelization factor should be '4'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Direct the HLS tool to optimize the given function for hardware acceleration without automatic optimization directives for loop pipelining, tiling, or parallel execution. Specifically, instruct it to:\n\n- Identify the 'kernel_adi' function as a hardware kernel\n- For each loop contained within this function, apply the following:\n  - Prevent automatic loop pipelining\n  - Disable loop tiling by keeping loops unaltered\n  - Keep loop iterations sequential without parallel execution\n\nAdditionally, for the last loop within the Row Sweep phase, allow manual parallel execution with a factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the 'kernel_2mm' function as a dedicated hardware kernel. Maintain the outermost loop in sequential execution without pipelining, keeping iterations in full chunks and without parallelism. Within this loop, flatten the pipeline for the second nested loop and allow a small degree of parallel execution with four simultaneous iterations, but do not tile this loop. Inside the deepest loop for the first half of the kernel computation, enable reduction on 'tmp' array without parallelism. For the second outermost loop of the kernel, default to automatic pipelining while keeping iterations in full chunks and with a slightly higher parallelism factor of five. In the deepest loop for the second half, substantially parallelize the iterations with a factor of fifty but perform reduction on the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function called 'kernel_2mm' as a hardware acceleration kernel tasked with matrix multiplication and addition, where the outer loops are executed sequentially without pipelining or tiling optimizations, while different levels of parallelism are applied within the nested loops:\n\n1. In the first nested loop, enforce that operations are executed one after another and do not use any pipeline optimization; only split the innermost loop into 4 parallel threads without any loop tiling.\n\n2. In calculating the 'tmp' matrix, allow a large degree of parallelism within the innermost loop by permitting 16 parallel threads, while designating the 'tmp' array for any necessary reductions due to concurrent updates.\n\n3. In the second outer loop, flatten the control flow hierarchy for potential pipelining improvements but do not apply any loop tiling; configure the inner loop to run 16 parallel threads for the 'j' iteration space.\n\n4. When updating the 'D' matrix, do not use any parallel optimization in the innermost loop, running all iterations sequentially.\n\nTo summarize, use these pragma directives respectively:\n\n- For the first level of nested loops, use these pragma values to prevent any optimization or parallel execution on the first loop; enforce non-parallel execution on the second loop, and define a moderate level of parallelism for the third loop:\n  - `#pragma ACCEL PIPELINE auto{off}`\n  - `#pragma ACCEL TILE FACTOR=auto{1}`\n  - `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n  - `#pragma ACCEL PIPELINE auto{off}`\n  - `#pragma ACCEL TILE FACTOR=auto{1}`\n  - `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n\n- For the second level of nested loops, use these pragma values to prevent any loop optimization or parallel execution on the outer loop; flatten the second loop for potential pipelining, and allow a high degree of parallelism on the innermost loop:\n  - `#pragma ACCEL PIPELINE auto{off}`\n  - `#pragma ACCEL TILE FACTOR=auto{1}`\n  - `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n  - `#pragma ACCEL PIPELINE auto{flatten}`\n  - `#pragma ACCEL TILE FACTOR=auto{1}`\n  - `#pragma ACCEL PARALLEL FACTOR=auto{16}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS code generator to treat the 'kernel_2mm' function as a hardware kernel for acceleration, disable automatic pipelining for the outer loops, flatten the innermost 'j' loop in the first major nested loop to enhance parallelism, allowing four simultaneous iterations while the 'k' loop performs reductions on 'tmp' array elements sequentially. Additionally, set up the second major nested loop to allow for 80 simultaneous iterations of the innermost 'j' loop while performing reductions on 'D' array elements sequentially, ensuring all loops are processed in their full form without tiling transformation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a kernel function that performs 2mm (two-matrix multiply) with nested loops, where the top-level loops are not pipelined and execute iterations in their original sequence, but the second-level loop is pipelined without any specific optimizations indicated. The innermost loop in the first block performs parallel reduction on 'tmp' over the full range with 70 iterations in parallel, and the innermost loop in the second block multiplies and accumulates with 'D', but only executes a single iteration at a time in parallel. Here are the specific directives:\n\n1. Treat the 'kernel_2mm' function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n2. Do not automatically pipeline the top-level 'for' loop for the variable 'i': `#pragma ACCEL PIPELINE auto{off}`\n3. The loop should maintain its full form, without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Execute iterations of the top-level 'for' loop serially, without parallelization: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n5. For the second-level 'for' loop over the variable 'j' within the first major block, pipeline this loop without additional specifications: `#pragma ACCEL PIPELINE auto{}`\n6. Again for the loop over 'j', keep it in its full form, without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n7. Execute pairs of iterations of the second-level 'for' loop in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n8. Perform a parallel reduction on 'tmp' in the innermost 'for' loop over 'k' with all possible iterations (70) in parallel for computation: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{70}`\n9. Pipeline the second big block's second-level 'for' loop over 'j' without specifying how the pipelining should be done, defaulting to the compiler's auto mode, and also flatten any nested loops: `#pragma ACCEL PIPELINE auto{flatten}`\n10. Keep the second big block's loop over 'j' in its original form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n11. In the second 'for' loop over 'j', run eighty iterations in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{80}`\n12. For the innermost loop in the second block over 'k', which computes 'D', execute iterations serially: `#pragma ACCEL PARALLEL reduction=D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Generate HLS C code for a 2D Alternating Direction Implicit (ADI) solver kernel, which is treated as a hardware acceleration kernel, with all loops being executed sequentially in their original form and without any automatic pipelining or partitioning optimizations applied. Specifically, disable loop pipelining, set tiling factors to 1 to avoid loop tiling, and ensure that all loops, except the parallel inner loops within the 'Row Sweep' section, run sequentially with no parallelism factor set. For the parallel loops within the 'Row Sweep', set one parallel factor to 16 and the other, which reverses the iteration order, to 29.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes in C with specific #pragma values, use the following comprehensive but concise natural language instruction:\n\n\"Define a hardware acceleration kernel function 'kernel_2mm' designed to perform a two-matrix multiplication with alpha and beta scaling on elements of multi-dimensional arrays. Disable automatic pipelining for all outer loops, ensure the original structure of each loop is preserved without tiling, and enforce serial execution. For the 'j' loop inside the first main 'i' loop, allow partial parallel execution with a factor of 4, and for the innermost 'k' loop within, enable a high degree of parallelism for the reduction operation on 'tmp' with a factor of 32. For the second set of loops processing 'D', similarly disable pipelining and tiling for the outer layers, but allow the 'j' loop to execute in parallel with a factor of 32, and for the reduction operations on 'D', use a parallel factor of 2.\" \n\nThis instruction translates to:\n\n- Treat the function 'kernel_2mm' as a hardware kernel for acceleration by including `#pragma ACCEL kernel` before the function definition.\n- Within the 'i' loop, disable pipelining and tiling and maintain serial execution with `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}`, respectively.\n- In the nested 'j' loop inside the first 'i' loop, disable pipelining, maintain the original loop form, and specify a parallel factor of 4 with related pragmas. For the inner 'k' loop, use `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}` to parallelize the reduction operation on 'tmp' with a factor of 32.\n- Repeat a similar pragma pattern for the second main 'i' loop and its nested 'j' loop, but increase the inner 'j' loop's parallel factor to `#pragma ACCEL PARALLEL FACTOR=auto{32}`, and apply a parallel factor of 2 for the reduction on 'D' with `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is the comprehensive but concise instruction to generate the given HLS codes:\n\n\"Define a kernel function named 'kernel_3mm' that multiplies three pairs of matrices (A*B, C*D, and E*F) without pipelining or tiling optimizations but with specific parallelism factors for the innermost loops. The first matrix multiplication should compute matrix E with no parallelism, the second should compute matrix F with maximum outer parallelism, and the third should compute matrix G with medium outer parallelism. Additionally, do not optimize the outer loops for pipelining or tiling.\"\n\nSpecific #pragma values to use:\n\n1. Convert the function to a hardware acceleration kernel:\n   `#pragma ACCEL kernel`\n\n2. For the outer loops (i and j) of each matrix multiplication:\n   - Disable automatic pipelining:\n     `#pragma ACCEL PIPELINE auto{off}`\n   - Process the loop without tiling:\n     `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Run the iterations sequentially, without parallelization:\n     `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n3. For the innermost loop (k) of each matrix multiplication:\n   - For the E matrix multiplication, where the innermost loop runs over dimension nk (60 iterations):\n     `#pragma ACCEL PARALLEL FACTOR=auto{60}`\n   - For the F matrix multiplication, where the innermost loop runs over dimension nl (80 iterations):\n     `#pragma ACCEL PARALLEL FACTOR=auto{80}`\n   - For the G matrix multiplication, where the innermost loop runs over dimension nj (50 iterations):\n     `#pragma ACCEL PARALLEL FACTOR=auto{50}`\n\nThe `auto{}` notation is used to automatically determine the default value if not explicitly set; in the example, the specifics are provided within the braces.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to designate the function `kernel_2mm` as a hardware acceleration kernel, to run the outermost loops for computation as single uninterrupted processes without any form of parallelism or loop transformation, while allowing the inner loops to run with different optimization strategies.\n\nSpecifically, prevent the initial part of the computation (`tmp[i][j] = alpha * A[i][k] * B[k][j]`) from being pipelined and parallelized, but with an exception that allows for parallel reduction with a factor equivalent to the full innermost loop nesting for the `k` index. Then, in calculating `D[i][j] += tmp[i][k] * C[k][j]`, explicitly allow the loop to pipeline the operations. Tile the innermost loop of this calculation with a factor of 8 and parallelize with the same factor to optimize for hardware resources and performance. Use pragma values `off` for no pipelining, `1` for no tiling, `auto` for automatic inference where no specific value is given, and the specific number factors where performance optimizations have been applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following comprehensive but very concise natural language instruction:\n\n\"Define a kernel function to perform three matrix multiplication operations (E=A*B, F=C*D, G=E*F) with input dimensions specified by ni, nj, nk, nl, and nm for matrices A, B, C, D, E, F, and G. Within each multiplication operation, do not apply automatic pipelining, tiling, or parallelization to the outermost loop (i-loop). In the middle loop (j-loop) for E=A*B and G=E*F, enable automatic pipelining, but keep tiling and parallelization to the original single iteration form. Allow significant parallelization in the innermost loop (k-loop) with a parallel factor of 12 for E=A*B, 32 for F=C*D, and 50 for G=E*F. However, for the middle loop in the G=E*F operation, slightly reduce parallelization with a factor of 7. Ensure the function is treated as a hardware acceleration kernel.\" \n\nHere are the specific #pragma values to use:\n\n- Treat the function as a hardware acceleration kernel: `#pragma ACCEL kernel`\n\nFor the matrix multiplication E=A*B:\n- Do not automatically pipeline the i-loop: `#pragma ACCEL PIPELINE auto{off}`\n- Keep the i-loop in its original form: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the i-loop iterations in series: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Automatically pipeline the j-loop, but without specific pragma values: `#pragma ACCEL PIPELINE auto{}`\n- Keep the j-loop in its original form: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the j-loop iterations in series: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Significantly parallelize the k-loop with a factor of 12: `#pragma ACCEL PARALLEL FACTOR=auto{12}`\n\nFor the matrix multiplication F=C*D:\n- Follow similar instructions as for E=A*B, but with the k-loop parallel factor of 32: `#pragma ACCEL PARALLEL FACTOR=auto{32}`\n\nFor the matrix multiplication G=E*F:\n- Automatically pipeline the j-loop, maintain original form for tiling: `#pragma ACCEL PIPELINE auto{} and #pragma ACCEL TILE FACTOR=auto{1}`\n- Set the parallel factor of the j-loop to 7: `#pragma ACCEL PARALLEL FACTOR=auto{7}`\n- Set the k-loop parallel factor to 50: `#pragma ACCEL PARALLEL FACTOR=auto{50}`\n\nPlease note that \"auto{off}\" and \"auto{1}\" indicate that those features should not be applied or should maintain the default behavior, and \"auto{}\" allows the tool to choose the best setting where the specific value is not provided.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Inside this function, for the outer loops handling array 'tmp' and 'D', disable automatic pipelining, process these loops in their entirety without splitting them into smaller chunks, and do not parallelize their iterations. \n\nHowever, automatically pipeline the first nested loop inside the 'tmp' array computation, retain its full structure and parallelize the innermost loop with a custom parallelization factor, while also enabling reduction on 'tmp'. For the nested loop handling array 'D', flatten the pipelining, double the loop's tiling factor compared to the outer loops, and drastically increase its parallel factor. On the most inner loop, enable reduction on 'D' with default parallelization.\n\nHere's the HLS directive breakdown for each loop:\n- Disable pipelining for the outer loops: `#pragma ACCEL PIPELINE off`\n- Retain the full structure of the outer loops: `#pragma ACCEL TILE FACTOR=1`\n- Do not parallelize the iterations of the outer loops: `#pragma ACCEL PARALLEL FACTOR=1`\n- Automatically pipeline the first nested loop within the 'tmp' array loop: `#pragma ACCEL PIPELINE`\n- Retain the full structure of the first nested loop: `#pragma ACCEL TILE FACTOR=1`\n- Parallelize the innermost loop when computing 'tmp' with a custom factor: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=7`\n- Flatten the pipelining of the nested loop inside the 'D' array loop: `#pragma ACCEL PIPELINE flatten`\n- Double the tiling factor of the loop within the 'D' array loop: `#pragma ACCEL TILE FACTOR=2`\n- Increase the parallel factor of the loop within the 'D' array loop: `#pragma ACCEL PARALLEL FACTOR=16`\n- Allow for reduction on the 'D' array with default parallelization: `#pragma ACCEL PARALLEL reduction = D`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes with appropriate pragmas, please instruct the code generator to \"mark a function as a hardware acceleration kernel and apply optimizations on nested loops for matrix multiplication with specific tuning directives\":\n\n1. Treat 'kernel_3mm' function as a hardware kernel for acceleration.\n2. For the outer loops of each matrix multiplication ('E := A * B', 'F := C * D', 'G := E * F'), turn off automatic pipelining, set tiling to the smallest loop iteration size which is 1, and execute loop iterations sequentially by setting parallel factor to 1.\n3. Within the innermost loop of the first matrix multiplication 'E := A * B', enable default pipelining, keep tiling to a factor of 1, and fully unroll the loop by setting the parallel factor to the loop iteration size, which is 60.\n4. In the second matrix multiplication 'F := C * D', turn off pipelining for both outer and inner loops, set tiling to 1 for all loops, and partially unroll the innermost loop with a parallel factor of 16.\n5. For the third matrix multiplication 'G := E * F', turn off pipelining and set the tiling factor to 1 for the outer loop, flatten pipeline the next loop, allowing razor's pipeline to consolidate the loops containing this pragma, set a tiling factor of 1, and partially unroll the innermost loop with a parallel factor of 7. Finally, run the last loop in the sequence without any unrolling by setting the parallel factor to 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the defined function as a hardware acceleration kernel, ensure that the outermost loop and all nested loops are neither automatically pipelined nor tiled, and that they execute sequentially without parallelism with the exception of the final nested loop which should employ parallel processing with eight simultaneous operations.\n\nIn the provided code:\n\n1. Use `#pragma ACCEL kernel` to indicate that function `kernel_adi` should be treated as a hardware acceleration kernel.\n2. Apply `#pragma ACCEL PIPELINE off` to all loops to prevent the compiler from automatically pipelining them.\n3. Apply `#pragma ACCEL TILE FACTOR=1` to maintain the original loop structure without tiling.\n4. Apply `#pragma ACCEL PARALLEL FACTOR=1` to run iterations of each loop sequentially except for the last nested loop within the 'Row Sweep' section.\n5. For the last nested loop in the 'Row Sweep' section, use `#pragma ACCEL PARALLEL FACTOR=8` to indicate that up to eight iterations of the loop should be processed in parallel.\n\nCorresponding #pragma statements are to be used as follows:\n\n- `#pragma ACCEL kernel` before the function declaration.\n- `#pragma ACCEL PIPELINE off`, `#pragma ACCEL TILE FACTOR=1`, and `#pragma ACCEL PARALLEL FACTOR=1` before each loop except for the last correction, where `#pragma ACCEL PARALLEL FACTOR=8` should be used to enable parallel processing.\n- Ensure that `#pragma ACCEL PARALLEL FACTOR` overrides any automatic parallel factor inference by the compiler, as it should be explicitly set as instructed.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a hardware-accelerated matrix multiplication function with three sets of matrix multiplications. Ensure that the function itself is treated as a hardware acceleration kernel. Within each multiplication set, do not automatically pipeline the outermost loop, but process this loop without tiling or parallelization. For the intermediate loops, enable automatic pipelining, but again, do not apply tiling or parallelization. In the innermost loops of each multiplication set, enable parallelization with specific factors: 15 for the AxB multiplication, 32 for the CxD, and 50 for the ExF, and leave the respective outer loops with a parallel factor of 1. Additionally, in the final matrix multiplication set of ExF, also set the parallel factor of the intermediate loop to 7.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the 'kernel_3mm' function as a hardware acceleration kernel and configure the associated loop optimizations as follows:\n\n1. For each of the three main matrix multiplication sections within 'kernel_3mm':\n\n    a. Prevent automatic loop pipelining.\n    b. Ensure the loop is not tiled, i.e., it is processed in its full, original form.\n    c. Ensure the loop iterations are processed sequentially, not in parallel.\n\n2. Within the first matrix multiplication block (E := A*B):\n\n    a. For the innermost 'k' loop, allow parallel processing of all iterations, corresponding to the maximum width of the 'B' matrix.\n\n3. In the second matrix multiplication block (F := C*D):\n\n    a. In the 'i' loop, flatten loop iterations to allow for potentially optimized hardware implementation without specific parallelism directives.\n    b. Keep the inner 'k' loop sequential, with no parallelism or unrolling applied.\n\n4. During the final matrix multiplication (G := E*F):\n\n    a. Flatten the 'i' loop similar to the previous matrix multiplication for potential optimization.\n    b. Adjust the 'j' loop to process eight iterations in parallel.\n    c. Keep the 'k' loop sequential, with each iteration processed one after the other.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the provided HLS C code by using pragmas to designate the following function 'kernel_adi' as a hardware acceleration kernel and disable automatic optimizations for loop pipelining, loop tiling, and loop parallelism by specifying the values 'off' for pipelining and setting both tile and parallel factors to '1' for the entire function, except for one inner loop where you should set the parallel factor to '16'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel, purposefully avoid pipelining, tiling, or parallelizing any loops within the code, maintain the column-wise calculation without parallelism, while allowing parallel execution with a factor of 16 during the row-wise calculation of q[i][j] and a factor of 58 in the final backward traversal of u[i][_in_j]. Use the appropriate pragmas to reflect these specifications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated C function named 'kernel_adi' that iteratively processes a 60x60 matrix to simulate Alternating Direction Implicit (ADI) solving method. Set all pragmas to disable automatic optimization features and to handle loops sequentially as follows:\n\n1. Annotate the function as a hardware acceleration kernel.\n2. Initialize constants DX, DY, DT, B1, B2, mul1, mul2, a, b, c, d, e, and f based on given equations.\n3. For the outermost time-stepping loop 't' (from 1 to 40), and for both the 'Column Sweep' and 'Row Sweep' nested loops:\n    - Turn off loop pipelining.\n    - Disable loop tiling (prevent loop from being unrolled or divided into smaller chunks).\n    - Preserve the original loop iteration order by setting parallel factor to 1.\n4. In the Column Sweep 'i' loop (from 1 to 58):\n    - Assign boundary conditions to 'v[0][i]' and 'v[60 - 1][i]'.\n    - Compute 'p[i][j]' and 'q[i][j]' dependencies sequentially in the 'j' loop for 'v' updates.\n    - Backward compute 'v' in reverse 'j' order (from 57 to 0) while maintaining sequential execution.\n5. In the Row Sweep 'i' loop (from 1 to 58):\n    - Assign boundary conditions to 'u[i][0]' and 'u[i][60 - 1]'.\n    - Compute 'p[i][j]' and 'q[i][j]' dependencies sequentially in the 'j' loop for 'u' updates.\n    - Backward compute 'u' in reverse 'j' order (from 57 to 0), but this time, allow the last parallel loop to potentially run parallel iterations with a factor of 32.\n\nUse pragma directives '#pragma ACCEL KERNEL' to indicate kernel function, '#pragma ACCEL PIPELINE off', '#pragma ACCEL TILE FACTOR=1', and '#pragma ACCEL PARALLEL FACTOR=1' (or '32' for the specific case) to achieve these controls. Indicate the beginning and end of the area of interest with '#pragma scop' and '#pragma endscop' respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as a target for hardware acceleration, and for every loop within it, specify to not introduce pipelining, not to tile the loops, and to execute all iterations in series, except the final backward substitution loop in the row sweep step, which should allow 16 iterations to be processed in parallel.\n\nHere are the corresponding pragmas:\n\n- Use `#pragma ACCEL kernel` above the function declaration to mark 'kernel_adi' as an acceleration kernel.\n- For every loop in the kernel, disable automatic pipelining with `#pragma ACCEL PIPELINE AUTO off`.\n- For every loop in the kernel, prevent tiling by setting `#pragma ACCEL TILE FACTOR=auto{1}`.\n- For every loop in the kernel, except the final backward substitution loop in the row sweep, enforce serial execution with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the final backward substitution loop in the row sweep, allow parallelism with `#pragma ACCEL PARALLEL FACTOR=auto{16}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a two-matrix multiplication kernel function with the following constraints:\n1. Treat the entire function as a hardware kernel for acceleration.\n2. Do not automatically pipeline the outermost loops 'i' in both main loop nests.\n3. Process the outermost loops 'i' exactly as they are without tiling or parallelization enhancements.\n4. In the first main loop nest where 'j' and 'k' indices are used, automatically flatten the 'j' loop, tile it without chunking, and parallelize the 'k' loop with a factor of 4 for simultaneous execution, ensuring that parallel execution reduces elements into the 'tmp' array properly.\n5. In the second main loop nest where 'j' and 'k' indices are used, automatically flatten the 'j' loop, tile it without chunking, and parallelize the 'k' loop with a factor of 32 for simultaneous execution, ensuring that parallel execution reduces elements into the 'D' array properly.\n\nPlease state the pragma directives as follows:\n- For not automatically pipelining loops, use `#pragma ACCEL PIPELINE auto{off}` above the respective loops.\n- For processing the loops without tiling or parallel enhancements, place `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}` above the loops.\n- For automatically flattening loops, use `#pragma ACCEL PIPELINE auto{flatten}` above the 'j' loops.\n- For the 'k' loops in the first nest, place `#pragma ACCEL PARALLEL FACTOR=auto{4}` and specify the array to be reduced, `tmp`, using `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` above it.\n- For the 'k' loops in the second nest, place `#pragma ACCEL PARALLEL FACTOR=auto{32}` and specify the array to be reduced, `D`, using `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` above it.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2MM kernel function with specific optimization constraints using these natural language instructions:\n\n- Identify the function 'kernel_2mm' with parameters for matrix dimensions and matrices 'alpha', 'beta', 'tmp', 'A', 'B', 'C', and 'D' as a hardware acceleration kernel.\n- Inhibit automatic pipelining of the outermost loops 'i'.\n- Do not partition the outermost loops 'i' and process them iteratively without tiling or parallel execution.\n- For the inner loop 'j' over 'tmp' and 'D', enable pipelining but do not tile or split the iterations, also parallelize to a fixed factor for 'tmp' update and a maximum available factor for 'D' update.\n- For loop 'k', enable reduction on 'tmp' and 'D' matrix updates and process the iterations sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a hardware-accelerated ADI kernel function with the following characteristics: the function `kernel_adi` should be identified as a hardware kernel for acceleration, all loops should not be automatically pipelined, but operate in their entirety, and the loop iterations should be executed sequentially, not in parallel. Specifically, for the inner loop of the 'row sweep' block, enable parallel processing with a factor of 16. Use appropriate #pragma directives for kernel functions, pipeline control, tiling, and parallel loop execution, setting 'auto' pragmas to the prescribed factors where indicated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS C code with corresponding pragmas, please provide the following concise instruction:\n\n\"Define a hardware-accelerated kernel for the Alternating Direction Implicit (ADI) method applied to a computational grid. For the kernel body, and the outermost and two inner loops, disable automatic pipelining to keep the original sequential loop execution. Ensure no tiling or parallel optimization is applied to the nested loops by using the smallest factor to preserve their full iteration range without sub-blocking or concurrent execution. In the second innermost loop, employ a flattened pipeline pragma to allow the compiler to optimize the loop flattening for potentially higher throughput.\"\n\nSpecific pragma values to mention:\n1. To treat the following function as a hardware kernel, use `#pragma ACCEL kernel`.\n2. To disable automatic pipelining, use `#pragma ACCEL PIPELINE auto{off}`.\n3. To avoid tiling the loops, use `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. To keep the original sequential iteration of the loops, use `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n5. To allow loop flattening optimizations in the pipeline, use `#pragma ACCEL PIPELINE auto{flatten}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` as a hardware acceleration kernel, then sequentially process the outer loops without pipelining or parallel execution and operate on the inner loops with varying degrees of parallelism.\n\nFor the first two nested loops: do not pipeline the outermost loop, do not tile its iterations, and run them sequentially. In the second loop, apply automatic pipelining, leave the iteration chunks whole, and parallelize the processing by a factor of 4. For the innermost loop, enable a reduction on `tmp` and set a parallel factor of 10.\n\nFor the last two nested loops: again, do not pipeline the outermost loop, do not tile its iterations, and run them sequentially. Pipeline the second outermost loop with automatic settings, do not apply tiling, and set parallel execution with a factor of 16. In the innermost loop, apply a reduction on `D` with a parallel factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code to create a kernel function named 'kernel_adi' which takes 'tsteps' and 'n' as integer parameters, as well as four 2D arrays 'u', 'v', 'p', and 'q'. Inside the function, initialize several double precision variables with specific equations related to dimensions 60 by 60 and a timestep dimension of 40. Use pragmas to disable automatic pipelining, enforce original loop tiling with a factor of 1, and execute loop iterations sequentially with no parallelization for all loops in the function. Moreover, use a parallelization factor of 2 only for the inner loops within the 'Row Sweep' section. Ensure that initial and last values of 'v' and 'u' arrays in the sweeps are set to 1.0, and apply backward substitutions using 'p' and 'q' arrays to update 'v' and 'u' arrays respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code for a function designated as a hardware kernel that performs a 2-matrix multiplication and addition operation on matrices A, B, and C, updating matrix D with the result, scaled by alpha and beta respectively. The outer loops should not be automatically pipelined and should execute fully without being split into subloops. The first intermediate loop should be flattened into a single pipeline stage when possible and allow a limited amount of parallel execution of iterations. The innermost loop of the first part should compute the products in a sequential fashion but allow the sum to be computed in parallel. The second part of the function should not pipeline the outer loop and partially parallelize the intermediate loop, while fully parallelizing the innermost loop for reduction in a highly parallel manner. Apply these pragmas accordingly:\n\n1. Treat the entire function as a hardware kernel.\n2. Disable automatic pipelining for all top-level loops.\n3. Prevent the top-level loops from being tiled or parallelized.\n4. Flatten the second loop into a single pipeline stage.\n5. Tile the second intermediate loop with a specific factor to encourage data reuse.\n6. Parallelize the deepest loop in the first part with a factor that permits some parallelism but not full parallelism, allowing the reduction result on 'tmp' to be computed in parallel.\n7. In the second main loop, only allow a double amount of parallelism compared to the first part's intermediate loop, ensuring the reduction operation on 'D' is highly parallel by choosing a significantly higher factor than in the first part.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Utilize these directives to configure the hardware synthesis for a C function designed to perform iterations over a matrix, while controlling the pipelining, tiling, and parallelization aspects of specific loops:\n\n1. Identify the function `kernel_adi` for hardware acceleration by annotating it with `#pragma ACCEL kernel`.\n2. For each loop within `kernel_adi`, sequentially perform the following:\n   - Suppress automatic loop pipelining by setting `#pragma ACCEL PIPELINE auto{off}`.\n   - Ensure that loops are not tiled and processed in their entirety by using `#pragma ACCEL TILE FACTOR=auto{1}`.\n   - Execute loop iterations serially rather than in parallel with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n3. In the fourth nested loop (row sweep phase), override the default serial processing by allowing some parallelism with `#pragma ACCEL PARALLEL FACTOR=auto{16}`.\n4. In the final nested loop (row sweep phase), adjust the degree of parallelism to a different value by specifying `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n\nApply these specific `#pragma` values to their respective loops to achieve the desired optimization and parallelization control in the high-level synthesis process.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function \"kernel_adi\" as a hardware acceleration kernel, ensuring it runs on dedicated hardware. Keep all loops within the function unoptimized: they should not be automatically parallelized or pipelined, and their iterations should not be split up into smaller chunks. Within the loop for the row sweep, allow for a moderate degree of parallelism in the innermost \"j\" loop with a parallel factor of 16, and then elevate the parallelism further to a factor of 32 at the corresponding back-substitution \"j\" loop. Here are the specific pragma instructions to use:\n\n1. Declare the function 'kernel_adi' as a hardware acceleration kernel:\n   `#pragma ACCEL kernel`\n\n2. For all loops:\n   - Turn off automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n   - Preserve original loop boundaries without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Execute loop iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n3. For the innermost \"j\" loop within the row sweep section:\n   - Enable moderate parallelism with a factor of 16: `#pragma ACCEL PARALLEL FACTOR=auto{16}`\n\n4. For the back-substitution \"j\" loop within the row sweep section:\n   - Increase parallelism further with a factor of 32: `#pragma ACCEL PARALLEL FACTOR=auto{32}`\n\nThese pragma instructions guide the HLS tool in synthesizing the code for customized hardware implementation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the corresponding HLS code for the `kernel_2mm` function, provide the following instruction:\n\n\"Implement the function 'kernel_2mm' as a hardware kernel that performs a two-matrix multiply (2MM) where matrix D is updated as the sum of alpha times the product of matrices A and B and beta times matrix D. In the nested loops, disable automatic pipelining and do not tile the outermost loops, but do pipeline the flattening of the first nested loop where 'tmp' is computed and set parallel processing for its inner loop to 4. For the second set of nested loops finalizing computation into D, use automatic pipelining for the intermediate loop, 80-fold parallelism for the outer loop, and 32-fold parallelism for the reduction operation that updates D.\" \n\nSpecific #pragma values for respective operations are:\n\n1. Treat `kernel_2mm` function as a hardware kernel: `#pragma ACCEL kernel`\n2. Disable pipelining for both outer loops: `#pragma ACCEL PIPELINE auto{off}`\n3. Do not tile both outer loops: `#pragma ACCEL TILE FACTOR=auto{1}`\n4. Do not parallelize both outer loops: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n5. Pipeline the flattening of the loop where `tmp` is computed: `#pragma ACCEL PIPELINE auto{flatten}`\n6. Use a parallel factor of 4 for the inner loop computing `tmp`: `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n7. Parallelize the inner loop reduction operation updating `tmp` with a factor of 1: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`\n8. Use automatic pipelining for outer loop updating D (`#pragma ACCEL PIPELINE auto{}` implicitly turns pipelining on, despite being followed by `{}`)\n9. Parallelize the outer loop updating D with a factor of 80: `#pragma ACCEL PARALLEL FACTOR=auto{80}`\n10. Parallelize the reduction operation updating D with a factor of 32: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}`\n\nEnsure the code adheres to these instructions to map to the provided HLS pragmas correctly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware-accelerated kernel, making sure that all loops are executed in the original sequential order without parallelism or pipelining, except for the second inner loop in the 'Row Sweep' section, which should be parallelized with a parallelism factor of 2, and the final inner loop of the same section should be parallelized with a parallelism factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a directive to identify the 'kernel_2mm' function as a hardware acceleration kernel. Then, apply specific loop optimization directives to the nested loops within this function. For the outermost loop, ensure it's not pipelined and that it is fully unrolled without any tiling. Within the first inner loop, enable full loop flattening, perform no tiling, and enable limited parallelism by a factor of four. Inside the innermost loop, enable parallelism with no specific factor for reduction on 'tmp'.\n\nFor the second outermost loop, disable pipelining again and enforce no tiling with no factor change for parallel execution. In its first inner loop, activate the default pipeline optimization without specifying the tiling while setting aggressive parallel execution by a factor of 80. Subsequently, in the most nested loop, enable parallelism with reduction on 'D' and restrict the parallel factor to four.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_adi' function as a hardware acceleration kernel, disable automatic pipelining, maintain original loop form, and prevent loop iteration parallelization for all loops except for the second inner 'j' loop in the 'Row Sweep' section, which should have a parallel factor of 29, and the last inner 'j' loop in the 'Row Sweep' section, which should have a parallel factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to `designate the function kernel_2mm as a hardware acceleration kernel, disable automatic loop pipelining for the outer loop computations, maintain the original loop structure without tiling, enforce sequential execution of the outer loop iterations, flatten the pipelining of the second loop to dynamically optimize resource usage, allow minimal tiling, parallelize the inner loop with four iterations computed simultaneously, and apply a parallel reduction strategy to accumulate on the tmp array with a single iteration at a time. For the final loop nest, do not pipeline or tile, allow all iterations of the second loop to run in parallel, and then apply a parallel reduction strategy with a group size of 32 when accumulating on the D array`. Here's the concise instruction for the specified pragmas:\n\n- Treat `kernel_2mm` as a function to be accelerated in hardware.\n- For the first pair of nested loops:\n  - Disable automatic pipeline optimization for the first loop.\n  - Maintain the loop's original structure without partitioning by setting the tile factor to 1.\n  - Execute the first loop iterations sequentially by setting the parallel factor to 1.\n  - Within the second loop:\n    - Enable dynamic optimization by flattening pipelining.\n    - Allow minimal loop restructuring by setting the tile factor to 1.\n    - Parallelize the innermost loop with a group of 4 iterations running together, while reducing the `tmp` array one iteration at a time.\n- For the second pair of nested loops:\n  - Disable automatic pipeline optimization for both loops.\n  - Maintain the loops' original structure without partitioning by setting the tile factor to 1.\n  - Allow the second loop to execute all iterations in parallel by setting the parallel factor to 80.\n  - Apply a parallel reduction on the `D` array with a group size of 32 during the accumulation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code using C by treating the 'kernel_2mm' function as a hardware acceleration kernel. Avoid pipeline optimization entirely for the outermost loops, but for inner loops, apply full loop flattening in the first set of nested loops and allow the second set to be pipelined with an unspecified factor. Do not break the outermost loops into smaller chunks, but inside the first nested loop, enable parallelization with a factor of 4. In the reduction steps, don't parallelize the outer parts, but set an unspecified parallel factor for the innermost reductions while maintaining the requested reduction variables. Apply a parallel factor of 80 for the second set of inner loops and 8 for the innermost reductions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function that performs the Alternating Direction Implicit (ADI) method on a 60x60 grid with specific optimization directives:\n\n1. Declare the function as a hardware acceleration kernel.\n2. Disable automatic pipelining for each loop within the function.\n3. Prevent loop tiling optimizations by setting the tile factor to 1 for each loop.\n4. Ensure each loop runs sequentially by using a parallel factor of 1. However, set the innermost loop of the 'Row Sweep' section to have a parallel factor of 16 and the second innermost loop of the 'Row Sweep' to have a parallel factor of 8, implying more parallel execution for these loops specifically.\n\nUse the following pragmas accordingly: \n- For declaring the function as a hardware kernel, use `#pragma ACCEL kernel`.\n- To disable automatic pipelining, use `#pragma ACCEL PIPELINE auto{off}` before the loop.\n- To set the tile factor to 1, use `#pragma ACCEL TILE FACTOR=auto{1}` before the loop.\n- For sequential loop execution, use `#pragma ACCEL PARALLEL FACTOR=auto{1}` for all loops except the ones specified with factors of 16 and 8, where you should use `#pragma ACCEL PARALLEL FACTOR=auto{16}` and `#pragma ACCEL PARALLEL FACTOR=auto{8}` respectively before the loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. In the first double-nested loop, allow the compiler to automatically optimize the outer loop with pipelining, tiling, and parallel processing, with parallel processing set for 2-way concurrency in the inner loop and 32-way concurrency for the reduction operation within the innermost loop. In the second double-nested loop, prevent pipelining optimizations on the outer loop, require the inner loop to be treated as fully unrolled, and for the innermost loop, permit the default sequential processing without parallel optimization, except for treating the reduction operation on 'D' sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the provided HLS C code and its associated pragmas into an HLS code generation instruction to achieve the specified behavior for loop optimizations and kernel acceleration:\n\n\"Generate HLS code for a parallelized alternating direction implicit (ADI) solver kernel that accelerates the entire function as a hardware kernel. Within the kernel, maintain the original loop structure without pipelining, tiling, or parallelizing any of the loops, hence preserving the original sequential execution order. Specifically, for the outer time loop and the first layers of both column and row sweep loops, disable automatic pipelining and set the tiling and parallel factors to 1 to execute iterations sequentially. However, in the second layer of the row sweep loop, apply a parallel factor of 2 for mild concurrency, and for the backward substitution phase within the row sweep, engage a higher level of parallelism by setting the parallel factor to 29.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the function 'kernel_adi' into a hardware kernel for acceleration, ensuring that none of the loops within the function are pipelined or tiled, and that each loop iteration is processed sequentially. However, explicitly set the parallelism factor of the second inner loop in the 'Row Sweep' section to 29 and the parallelism factor of the last loop to 32, to enable limited parallel execution for these loops only.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_2mm` as a hardware acceleration kernel that takes matrix dimensions and matrix data as arguments, performs matrix multiplication followed by matrix addition according to the formula `D := alpha*A*B*C + beta*D`, and ensures the following compilation directives:\n\n1. Disable automatic pipelining for the outermost loops that iterate over the first dimension of `tmp` and `D`.\n2. Prevent tiling optimization for the outermost loops, ensuring they run in full without being split.\n3. Execute the outermost loops without parallelizing their iterations.\n4. For the `j` loops nested inside the `i` loops, allow a complete flattening transformation for pipelining.\n5. Similarly, prevent tiling optimization for the `j` loops.\n6. Instruct the compiler to attempt to parallelize the inner loop iterations for the `j` loop while calculating `tmp` with a factor of 4, and for the `j` loop while updating `D` with a factor of 5.\n7. For the innermost `k` loops, ensure a parallel reduction on the `tmp` and `D` arrays with no additional parallelism beyond the original loop structure.\n\nSet specific `#pragma` values for each of the compiler directives as follows:\n\n- Use `#pragma ACCEL kernel` before the function definition to indicate its role as an acceleration kernel.\n- Use `#pragma ACCEL PIPELINE auto{off}` before the main `i` loops to turn off automatic pipelining.\n- Set `#pragma ACCEL TILE FACTOR=auto{1}` before the main `i` loops to avoid tiling optimization.\n- Set `#pragma ACCEL PARALLEL FACTOR=auto{1}` before the main `i` loops to execute iterations sequentially.\n- Place `#pragma ACCEL PIPELINE auto{flatten}` before the nested `j` loops to enable complete loop flattening for pipelining.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{4}` and `#pragma ACCEL PARALLEL FACTOR=auto{5}` before the nested `j` loops for the `tmp` and `D` calculations respectively to set the desired parallelization factors.\n- Add `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` before the innermost `k` loops for reduction operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function with the following directives:\n\n- Declare the function as a hardware acceleration kernel.\n- Enable automatic pipelining for the first two nested loops but disable it for the outer loop in the second two nested loops. Flatten the pipeline for the inner loop within the second pair of nested loops.\n- Set the automatic tiling factor to the default for all potential tiling optimizations.\n- Parallelize the first and third nested loops at FACTOR=8 and FACTOR=2, respectively, for the multiplication of matrices A and B, and set the reduction variable to 'tmp'.\n- In the multiplication updating matrix D with tmp and C, disable parallelism by setting the parallel factor to 1 and the reduction variable to 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a High-Level Synthesis (HLS) code for a hardware kernel function named 'kernel_adi' that performs Alternating Direction Implicit (ADI) iterations on four 2-dimensional arrays 'u', 'v', 'p', and 'q'. The function initializes several scalar variables for computation within. Ensure the top-level loop over time steps and the nested loops for 'Column Sweep' and 'Row Sweep' are not automatically pipelined nor partitioned; instead, run their iterations serially. The innermost loops within 'Column Sweep' and 'Row Sweep' are also to run serially, with the original loop order for the reverse iterations along the 'j' index preserved. Use appropriate pragma directives to prevent the compiler from applying optimizations that would pipeline or parallelize the loops automatically, parallelize the loops with a factor of 1, or tile the loops with a factor of 1, while keeping the structure of the nested loops intact.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS C code for a kernel function `kernel_2mm` that accelerates the computation of `D := alpha*A*B*C + beta*D` with the following constraints:\n\n1. Declare the function as a hardware acceleration kernel.\n2. Ensure the outermost loops for computing `tmp` and `D` are not automatically pipelined.\n3. For the computation of `tmp`, do not split the `i` and `j` loops into smaller chunks. Process two iterations of the `j` loop in parallel and pipeline its inner contents with the default behavior.\n4. The reduction on `tmp` within the `k` loop should be performed with 32 parallel computations.\n5. For the computation of `D`, ensure that the loops are processed in their entirety, without automatic tiling or parallelization, and that iterations are executed sequentially.\n6. Flatten the pipelining for the `i` and `j` loops in the computation of `D`.\n7. Perform the reduction on `D` within the `k` loop without any parallel computation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for the 2mm kernel function, which processes a two-matrix multiplication followed by a matrix addition with scalars. In the first main loop, do not employ automatic pipelining, use the original loop form, and execute loop iterations serially. Within the nested loop, flatten the loop for pipelining, use original loop form, and parallelize with a factor of 4 while also performing a reduction operation on 'tmp' without parallelism. For the second main loop iteration, again avoid automatic pipelining, use the original loop form, execute iterations serially, and within the nested loop, disable pipelining, use the original loop form, but parallelize with a factor of 80 and perform a reduction operation on 'D' with a parallel factor of 50. Treat the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code by marking the `kernel_3mm` function as a hardware acceleration target and setting the following loop pragmas: for all main loops, disable pipelining, use the original loop form without tiling, and execute iterations sequentially. For the innermost loop calculating `E` matrix, allow the k-loop to parallelize with a factor of 6. For the innermost loop calculating `F` matrix, use the maximum allowed parallelism of 80 for the k-loop. Finally, for the `G` matrix computation, enable pipelining with a loop flattening directive on the i-loop and parallelize the j-loop with a factor of 8, while keeping the innermost k-loop sequential.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{6}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes with appropriate pragmas for the `kernel_2mm` function, you should provide the following instruction:\n\n\"Implement a hardware-accelerated kernel function for a two-matrix multiplication followed by a matrix addition, where the first multiplication loop is not pipelined, tiled, or parallelized at the top level, but the innermost loop for the first multiplication is parallelized with maximum unrolling. For the second part of the computation, only the innermost loop is parallelized with moderate unrolling, while other loops are not pipelined, tiled, or parallelized.\"\n\nSpecifically:\n\n- Treat the `kernel_2mm` function as a hardware acceleration kernel.\n- For the first outer loop `i`, do not apply pipeline optimization, do not tile the loop, and do not parallelize it by running iterations sequentially.\n- Inside the first `j` loop, enable automatic pipeline optimization, tile the loop with a factor of 1 (equivalent to not tiling), and parallelize it with a factor of 4.\n- Inside the innermost `k` loop of the first matrix multiplication, enable maximum parallelization with a reduction operation on the `tmp` array, with a suggested unrolling factor of 32.\n- For the second outer loop `i`, do not apply pipeline optimization, do not tile the loop, and do not parallelize it by running iterations sequentially.\n- Inside the second `j` loop, do not apply pipeline optimization, do not tile the loop, and only parallelize it with a moderate factor of 5.\n- Inside the innermost `k` loop of the matrix addition, use parallelization with a reduction operation on the `D` array, with a suggested unrolling factor of 4.\n\n#pragma values to use:\n\n- `#pragma ACCEL kernel` to indicate the function is for hardware acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` to prevent the pipelining of a loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to keep the loop in its full form without tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to run the loop iterations sequentially (not in parallel).\n- `#pragma ACCEL PARALLEL reduction = <array> FACTOR=auto{<unroll_factor>}` to enable parallelization with a specified unrolling factor and a reduction on the given array.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "You want to declare a kernel function for HLS that performs matrix multiplication on three pairs of matrices (A*B, C*D, E*F) without pipelining any of its outer loops, ensuring that the entire loop is executed in its original form and run sequentially. Additionally, set specific parallel factors for the innermost loop of each matrix pair multiplication to optimize for different degrees of parallelism. Here are the concise instructions:\n\n- Treat the entire function as a hardware kernel designated for acceleration.\n- For each outer matrix multiplication loop over `E=A*B`, `F=C*D`, and `G=E*F`, do not implement any loop pipelining, do not tile the loops, and ensure the loop iterations execute sequentially.\n- Specifically, for the innermost loops:\n  - In the multiplication for `E=A*B`, allow a parallel factor of 12 in the accumulation loop over `k`.\n  - In the multiplication for `F=C*D`, set the parallel factor to 32 for the accumulation loop over `k`.\n  - For `G=E*F`, set the parallel factor to 7 for the second-level loop over `j` and 50 for the innermost loop over `k`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{12}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C function named `kernel_adi` that iteratively updates four 2D arrays over a series of time steps. Mark the function for hardware acceleration. Within the function, initialize constants for discretization steps and coefficients for an Alternating Direction Implicit (ADI) method. For the main time-stepping loop and its nested loops, disable automatic loop pipelining, set the loop tile factors to fully unroll the loops (treat as a single chunk of computation without tiling), and force parallel loop execution with a single iteration per cycle (effectively disabling parallelism). Within the nested loops, conduct two sweeps - a column sweep and a row sweep - updating two arrays in each sweep with adjustments in row updates to use a higher parallel factor in the inner loop, only in the case of backward iteration. Use appropriate pragmas before each loop to apply these settings, and denote the start and conclusion of the key computational region with `#pragma scop` and `#pragma endscop`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) code for a two-matrix multiplication kernel that follows these guidelines:\n\n1. Define the kernel function for acceleration, with parameters for matrix dimensions ni, nj, nk, nl and scalar values alpha, beta, and matrices tmp, A, B, C, and D.\n2. Implement the first computation phase, `D := alpha*A*B*C + beta*D`, without auto-pipelining and with the default tiling and parallelization factors for the outermost loop.\n3. In the inner loop that computes the temporary matrix `tmp`, employ automatic pipelining but disable tiling, and set a parallel factor of 2 for the loop iterating over 'j'.\n4. Utilize a heavy parallel factor of 32 for the innermost loop that computes the `tmp` values with reduction on `tmp`, reflecting a significant amount of paralleled computation in summing up the matrix products.\n5. For the final computation phase updating matrix `D`, turn off pipelining for the outer loop, use the default tiling, and specify that iterations of this loop run sequentially (parallel factor of 1).\n6. Within the inner loop that updates `D`, use the 'flatten' directive for pipelining to ensure that loops are fully unrolled if possible, and apply the default tiling and parallelization factors.\n7. For the innermost loop of this phase, keep the reduction on `D`, with all iterations processed sequentially, by setting a parallel factor of 1 to avoid parallelization.\n\nEnsure to place the appropriate pragmas with their values before each loop or computational block respectively based on the above instructions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function named 'kernel_adi' that accelerates the alternating direction implicit (ADI) method, using the following specifications:\n\n1.  Designate the 'kernel_adi' function as a hardware acceleration kernel.\n2.  Initialize constant values and calculate coefficients at the beginning without any optimization pragmas.\n3.  Set up a time-stepping loop for 40 iterations:\n    - Within the loop, perform a 'Column Sweep':\n      - Disable loop pipelining.\n      - Disable loop tiling.\n      - Disable loop parallelization.\n      - For each inner loop, execute the loop iterations sequentially, without unrolling or interleaving.\n      - Initialize the edges of matrix 'v' and set up tridiagonal matrices 'p' and 'q' with forward substitution, iterating normally without reversing the loop direction.\n      - Use backward substitution to obtain 'v' without reversing the loop directions in the code.\n    - Perform a 'Row Sweep' with similar constraints as the 'Column Sweep':\n      - Disable pipelining.\n      - Disable tiling.\n      - Disable parallel handling of the inner loop.\n      - Apply forward substitution and then backward substitution using sequential loop iterations.\n\nSpecific pragma comments for HLS directives to use in the appropriate locations are as follows:\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{off}` for each loop where pipelining is disabled.\n- Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}` for each loop where tiling is disabled.\n- Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}` for each loop where parallelization is disabled.\n\nNote: The backward substitution loop at the end of the Row Sweep has been set to partially unroll with a `#pragma ACCEL PARALLEL FACTOR=auto{58}`. This should be kept as the only exception to the otherwise sequential operation of loop iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following HLS pragmas and their settings into concise natural language instructions:\n\n- Treat the `kernel_2mm` function as a hardware kernel to be accelerated.\n- For the outer loop iterating over `i`, attempt automatic optimization to improve performance without a specific strategy.\n- For the inner loop iterating over `j` within the first outer loop, attempt automatic optimization to improve performance without a specific strategy, and execute iterations with an automatic determination of parallelism using 2 concurrent processes.\n- For the loop iterating over `k` within the inner loop for calculating `tmp`, parallelize the reduction on `tmp` automatically with a suggestion of 32 simultaneous operations.\n- For the second occurrence of the outer loop iterating over `i`, do not attempt to pipeline the loop automatically.\n- For the inner loop iterating over `j` within the second outer loop, flatten nested loops and fully unroll if possible for hardware implementation.\n- For the loop iterating over `k` within the second inner loop for updating `D`, keep the original, full loop form with sequential execution, i.e., with a single process at a time.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a kernel function 'kernel_adi' designed for accelerating an Alternating Direction Implicit (ADI) solver, maintaining the original loop structures and not introducing any loop pipelining or additional parallelism, except for the innermost loop of the second sweep, where parallelism is doubled.\n\n1. Indicate the 'kernel_adi' function as a hardware accelerator kernel.\n2. Set global loop directives to prevent pipelining, tiling, and parallelization for all loops in the function:\n   - Disable automatic loop pipelining.\n   - Prevent loop tiling (i.e., do not break loops into smaller chunks).\n   - Enforce sequential execution of loop iterations.\n3. Within the row sweep section, for the second nested loop, increment parallelism to a factor of two.\n\nThe associated #pragma directives to place before each loop (and the one for the function) will be as follows:\n\n- Use `#pragma ACCEL kernel` to indicate the function should be treated as a hardware accelerator kernel.\n- Apply `#pragma ACCEL PIPELINE auto{off}` before each loop to disable loop pipelining.\n- Apply `#pragma ACCEL TILE FACTOR=auto{1}` before each loop to keep the loop structure unaltered (no tiling).\n- Apply `#pragma ACCEL PARALLEL FACTOR=auto{1}` before the loops that require sequential iteration.\n- Specifically, use `#pragma ACCEL PARALLEL FACTOR=auto{2}` before the innermost loop of the row sweep to allow that loop to process two iterations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, within which you should:\n\n1. For all loops by default, disable automatic pipelining, keep loops unbroken and run iterations sequentially.\n   \n2. In the first nested level, specifically for the second loop, facilitate parallel execution with a factor of four but without loop pipelining or tiling.\n\n3. Within the second nested level, ensure that the temporary array 'tmp' can accumulate values concurrently across iterations but with sequential processing.\n\n4. For the outer loop in the second major block, apply loop flattening to allow pipelining optimizations and set the parallelism factor to eighty for the second loop, while keeping the loop as a whole without tiling and sequential iteration of the innermost loop, also guaranteeing that the 'D' array accumulates values concurrently across iterations.\n\nCorrespondingly, use the following pragmas:\n\n1. Before each loop, insert `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` to meet the default behavior of no pipelining, no tiling, and sequential iteration.\n\n2. Before the second loop within the first major loop-block, insert `#pragma ACCEL PARALLEL FACTOR=auto{4}` while maintaining the default no pipelining and no tiling for this loop.\n\n3. For the innermost loop in the first major loop-block, use `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` to enable concurrent accumulation in 'tmp'.\n\n4. In the second major loop-block, use `#pragma ACCEL PIPELINE auto{flatten}` before the outer loop for pipelining with loop flattening and insert `#pragma ACCEL PARALLEL FACTOR=auto{80}` before the second loop to allow a high degree of parallel processing. Within the innermost loop, use `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` to enable concurrent accumulation in 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a three-matrix multiplication kernel function that disables pipelining at the top level of each multiplication but enables it within the innermost loop, with all loops processing in their original form without tiling, except for the loop where matrices E and B multiply, which should exploit full parallelism available in B's iterations. Make sure the function treats itself as a hardware kernel and appropriately tag only specific loops to run iterations sequentially, particularly ensuring the loop that multiplies matrices C and D runs completely in sequence and the final multiplication loop running two iterations in parallel. \n\nUse the following pragma statements:\n\n1. `#pragma ACCEL kernel` to mark the function as a hardware kernel.\n2. `#pragma ACCEL PIPELINE \"off\"` to disable pipelining for the three top-level matrix multiplication sections.\n3. Inside each of the top matrix multiplication sections:\n   - `#pragma ACCEL PIPELINE` to enable automatic pipelining within the innermost loop without constraints.\n   - `#pragma ACCEL TILE FACTOR=1` after the `PIPELINE` pragma to indicate no tiling should occur.\n   - `#pragma ACCEL PARALLEL FACTOR=1` to enforce sequential processing of iterations unless otherwise specified.\n4. In the loop multiplying matrices A and B that fills matrix E:\n   - `#pragma ACCEL PARALLEL FACTOR=60` within the `k` loop to fully parallelize the innermost loop across B's columns.\n5. In the loop multiplying matrices C and D that fills matrix F:\n   - `#pragma ACCEL PARALLEL FACTOR=1` within the `k` loop to enforce sequential processing.\n6. In the loop multiplying matrices E and F that fills matrix G:\n   - `#pragma ACCEL PARALLEL FACTOR=2` right before the `j` loop to allow two iterations of the loop to run in parallel.\n   - `#pragma ACCEL PARALLEL FACTOR=50` within the `k` loop to parallelize across E's columns.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm (two matrix multiply) kernel performing D = alpha * A * B * C + beta * D with the following characteristics:\n\n1. Mark the function as a hardware acceleration kernel.\n2. Use automatic pipeline optimization for the outer loops.\n3. For the first set of nested loops:\n   - Automatically determine the tiling factor for the outer loop.\n   - Parallelize the innermost loop with an automatically chosen factor that allows for looping over 32 iterations in parallel, and enable reduction on the 'tmp' array.\n4. Deactivate automatic pipeline optimization for the second set of outer loops.\n5. Flatten the pipeline for the inner loop of the second set, while automatically determining its parallelism factor, aiming for 4 parallel iterations.\n6. Do not parallelize the innermost loop of the second set.\n\nEach directive should be in the form of `#pragma ACCEL <directive> auto{<value>}` corresponding to the respective optimizations and constraints described.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' for hardware acceleration without automatic pipeline optimization, and within this function, process the two nested loops individually with specific parallelism instructions:\n\n1. In the first two-level loop nest:\n    - Do not automatically pipeline the outermost loop, but allow for full loop unrolling with a parallelization factor of 32 for the 'i' loop.\n    - In the inner 'j' loop, apply default pipelining, refrain from tiling, and set a parallelization factor of 2 to allow a moderate level of concurrent loop iteration execution.\n    - In the most inner 'k' loop, enable aggressive parallel reduction on 'tmp' by fully unrolling the loop with a parallelization factor of 32.\n\n2. In the second two-level loop nest:\n    - Avoid automatic pipeline optimization and execute iterations of the 'i' loop sequentially, without parallelism or tiling.\n    - For the 'j' loop, apply pipeline flattening to process the entire loop in a single stage, and again, avoid tiling and parallelization to preserve sequential execution.\n    - Within the most inner 'k' loop, disable parallelization completely to ensure iterations are processed one after another, with a focus on preserving reduction operations on 'D' sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for an ADI kernel function handling a numerical PDE solver, using C pragmas to control loop optimization: designate the function as a hardware acceleration kernel; disable automatic loop pipelining; prevent loops from being tiled and run iteratively without parallelization, except for one specific loop with a parallelization factor of 32 and another with a factor of 16. Use the corresponding pragma values '#pragma ACCEL kernel', '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{1}', '#pragma ACCEL PARALLEL FACTOR=auto{1}', with factors adjusted to 32 and 16 as mentioned.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function 'kernel_2mm' that performs a matrix multiplication and addition operation on matrices of specific dimensions without pipeline optimization for the outer loops, but with varying levels of parallelism and tiling in the inner loops, as follows:\n\n1. Declare the function as a hardware kernel for acceleration.\n2. Disable automatic pipelining for the first level of the nested loops.\n3. Keep the first loop in its original, full form without tiling or parallelizing the iterations.\n4. For the inner loop within the first loop nest, flatten the iterations into a single pipeline stage, maintain a tiling factor of 1, and set a parallelism factor of 4.\n5. For the innermost loop of the first loop nest, ensure it reduces to a single value 'tmp' with an enforced parallelism factor of 1.\n6. Again, disable automatic pipelining for the second level of nested loops.\n7. For the second loop nest, maintain the loops as full and untiled with a parallelism factor of 1 for the outer loop.\n8. In the inner loop, enable automatic pipeline optimization and apply an extremely high parallelism factor of 80.\n9. For the innermost loop of the second loop nest, perform reduction over the 'D' matrix with a parallelism factor of 10.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use this natural language instruction to generate the described HLS C code:\n\n\"Define a function as a hardware acceleration kernel with the name 'kernel_adi' that takes time steps, a size parameter, and four two-dimensional arrays of doubles as input. Within the function, initialize scalar variables for constants and calculations. For each time step, iterate over 'i' and 'j' indices to perform a column sweep calculation and a row sweep calculation with dependencies between iterations. Avoid auto pipelining, tiling, and parallel processing for all loops. For the innermost loop that runs backwards in the column sweep code, and the corresponding innermost loop in the row sweep code, specifically override the parallel factor to the maximum achievable value only for the last loop.\"\n\nSpecific #pragma directive values to use for each case:\n\n- \"#pragma ACCEL kernel\" for stating the function as a hardware kernel.\n- \"#pragma ACCEL PIPELINE auto{off}\" for each loop to indicate that no automatic pipelining should be done.\n- \"#pragma ACCEL TILE FACTOR=auto{1}\" for each loop to make sure loops are processed in their full form without tiling.\n- \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" for all but the last loop to maintain sequential processing for each iteration without parallelism.\n- \"#pragma ACCEL PARALLEL FACTOR=auto{29}\" specifically for the last reversed loop to utilize the maximum achievable parallelism for that loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel that works as a hardware accelerator without automatic pipelining or tiling, but with specified degrees of parallelism. Specifically:\n- Treat the entire function `kernel_2mm` as a hardware kernel.\n- For the first outer loop with index `i`, disable automatic pipelining and use the full loop without tiling, but enable parallel processing for 16 iterations at a time.\n- Inside the first outer loop, for the loop with index `j`, enable pipelining automatically, avoid tiling, and process two iterations in parallel.\n- In the loop with index `k` within the `j` loop, reduce the `tmp` array while running 32 computations in parallel.\n- For the second outer loop with index `i`, process all operations serially, with neither pipelining nor parallelism enhancements, except flattening the pipeline for the inner `j` loop.\n- For the loop with index `k` within the second `j` loop, no parallelism or pipelining changes are necessary, just perform a simple reduction on the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_adi` as a hardware acceleration kernel, and ensure that all loops within the function are executed sequentially without any form of loop pipelining, tiling, or parallel processing optimization, except for one inner loop where parallel processing with a factor of 32 is applied. Use the appropriate pragmas to turn off automatic pipelining, set tiling to the default loop execution, and specify parallel factors accordingly, including overriding the parallel factor for one specific loop to 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware-accelerated kernel function named 'kernel_2mm' that performs a matrix multiplication and then a matrix addition operation. For the entire function, do not automatically manage loop pipelining, tiling or parallel execution; instead, make all decisions on these aspects manually for each loop:\n\n1. In the first pair of nested loops, disable pipelining for the outer loop, do not tile the iterations, and process loop iterations serially. In the innermost loop, for calculations involving the variable 'tmp', disable automatic loop pipelining but perform a reduction operation in parallel for 16 iterations at a time, with 'tmp' as the variable to be reduced.\n\n2. In the second pair of nested loops, disable automatic loop pipelining for the outer loop but flatten the nested loops where possible. Do not tile the iterations and run 20 iterations of the inner loop in parallel. For the innermost loop, disable pipelining and parallelism but enable a reduction operation for the variable 'D' without specifying a parallel factor.\n\nSpecifically, apply the following #pragma directives in your code:\n\n- `#pragma ACCEL kernel` to identify the function as a hardware kernel for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` outside both pairs of nested loops to prevent automatic loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{1}` prior to both pairs of nested loops to avoid loop tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the outer loops and `#pragma ACCEL PARALLEL FACTOR=auto{4}` for the first inner loop to adjust the degree of parallelism.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` for the innermost loops of each pair to specify variables for reduction and their parallel factors.\n- `#pragma ACCEL PIPELINE auto{flatten}` to enable loop flattening for the second pair of nested loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function to perform matrix multiplication and addition with the following directives:\n\n1. Declare the function as a hardware acceleration kernel using '#pragma ACCEL kernel'.\n2. In the first layer of nested loops, disable automatic pipelining for the parent loop using '#pragma ACCEL PIPELINE auto{off}', process the loop without tiling using '#pragma ACCEL TILE FACTOR=auto{1}', and prevent parallel processing of iterations using '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n3. Inside the second layer of nested loops, flatten the nested loop structure for pipelining using '#pragma ACCEL PIPELINE auto{flatten}', do not tile the loop using '#pragma ACCEL TILE FACTOR=auto{1}', and set the parallel factor to 2 for the j-loop using '#pragma ACCEL PARALLEL FACTOR=auto{2}'.\n4. Apply a parallel reduction pragma to the innermost k-loop with respect to the 'tmp' array to prevent data hazards and set the parallel factor to 1 using '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}'.\n5. For the second main loop block, again disable automatic pipelining, prevent tiling, and avoid parallel execution with the same pragmas as in the first layer.\n6. In the second nested loops, flatten the loop structure for pipelining, do not tile the loop, and set the parallel factor to the maximum loop iterations for the j-loop using '#pragma ACCEL PARALLEL FACTOR=auto{80}'.\n7. Apply a parallel reduction pragma to the inner k-loop with respect to the 'D' array to prevent data hazards and set the parallel factor to 1 using '#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C using natural language instructions, you would employ the following directives:\n\n1. Designate the function 'kernel_2mm' as a hardware acceleration kernel: \"Treat the 'kernel_2mm' function as a dedicated hardware module for acceleration.\"\n\n2. Inhibit loop pipelining at the first level 'i' of the nested loops: \"Do not automatically pipeline the outermost 'i' loop.\"\n\n3. Prevent tiling optimization at the first 'i' level loop: \"Execute the iterations of the first-level 'i' loop completely and sequentially without dividing it into smaller parallel blocks.\"\n\n4. Disallow parallel execution in the first 'i' level loop: \"Ensure that the iterations of the first-level 'i' loop are processed one after the other, not in parallel.\"\n\n5. Optimize the second 'j' level loop within the first level 'i' loop by flattening nested loops, if possible: \"Allow flattening of nested loops within the second-level 'j' loop to enhance parallelism opportunities.\"\n\n6. Forbid tiling optimization in the second 'j' level loop: \"Process the second-level 'j' loop iterations in their entirety, without sub-dividing them for parallel execution.\"\n\n7. Set a parallel factor for the second 'j' level loop: \"Divide the second-level 'j' loop into 4 concurrent execution threads to process iterations simultaneously.\"\n\n8. Use reduction parallelism for the innermost 'k' loop that updates 'tmp' array: \"Allow concurrent computation of partial sums for 'tmp' array updates in the innermost 'k' loop, with an enforced sequential addition.\"\n\n9. Disable loop pipelining for updating the 'D' array: \"Prevent automatic pipelining in the loop that updates the 'D' array.\"\n\n10. Enforce full loop execution without tiling for the 'D' array update loop: \"Execute the full loop for updating the 'D' array without dividing it into smaller chunks.\"\n\n11. Prohibit parallel loop execution for the 'D' array update loop: \"Ensure that the 'D' array update loop is processed sequentially from one iteration to the next.\"\n\n12. Flatten loop nests in the 'D' array update loop, if possible: \"Permit the flattening of nested loops in the 'D' array update loop to enhance parallelism opportunities.\"\n\n13. Allow a parallel factor of 2 for the loop that scales and updates 'D': \"Divide the iterations of the 'D' array scaling loop into 2 parallel threads for simultaneous processing.\"\n\n14. Enable parallel reduction for the innermost loop that updates 'D': \"Support parallel computation of partial results for updating the 'D' array, summing them sequentially at the end.\" \n\nThe #pragma values for auto{off}, auto{1}, and auto{flatten} are reflective of the instructions given and should be set accordingly to adhere to the described behavior. The 'auto' prefix is used to indicate that the pragma setting should be considered automatically by the tools, while the following value encodes the desired directive behavior.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code function named 'kernel_2mm' that treats the function as a hardware acceleration kernel, computes the matrix multiplication and addition of `alpha*A*B + beta*D`, and manages the loops with the following specifications:\n\n1. Do not pipeline the outer two loops, but let inner loops be automatically decided on pipelining.\n2. Do not tile the loops but let inner loops have an automatic tile factor.\n3. The first outer loop's iterations should be parallelized with a factor of 8 while the second outer loop's iterations should run serially.\n4. For the innermost loop of the first outer loop, enable a parallel reduction on the `tmp` array with a factor of 32.\n5. For the innermost loop of the second outer loop, enable a parallel reduction on the `D` array without parallelization.\n6. Flatten the pipelining of the loops wherever it is automatically determined as suitable.\n\nSpecifically, for #pragma directives, always use 'auto' except for:\n- Set PIPELINE to 'off' for the two outer loops.\n- Keep the TILE FACTOR to '1' to ensure loops are processed in full form.\n- Set PARALLEL FACTOR to '8' for the first outer loop and '1' for the second outer loop and inner loops.\n- Enable reduction with PARALLEL FACTOR set to '32' on 'tmp' and '1' on 'D' for the respective reductions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware kernel while optimizing the nested loop computations for a high-level synthesis process. For the first two layers of loops, flatten the nested iterations as appropriate and optimize with full parallelization while preventing automatic pipeline and tile factor adjustments. In the innermost loop, apply parallelization specifically with a reduction on `tmp` and a factor that ensures single iteration per cycle. For the subsequent set of loops that update matrix `D`, disable automatic pipelining for the outermost loop and enforce the iterations to run in sequence without tiling. For the second loop, enable pipelining without specifying a strategy, and in the innermost loop, employ a parallel reduction on `D` with a factor aiming for five operations concurrently. Here are the corresponding pragma directives for each instruction:\n\n- Treat the `kernel_2mm` function as a hardware acceleration kernel: `#pragma ACCEL kernel`.\n- For the first two layers of loops processing `tmp`, disable automatic loop pipelining and tiling: `#pragma ACCEL PIPELINE auto{off}` and `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Run the outermost loop for processing `tmp` sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Flatten the second loop (j-loop) that processes `tmp`: `#pragma ACCEL PIPELINE auto{flatten}`.\n- Retain the original loop form for the `j`-loop of `tmp`: `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Achieve a degree of parallelism that allows four iterations to compute concurrently in the `j`-loop when processing `tmp`: `#pragma ACCEL PARALLEL FACTOR=auto{4}`.\n- For the innermost loop (k-loop) calculating `tmp`, utilize parallelism with a reduction on `tmp` and aim for one cycle per iteration: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}`.\n- For the loops updating `D`, turn off loop pipelining for the outer loop: `#pragma ACCEL PIPELINE auto{off}`.\n- Enforce the outer loop for `D` to process one iteration at a time without tiling: `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Enable pipelining for the second (j-loop) updating `D`, with no specific optimization strategy given: `#pragma ACCEL PIPELINE auto{}`.\n- Specify no tiling for the j-loop of `D`: `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Optimize the parallel computation in the j-loop for updating `D` to handle 80 concurrent operations: `#pragma ACCEL PARALLEL FACTOR=auto{80}`.\n- In the innermost (k-loop) updating `D`, apply parallelism with a reduction on `D` and a concurrent operation factor of five: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the outermost loop 'i' with automation for pipelining and tiling, but without modifying these settings in the inner loops. In the 'j' loop for the first multiplication (E=A*B), do not use automation for pipelining, and ensure no tiling or parallelization modifications are made to the 'k' loop. For the 'j' loop in the second multiplication (F=C*D), again do not use pipelining but allow for automatically determined parallelism in the 'k' loop. Finally, for the 'j' loop in the third multiplication (G=E*F), do not use pipelining and run loops in its original form without any parallel or tiling optimization. Additionally, declare the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{14}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as a hardware acceleration kernel, disabling automatic pipeline optimization for all loops within it, and ensuring that each loop is executed in full without tiling or parallelizing. Specifically, for the innermost loop of the 'Row Sweep' section, allow parallel processing with a parallelization factor of 32, and for the reverse iteration loop in the same section, set the parallelization factor to 58.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function to be synthesized as hardware acceleration, disable any automatic loop pipelining, maintain the original loop structure without tiling, execute all loop iterations sequentially without parallelism, except for one specific inner loop where you allow a limited form of parallelism with a factor of 4 in the 'Row Sweep' section and a factor of 2 in the reversed loop within the same section.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware kernel function, `kernel_adi`, used for alternating direction implicit (ADI) method, ensuring:\n\n1. The `kernel_adi` function should be identified as a hardware acceleration kernel.\n2. Ensure no loop pipelining optimization by setting the `PIPELINE` pragma to 'off'.\n3. Preserve the original loop structure by setting `TILE FACTOR` to 1, preventing loop tiling.\n4. Execute loop iterations sequentially with `PARALLEL FACTOR` set to 1, except in the second inner loop of the row sweep phase where it should be set to 2 and in the reversed loop of the row sweep phase to 4.\n\nMake sure to apply these #pragma directives before all top-level loops and nested loops where indicated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS kernel function called `kernel_adi` designed to perform alternating direction implicit (ADI) computations for a 60x60 grid over a given number of timesteps. Scale the grid by setting DX and DY to the inverse of 60, and DT to the inverse of 40. Define B1 as 2.0, B2 as 1.0, and calculate `mul1` and `mul2` based on B1, B2, and DT. Initialize a, b, c with scaled negative and positive values involving `mul1`, and d, e, f with those involving `mul2`. Within the loop for time steps, perform calculations for the 'Column Sweep' and 'Row Sweep'. Prevent pipeline optimization, parallel execution, and tiling for loop iterations by setting their corresponding pragmas (#pragma ACCEL PIPELINE, #pragma ACCEL PARALLEL FACTOR, #pragma ACCEL TILE FACTOR) to auto{off}, auto{1}, and auto{1}, respectively. However, allow parallel execution for the innermost loops with a partial degree of 29 for the 'Row Sweep' by setting #pragma ACCEL PARALLEL FACTOR to auto{29}.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_adi` as a hardware acceleration kernel targeting the computation of the Alternating Direction Implicit (ADI) solver on a 60x60 grid for a fixed number of time steps, and enforce the following restrictions on loop optimization:\n\n1. Do not apply loop pipelining at any level of the loop nest.\n2. Do not tile the loops, meaning each loop should iterate over its full range without being split into smaller, tiled loops.\n3. Do not execute any of the loops in parallel; each iteration should depend on the completion of its predecessor.\n4. Specifically, for the innermost loop in the second row sweep operation, enable 29-fold parallelism on the iterations.\n5. Additionally, in the final loop of the same row sweep, experiment with 2-fold parallelism instead.\n\nInclude these instructions specifically as pragmas within the function:\n\n- Before each loop, apply `#pragma ACCEL PIPELINE off` to prevent automated loop pipelining.\n- After the pipeline pragma, use `#pragma ACCEL TILE FACTOR=1` to enforce that the loop is not tiled.\n- Follow the tile pragma with `#pragma ACCEL PARALLEL FACTOR=1` to ensure that loop iterations are not parallelized, except for the two loops mentioned above where the parallel factor pragmas should be set to 29 and 2 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for an ADI kernel function that processes a 2D grid through time steps, using specific pragmas: \n- Treat the function as an acceleration hardware kernel.\n- Avoid automated pipelining, tiling, and parallelization for the outer time loop and the first inner loops for column and row sweeps.\n- In the nested loops, keep the default single iteration per cycle for the column sweep but allow parallel processing of 29 iterations per cycle in the first nested 'row sweep' loop and 8 iterations per cycle in the second nested 'row sweep' loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel that does not have its outer loops automatically pipelined, nor tiled, nor parallelized. However, within the first two nested loops, enable automatic pipelining for the second level loop processing matrix `tmp`, with manual tiling and a parallel factor of 4. Inside the innermost loop of the first double-nested loop, employ a parallel reduction with a factor of 16 for accumulating into matrix `tmp`. For the next two nested loops, again do not pipeline the outer loop, manually tile, and use a parallel factor of 4 for the second level loop that updates matrix `D`, while applying a parallel reduction with a factor of 2 inside its innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware kernel without applying any automatic optimization techniques like pipelining, loop tiling, or parallel execution for the majority of the loops, except parallelize one loop with a factor of 32 and another with a factor of 29.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the following kernel function for hardware acceleration with specific constraints: Define the function as a hardware kernel for acceleration, and within its processing loops, deactivate automatic pipelining, preserve the original loop iteration space without tiling, and execute loop iterations sequentially. However, parallelize two specific inner loop nests within the 'Row Sweep' section with factors of 29 and 58 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration target; then optimize matrix multiplications for 'E = A * B', 'F = C * D', and 'G = E * F' within it. For matrix 'E', automate pipelining and tile it with an optimal block size, iterating serially through the matrix dimensions. For matrices 'F' and 'G', prevent pipelining and tiling, executing the outer loops distinctly. Operate innermost loops for all matrices sequentially, with no concurrent iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{40}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, you can use the following comprehensive but concise natural language instruction:\n\n\"Define a hardware acceleration kernel for a 2D ADI solver with given timesteps and dimensions that inhibits loop pipelining, enforces original loop boundary processing without tiling or parallel execution enhancements for all but one inner loop, where you should specify a parallel factor of 16 for traversing elements in the 'Row Sweep' section and a parallel factor of 4 for the reversed loop within the same section.\"\n\nSpecific #pragma directives and values to use:\n\n1. To declare the function as a hardware acceleration kernel:\n   `#pragma ACCEL kernel`\n\n2. To prevent automatic loop pipelining:\n   `#pragma ACCEL PIPELINE auto{off}`\n\n3. To process loops without tiling:\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n\n4. To enforce non-parallel loop execution for the majority of loops:\n   `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n5. To enable some parallel execution with a factor of 16 in a specific inner loop in the 'Row Sweep' section:\n   `#pragma ACCEL PARALLEL FACTOR=auto{16}`\n\n6. To set a parallel factor of 4 for the reversed loop in the 'Row Sweep' section:\n   `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n\nThese directives and values should appear before the corresponding loops in the ADI solver code respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' for hardware acceleration without automatic code transformation optimizations. In the first double-nested loop, disable loop pipelining for the outermost loop while internally flattening the pipeline for the innermost loop with unrolled parallel processing for 4 elements at a time. For the multiplication within the innermost loop, parallelize without unrolling or partitioning. In the second double-nested loop, disable loop pipelining for the outermost loop with a single iteration per cycle, while allowing the compiler to determine pipelining strategies for the inner loop, and fully unroll the innermost loop with 16-fold parallelized operations for the reduction on 'D'. Ensure that neither loop is broken into smaller chunks throughout the process.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function with specific pipeline, tile, and parallel optimization directives. The outermost loop should remain sequential without automatic pipelining, maintaining the original full iteration size without tiling, and with strictly serial execution. The inner loop responsible for computing the matrix 'tmp' should allow automatic pipelining, restrict tiling to single iterations, and parallelize its innermost loop with a factor of 4. The innermost loop should perform a reduction on 'tmp' and be parallelized with a factor of 16. For the second major loop block updating 'D', prevent automatic pipelining while ensuring loops are completely flattened for hardware, limit each tiling to single iterations, and set a parallel factor of 20 for the intermediate loop. Finally, enforce serial execution within the innermost loop that updates the matrix 'D'. Treat the entire function as a hardware kernel for acceleration purposes.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) C code for an accelerated kernel function named 'kernel_adi' that performs Alternating Direction Implicit (ADI) computation on two-dimensional arrays `u`, `v`, `p`, and `q` with size [60][60]. The function accepts the number of timesteps `tsteps` and the size `n`. Define constants inside the function for `DX`, `DY`, `DT`, `B1`, `B2`, `mul1`, and `mul2`. The variable coefficients `a`, `b`, `c`, `d`, `e`, and `f` should also be computed based on these constants.\n\nFor this function, annotate it as a hardware kernel for acceleration, but avoid automatic loop pipelining and tiling optimization of the main time loop and the two spatial loops for column and row sweeps. Additionally, disable automatic parallelism for all nested loops by configuring it to run only one iteration at a time.\n\nIn HLS C pragma syntax:\n\n- Use `#pragma ACCEL kernel` at the start to indicate that the following function is a hardware kernel.\n- After defining the function's constants, apply `#pragma ACCEL PIPELINE auto{off}` to prevent automatic loop pipelining for all loops.\n- Use `#pragma ACCEL TILE FACTOR=auto{1}` after the pipeline pragma for each loop to disable the tiling optimization that divides loops into smaller chunks.\n- Immediately follow with `#pragma ACCEL PARALLEL FACTOR=auto{1}` for each loop to enforce sequential execution of iterations with no parallelism.\n\nMake sure to set each pragma directive as instructed above, right before each loop construct within the function, and adhere to loop boundary conditions accurately for proper execution of the ADI algorithm.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a C function designed to be compiled to high-level synthesis (HLS) for hardware acceleration that performs a two-matrix multiplication (2mm) with double-precision floating-point elements. The function 'kernel_2mm' has parameters for matrix dimensions and scalars 'alpha' and 'beta', as well as arrays for matrices 'tmp', 'A', 'B', 'C', and 'D'. Apply the following directives: \n\n1. Treat the 'kernel_2mm' function as a hardware acceleration kernel.\n2. For the first two-level nested loops iterating over 'i' and 'j', enable pipelining at an automatically determined factor, tile the loops with an automatically chosen factor, and parallelize the innermost loop iterating over 'k' with a factor of 8 to operate on eight elements concurrently when accumulating into 'tmp'.\n3. For the innermost loop inside the first two-level nested loops, enable an array reduction optimization for 'tmp' with a parallel factor of 32, aiming to use 32 processing elements.\n4. For the second-level nested loops iterating over 'i' and 'j' to update the 'D' matrix, do not automatically pipeline the loop over 'i', ensure the loop over 'j' executes iterations all at once by flattening, and parallelize the loop over 'k' with a factor of 4.\n5. For the innermost loop inside the second two-level nested loops, enable a reduction for the array 'D' with a parallel factor of 1, processing elements serially.\n\nTranslate the above into HLS-compatible C code with the appropriate #pragma directives to guide the synthesis tool for the desired optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{40}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a 2mm kernel function, designed to perform a two-matrix multiplication followed by the addition of a scaled matrix with the following instructions:\n\n1. Declare the function as a hardware acceleration kernel.\n2. For the first level loop 'i', automatically determine an optimized pipeline strategy, use default tiling, and parallelize with a factor of 8 for acceleration.\n3. In the nested loop 'j' within the first main loop, again, apply an automatic pipelining strategy and default tiling, but with a parallelism factor of 2.\n4. Inside the innermost loop 'k', where the partial products are accumulated, enable parallel reduction of the 'tmp' array with a high parallelization factor of 32, letting the tool automatically determine the strategy.\n5. For the second main loop 'i' responsible for updating matrix 'D', turn off pipelining, use the default tiling, and process the iterations sequentially, without any parallelism.\n6. Within the nested loop 'j' in the second main loop, flatten the loop's hierarchy while pipelining, use default tiling, and set a parallel factor of 4.\n7. In the innermost loop 'k', apply parallel reduction for the 'D' array with a factor of 1 allowing the tool to determine the appropriate strategy automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code that implements an alternating-direction implicit solver with specific optimization directives:\n\n1. Treat the entire `kernel_adi` function as a hardware acceleration kernel.\n2. Within the function, for each loop, ensure the following:\n   - Disable automatic pipelining to execute loop iterations sequentially.\n   - Set the tiling factor to 1 to disable loop tiling and maintain the original loop bounds.\n   - Set the parallel factor to 1 for the inner loops to enforce serial execution of iterations.\n\nFor the specific loop that runs the 'row sweep' operation, utilize a parallel factor of 32 to allow for parallel execution of 32 iterations at a time, improving throughput. Apply this enhanced parallelization to both instances of the j-loop within the 'row sweep' section.\n\nThe appropriate pragmas to use are:\n- `#pragma ACCEL kernel` to identify the function as a kernel for hardware acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` to disable automatic loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to prevent loop tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to maintain sequential execution of loop iterations.\n- `#pragma ACCEL PARALLEL FACTOR=auto{32}` specifically for the 'row sweep' j-loops to enable parallel execution of 32 iterations.\n\nNote that the pragmas should be inserted just before each loop where the respective optimizations are to be applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Apply the following HLS pragmas to the 3mm kernel function in C: Mark the function as a hardware kernel for acceleration, and for each nested loop within the matrix multiplications for E, F, and G, disable automatic pipelining, set tiling factors to ensure loops aren't broken into smaller chunks, maintain the original sequential ordering of loop iterations by running them one after the other, and specifically for the second and third level loops in F and G, flatten the loops to optimize for loop hierarchy simplification.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code to implement an accelerated kernel for solving the 2D Alternating Direction Implicit (ADI) method for partial differential equations. Set up the grid dimensions and time steps as inputs, and initialize the coefficients for the calculations based on the problem's discretization parameters. Assign the main computational loops to not be automatically pipelined, tiled, or parallelized, except for one inner loop where the parallel factor is customized to '29', and another where it is set to '4'. Use the pragma directives `#pragma ACCEL kernel` to define the accelerated kernel function, and apply the appropriate `#pragma ACCEL PIPELINE`, `#pragma ACCEL TILE FACTOR`, and `#pragma ACCEL PARALLEL FACTOR` pragmas with the 'off', '1', '29', and '4' parameters for pipelining, tiling, and parallelizing respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a 2D Alternating Direction Implicit method, ensuring that the function 'kernel_adi' is optimized for hardware acceleration and the loops do not benefit from pipelining, tiling, or parallelization optimizations, except for the innermost loop within the 'Row Sweep' process, which should be fully parallelized.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for an alternating-direction implicit (ADI) solver kernel, labeling the function for acceleration and configuring the loops within to execute sequentially without automatic pipelining, parallelization, or tiling optimization except for one inner loop where iterations should be allowed to proceed concurrently by a factor of 29.\n\n1. Define the function 'kernel_adi' with appropriate parameters for time steps, grid size and matrices 'u', 'v', 'p', 'q'.\n2. Initialize constants 'DX', 'DY', 'DT', 'B1', 'B2', and calculate 'mul1', 'mul2', 'a', 'b', 'c', 'd', 'e', 'f' before the loop starts.\n3. Use '#pragma ACCEL kernel' before the function to indicate it's a hardware acceleration candidate.\n4. For the main time-stepping loop and the subsequent column and row sweep loops, disable automatic pipelining by using the pragma '#pragma ACCEL PIPELINE auto{off}'.\n5. Set the tiling factor to 1 for these loops using '#pragma ACCEL TILE FACTOR=auto{1}', ensuring that the original loop form is maintained.\n6. For each of these loops, also set the parallel factor to 1 using '#pragma ACCEL PARALLEL FACTOR=auto{1}' to enforce sequential execution of iterations.\n7. Inside the row sweep loop, there is one inner loop where computations can proceed in parallel \u2013 enable this by setting the parallel factor to 29 with '#pragma ACCEL PARALLEL FACTOR=auto{29}'.\n8. For the backward substitution loops, they are manually standardized to loop from '0' to '57' with an inverse index for clarity and to fit the pragma model.\n9. Ensure all the computational statements and assignments within the loops are unchanged.\n10. Conclude the function with end scope pragma '#pragma endscop' after the time-stepping loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function for matrix multiplication with loops that are not automatically pipelined, and whose outer loops are executed sequentially without tiling or parallel optimization, while specific inner loops are manually set to run with unrolled parallelism at different factors, and one loop flattened to expose more parallelism.\n\n1. Mark the `kernel_3mm` function as a hardware kernel for acceleration.\n2. For the initial loop calculating matrix `E`, turn off automatic pipelining, disable tiling, and run the outer loop sequentially. For the innermost loop, set a parallelism factor of 32 to allow for simultaneous computation of the matmul operation.\n3. For the loop calculating matrix `F`, again turn off pipelining for the outermost loop, don't apply tiling, and flatten the middle loop to uncover more parallel opportunities. Run the innermost loop without parallelism.\n4. For the final loop computing matrix `G`, disable pipelining and tiling for the outermost loop, set a parallelism factor of 2 for the middle loop, and increase innermost loop's parallelism factor to 32 for concurrent computation of matmul.\n\nUtilize the following pragma statements respectively for each loop nest and loop:\n- For the entire function: `#pragma ACCEL kernel`\n- Pipelining:\n  - To disable: `#pragma ACCEL PIPELINE \"off\"`\n  - To flatten: `#pragma ACCEL PIPELINE auto{flatten}`\n  - To auto pipeline without specific off/flattening: `#pragma ACCEL PIPELINE auto{}`\n- Tiling: `#pragma ACCEL TILE FACTOR=1`\n- Parallelism:\n  - To disable concurrency: `#pragma ACCEL PARALLEL FACTOR=1`\n  - To allow concurrency at a factor of 2: `#pragma ACCEL PARALLEL FACTOR=2`\n  - To highly unroll the loop for parallel execution: `#pragma ACCEL PARALLEL FACTOR=32`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, the following comprehensive but concise natural language instructions should be used:\n\n\"Treat the function 'kernel_3mm' as a hardware acceleration kernel. Within the function, for each of the three matrix multiplication loops ('E = A*B', 'F = C*D', and 'G = E*F'), disable automatic pipelining, set tiling to the smallest granularity, which means no tiling, and execute the outer loop iterations sequentially. For the innermost loop of the first matrix multiplication ('E = A*B'), enable flattening in pipelining to combine nested loops into a single loop level where possible. For the second matrix multiplication ('F = C*D'), allow the innermost loop to process four iterations in parallel. For the third matrix multiplication ('G = E*F'), allow the second loop to process seven iterations in parallel and the innermost loop to process fifty iterations in parallel.\"\n\nSpecific #pragma values:\n\n1. Use `#pragma ACCEL KERNEL` at the beginning of the function 'kernel_3mm' to specify that it should be treated as a hardware kernel.\n2. Apply `#pragma ACCEL PIPELINE auto{off}` to each matrix multiplication code block to turn off automatic loop pipelining.\n3. Set `#pragma ACCEL TILE FACTOR=auto{1}` in each loop to indicate that no loop tiling should be performed.\n4. Define `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the outer loops to run iterations sequentially.\n5. For the loop that calculates 'E', use `#pragma ACCEL PIPELINE auto{flatten}` to suggest that loop flattening should be applied to the pipeline.\n6. In the innermost loop of 'F', employ `#pragma ACCEL PARALLEL FACTOR=auto{4}` to enable four parallel iterations.\n7. For the 'G' computation, in the second loop, set `#pragma ACCEL PARALLEL FACTOR=auto{7}` to enable seven parallel iterations, and in the innermost loop, use `#pragma ACCEL PARALLEL FACTOR=auto{50}` to parallelize fifty iterations.\n\nNote that in this code `auto{...}` is used to denote that a certain value should be automatically used within the context of the specified HLS directive.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Implement a hardware-accelerated kernel function for an Alternating Direction Implicit (ADI) method with the following specifications:\n\n- Define the function 'kernel_adi' with int parameters 'tsteps' and 'n', as well as four 2D double array parameters 'u', 'v', 'p', and 'q', each of size 60 by 60.\n- Use '#pragma ACCEL kernel' to indicate that this function should be optimized for hardware acceleration.\n- Set the loop pipelining to off inside all loops with '#pragma ACCEL PIPELINE auto{off}'.\n- Prevent loop tiling by setting the tile factor to 1 with '#pragma ACCEL TILE FACTOR=auto{1}'.\n- Disable parallel loop execution by setting the parallel factor to 1 with '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n- However, explicitly allow for a parallel factor of 32 in one of the inner loops nested in the 'Row Sweep' section, using '#pragma ACCEL PARALLEL FACTOR=auto{32}', to increase loop iteration parallelism.\n- In a different inner loop nested in the 'Row Sweep' section, allow for a reduced parallel factor of 2, with '#pragma ACCEL PARALLEL FACTOR=auto{2}'.\n- Initialize physical constants outside all loops and calculate variables 'a', 'b', 'c', 'd', 'e', and 'f' based on the given 'DX', 'DY', 'DT', 'B1', and 'B2'.\n- Structure the algorithm to execute 'Column Sweep' before 'Row Sweep' for each time step, performing necessary matrix calculations.\n- Implement the reverse loop iterations in a standardized form using a decrementing loop index with positive bounds.\n- Encapsulate the computational part of the function within '#pragma scop' and '#pragma endscop' to define the scope of the optimizations. \n\nEnsure that this instruction complies with the requirements of High-Level Synthesis (HLS) to produce efficient hardware designs for ADI kernels.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the provided HLS C code, use the following comprehensive instructions to describe the #pragma directives:\n\n- \"Define the function `kernel_3mm` as a hardware acceleration kernel.\"\n- \"Automatically optimize the top-level loops for throughput and resource usage without changing their loop structure.\"\n- \"In the nested loops within the matrix multiplication operations, do not apply any form of loop pipelining.\"\n- \"Process each nested loop's iterations sequentially and do not tile or parallelize them.\" \n\nApplying these instructions results in:\n\n- `#pragma ACCEL kernel`: Treat the `kernel_3mm` function as a dedicated hardware module to be accelerated.\n- `#pragma ACCEL PIPELINE auto{flatten}`: Optimize the top-level loops (the outermost loops in the matrix multiplication operations) to improve throughput, and consider flattening any nested loops for potential optimization.\n- All usages of `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` within the nested loops: Don't pipeline these loops, don't split them into smaller chunks, and execute loop iterations sequentially, ensuring no attempt is made to parallelize them, in order to strictly maintain original loop execution order and dependencies.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the loop execution strategy for the algorithm of Alternating Direction Implicit (ADI) method by specifying each loop should not be pipelined, each outer loop has no tiling and parallel factor set, while the inner loop processing the row sweep should have parallel factor of 32 for the forward sweep and 4 for the backward sweep.\n\nHere are the #pragma descriptions in natural language:\n\n1. For the kernel annotation, instruct to \"Identify the function as a dedicated hardware acceleration kernel.\"\n2. For the outermost loop and the 'Column Sweep' loop pragmas, specify \"Do not perform automatic loop pipelining, tiling, or introduce parallelism.\"\n3. For the loops within the 'Column Sweep' section, instruct to \"Ensure that each iteration of the inner loops is run sequentially with no parallel execution.\"\n4. In contrast, within the 'Row Sweep' section, direct to \"Execute 32 operations in parallel for the forward directional loop and 4 operations in parallel for the backward directional loop to process the row computations efficiently.\"\n\nThe explicit #pragma values to use for each instruction are as follows:\n\n- `#pragma ACCEL kernel` for directive (1).\n- `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` for directive (2).\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` for directive (3).\n- `#pragma ACCEL PARALLEL FACTOR=auto{32}` for the forward loop and `#pragma ACCEL PARALLEL FACTOR=auto{4}` for the backward loop in directive (4).", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function 'kernel_3mm' as a hardware acceleration kernel. Within this function, for all matrix computations (\"E = A*B\", \"F = C*D\", and \"G = E*F\"), disable automatic pipelining, ensure that the loops execute without tiling, and that the outer loops run sequentially. Specifically, for \"G = E*F\", run the innermost loop with a parallelism factor of 2, while for all other loops, maintain sequential execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Disable all loop optimizations such as pipelining, tiling, and parallelization for all the loops within the `kernel_3mm` function, which performs matrix multiplications E=A*B, F=C*D, and G=E*F, by setting pipeline, tile, and parallel factors to 'off' and '1' where auto-tuning is suggested. Additionally, declare the function as a hardware kernel for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) code for a function named `kernel_2mm` which is treated as a hardware kernel for acceleration. This function calculates a matrix-matrix multiplication with scaling factors and is applied to input matrices A, B, C, and D with the following details:\n\n1. Define the function `kernel_2mm` with integer parameters `ni`, `nj`, `nk`, and `nl`, double precision floating-point parameters `alpha` and `beta`, and double precision floating-point matrix parameters `tmp`, `A`, `B`, `C`, and `D` with predefined sizes. Sizes are based on fixed values: `tmp[40][50]`, `A[40][70]`, `B[70][50]`, `C[50][80]`, `D[40][80]`.\n\n2. Within `kernel_2mm`, for the initial double-nested loop that initializes and computes the values of the `tmp` matrix, disable automatic pipelining, enforce the loop to be executed fully without tiling, and force a parallel factor of `4` specifically for the inner loop. Apply a parallel reduction pragma with a custom factor of `32` to the innermost loop, optimizing the accumulation in `tmp` using an automatic setting.\n\n3. For the subsequent double-nested loop responsible for updating matrix `D`, also disable automatic pipelining, refrain from tiling, use a parallel factor of `4` for the inner loop, and again apply a parallel reduction pragma while scaling down to a parallelization factor of `2` for the innermost loop. The reductions apply to the `D` matrix specifically.\n\n4. Ensure the primary computation for the `tmp` matrix involves multiplying elements from matrices `A` and `B` (in accordance with indices) and scaling that product by the value of `alpha`, while for the `D` matrix update, scale the existing `D` values by `beta` and then add the product of `tmp` and `C` matrices elements.\n\n5. Encapsulate the computation sections with appropriate `#pragma` scopes if needed.\n\nThe instruction should consider HLS-specific optimizations and coding best practices to generate an efficient hardware implementation of the given matrix-matrix multiplication algorithm.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following comprehensive but concise natural language instruction:\n\n\"Create a kernel function for hardware acceleration that performs 3 matrix multiplications (E=A*B, F=C*D, and G=E*F) without any automatic loop optimization. Ensure that each loop runs sequentially without pipelining, tiling, or parallel execution enhancements except for the innermost loop in the last matrix multiplication where parallelism of factor 10 is applied.\" \n\nSpecify the following #pragma directives:\n\n1. Treat the entire function as an acceleration kernel: `#pragma ACCEL kernel`.\n2. For each matrix multiplication:\n   - Tell the compiler not to apply automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`.\n   - Prevent the compiler from tiling the loops: `#pragma ACCEL TILE FACTOR=auto{1}`.\n   - Disable any automatic parallelization of the loops: `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n3. In the last matrix multiplication, apply parallel execution to the innermost loop with a factor of 10: `#pragma ACCEL PARALLEL FACTOR=auto{10}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, disabling automatic loop pipelining, prevent loop tiling and loop parallelization optimizations for the first two matrix multiplication operations, but allow a parallelization factor of 16 for the third matrix multiplication operation's innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a three-matrix multiplication accelerator kernel that does not pipeline nor tile the outer loops for computation of \\( E = A \\times B \\), \\( F = C \\times D \\), and \\( G = E \\times F \\) with specific matrix dimensions, but allow parallel computation with different factors within the inner loops. Specifically, do not pipeline outer loops and do not divide them into smaller chunks, run iterations sequentially, allow parallel computation of the E matrix with a factor of 32, parallel computation of the F matrix also with a factor of 32, and for the G matrix, allow parallel computation with a factor of 7 in the second outer loop and a factor of 5 in the inner loop. Use 'auto' where appropriate to let HLS tool decide the actual factor values based on hardware constraints.\n\nIn terms of #pragma directives, this translates as follows:\n- Annotate the function with `#pragma ACCEL kernel` to specify it as an accelerator kernel.\n- For each outer loop computing matrices E, F, and G:\n  - Disable automatic pipelining with `#pragma ACCEL PIPELINE auto{off}`.\n  - Prevent loop tiling with `#pragma ACCEL TILE FACTOR=auto{1}`.\n  - Ensure sequential execution of iterations with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the inner loop that computes the E matrix multiplication, use `#pragma ACCEL PARALLEL FACTOR=auto{32}` to allow parallelism with a factor of 32.\n- For the inner loop that computes the F matrix multiplication, use `#pragma ACCEL PARALLEL FACTOR=auto{32}` as well.\n- For the outer loop computing G matrix, use `#pragma ACCEL PIPELINE auto{}` and `#pragma ACCEL PARALLEL FACTOR=auto{7}` to allow limited parallelism with a factor of 7.\n- In the innermost loop computing G matrix, use `#pragma ACCEL PARALLEL FACTOR=auto{5}` to allow parallelism with a factor of 5.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` for hardware acceleration. Apply automatic loop pipelining optimization decisions throughout. Initially, allow the tool to determine loop tiling and parallelism factors for the first loop nest, but override parallel factor for the innermost loop to run 32 parallel operations, focusing on reducing the `tmp` array. In the second loop nest, enforce automatic pipelining with an additional directive to consider loop flattening, then explicitly set the parallel factor to 4 to concurrently process elements of the `D` array, leaving the innermost loop to proceed with a single operation at a time to handle reduction over `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{40}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with the appropriate pragma directives for a matrix-matrix multiplication kernel targeting hardware acceleration, use the following concise instruction:\n\nDefine a function as a hardware acceleration kernel performing three matrix-matrix multiplications sequentially. For each multiplication, disable automatic loop pipelining, prevent loop tiling, and enforce serial execution for all but the innermost loop in the third multiplication, where a parallelization factor of 16 should be set.\n\nSpecifically:\n- Declare the function 'kernel_3mm' as an acceleration kernel.\n- For the first multiplication 'E := A*B', disable pipelining, prevent tiling, and enforce serial loop execution.\n- For the second multiplication 'F := C*D', replicate the same pragma settings as the first.\n- For the third multiplication 'G := E*F', also disable pipelining and prevent tiling, and enforce serial loop execution for the outer loops, but in the innermost loop set the parallelization factor to 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to designate the function as a hardware acceleration kernel, disable any automatic pipelining for the entire function, and enforce a default execution policy for the loops without any additional tiling or parallel processing. However, in the first nested 'j' loop, allow automatic pipelining optimization, specify a tiling factor of 1, and set the parallel execution factor to 4 with a reduction on the 'tmp' array, where up to 14 iterations can be processed in parallel. In the second nested 'j' loop, employ an automatic pipelining with full loop flattening, set the tiling factor to 2, and arrange for 8 iterations of the loop to be capable of running in parallel with a reduction on the 'D' array, processing iterations sequentially.\n\nFor the specific #pragma statements:\n\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Don't automatically pipeline the outer loops in the function: `#pragma ACCEL PIPELINE auto{off}`\n- Process the outer loops in their original, full form without breaking them into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run the outer loop iterations sequentially, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- For the first nested 'j' loop: Allow the HLS to decide whether to pipeline the loop: `#pragma ACCEL PIPELINE auto{}`\n- Set the tiling factor and parallel factor respectively: `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{4}`\n- Allow the 'k' loop inside the first nested 'j' loop to process 14 iterations in parallel with a reduction on 'tmp': `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}`\n- For the second nested 'j' loop: Enable automatic pipelining with loop flattening: `#pragma ACCEL PIPELINE auto{flatten}`\n- Set the tiling factor and parallel factor respectively: `#pragma ACCEL TILE FACTOR=auto{2}` and `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n- Run the 'k' loop inside the second nested 'j' loop in a strictly sequential manner with a reduction on 'D': `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware accelerated alternating direction implicit solver. This function will iterate over time steps and perform column and row sweeps on 2D arrays. Disable automatic pipelining, tiling, and parallelization for all loops, except for the second innermost loop in the row sweeps where you should allow four parallel computations and for the final innermost loop in the row sweeps, enable thirty-two parallel computations. All other loops should run sequentially. Define the function as the hardware kernel to be accelerated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the following function as a hardware kernel for acceleration but do not automatically pipeline any loops within it; keep all the loops in their original forms, running their iterations strictly in sequence with only the last parallelizable loop within the row sweep section running eight iterations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to not pipeline any of the loops, process all loops in full without tiling, execute all loop iterations sequentially (not in parallel), and with the exception of one specific loop, where each iteration should be partially parallelized with a factor of 32, and another loop with a factor of 8 for parallel execution. Identify the kernel function to be accelerated using hardware resources.\n\n### Code Explanation and Pragma Details:\n\n1. `#pragma ACCEL kernel` indicates that the function `kernel_adi` is to be compiled as an independent hardware kernel for acceleration.\n\n2. The pragmas `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` are used multiple times before different loops to specify that:\n   - The pipeline optimization should be turned off (`PIPELINE \"off\"`).\n   - The loops should not be tiled, preserving their original structure (`TILE FACTOR=1`).\n   - Loop iterations are to be executed one after another without any parallelism (`PARALLEL FACTOR=1`).\n\n3. There are exceptions for two loops with the pragmas:\n   - `#pragma ACCEL PARALLEL FACTOR=auto{32}` where the loop iterations are to be parallelized with a factor of 32.\n   - `#pragma ACCEL PARALLEL FACTOR=auto{8}` where the loop iterations are to be parallelized with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_3mm' as a hardware acceleration target, allowing for automatic optimization decisions in the first loop construct concerning pipelining, but prevent automatic pipelining and retain default unchunked and sequential execution for subsequent nested loop constructs, except parallelize the innermost loop of the final matrix multiplication with a factor suggesting potential concurrent execution suggestions, while processing loop iterations sequentially for other operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a high-level synthesis (HLS) C code for a kernel function named 'kernel_adi' that performs an Alternating Direction Implicit (ADI) method with given parameters 'tsteps' and 'n', operating on four 2D arrays 'u', 'v', 'p', and 'q' of size 60x60. Initialize specific constants inside the function scope for calculations. The code should include directives to disable automatic pipelining, to retain loop structures without tiling, and to enforce non-parallel execution of the loops. Also, annotate a specific loop to parallelize only 58 and 16 iterations for two different innermost for-loops. Use '#pragma ACCEL kernel' to signal the function as an acceleration kernel, and then employ '#pragma ACCEL PIPELINE' with the value \"off\", '#pragma ACCEL TILE FACTOR' with the value set explicitly to 1, and '#pragma ACCEL PARALLEL FACTOR' set explicitly to 1 for general loops, to 58 for parallel processing of 58 iterations, and finally to 16 for parallel processing of 16 iterations for the respective loops. Make sure to use '#pragma scop' and '#pragma endscop' to delineate the computationally intensive part of the code that is suitable for acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, and in all of the loops, disable automatic pipelining, prevent the loops from being split into smaller tiled sections, and ensure the iterations run sequentially without parallelism except for the second inner loop in the row sweep section which should be parallelized with a factor of 58 and the reverse loop in the same section with a parallelization factor of 29. Use the appropriate pragma directives for these specifications.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function 'kernel_adi' to be used as a hardware acceleration kernel that takes 'tsteps', 'n', and four 2D arrays 'u', 'v', 'p', 'q' as inputs. Initialize constants for the discretization and constants 'B1' and 'B2'. For the main time iteration loop and the two inner processing loops (Column Sweep and Row Sweep), disable automatic pipelining, do not tile the loops, and disable automatic parallelization across the loop iterations. For the first and third inner nested loop within the Column Sweep, maintain sequential execution of iterations. However, in the second nested loop within Row Sweep, allow parallel execution of the 58 iterations and for the last nested loop, set parallel factor to 2 to allow a pair of operations to happen concurrently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following C function into an HLS-accelerated kernel, turn off automatic pipelining optimization for all loops, preserve the original single-iteration-per-clock-cycle behavior by setting the tiling factor to 1 and the parallelism factor to 1 for all loops, except set the parallelism factor for the inner loop during the 'Row Sweep' to 58 and to 32 for the reverse iteration of the same sweep.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following concise natural language instructions:\n\n1. Designate the function `kernel_2mm` as a hardware acceleration kernel.\n2. Automatically optimize the outer two loops for pipelining without specifying a particular factor.\n3. Automatically select appropriate loop tiling factors for the initial matrix multiplication (`tmp = alpha * A * B`) loops, but keep the tile size as it is originally without decomposition.\n4. Parallelize the innermost loop of the initial matrix multiplication with an unspecified factor, aiming for as much parallelism as hardware resources allow, but with a hint of up to 32.\n5. For the second part, where `D` is updated (`D = beta * D + tmp * C`), turn off automatic pipelining on the outer loop, while the inner loop should be flattened if possible.\n6. Parallelize the inner loop of the `D` matrix update with a hint for a moderate amount of parallel execution with a factor up to 5 for the outer loop and up to 4 for the nested loop.\n7. The innermost loop of the `D` matrix update should not be parallelized.\n\nCorresponding #pragma directives:\n\n1. `#pragma ACCEL kernel` - Treat the following function as a hardware kernel for acceleration.\n2. `#pragma ACCEL PIPELINE auto` - Allow the compiler to automatically decide the pipelining strategy for loops.\n3. `#pragma ACCEL TILE FACTOR=auto` - Automatically determine the tiling factor for loops while preserving the full original loop size.\n4. `#pragma ACCEL PARALLEL FACTOR=auto` - Specify automatic determination of parallel factor, allowing up to the number stated after `FACTOR=`.\n5. `#pragma ACCEL PIPELINE auto{off}` - Do not automatically pipeline this particular outer loop.\n6. `#pragma ACCEL PARALLEL FACTOR=auto{}` with a value hint - Suggests an upper limit when automatically determining the parallel factor for loop parallelization.\n7. `#pragma ACCEL PARALLEL reduction = VARIABLE FACTOR=auto` - Specifies a parallel reduction operation on a given variable, without specific parallel factor details.\n\nNote that for the loops where `FACTOR=auto{}` is specified with a hint value, it tells the compiler that the parallelism factor should be automatically determined, but the number in braces provides guidance on the upper limit that should be considered within the hardware resource constraints.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{5}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function named `kernel_3mm` for computing the matrix multiplications E = A * B, F = C * D, and G = E * F without pipelining, tiling, or parallel optimizations for the first two multiplications, and with a parallel factor of 16 for the last multiplication G = E * F. Use pragmas to disable automatic pipeline, set tile factor to 1, and set parallel factor to 1 where not specified otherwise. For the final loop computing G, use a parallel factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel, automatically adjust the pipelining of the first loop block for optimum performance, and automatically determine the tiling factor for the inner loops while setting parallel loop execution with a factor of 8 for the first loop and 2 for the nested loop, allowing parallel reduction for the innermost loop with a parallel factor of 32. In the second loop block, prevent pipelining from being applied automatically and specify that the pipeline should not be broken into smaller parts. Ensure the outer loops run with a parallel factor of 8 and 4 respectively, and allow the deepest nested loop to run serially with parallel reduction on variable 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the 'kernel_adi' function as a hardware acceleration kernel and disable automatic pipelining, tiling, and parallel processing for all the loops within the 'kernel_adi' function except for the innermost loop within the second 'Row Sweep' section, where parallel processing should be applied with a factor of 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, instruct the code generator with the following comprehensive but concise natural language instruction:\n\n\"Implement a hardware-accelerated kernel function called 'kernel_adi' that performs alternating direction implicit (ADI) integration on a 60x60 grid over a specified number of time steps. Use double-precision floating-point arithmetic for calculations and initialize the grid with boundary values of 1.0. Prevent automatic pipelining and automatic loop tiling optimizations, while maintaining the original sequential execution of iterations within the loops. For the row sweep phase, apply a parallelism factor of 2 in the first inner loop to allow some concurrent execution, followed by a higher parallelism factor of 8 in the standardized backward loop to exploit more parallelism. Apply a pragma to mark the region of interest as a scope for optimization, and ensure that all the pragmas have a setting to turn their automatic behavior 'off' except for the customized parallelism factors in the row sweep where you should use factors of 2 and 8 as specified.\" \n\nThis description should result in the following #pragma specifications:\n\n- `#pragma ACCEL kernel` to indicate the function is designated for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` to prevent automatic pipelining optimization.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to prevent automatic loop tiling optimization and keep the original loop bounds.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to maintain sequential execution of loop iterations, except for specific loops within the row sweep phase.\n- For the row sweep phase, `#pragma ACCEL PARALLEL FACTOR=auto{2}` and `#pragma ACCEL PARALLEL FACTOR=auto{8}` are used to set specific parallelism factors different from the default setting.\n- The use of `#pragma scop` and `#pragma endscop` to indicate the region of the code that is subject to optimization.\n\nIt is important to note that \"auto{}\" with \"off\" or specific numbers inside is a placeholder syntax to enable or disable automatic features or set specific values for those features in the code. It is not standard OpenCL or HLS pragma syntax and would likely need to be replaced with the actual pragma syntax supported by the specific HLS toolchain being used.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the given 3 matrix-matrix multiplication (3MM) code into High-Level Synthesis (HLS) by specifying each loop should not be automatically pipelined, should not be tiled into smaller chunks (i.e., use single iteration per tile), and iterations should run sequentially rather than in parallel, with the exception of the innermost loop in the third matrix multiplication where 25 iterations may run in parallel. Additionally, treat the whole function as a hardware kernel for acceleration and flatten the pipeline for the second matrix-matrix multiplication part.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a three-matrix-multiplication kernel that treats the entire function as a hardware acceleration kernel, where each of the three nested loop blocks calculating E=A*B, F=C*D, and G=E*F respectively, should not be automatically pipelined, the loops should be processed in their full original form without tiling, each loop iteration should be run sequentially (not in parallel), and innermost multiplications should be parallelized with a factor of 1. Additionally, for the loops calculating the initial values of matrices E and G, ensure that the outer loop is fully unrolled. \n\n- Treat the function `kernel_3mm` as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n- Declare loop iteration variables `i, j, k` within the function for indexing matrices.\n- For the loops calculating matrix E:\n  - Do not automatically pipeline the outer loop (i-loop) using `#pragma ACCEL PIPELINE auto{off}`.\n  - Do not tile the outer loop (i-loop) using `#pragma ACCEL TILE FACTOR=auto{1}`.\n  - Do not run the outer loop iterations in parallel using `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n  - Fully unroll the intermediate loop (j-loop) using `#pragma ACCEL PIPELINE auto{flatten}`.\n  - Ensure iterations in the innermost loop (k-loop) concerning the element-wise multiplication run one after the other using `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Repeat the same set of pragmas for the loops calculating matrix F.\n- For the loops calculating matrix G:\n  - Use the same set of pragmas as for loops calculating matrix E, ensuring the intermediate loop (j-loop) is fully unrolled with `#pragma ACCEL PIPELINE auto{flatten}`.\n- Finally, wrap the loop blocks with `#pragma scop` and `#pragma endscop` comments to define the scope of the optimization pragmas within the function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Optimize the nested matrix multiplication kernel for hardware acceleration by marking the function as an acceleration kernel, allowing automated loop pipelining for the first multiplication phase, and sequentially processing the iterations for the second and third multiplication phases with no loop unrolling or tiling.\n\n- Mark the entire function as a hardware acceleration kernel.\n- For the first matrix multiplication E=A*B, enable automatic pipelining for the outermost loop with loop flattening enabled, but no specific tile or parallel factor.\n- In the second layer of the first multiplication, disable pipelining and again do not apply tiling or parallel execution to the loops, forcing them to run sequentially.\n- For the second matrix multiplication F=C*D, turn off pipelining for both levels of loops and keep the loops sequentially executed without tiling or parallelism.\n- In the final matrix multiplication G=E*F, disable pipelining for the outer loops. In the innermost loop, enable parallel execution with a factor of 16 while keeping loops sequential and without tiling at all other levels.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the encompassed function as a candidate for hardware acceleration while preventing loops from automatic pipelining and tiling, and ensuring that all loop iterations execute sequentially, except for the second innermost parallel loop within the 'Row Sweep' section, which should partially overlap execution with 8 iterations running concurrently, and the final parallel loop in the same section, where 29 iterations can proceed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Generate a high-level synthesis (HLS) C function to perform a triple matrix multiplication sequence\u2014where matrix `E` is the product of `A` and `B`, matrix `F` is the product of `C` and `D`, and matrix `G` is the product of `E` and `F`\u2014with the following constraints: designate the function as a hardware acceleration kernel, prevent automatic loop pipelining for the outer loops, maintain the original loop structure without tiling or additional parallelism for all loops, except the innermost multiplication loop in the computation of `E` which should be completely flattened for pipelining optimization.\" \n\n- Use `#pragma ACCEL kernel` to designate the function as an acceleration kernel.\n- Use `#pragma ACCEL PIPELINE \"off\"` for each top-level matrix multiplication to indicate no automatic pipelining should be applied to the outer loops.\n- Use `#pragma ACCEL TILE FACTOR=1` along with `#pragma ACCEL PARALLEL FACTOR=1` for all loops, indicating the preservation of the original loop structure without tiling or parallelization, ensuring the loop iterations execute sequentially.\n- Specifically for the innermost loop of the computation of matrix `E`, apply `#pragma ACCEL PIPELINE auto{flatten}` to enable full loop flattening, enhancing pipelining at the innermost loop level for potential performance gains.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a three-matrix multiply kernel where each loop is not automatically pipelined, executed in its original form without tiling, and run serially, except for the innermost loop in the last multiply which should have a parallel factor of 25. The outer function should be specified as a hardware kernel for acceleration. Use '#pragma ACCEL kernel' at the beginning of the function, and for each loop set pipeline to 'off', tile factor to '1', and parallel factor to '1' except for the innermost loop of the last matrix multiplication, where you should use a parallel factor of '25'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a two-matrix multiplication (2mm) kernel function that handles four matrices A, B, C, and D. Utilize the following directives for optimization:\n\n1. Specify the function as a hardware kernel targeted for acceleration.\n2. For the outer loops 'i' processing columns of the result matrix, disable loop pipelining, ensure the loop runs without tiling, and execute the loop iterations sequentially without parallelization.\n3. Within the first 'i' loop, as it iteratively computes rows of the temporary matrix 'tmp', turn off the loop pipelining, run without loop tiling, and allow limited parallelization, with eight iterations of the 'j' loop running concurrently.\n4. For the nested 'k' loop within the 'j' loop, while accumulating temporary results in 'tmp', enable parallel execution with a large parallelization factor, allowing for significant concurrency, specifically thirty-two iterations running in parallel, and perform a reduction operation on the 'tmp' array.\n5. In the second 'i' loop, as it updates matrix 'D', flatten the nested 'j' loop with pipelining, without loop tiling, and with only sequential execution of the loop iterations.\n6. In the innermost 'k' loop during the update of 'D', disable parallelization while performing a reduction operation on the 'D' array, indicating a sequential fetch and accumulation of results.\n\nSpecifically, use `#pragma ACCEL kernel` to mark the function as an acceleration kernel and set each #pragma's value as follows:\n\n- To disable automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- To process loops in their original full form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- To enforce sequential loop execution: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- To parallelize a loop with a factor of eight: `#pragma ACCEL PARALLEL FACTOR=auto{8}`\n- To parallelize a loop with a factor of thirty-two doing a reduction on 'tmp': `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}`\n- To ensure a loop operations are flattened with pipelining enabled: `#pragma ACCEL PIPELINE auto{flatten}`\n- To parallelize a loop with a factor of one doing a reduction on 'D': `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated function named `kernel_adi` designed to perform an Alternating Direction Implicit (ADI) solver operation with fixed-size 60x60 double-precision floating-point arrays for variables `u`, `v`, `p`, and `q`. Define the following:\n\n- Do not apply automatic pipelining to any loops. Indicate this by using `#pragma ACCEL PIPELINE auto{off}` before each loop where you would like pipelining to be disabled.\n- Process all loops in their original, full form rather than breaking them into smaller chunks. Apply `#pragma ACCEL TILE FACTOR=auto{1}` before each loop to avoid tiling optimizations.\n- Execute iterations of each loop sequentially without parallel execution, but optimize the innermost loop of the row sweep with a factor of 4 to allow some loop iterations to run in parallel. Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` for sequential loop execution and `#pragma ACCEL PARALLEL FACTOR=auto{4}` for the optimized innermost loop in the row sweep to indicate these respective parallelization strategies.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the provided C code with HLS pragmas into a hardware-accelerated kernel that does not automatically pipeline any of the loops, processes all loops in their entirety without tiling, and executes all loops sequentially rather than in parallel\u2014except for one inner loop during the row sweep phase, which should run with a parallelism factor of 8, and another inner loop in the same phase which should run with a parallelism factor of 16. Use the following directives: treat the `kernel_adi` function as a hardware kernel; do not pipeline the main time-stepping loop, the column sweep loops, or the row sweep loops; do not tile any of the loops; run all loops with a parallelism factor of 1 except for the specific row-sweep inner loops, where you should use a parallelism factor of 8 for the first row-sweep inner loop and a parallelism factor of 16 for the second row-sweep inner loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a function `kernel_adi` that performs an Alternating Direction Implicit (ADI) method on a 60x60 grid with given time steps `tsteps` and boundary `n`. \n\nInitialize the constants `DX`, `DY`, `DT`, `B1`, `B2`, `mul1`, `mul2`, `a`, `b`, `c`, `d`, `e`, and `f` using predefined formulas. \n\nFor the actual computation, include the following directives: \n\n1. Mark the function `kernel_adi` as a hardware kernel for acceleration with `#pragma ACCEL kernel`.\n2. For each loop in the code, disable automatic pipelining using `#pragma ACCEL PIPELINE auto{off}`.\n3. Ensure each loop runs with a single iteration processed at a time by setting the tiling factor to 1 with `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. Do not run iterations in parallel within any loop, indicated by `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the outer loops and all inner loops, except for one of the inner loops within the 'Row Sweep' section, where you should set it to 4 with `#pragma ACCEL PARALLEL FACTOR=auto{4}` and for the final loop in the 'Row Sweep' where the parallel factor is set to 29 with `#pragma ACCEL PARALLEL FACTOR=auto{29}`. \n\nLastly, make sure that the loops iterating over `j` in reverse are standardized into a forward loop by calculating an inverse index `_in_j_0` and `_in_j`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code that defines a hardware kernel function called `kernel_atax` that processes two-dimensional arrays and vectors for a matrix-vector multiplication and its transpose without automated pipelining, without tiling, with serial loop execution in the outer loop, with a parallel reduction on `tmp` with a single instance, and with a parallel reduction on `y` with four instances in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS C code to implement a kernel function named 'kernel_adi' that operates on a 60x60 grid using four 2D arrays: 'u', 'v', 'p', and 'q'. The process involves three nested loops to perform an ADI method over 'tsteps' iterations on this grid. Specify that the outer loop and the two inner loops should not be automatically pipelined, with the second inner loops to be executed serially. However, within the 'Row Sweep' phase, the second inner loop iterations should be parallelized with a factor of 8, while the subsequent loop should be parallelized with a factor of 2. All loops should avoid tiling optimizations by maintaining the original iteration space without subdivision. The variables 'DX', 'DY', 'DT', 'B1', 'B2', 'mul1', 'mul2', 'a', 'b', 'c', 'd', 'e', and 'f' should be declared and initialized before starting the loop nests. Also, utilize '#pragma scop' and '#pragma endscop' to define the region of interest for code optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for accelerating a matrix-vector multiplication and subsequent vector update, where matrix A has dimensions 390x410, vector x has size 410, and vectors y and tmp have size 410 and 390, respectively. Apply the following directives: Mark the surrounding function as a hardware acceleration kernel, disable automatic loop pipelining, ensure the first inner loop runs without parallelism or tiling, the second inner loop uses a parallelism factor of 1 with a reduction variable named 'tmp', and the third inner loop uses a parallelism factor of 8 with a reduction variable named 'y'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' for hardware acceleration. Within this function, initialize the 'y' array elements to zero without automatic pipelining. Preserve the first nested loop in its entirety and execute its iterations sequentially. For the second nested loop, summate the product of 'A' elements and the 'x' vector into 'tmp', enabling parallel execution with a factor hint of 10, while targeting parallel reduction optimization. Finally, in the last loop, parallelly sum the product of 'A' elements and 'tmp' into the 'y' array with a parallel factor of 1, focused on optimizing parallel reduction.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a matrix-vector multiplication and subsequent accumulation kernel, marking the function as a hardware kernel using `#pragma ACCEL kernel`. Disable automatic pipelining for the first outer loop, ensure that the initial outer loop doesn't split into smaller chunks and is fully serialized with `#pragma ACCEL TILE FACTOR=1` and `#pragma ACCEL PARALLEL FACTOR=1`. Inside the first nested loop, parallelize the computations with a reduction on the temporary array 'tmp' and set a parallel factor of 10 using `#pragma ACCEL PARALLEL reduction=tmp FACTOR=10`. Similarly, parallelize the second nested loop with a reduction on the 'y' array, also with a parallel factor of 10 using `#pragma ACCEL PARALLEL reduction=y FACTOR=10`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel, and automatically optimize the pipelining for the first top-level loop over array 'y'. For the second top-level loop over 'tmp', utilize automatic tiling to split its iterations without specifying the factor size, while running its iterations sequentially. Inside this loop, apply parallelization with a specified factor of 10 for the reduction on 'tmp' across the innermost loop, and force the last innermost loop to process its iterations sequentially while performing reduction on 'y'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{390}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a kernel function that performs three matrix multiplications without pipelining or parallelizing any of the loops, except for the innermost loop in the last matrix multiplication sequence, where that loop should be parallelized by a factor of 10. Use '#pragma ACCEL kernel' to identify the function as a hardware acceleration candidate. Also, prevent loop tiling and ensure the pipeline directive is set to 'off'. However, for the second layer loop of the last matrix multiplication sequence, adjust the pipeline directive to 'flatten'. Set all other tiling and parallel factors to '1' to maintain the original loop structure.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, the following concise natural language instruction can be given:\n\n\"Create a hardware accelerated kernel function named 'kernel_adi' for an Alternating Direction Implicit solver with specified parameters for time steps and grid size. Ensure that main computation loops process in an unoptimized serial fashion, without automatic pipelining, tiling, or parallelization optimizations. However, in the row sweep section, partially parallelize the inner `j` loop with a parallelization factor of 4 initially, then switch to a factor of 8 in the backward iteration. Define all loop bounds and constants, and maintain a standardized loop structure for indexing.\" \n\nSpecifically, use the following #pragma directives:\n\n- `#pragma ACCEL kernel` to identify the function as a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` to disable automatic loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to prevent loop tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to execute iterations serially for most loops.\n- Within the row sweep, change the inner `j` loop with `#pragma ACCEL PARALLEL FACTOR=auto{4}` and `#pragma ACCEL PARALLEL FACTOR=auto{8}` for different parts of the sweep to alter the parallelization factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as a hardware acceleration kernel, disallow loop pipelining, and enforce sequential processing without tiling or parallelizing loops, except for one nested loop with a parallel factor of four and a second with a parallel factor of sixteen.\n\nIn HLS code:\n\n- Use `#pragma ACCEL kernel` to signal that the function 'kernel_adi' should be compiled as a hardware acceleration kernel.\n- Apply `#pragma ACCEL PIPELINE off` to prevent automatic pipelining of loops.\n- Set `#pragma ACCEL TILE FACTOR=1` to avoid loop tiling and ensure loops are processed in their original form.\n- Utilize `#pragma ACCEL PARALLEL FACTOR=1` to enforce sequential iteration of loops.\n- For the specific nested loop processing 'j' within the row sweep section, change to `#pragma ACCEL PARALLEL FACTOR=4` for a light parallel execution.\n- For another specific nested loop in the row sweep section, modify to `#pragma ACCEL PARALLEL FACTOR=16` to allow for more aggressive parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_atax' function as a hardware acceleration candidate, disable automatic loop pipelining for the first loop to maintain it as a single entity, keep each loop iteration in sequence without parallelization or tiling for the second loop, increase parallelization to tenfold while performing reduction on 'tmp' for the third loop, and finally, double the parallelization factor while performing reduction on 'y' for the fourth loop.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given HLS C code into a set of instructions using specific directives for loop and function optimization in the following way:\n\n1. Declare the function 'kernel_adi' as a hardware acceleration kernel.\n2. For each loop in the function where the pipeline, tile, and parallel pragmas are set to 'auto{off}', 'auto{1}', and 'auto{1}' respectively, instruct the compiler to:\n   - Not automatically pipeline the loop. Instead, execute it sequentially without software pipelining optimization.\n   - Not tile the loop, processing it in its original, full form without subdivision.\n   - Execute loop iterations sequentially, without introducing parallel hardware computation.\n\n3. For the innermost loop within the 'Row Sweep' section with a parallel pragma set to 'auto{4}', allow limited parallel execution with a factor of 4, meaning it can process 4 iterations concurrently.\n\n4. For the last loop within the 'Row Sweep' section with a parallel pragma set to 'auto{58}', fully parallelize the loop iterations, allowing them to execute concurrently up to a factor of 58 at the hardware level.\n\nThe specific #pragma values to be used for this HLS code are as follows:\n\n- `#pragma ACCEL kernel` to mark the function for acceleration.\n- `#pragma ACCEL PIPELINE auto{off}` to prevent loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to prevent loop tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to allow for sequential loop execution.\n- `#pragma ACCEL PARALLEL FACTOR=auto{4}` to enable limited parallel execution with a factor of 4.\n- `#pragma ACCEL PARALLEL FACTOR=auto{58}` to enable full parallel execution with a factor of 58.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate C code for an HLS hardware kernel to perform an Alternating Direction Implicit (ADI) method on a 60x60 grid, using the given variables. Prevent the tool from automatically optimizing loop pipelining, tiling, and parallelization. For the final backward loop in the rows sweep, parallelize with a factor of 4. Use the directives as follows:\n\n1. Mark the function as a hardware kernel using `#pragma ACCEL kernel`.\n2. Deactivate automatic pipelining for all loops using `#pragma ACCEL PIPELINE auto{off}`.\n3. Disable automatic tiling by setting the tile factor to 1 using `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. For each loop, except the final backward loop in the rows sweep, ensure the iterations run sequentially by setting parallel factor to 1 using `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n5. For the final backward loop in the rows sweep, allow four iterations to be processed in parallel using `#pragma ACCEL PARALLEL FACTOR=auto{4}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please generate HLS (High-Level Synthesis) code for a 3-matrix multiplication kernel, ensuring the following directives:\n\n1. Indicate the main function as a hardware acceleration kernel.\n2. Automatically determine whether to flatten nested loops for the first matrix multiplication, 'E = A * B', to optimize pipeline performance.\n3. Preserve the original loop structure without tiling for the first matrix multiplication, 'E = A * B', and run iterations sequentially for both outer and inner loops.\n4. Do not pipeline the second matrix multiplication, 'F = C * D', and third matrix multiplication, 'G = E * F', maintaining the default iteration flow by running iterations sequentially and without any tiling.\n5. For the last matrix multiplication, 'G = E * F', apply tiling to the outer loop 'i' with a factor of 40 to optimize for data locality.\n\nYou should apply the following #pragma values:\n- `#pragma ACCEL kernel` at the beginning of the function to treat it as an acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{flatten}` and `#pragma ACCEL PIPELINE auto{off}` to manage loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL TILE FACTOR=auto{40}` to control loop tiling.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to ensure that loop iterations run sequentially.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{40}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_atax' as a hardware acceleration kernel, disable automatic pipelining for the first loop which initializes the array 'y', process the second loop without tiling and keep iterations sequential, for the nested loops within the second loop use parallelism with a reduction on 'tmp' and 'y' using factors of 10 and 4 respectively.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function called 'kernel_adi' that is treated as a hardware acceleration kernel, to perform the Alternating Direction Implicit (ADI) method for 'tsteps' time steps on a 60x60 grid of 'u' and 'v' values. Utilize provided 'p' and 'q' arrays for intermediate computations. Define and calculate constants 'DX', 'DY', 'DT', 'B1', 'B2', 'mul1', and 'mul2', along with the coefficients 'a', 'b', 'c', 'd', 'e', and 'f'. In the loop structure, disable automatic pipelining, and ensure the loop body is processed as a single tile without partitioning. Make each loop iteration run sequentially without parallelization. In the column sweep phase, apply the iterative calculation in a sequential manner. However, modify the final parallel loop within the row sweep by allowing up to 58 iterations to run in parallel, and the very last loop to use a parallelism factor of 4. Assign boundary conditions with the value '1.0' where appropriate.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, disable automatic loop pipelining, maintain the original loop structure without tiling, enforce strictly sequential loop iterations, utilize unparallelized reduction for accumulating into the 'tmp' array, and allow limited parallel iterations with a parallelization factor of five for the reduction into the 'y' array.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an AES256 encryptor function in C language with accelerator pragmas. The function first initializes an AES256 context with an input key and prepends a round constant to ctx -> deckey. Then, with auto 'off' pipeline pragma, it calls aes_expandEncKey_1 function to prepare the decryption key. Afterwards, it calls aes_addRoundKey_cpy_1 to copy the added round key to the context.\n\nGoing into the encryption routine, the function uses the pipeline flatten and tile factor pragmas set to 1 to decrypt the input buffer in sequence, treating the whole operation as one large tile. It calls aes_subBytes_1, aes_shiftRows_1, and aes_mixColumns_1 functions in each round of the decryption, alternately adding the round key from either ctx -> key[16] or ctx -> key depending on the round index. Once the 14 rounds are done, it calls aes_subBytes_1 and aes_shiftRows_1 once more, before expanding the ctx -> key with aes_expandEncKey_1, and performing the final round key addition with aes_addRoundKey_1. The entire function is marked as an accelerator kernel.\n", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a hardware acceleration kernel, choose optimal pipeline settings for the code automatically, determine an appropriate tiling factor for the outer loop, allow the first inner loop to run in parallel with an automatically decided factor using the reduction clause for the `tmp` array, and let the second inner loop run sequentially but still use parallel reduction for updates to the `y` array.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{390}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes in C, use the following comprehensive, concise instruction:\n\nDesignate a function as a hardware acceleration kernel and perform three matrix multiplications (E=A*B, F=C*D, G=E*F) in sequence. For each matrix multiplication loop:\n\n1. For the outermost loops (`i` loops), apply automatic pipelining and tiling, but do not modify the loop structure or iteration scheduling.\n2. For the middle loops (`j` loops):\n   - Turn off automatic pipelining.\n   - Do not tile or modify the iteration scheduling.\n   - Initialize the resulting matrix (`E`, `F`, or `G`) elements to zero.\n3. For the innermost loops (`k` loops), do not apply parallelism, tiling, or modify the iteration schedule, but for the last multiplication where `G` is calculated, allow parallel processing with a factor suggested by the tool.\n\nSpecifically, use the following #pragma directives:\n- `#pragma ACCEL kernel` to treat the function as an acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{flatten}` to automatically pipeline the loops with the possibility of flattening nested loops.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to not tile the loop (effectively using a factor of 1).\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to indicate no parallelism, but for the loop calculating `G`, a parallel factor is up to the tool to be decided (use `#pragma ACCEL PARALLEL FACTOR=auto{2}`).\n- Use `#pragma ACCEL PIPELINE auto{off}` to disable automatic pipelining for certain loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The function \"aes256_encrypt_ecb\" encrypts data using the AES algorithm with a 256-bit key in \"ecb\" mode. Inside the function, we define a context called \"ctx\" which has the 256-bit key, 2 round keys (enckey and deckey), and a buffer. The function contains numerous operations, such as addRoundKey, subBytes, shiftRows, and expandEncKey. Arrays are processed using multiple iterations (loops). \n\nFor kernel optimization, '#pragma ACCEL kernel' is used to inform HLS that the aes256_encrypt_ecb function is a hardware kernel for acceleration. There are two '#pragma ACCEL PIPELINE' directives used here specifying 'auto{off}'. This tells HLS not to automatically pipeline those loops. Additionally, there's one '#pragma ACCEL TILE' directive marking the loop with 'FACTOR=auto{4}', which will divide this loop into smaller chunks with a size factor of 4 for parallel execution. This can lead to greater concurrency and performance.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a candidate for hardware acceleration. For the outer two loops, enable automatic optimization for pipelining without specifying the pipelining factor. Automatically define tiling parameters for use with loop nest optimization, and leverage parallel processing with specific factors: 8 for the first outer loop and 10 for the second outer loop.\n\nWithin the first nested loop, facilitate automatic pipelining and tiling, while enabling parallel computation with a factor of 2 and applying a reduction strategy on 'tmp' with a parallel factor of 32 for the innermost loop.\n\nFor the second outer loop, turn off automatic pipelining optimization, insisting on full loop forms. Flatten the subsequent nested loop for pipelining optimization. Specify a parallel factor of 4 while leaving the innermost loop to run serially with a reduction strategy on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Translate the following matrix multiplication functions into HLS C code optimizing the outer loops with automatic pipeline decisions but without pipelining the innermost loops, ensure that loops are not tiled or parallelized beyond the default single iteration processing, except for the innermost loop in the final multiplication stage which should be parallelized by a factor of 10, and encapsulate the entire function as a hardware acceleration kernel.\"\n\nThe specific #pragma values to use are:\n\n1. For the outermost loops, automatically determine if they should be pipelined:\n   `#pragma ACCEL PIPELINE auto{flatten}`\n\n2. For the middle loops (i.e., the second level of nested loops), pipeline should be turned off, loops should not be tiled beyond the default, and there should be no extra parallelism:\n   `#pragma ACCEL PIPELINE auto{off}`\n   `#pragma ACCEL TILE FACTOR=auto{1}`\n   `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n3. For the innermost loops of the first two matrix multiplications, process each iteration serially:\n   `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n4. For the innermost loop in the final matrix multiplication, use a parallelism factor of 10:\n   `#pragma ACCEL PARALLEL FACTOR=auto{10}`\n\n5. Encapsulate the entire function as a hardware kernel:\n   `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function 'kernel_atax' that processes matrices and vectors. Disable automatic pipelining for the outermost loop, prevent the loop from being tiled and parallelized, and apply specific parallelization factors with reduction for the two inner loops, ensuring that the 'tmp' array updates are performed with a factor of 10, and 'y' array updates with a factor of 8.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C function that identifies a specific function for hardware acceleration. In the outermost loop, prevent the compiler from automatically pipelining the iterations. For the subsequent first nested loop, instruct the compiler to treat the entire loop unaltered and to execute its iterations sequentially. In the second nested loop, designate a parallel reduction on the 'tmp' array with a specific degree of parallelism. Finally, in the third nested loop, specify another parallel reduction that operates on the 'y' array with an increased parallelism compared to the 'tmp' reduction. \n\nFor the pragmas:\n\n- Use \"#pragma ACCEL kernel\" to declare the function a hardware acceleration kernel.\n- Disable automatic pipelining with \"#pragma ACCEL PIPELINE auto{off}\".\n- Retain the original loop form with \"#pragma ACCEL TILE FACTOR=auto{2}\".\n- Ensure sequential execution of the first nested loop with \"#pragma ACCEL PARALLEL FACTOR=auto{1}\".\n- Set parallelism in the 'tmp' reduction loop to a specific value with \"#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\".\n- Finally, for the 'y' array reduction, increase parallelism with \"#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\".", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This program defines several functions to perform an AES-256 encryption in electronic codebook (ECB) mode on a 16-byte block of data. The code starts by defining the lookup tablle needed by the AES algorithm and declaring the structure that will contain keys for encryption, decryption, and interim computations.\n\nThe encryption process happens in the \"aes256_encrypt_ecb\" function. The function first initializes the internal keys and expands them using the \"aes_expandEncKey_1\" function. Then, it performs 14 rounds of transformations as specified by the AES algorithm (substitution, row shifting, column mixing, and round key addition), which uses other defined functions such as \"aes_subBytes_1\", \"aes_shiftRows_1\", \"aes_mixColumns_1\", and \"aes_addRoundKey_1\". \n\nThe \"#pragma ACCEL\" directives indicate specific compilation or optimization instructions for a hardware accelerator. Specifically, \"#pragma ACCEL kernel\" indicates that 'aes256_encrypt_ecb' should be identified as a hardware-accelerated function. \"#pragma ACCEL PIPELINE auto{flatten}\" suggests the compiler to automatically pipeline and flatten the structure of a for loop. \"#pragma ACCEL PIPELINE auto{off}\" turns off automatic pipelining and \"#pragma ACCEL TILE FACTOR=auto{13}\" allows the compiler to decide on a blocking factor for a loop nest, and the suggested tile size is 13.\n", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes to define a function `kernel_3mm` that computes three matrix multiplications (E=A*B, F=C*D, and G=E*F) with specified dimensions and matrices. The function should be accelerated in hardware. For the loops computing these multiplications, ensure the following:\n\n1. Disable automatic loop pipelining, retain the loops in their full, original form without any unrolling or pipelining modifications.\n2. For the outer two loops of each multiplication, prevent tiling optimizations and ensure the loop iterations execute sequentially.\n3. In the loop that computes G, implement only the innermost loop with a parallelization factor of 10, while keeping the others executing in a serial, unmodified fashion. \n\nEnsure that you preserve the functionality of the original function's structure and variable names. Use appropriate #pragma directives to control loop tile factors, pipelining, and parallel execution factors as specified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define an AES 256 encryption function `aes256_encrypt_ecb`, which takes a structure `aes256_context` that contains encryption, decryption keys, and the initial key `k[]`, along with the buffer `buf[]`. \n\n- First, store the encryption and decryption keys in the `aes256_context`. The encryption keys are expanded using the `aes_expandEncKey_1` function. Loop over this 8 times, each time updating the `rcon` object.\n- Then use the `aes_addRoundKey_cpy_1` function to add the round key, also an important part of the AES algorithm.\n- Then loop over the following sequence 13 times using the utility functions. Perform substitution of bytes (`aes_subBytes_1`), rotations (`aes_shiftRows_1`) and mix columns (`aes_mixColumns_1`) operations. If the loop index is odd, add the round key using `aes_addRoundKey_1` else update keys (`aes_expandEncKey_1`).\n- Conclude by performing substitution and rotations, expanding the key and adding round key once again. \n\nUse pragma `ACCEL kernel` to turn the whole `aes256_encrypt_ecb` function into an HLS accelerator. Use pipeline pragma with AUTO and FLATTEN directives in different loops to enable automatic pipelining of loops and flattening for loop nests respectively. Also use auto tiling with the tile factor set to 8 for another loop to get better performance.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a function named \"aes256_encrypt_ecb\" that takes an AES256 context and a 32-byte key and a 16-byte data buffer. This function will encrypt the data buffer in AES256 ECB mode. The code begins by initializing the encryption and decryption keys from the input key. Then it expands the decryption key eight times. Following that, it starts the encryption rounds. For each round, it substitutes bytes, shifts rows and mixes columns. Then, depending on whether the round number is odd or even, adds the round key. These steps are repeated 14 times. In the last round, byte substitution and row shifting is done again. The expanded key is used to add the last round key. \n\nThe pragma statement '#pragma ACCEL kernel' tells the HLS compiler to treat this function as a hardware kernel. No automatic pipelining is done in the loop that expands the decryption key, as indicated by the pragma annotation '#pragma ACCEL PIPELINE auto{off}'. The annotation '#pragma ACCEL PIPELINE auto{}' before the encryption rounds allows the compiler to use automatic hardware pipelining. The '#pragma ACCEL TILE FACTOR=auto{13}' annotation tells the compiler it can split the encryption rounds into smaller chunks to allow more parallelism.\n", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Direct the HLS tool to convert the `kernel_atax` function into a dedicated hardware module, avoid pipelining the first loop, refrain from tiling and parallelizing the second loop, while for the two nested loops allow reduction-style parallelism with varying intensities - 10-fold parallelism for computing `tmp` and 32-fold parallelism for updating `y`. Specifically, ensure the following:\n\n1. Mark the function `kernel_atax` as a hardware kernel.\n2. Disable automatic loop pipelining for the loop initializing array `y`.\n3. Disable loop tiling and parallel processing for the loop initializing array `tmp`.\n4. Enable loop parallelism with a factor of 10 for calculating the `tmp` array using reduction to update `tmp[i]`.\n5. Enable loop parallelism with a factor of 32 for the final loop reducing values into the `y` array with a reduction pattern.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code defines a function, aes256_encrypt_ecb, that encrypts a block of text using the AES-256 encryption standard in electronic code book (ECB) mode. The encryption process consists of several steps including key expansion, substitution, shifting rows, mixing columns, and adding round keys.\n\nIn the code, firstly, encryption key is expanded and stored in ctx-> enckey, decryption key is expanded and stored in ctx -> deckey.\n\nIt then enters a stage where key expansion loop is NOT pipelined indicated by '#pragma ACCEL PIPELINE auto{off}' to minimize access time.\n\nThen, it prepares for the decryption by copying the cipher key from input to the encryption/decryption keys, and expands the decryption key for the first AES round.\n\nThen, it enters the main encryption loop, which consists of fourteen rounds. Each round includes steps for SubBytes transformation, ShiftRows transformation, MixColumns transformation (except on the final loop iteration), and AddRoundKey transformation. The loop is tiled into chunks with a size automatically determined by the compiler (auto{13}), suggested by '#pragma ACCEL TILE FACTOR=auto{13}'. \n\nFinally, the function concludes with additional SubBytes, ShiftRows, and AddRoundKey transformations.\n\nThe full process of this function is delineated as an acceleration kernel with '#pragma ACCEL kernel'.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The provided code illustrates an encryption method, aes256_encrypt_ecb, within the type 'aes256_context', coded in C syntax. The encryption uses several sub-processes such as aes_expandEncKey_1, aes_addRoundKey_cpy_1, aes_subBytes_1, aes_shiftRows_1, aes_mixColumns_1, and aes_addRoundKey_1 for transforming the input in different ways.\n\nAs for #pragma directives:\n\n- #pragma ACCEL kernel is utilized to instruct the HLS compiler that the associated function, aes256_encrypt_ecb, is to be sped up as a hardware kernel.\n\n- #pragma ACCEL PIPELINE auto{off} is used for the purpose of specifying that the loop directly inside the pipeline should not be pipelined automatically.\n\n- Lastly, in #pragma ACCEL PIPELINE auto{flatten} and  #pragma ACCEL TILE FACTOR=auto{4}, the first pipeline clause is optimizing loop flattening while the second TILE directive with FACTOR auto{4} is suggesting that loop tiling should be carried out in an automated manner with a tiling factor of 4. These pragmas are applied to the subsequent 'for' loop in the code.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given C code comprises of a number of functions to perform AES256 encryption in ECB mode. The process begins with key initialization. The encryption key is then expanded and used to cipher the plaintext block. This is done in multiple rounds, each round consisting of four operations: SubBytes, ShiftRows, MixColumns and AddRoundKey. \n\nThe key points for generating this code are summarized below:\n\n1. Firstly, there is an initialization stage where the encryption and decryption keys are copied into context structure. This is done in the ecb1 loop with the #pragma \"ACCEL PIPELINE auto{flatten}\" execution strategy. \n\n2. Secondly, for each round except the last one, the current round key is derived from the previous one and the encryption process happens. This includes the SubBytes, ShiftRows, MixColumns, and AddRoundKey operations. It's done in the ecb3 loop with the #pragma \"ACCEL PIPELINE auto{off}\" execution strategy and #pragma \"ACCEL TILE FACTOR=auto{8}\" tiling strategy. \n\n3. Lastly, the last round of encryption is slightly different from the others. It includes SubBytes and ShiftRows operations, expands the key, and performs AddRoundKey operation but skips the MixColumns operation.\n\nNote that pragmas \"ACCEL PIPELINE\" and \"ACCEL TILE\" are used to guide the compiler about specific parallel patterns to exploit, such as loop pipelining and loop tiling effectively.\n\nThe complete code is enclosed within '#pragma ACCEL kernel' indicating that the 'aes256_encrypt_ecb' function should be synthesized into an accelerator.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function named 'kernel_atax' to accelerate the matrix-vector multiplication and accumulation process with the following constraints: the outer loop initializing 'y' should not be pipelined, the second outer loop 'i' iterating over 'tmp' should be executed in a non-parallel, non-tiled, original manner but should parallelize the nested 'j' loop with a reduction on 'tmp' using 16 processing elements, and then it should execute the subsequent 'j' loop in a non-parallel way with a reduction on 'y'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code is essentially an encrypted AES (Advanced Encryption System) 256 function that encrypts a plaintext input buffer into a ciphertext output. It defines the AES operations like SubBytes which substitutes each byte in the state from a lookup table, ShiftRows which shifts rows, MixColumns which mixes the data in each column and AddRoundKey that simply adds a generated key with the state.\n\n1. The first loop initializes the encryption and decryption keys and stores them in the context ctx.\n2. The second loop expands the decryption key `deckey`.\n3. The third loop contains the main encryption routine. \n During each iteration, `SubBytes`, `ShiftRows`, and `MixColumns` operations are executed, alongside `aes_expandEncKey_1` function which expands the encryption key depending on the iteration, and `aes_addRoundKey_1` function that adds (XORs) the round key to the state.\n4. The operation ends with `SubBytes`, `ShiftRows`, final expansion of keys and finally adding round key using `aes_addRoundKey_1`.\n    \nFor generating the code below, you'll essentially need to follow the above steps.\n\nThe `#pragma ACCEL kernel` directive specifies to the compiler to generate an accelerated version of the subsequent function, treating it as a hardware kernel for acceleration.\n\nThe `#pragma ACCEL PIPELINE` directive specifies pipelining to the compiler. \n\nThe `#pragma ACCEL TILE` directive sets the tile factor to automatically set by the compiler, which may control data locality in a loop when number of iteration is enough.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to: \n1. \"Accelerate the following function as a dedicated hardware module.\"\n2. For each loop within the function: \n     - \"Make no attempt to automatically optimize the loop's operation ordering.\"\n     - \"Do not partition the loop iterations, execute it in full.\"\n     - \"Execute iterations sequentially, do not introduce parallelism.\"\n\nFor the two inner loops labeled 'Row Sweep', further specify: \n- \"Allow parallel processing of iterations, with a particular factor enabling multiple operations concurrently within the available hardware resources.\"\n\nApplying these instructions:\n- Use '#pragma ACCEL kernel' at the beginning of the function to designate it as a hardware acceleration kernel.\n- Apply '#pragma ACCEL PIPELINE off', '#pragma ACCEL TILE FACTOR=1', and '#pragma ACCEL PARALLEL FACTOR=1' to each loop to indicate the above loop operation settings.\n- Specifically, for the 'Row Sweep' inner loops, use '#pragma ACCEL PARALLEL FACTOR=16' to inform the tool to process 16 loop iterations in parallel, if the hardware allows it.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS kernel function named `kernel_atax` that accelerates the matrix-vector multiplication and subsequent vector update. The inputs are a matrix A of size 390x410, and a vector x of length 410. Initialize a vector y of length 410 to zeroes. Employ tiling and full parallelism within the two inner loops that respectively compute the product of A's rows and x into a temporary vector tmp, and update y with the product of A's rows and tmp. Automatically manage the pipelining of the first loop, while for the two inner loops, invoke parallelism with a pre-determined factor suggested by the HLS tool and employ reduction to accumulate values into tmp and y. The vector y will be updated without breaking the second loop into smaller chunks.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The code defines the AES-256 encryption process using Electronic Codebook (ECB) mode. To generate this code:\n\n1. Start by defining a 256-element substitution box 'sbox' and a struct 'aes256_context' that holds vectors for a key, encryption key and decryption key.\n\n2. Create several helper functions: 'aes_expandEncKey_1' for key expansion, 'aes_addRoundKey_cpy_1' for adding round key, 'aes_subBytes_1' for byte substitution, 'aes_shiftRows_1' for shifting rows, 'rj_xtime_1' for time calculation in AES MixColumns step, 'aes_mixColumns_1' for mixing columns, and 'aes_addRoundKey_1' for applying XOR operation with just the key.\n\n3. The main function 'aes256_encrypt_ecb' gets an aes256_context structure and an input buffer as parameters, indicating the information to be encrypted.\n\n4. Use '#pragma ACCEL kernel' at the start of 'aes256_encrypt_ecb' function to mark it for potential optimization by the compiler.\n\n5. Following that, create a copy of the key, expand it and apply it to the input buffer.\n\n6. Finally, perform the main AES steps (subBytes, shiftRows, mixColumns, addRoundKey) in a loop 14 times. Sequential iterations of these steps are compelled to be run in parallel by '#pragma ACCEL PIPELINE auto{flatten}' and in ordered manner by using '#pragma ACCEL TILE FACTOR=auto{2}'.\n\n7. Conclude the AES encryption process by applying subBytes, shiftRows, and an additional expandEncKey/addRoundKey operations once more.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a target for hardware acceleration. Prevent the pipeline optimization of the outer-most loop that initializes array `y` to zero. Execute the loop that calculates `tmp` array without splitting it into multiple smaller tasks, but allow the iterations to run sequentially. For the inner-most loop that computes the `tmp` array, enable parallel reduction with a factor that means 16 concurrent operations, while for the loop that updates the `y` array, enable parallel reduction with a factor corresponding to 32 concurrent operations.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{390}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The following AES256 encryption HLS C code contains a number of functions that perform different steps of an AES encryption. \n\nThe \"aes256_encrypt_ecb\" function initiates the main series of computations, first initialising the encryption keys, before progressing through a series of iterations to encrypt the block of plaintext. In the process, the function performs key expansion, byte substitution, shiftRows operation, mixColumns operation, addRoundKey operation, and finally applies a two-stage pipelining process for optimization purposes. \n\nThe '#pragma ACCEL kernel' directive indicates that HLS should apply hardware acceleration to the \"aes256_encrypt_ecb\" function. \n\nThe '#pragma ACCEL PIPELINE auto{off}' directive informs HLS not to apply pipelining on 'for-loop' within \"aes256_encrypt_ecb\" function.\n\nThe '#pragma ACCEL PIPELINE auto{flatten}' directive in the same function indicates to HLS that nested for loops should be consolidated into one flattened loop. \n\nThe '#pragma ACCEL TILE FACTOR=auto{8}' directive specifies that the parallel execution instances of the 'for-loop' within 'aes256_encrypt_ecb' function should be unrolled eight times for potential performance optimization. \n\nEach individual function in the HLS code performs one element of the AES encryption process and most of these functions have been coded to perform their operations in an iterative, piecemeal way. The 'pragma' directives have been used to modify the behaviour of these functions to improve computational performance.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, I'll describe the process and the specific pragma values in the following way:\n\n- Treat the 'kernel_3mm' function as a dedicated hardware acceleration kernel.\n- For each of the three matrix multiplication loops ('E := A*B', 'F := C*D', 'G := E*F'):\n  - Avoid applying automatic pipelining optimizations to the loops.\n  - Ensure the loops are processed in their original form without tiling optimizations.\n  - Run the iterations of each loop sequentially rather than in parallel.\n\nFor the final loop where 'G := E*F' is computed:\n- Apply pipeline flattening instead of turning it off to potentially combine the nested loops into a single pipeline stage when it is beneficial for the design, this should only be done for the outermost 'i' loop.\n\nThe #pragma values to use respectively in natural language are:\n- 'Do not pipeline this loop' for '#pragma ACCEL PIPELINE \"off\"'.\n- 'Do not tile this loop' for '#pragma ACCEL TILE FACTOR=auto{1}'.\n- 'Do not parallelize this loop' for '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n- 'Allow for potential flattening optimizations in pipelining the outer 'i' loop of the 'G := E*F' computation' for '#pragma ACCEL PIPELINE auto{flatten}'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This is how you would generate the provided code:\n\n1. Generate a method that creates an array `sbox[256]` of type `unsigned char`.\n\n2. Declare a struct `aes256_context` with three `unsigned char` arrays as member.\n\n3. Generate six helper methods `aes_expandEncKey_1`, `aes_addRoundKey_cpy_1`, `aes_subBytes_1`, `aes_shiftRows_1`, `rj_xtime_1`, and `aes_mixColumns_1`, which manipulate and process the keys and buffers used in AES 256-bit encryption.\n\n4. Generate the `aes_addRoundKey_1` method that executes an XOR operation on the buffer and the key.\n\n5. Lastly, generate the `aes256_encrypt_ecb` method, which performs AES 256-bit encryption by executing the required transformations mentioned in the AES algorithm on each 128-bit block which include key expansion, initial round key addition, nine rounds of substitutions, row shifts, column mixes and round key additions and one final round of substitutions, row shifts and round key additions. \n\n6. In the method `aes256_encrypt_ecb`, use '#pragma ACCEL kernel' to treat the whole function as a hardware kernel for acceleration, '#pragma ACCEL PIPELINE auto{flatten}' to apply pipeline optimizations, saying that it's okay to \"flatten\" loops (i.e., remove loop nesting if possible), and '#pragma ACCEL PIPELINE auto{off}' to disable automatic pipelining for some code, and '#pragma ACCEL TILE FACTOR=auto{1}' to keep the loop iterations together without breaking them down into smaller chunks.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the presented HLS code snippets, apply the following steps:\n\n1. Define a static unsigned character array, sbox[], with 256 elements.\n2. Declare a structure, aes256_context, with three unsigned character arrays of 32 elements each, key[], enckey[], and deckey[].\n3. Create seven static functions named aes_expandEncKey_1, aes_addRoundKey_cpy_1, aes_subBytes_1, aes_shiftRows_1, rj_xtime_1, aes_mixColumns_1, and aes_addRoundKey_1. Each function utilizes bitwise arithmetic and loop iterations for manipulating the buffer and contextual keys for AES encryption.\n4. Define an accelerated kernel function, aes256_encrypt_ecb, which takes as input a pointer to the aes256_context struct and two unsigned character arrays for the key and buffer. This function first copies key values to the enckey and deckey fields of the struct, then it applies several encryption steps in a loop including key expansion, sub-bytes transformation, row shifting, and column mixing.\n5. To control how the operations in the encryption function are parallelized, initialize different loop-related pragmas: 'pragma ACCEL PIPELINE auto' with the value \"off\" to prevent automatic pipelining for the aes_expandEncKey_1 function inside the 'ecb2' loop. Next, activate automatic pipelining with the same pragma without specifying a value. Lastly, apply 'pragma ACCEL TILE FACTOR' as auto with a factor of 1, ensuring the 'ecb3' loop processes one iteration at a time without breaking it into smaller chunks.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel, disable automatic pipelining for the outermost loop initializing 'y', set tiling of the first 'tmp' initialization loop to a factor of 2 while keeping its iterations sequential, and apply parallelization with a reduction on 'tmp' and 'y' to their respective internal loops with a factor of 16 for both.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a hardware acceleration target. Disable pipelining for the first loop iterating over `y`. In the second loop iterating over `tmp`, tile the iterations into chunks with a factor automatically determined but suggested to be 2. Execute these loops strictly in sequence without parallelization. Inside the nested loops for computing `tmp` and `y`, apply parallel reduction with a recommendation of 16 and 32 compute units, respectively, while leaving the actual factor to be automatically chosen by the HLS tool.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the High-Level Synthesis (HLS) tool to optimize a function designed for matrix multiplication and addition using directives that establish how loops should be parallelized, pipelined, and tiled. Apply the following directives specifically:\n\n1. Treat the entire function `kernel_2mm` as a hardware acceleration kernel.\n2. For the outer two loops iterating over `i` and `j` in the first part of the kernel (matrix multiplication):\n    - Enable automatic determination of the optimal pipeline setting for the loops.\n    - For the first loop over `i`, allow the tool to automatically decide the loop tiling factor, but also test not tiling by using a factor of 1.\n    - For the parallelization, experiment with the outer loop over `i` at a factor of 8 and the inner loop over `j` at a factor of 2.\n3. Within the innermost loop over `k`, which accumulates the multiplication result, implement parallel reduction on array `tmp` while also enabling the automatic determination of the optimal parallel factor (up to a maximum of 32).\n4. For the outer two loops in the second part of the kernel (matrix addition):\n    - Disable pipelining for the outer loop over `i`.\n    - Flatten the pipeline for the inner loop over `j`.\n    - Maintain a loop tiling factor of 1 as determined by the HLS tool.\n    - Set an automatic parallel factor for both outer and inner loops, but experiment specifically with a factor of 4 for the outer loop and the default (1) for the inner loop.\n5. In the innermost loop over `k` within the second part, implement a parallel reduction on array `D`, letting the tool automatically pick an appropriate factor up to a maximum of 1.\n\nNote: Ensure you use the appropriate syntax for `#pragma ACCEL` to inform the HLS tool when and where to use automatic decision-making (`auto`) and when specific values are being tested or applied.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel that contains three matrix multiplication operations (E = A * B, F = C * D, G = E * F) with explicit no pipelining, simple tiling, and parallel execution constraints for specific loops, as detailed below:\n\n1. Treat the entire `kernel_3mm` function as a hardware acceleration kernel.\n2. In the first matrix multiplication (E = A * B), avoid automatic pipelining for the outermost loop, use a tile factor of 1 to prevent tiling, and ensure the outermost loop runs sequentially without parallelism. In the innermost loop of this matrix multiplication, enable parallel computation with a parallel factor equal to the loop bound for `k` (60).\n3. For the second matrix multiplication (F = C * D), similarly, avoid automatic pipelining for the outermost loop, use a tile factor of 1 to prevent tiling, and ensure the outermost loop runs sequentially without parallelism. However, use the 'flatten' directive for pipelining the second loop to merge nested loops and run them as a single pipeline. Keep the innermost loop sequential by using a parallel factor of 1.\n4. In the third matrix multiplication (G = E * F), disable pipelining for both the outer and second loop, prevent tiling by using a tile factor of 1, and apply a parallel factor of 7 for running the second loop with limited parallel execution. For the innermost loop of this operation, enforce parallel execution with a parallel factor of 10.\n\nUse the HLS pragma directives accordingly to reflect these instructions, ensuring to place each directive before the relevant loop or code section it is supposed to affect.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{60}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{7}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a hardware acceleration kernel. Inside this function, initialize a vector `y` to zeros. For the main computation: First, disable automatic pipelining for the outermost loop iterating over `A` rows. Opt out of loop tiling by explicitly setting the tile factor to 1 for this loop. Ensure that the iterations of this outermost loop are not parallelized by specifying a parallel factor of 1. Within this loop, calculate a temporary vector `tmp` such that each element is the dot product of a row of matrix `A` with vector `x`, and parallelize the reduction across the `tmp` calculation with a factor of 16. Further, update vector `y` by accumulating the outer product of vector `tmp` with the columns of matrix `A`, parallelizing the reduction on `y` with a factor of 10.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a high-level synthesis (HLS) C code for a kernel function named `kernel_atax` that processes matrices and vectors for an ATAX algorithm with the following specifications:\n\n1. Treat the entire function `kernel_atax` as a hardware acceleration kernel.\n2. Initialize the vector `y` without automatic pipelining.\n3. Keep the outer loop for processing `tmp` in its natural sequential order without tiling and without parallel execution of its iterations.\n4. In the inner loop that accumulates into `tmp`, employ parallel reduction with a factor that allows for 16 operations to be performed in parallel.\n5. Similarly, use parallel reduction for the inner loop that updates `y`, again allowing for 16 parallel operations.\n\nFor each directive, ensure you override any automatic factor assignment with the specifically given values.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` to operate as a dedicated hardware acceleration kernel. Within this kernel, initialize the array `y` sequentially. Apply automatic pipelining to the outermost loop which initializes the `tmp` array in order to potentially increase its execution speed by overlapping loop iterations. Automatically determine and apply tiling to the second loop to boost performance by executing chunks of the loop iterations in parallel, but enforce that iterations of this loop execute sequentially by setting a parallel factor of 1. In the innermost loops that compute the dot products, allow for parallel execution of loop iterations with the assistance of automatically selected parallel factors but ensure that updates to the `tmp` and `y` arrays are safely accumulated using reduction.\n\nHere are the corresponding #pragma descriptions for each operation:\n\n- `#pragma ACCEL kernel`: Designate `kernel_atax` as a hardware accelerator kernel.\n- `#pragma ACCEL PIPELINE auto{}`: Enable automatic pipelining for the loop initializing the `tmp` array.\n- `#pragma ACCEL TILE FACTOR=auto{2}`: Automatically apply tiling to enhance performance while keeping the loop iterations sequential.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Ensure the tiled loop runs sequentially, despite being tiled.\n- `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}`: Automatically select a factor to parallelize the loop updating `tmp` and protect the accumulation with reduction.\n- `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}`: Similarly, automatically select a factor for parallelizing the loop updating `y` while using reduction.\n\nNote: The `auto{}` and `auto{number}` pragma values indicate that the compiler should automatically determine the most appropriate value, whereas specifying `FACTOR=1` explicitly sets the factor to 1 for sequential execution.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given code represents an implementation of the AES-256 encryption algorithm in HLS (High-Level Synthesis) style C code. We have a provided a 256-element lookup table, a structure definition for the AES context, multiple helper functions for different AES operations, and the primary encryption function that makes use of these helpers, with specific HLS pragma values.\n\n- \"#pragma ACCEL kernel\" is applied to the 'aes256_encrypt_ecb' function which indicates that this function should be the primary hardware component for acceleration.\n- \"#pragma ACCEL PIPELINE auto{off}\" is used in the loop 'ecb2' which means this loop is to be executed in its natural order and not pipelined. \n- \"#pragma ACCEL PIPELINE auto{flatten}\" is used before the loop 'ecb3', asking the compiler to make efforts to pipeline all the loops within the scope with the maximum parallelism and no need to worry about the hierarchy of the nested loops. \n- \"#pragma ACCEL TILE FACTOR=auto{2}\" at loop 'ecb3' is used to instruct the HLS compiler to partition the iterations into smaller sets or 'tiles' to enable data reuse, where '2' is the suggested tile factor giving the compiler a clue without forcing it.\n\nIt's important to note that AES-256 encryption algorithm involves a number of transformations like substitute bytes, shift rows, mix columns and add round key operations, with certain steps being looped over a number of iterations. The HLS pragmas are used strategically to optimize these operations for a hardware implementation.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the following code, we first declare an array called 'sbox' and a struct 'aes256_context' which holds three arrays key, enckey, and deckey each with 32 unsigned characters. In the main function 'aes256_encrypt_ecb', perform the following:\n\n1. Initialize the 'enckey' and 'deckey' arrays in 'aes256_context' by copying the array 'k' using a loop. The pragma directive for this loop should be 'ACCEL PIPELINE auto{off}' to prevent automatic pipelining.\n\n2. Use a pipelined loop (with 'ACCEL PIPELINE auto{}') to call 'aes_expandEncKey_1' function where it alters/enhances the initial key for further stages of encryption.\n\n3. Inside another loop indexed by '_s_i' (with tiling factor set to 4 by using 'ACCEL TILE FACTOR=auto{4}'), perform the following steps:\n\n  - Call four functions, which are: 'aes_subBytes_1' to substitute bytes in the buffer with some from a predetermined table, 'aes_shiftRows_1' to permute the bytes between certain positions, 'aes_mixColumns_1' to mix the columns of the bytes, and in each second iteration, 'aes_addRoundKey_1' to round off the key bytes or 'aes_expandEncKey_1' & 'aes_addRoundKey_1' to generate the next round key and round off the initial key bytes.\n\n4. Perform the last round of operations on the buffer by calling 'aes_subBytes_1', 'aes_shiftRows_1', 'aes_expandEncKey_1' and 'aes_addRoundKey_1' functions accordingly.\n\n5. Finally, use '#pragma ACCEL kernel' to indicate that this function should be treated as a hardware kernel for acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_atax' as a hardware acceleration kernel, disable automatic pipelining for the outermost 'y' initialization loop, execute the 'tmp' initialization loop without modification, perform the computation of 'tmp[i]' without parallelization, but process the 'tmp' updates with an assumed optimal parallelism factor of 16 and similarly for the final 'y' update loop with a parallelism factor of 4.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel for a function named `kernel_atax` that computes the ATAX matrix-vector product. Initialize the `y` vector with zeros and use automatic pipelining. Inside the outer loop that iterates over the `tmp` array elements, also enable automatic tiling but keep the loop intact as one complete unit by not specifying an explicit factor. Then, calculate `tmp[i]` by accumulating in parallel with a reduction operation using automatic factor determination with an optimal factor, and repeat the process for updating `y[i]` by parallelizing with reduction on the `y` array with a different automatic factor optimized for potential parallel execution.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code is a hardware accelerated implementation of the AES256 encryption algorithm in ECB mode, utilizing the HLS directives. To generate the code, follow the steps below:    \n- Define an array of 256 unsigned characters called `sbox` for the Rijndael S-box\n- Define a struct called `aes256_context` with three arrays of 32 unsigned characters each: `key`, `enckey`, and `deckey`\n- Define six helper functions: `aes_expandEncKey_1`, `aes_addRoundKey_cpy_1`, `aes_subBytes_1`, `aes_shiftRows_1`, `rj_xtime_1`, and `aes_mixColumns_1`, which each perform a specific operation of the AES algorithm\n- Annotate the main encryption function, `aes256_encrypt_ecb`, with `#pragma ACCEL kernel` to indicate that the following function is the hardware kernel for acceleration\n- Initialize a Rijndael key schedule using AES key from user and performs AES key expansion using `aes_expandEncKey_1`\n- Implement the main loop of AES256 ECB encryption, where each round consists of four stages: SubBytes, ShiftRows, MixColumns, and AddRoundKey\n- Use `#pragma ACCEL PIPELINE auto{flatten}` directive before the loop that expands the encryption key, to enable loop pipelining. Since it's an `auto{flatten}` pragma, the compiler will try to optimize the pipeline to achieve the best performance\n- Use `#pragma ACCEL PIPELINE auto{off}` directive before the loop that performs AES encryption, to disable loop pipelining\n- Use `#pragma ACCEL TILE FACTOR=auto{4}`, it tells HLS to break iterating over the buffer into smaller chunks as it optimizes the process. As a result, simultaneous iterations can be run in parallel on different sections of the data. Here, the compiler will try to optimize the tile factor to achieve the best performance", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration target. Initialize the array 'y' sequentially. Automatically optimize loop pipelining for subsequent loops. Delegate loop tiling to the HLS tool with a suggested factor of 8 for the 'i' loop iterating over 'tmp'. Run the 'i' loop sequentially, without parallelism. For the inner 'j' loop calculating 'tmp', apply parallelism and enable reduction with a suggested parallel factor of 16. In the final 'j' loop updating 'y', enable parallel reduction with a suggested parallel factor of 32.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Implement the 'kernel_2mm' function as a hardware kernel to accelerate the computation of matrix multiplication and addition where the first phase calculates the intermediate matrix 'tmp' by multiplying matrices 'A' and 'B', scaling the result by 'alpha', and the second phase updates matrix 'D' by multiplying the 'tmp' matrix with 'C', scaling 'D' by 'beta', and adding the results. For optimization, allow automatic determination of pipeline opportunities, tiling factors, and parallelization factors for outer loops, with the innermost loops utilizing specified parallel factor reductions: '32' in the first matrix multiplication and no parallelization in the subsequent matrix addition, as well as disabling the loop pipelining in the second phase outermost loop while flattening the pipeline for the corresponding inner loop.\" \n\n#pragma specifics:\n\n1. Treat the 'kernel_2mm' function as a hardware kernel for acceleration.\n2. Allow the compiler to automatically decide whether to pipeline the outer loops and enable loop flattening only for the second phase's inner loop.\n3. Enable automatic tiling for all loops.\n4. Allow the compiler to automatically determine the parallelization factor for the outer loops, with '32' and '1' used specifically for the two inner loops' parallel reductions.\n5. Disable automatic pipelining for the second phase's outer loop. \n\nThe code should be generated with the correct pragmas to reflect these optimizations. Here are the respective pragma translations based on the given description:\n\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`.\n- Automatically determine pipeline opportunities for each loop: `#pragma ACCEL PIPELINE auto{}` unless otherwise specified.\n- Enable loop flattening for the second phase's inner loop: `#pragma ACCEL PIPELINE auto{flatten}`.\n- Specify parallel factor reductions for inner loops: `#pragma ACCEL PARALLEL reduction=<variable> FACTOR=auto{<value>}` with `<variable>` and `<value>` replaced by 'tmp', '32' and 'D', '1' respectively.\n- Allow the compiler to automatically determine tiling factors: `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Disable automatic pipelining for the second phase's outer loop: `#pragma ACCEL PIPELINE auto{off}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the 'kernel_atax' function as a dedicated hardware acceleration kernel, disable pipelining for the outermost loop, process the first inner loop without tiling or parallelization, and within the nested loops, apply a parallel reduction with a factor of 2 for the 'tmp' array updates and a factor of 4 for the 'y' array updates.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The code below starts by assigning a variable rcon and initializing it with value 1. Thereafter, it creates a loop with a counter that ranges between 0 <= b <= 31. Inside the loop, ctx -> enckey[_s_i_0] and ctx -> deckey[_s_i_0] are assigned the value of k[_s_i_0]. After week, a for loop is created to iterate from 8 to 1, where the aes_expandEncKey_1 function is called.\n\nThen in the next loop, the counter increments from 1 to 13. Inside this loop, the functions aes_subBytes_1, aes_shiftRows_1, and aes_mixColumns_1 are applied to the buf variable. If the counter is an odd number, the aes_addRoundKey_1 function is applied to buf & ctx->key[16]. Otherwise, the aes_expandEncKey_1 function is applied to ctx->key & rcon, and the aes_addRoundKey_1 function is applied to buf & ctx->key.\n\nHere are the specific pragmas to use respectively: \n- Preprocessing pragma that treats the following function as a hardware kernel for acceleration for 'void aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])'\n- Instruction used to disable automatic pipelining for the loop 'for (i = ((unsigned char )8); --i; ) {    aes_expandEncKey_1(ctx -> deckey,&rcon);}'\n- Instruction to process the loop 'for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i)' in parallel\n- For the loop 'for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i)', tiling is enabled where the optimal factor is chosen by the compiler.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a dedicated hardware acceleration kernel. Within the function, for the main matrix multiplication and addition operations, ensure that:\n\n1. The outermost loops (over dimensions 40 and 80) are not automatically pipelined, parallelized, or tiled. They should execute sequentially without any compiler optimizations that modify their structure or execution order.\n\n2. The innermost loops (over dimensions 50 and 70 for the first operation, and 50 for the second operation) applying the reduction operation to 'tmp' and 'D' are to be parallelized with a parallel factor of 4 for incrementing 'tmp' through matrix A*B and a factor of 80 for incrementing 'D' through tmp*C, but without any optimization for the intermediate loops.\n\n3. When updating the 'tmp' array, do not alter the pipelining of the loop, but set the innermost loop to parallelize with a minimum or no specified factor for reduction on 'tmp', meaning it will largely follow the original sequential execution pattern but still allow for some parallel execution.\n\n4. When updating the 'D' array, automatically flatten the second outermost loop to potentially parallelize its iterations while keeping the innermost loop executing reductions on 'D' as mostly sequential for each iteration of its loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a hardware acceleration candidate, disable automatic pipelining for the top-level loop within it, ensure the two inner loops iterate in their full, uninterrupted sequences, and enforce no parallel operation for the second-level loop. For the nested loops inside, set up parallel reductions on arrays `tmp` and `y` with a parallelism factor of 10 and 5, respectively.\n\nThe respective pragmas you need are:\n- `#pragma ACCEL kernel` for identifying the function to be accelerated.\n- `#pragma ACCEL PIPELINE auto{off}` to prevent the loop from being automatically pipelined.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to indicate that the loop should not be tiled and should run in its entirety.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the second loop to forbid parallelization.\n- `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}` to allow parallel reduction on `tmp` with a factor of 10.\n- `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}` to allow parallel reduction on `y` with a factor of 5.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following hardware acceleration directives into C code for the specified matrix operation function, ensuring to apply the following constraints and optimizations:\n\n1. Declare the function as a hardware kernel to expedite processing.\n2. For the initialization of vector `y`, prevent the automatic pipelining optimization to enforce original loop execution.\n3. Ensure the main processing loop for vector `tmp` is executed in full without tiling or parallel processing optimizations.\n4. Within the loop that calculates `tmp`, parallelize the reduction operation on `tmp` with a factor of 16, allowing simultaneous updates on the `tmp` vector.\n5. For the loop updating vector `y`, apply a parallel reduction with a reduced parallelism factor of 8, allowing concurrent updates on `y` with moderate parallelism.\n\nUse the appropriate `#pragma` directives before each loop or function to respect these guidelines.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This task requires the generation of a C code for the AES-256 encryption algorithm where it deals mainly with several operations and iterations that form the AES-256 rounds.\n\nBelow are the key points stating what each function does and the suitable pragma for each:\n\n1. \"aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\": It is the main encryption function that applies the AES-256 encryption on input text data. To make it applicable for hardware acceleration, this function would be defined as a kernel function with \"#pragma ACCEL kernel\" directive. \n\n2. \"aes_expandEncKey_1\": This is used to expand AES keys. It's executed in the body of a loop with the pragma directive to flatten it using \"#pragma ACCEL PIPELINE auto{flatten}\" for efficient pipelining.\n\n3. \"aes_addRoundKey_cpy_1\": This function adds (XORs) the round key to the state and is also subject for pipelining to enhance the performance with \"#pragma ACCEL PIPELINE auto{flatten}\".\n\n4. \"#pragma ACCEL TILE FACTOR=auto{4}\": This pragma directive is used to break iterations into smaller \"tiles\" for parallel execution. However, auto{4} clause means we're specifying the size of each \"tile\" here.\n\n5. \"aes_addRoundKey_1\": This applies each Round Key to the state. The rounds are performed in order, first the round0 key, then round1, and so on.\n\n6. \"aes_subBytes_1\", \"aes_shiftRows_1\", and \"aes_mixColumns_1\": These functions describe the basic AES round operations and are also pipelined for better data throughput.\n\nThe mentioned operations of the AES-256 encryption process are passed through a pipeline \"#pragma ACCEL PIPELINE auto{flatten}\" and tiled:\"#pragma ACCEL TILE FACTOR=auto{4}\" to better manage loop iterations and improve hardware parallelism. The AES expansion function is also marked as a kernel which suggests that it could be suitable for hardware acceleration.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_atax' function for hardware acceleration, but ensure that the first loop within remains sequential without optimization, operates at full granularity without tile subdivision, and runs its iterations one after another without parallelism. In the nested loops, first allow a reduction on 'tmp' with moderate parallelism by a factor of 10, and then a reduction on 'y' with increased parallelism by a factor of 16.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_atax' function as a hardware acceleration kernel, automatically enable pipelining for the top-level loop which initializes array 'y', automatically determine and apply the optimal tiling factor for the second-level loop that calculates 'tmp', ensure this second-level loop's iterations run sequentially, enable parallel processing with automatic factor selection for the reduction computation of 'tmp' within the nested loop, and also enable parallel processing with another automatic factor adjustment for the reduction computation of 'y' in the following nested loop.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a hardware-accelerated kernel function named 'kernel_atax' that computes an ATAX matrix-vector multiplication. Use a 2D array 'A' with dimensions 390x410, a vector 'x' with length 410, an output vector 'y' with length 410, and a temporary vector 'tmp' with length 390. For the first-level 'y' initialization loop, disable automatic pipelining. For the second-level loop computing 'tmp', retain full loop form without tiling, execute iterations sequentially, but within the innermost loops, enable parallel reduction on 'tmp' with a factor that doubles the default behavior. Lastly, for the final-level loop updating 'y', again allow parallel reduction with twice the default parallelization factor.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the code generation tool to:\n\n1. Designate the function 'kernel_atax' for hardware acceleration.\n2. Within 'kernel_atax', initially set all elements of the array 'y' to zero without applying any loop pipelining optimisation.\n3. In the first loop that iterates over 'tmp', prevent the loop from being tiled or parallelized beyond its original sequential behavior.\n4. In the nested loop that accumulates into 'tmp', apply loop parallelization with a factor of 32 to enable parallel reduction across the 'tmp' array.\n5. In the subsequent loop that updates the 'y' array, apply a reduced parallelization with a factor of 2 to allow for parallel reduction, but with lesser parallelism compared to the first nested loop.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The task requires generating a code to encrypt a message using AES 256 in Electronic Code Book (ECB) mode. Here's a walkthrough of the generated code:\n\n1. Defined an array (sbox) to represent the substitution box used in AES encryption.\n2. Defined a context structure (aes256_context), which holds the key and intermediate states.\n3. Implemented six helper functions (aes_expandEncKey_1, aes_addRoundKey_cpy_1, aes_subBytes_1, aes_shiftRows_1, rj_xtime_1, and aes_mixColumns_1), which are blocks of AES algorithm. These are involved in generating expanded keys, adding round keys, substituting bytes, shifting rows, multiplying by x in Galois field 2^8, and mixing columns respectively.\n4. Defined the main encryption function (aes256_encrypt_ecb). This function initiates the context with the encryption key, then runs 14 encryption rounds which include substituting bytes, shifting rows, mixing columns and adding round keys.\n5. Inside this function, the ACCEL pragma keywords are used:\n\n   - pragma ACCEL kernel denotes the function aes256_encrypt_ecb as kernel function.\n  \n   - pragma ACCEL PIPELINE auto{off} defines that this block of code should not be pipelined.\n  \n   - pragma ACCEL TILE FACTOR=auto{8} suggests to the compiler that it can automatically choose a suitable tile factor for the loop with a hint of using 8 as the factor. Tiles are used in order to enable the execution of loop iterations in a more parallelized manner on hardware.\n  \nPlease note that these explanations are intended to be concise, not an extensive discussion of AES algorithm details.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the given C code into a high-level synthesis (HLS) format to create hardware acceleration for the function 'kernel_atax'. Within the function, disable automatic pipelining for the first loop to execute 'as-is'. For the second loop, ensure it runs in full without breaking it into smaller pieces and without parallel processing of its iterations. In the nested loops, use parallelism with reduction to sum up elements multiplied by 'x' with a parallel factor of 16 and to accumulate the product of 'A' with 'tmp' into 'y' with a parallel factor of 32.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Identify the 'kernel_atax' function as a hardware acceleration kernel, disable automatic pipelining of the first nested loop, execute the first nested loop without any tiling optimization, and perform its iterations sequentially. For the innermost second and third nested loops, perform parallel processing with a reduction on 'tmp' with a minimal degree of parallelism and with a more aggressive factor on 'y', respectively.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided C function as a hardware acceleration kernel, do not automatically pipeline the first loop and execute it fully sequentially. For the second outer loop, execute iterations in sequence without any tiling optimizations. Within this loop, allow the inner first loop to execute two iterations in parallel with reduction applied to the `tmp` array, and have the inner second loop run its iterations sequentially with reduction applied to the `y` array.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function as a hardware acceleration kernel, disable automatic pipelining for the `y` initialization loop, set the `tmp` initialization loop to not be divided into smaller parts for parallel execution, enforce sequential execution for the `tmp` initialization loop, apply parallelism with automatic factor detection optimized for the architecture to the inner loop with reductions on `tmp`, and apply a parallelization strategy with automatic factor detection optimized for the architecture to the loop that accumulates into `y`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The process can be outlined as follows:\n\n- Firstly, define an unsigned character array variable `sbox[]` and a struct `aes256_content` which contains three 32-byte unsigned character array variables named `key`, `enckey` and `deckey`.\n- There are a series of functions prepared for AES256 encryption, like `aes_expandEncKey_1()`, `aes_addRoundkey_cpy_1()`, `aes_subBytes_1()`, `aes_shiftRows_1()`, `rj_xtime_1()`, `aes_mixColumns_1()`, `aes_addRoundkey_1`. Each function is doing a specific task in AES256 encryption. \n- Then, use the `#pragma ACCEL kernel` Pragma to specify the following function `aes256_encrypt_ecb()` for acceleration.\n- In the `aes256_encrypt_ecb()` function, initialize the `rcon` and `i` variables and use two for loops to copy the input `k` array into `enckey` and `deckey` arrays of `ctx` structure.\n- Use a loop with CELE PIPELINE Pragma directive to expand the decryption key 8 rounds by calling `aes_expandEncKey_1()` function.\n- Then enter the main processing of AES256 encryption: use a for loop with PIPELINE and TILE FACTOR Pragma directives to perform a series of operations including substituting bytes, shifting rows, mixing columns, expanding encryption key and adding round key.\n- After the loop, perform substituting bytes, shifting rows, expanding encryption key and adding round key to complete the AES256 encryption on input buffer. \n\nFor the Pragma values, \n- Use '`#pragma ACCEL kernel`' to indicate that the following function `aes256_encrypt_ecb()` should be processed as a hardware kernel for acceleration.\n- Use '`#pragma ACCEL PIPELINE auto{off}`' to specify that the loop should not be automatically pipelined.\n- Use '`#pragma ACCEL PIPELINE auto{}`' to specify that the loop should be automatically pipelined following the software's implementation.\n- Use '`#pragma ACCEL TILE FACTOR=auto{2}`' to request the compiler to automatically select the tile factor for the loop, and provide the maximum number of hardware resources that the compiler can utilize is 2.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel to accelerate a matrix-vector multiplication followed by a vector update routine. Disable automatic pipelining for the first level vector initialization loop. Preserve the full iteration domain of the main computational loop without tiling, and ensure the iterations are not executed concurrently. In the nested loops, enable parallelism with a factor of 4 and specify reduction for the accumulation of intermediate results `tmp` and final results `y`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a hardware acceleration kernel, disable automatic pipelining for the outer loop initializing array `y`, preserve the full, original structure of the first inner loop setting `tmp` values without partitioning it into smaller chunks or parallelizing its iterations, and within the two nested loops performing matrix-vector multiplications, allow parallel processing on the inner loops with a specified degree of parallelism that maps 32 operations to run concurrently while performing a reduction on `tmp` and `y` arrays respectively.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code that:\n1. Designates the provided function as a hardware acceleration kernel.\n2. Disables automatic pipelining for the first loop within the function.\n3. Ensures the second loop within the function maintains its original form without tiling, and executes iterations sequentially.\n4. Applies a parallel reduction to the inner loop with a factor of 32, targeting the 'tmp' array.\n5. Applies a parallel reduction to the second inner loop with a factor of 8, targeting the 'y' array.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel function called `kernel_atax` that operates on a matrix `A[390][410]` and vectors `x[410]`, `y[410]`, and `tmp[390]`. Start by zeroing the `y` vector. Then in the first outer for-loop that has 390 iterations, set each `tmp` element to zero but do not apply pipelining optimization to the loop. Tile the loop with the maximum factor while keeping the loop structure as one chunk. Run the iterations of this loop sequentially. Inside this loop, there are two nested loops each with 410 iterations: have the first perform a dot product of a row of `A` with `x`, accumulating into `tmp[i]`, and parallelize it with an unspecified factor that optimizes for the highest achievable parallelism and applies a reduction to prevent data hazards; have the second loop similarly update `y[j]` using `tmp[i]` and parallelize it similarly to the first nested loop.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{390}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' for hardware acceleration, initialize the 'y' array outside of the compute loops without pipelining, and for the first compute loop, use automatic optimization for pipelining and tiling within the loop while maintaining a sequential iteration pattern. Inside this loop, apply a parallel reduction to the 'tmp' array computation with a factor suggestion of 32, and for the second nested loop, invoke a parallel reduction on the 'y' array with a recommended factor of 8.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, you would instruct as follows:\n\n\"Define a function called 'kernel_atax' that will be treated as a hardware acceleration kernel. This function takes a two-dimensional array 'A' with dimensions 390 by 410, and three one-dimensional arrays 'x', 'y', and 'tmp' with sizes of 410, 410, and 390, respectively. The array 'y' is initially filled with zeroes.\n\nInside the function, create two nested loops. For the outer loop, iterating over 'i' from 0 to 389, automatically pipeline the loop, but do not specify the exact pipeline factor; let the HLS tool automatically decide. Tile this 'i' loop with some automatic factor but suggest using a factor of 8 as a starting point for the HLS tool's consideration.\n\nWithin this outer loop, set 'tmp[i]' to zero before entering the inner loops. Create an inner loop, iterating over 'j' from 0 to 409, where you want to perform parallel processing with reductions on 'tmp' with an automatic parallel factor, proposing 10 as a hint for the HLS tool to consider.\n\nIn the next inner loop, also iterating 'j' from 0 to 409, parallelize the operations with reductions on 'y' with an automatic parallel factor and suggest 32 as a hint for the parallelization degree.\n\nLastly, specify for the HLS tool that the 'i' loop should not be parallelized beyond its natural iteration order, meaning the iterations should occur sequentially, one after the other.\"\n\nThe respective #pragma values would be:\n\n1. '#pragma ACCEL kernel' describes treating the function as a hardware acceleration kernel.\n2. '#pragma ACCEL PIPELINE auto' allows for automatic pipelining of the loop, where the tool decides the pipelining factor.\n3. '#pragma ACCEL TILE FACTOR=auto{8}' allows the tool to automatically decide on the tiling factor, hinting at a starting factor of 8.\n4. '#pragma ACCEL PARALLEL FACTOR=auto{1}' indicates that the outer loop's iterations should be executed in sequence, suggesting no parallel factor beyond 1.\n5. '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}' specifies parallel reduction operations on 'tmp', hinting at a starting factor of 10.\n6. '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}' specifies parallel reduction operations on 'y', suggesting a starting factor of 32 for the tool's consideration.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided C function 'kernel_atax' to be synthesized as a hardware function for acceleration. Do not pipeline the first loop within the function. Ensure the second loop runs in its complete form without tiling, and execute its iterations sequentially. In the third loop, use parallelism to accumulate the 'tmp' array with a parallel factor of 4. Finally, for the fourth loop, apply parallel reduction to the 'y' array with a parallel factor of 16. Use the correct pragmas at the appropriate locations within the function to achieve these requirements.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware-accelerated kernel function `kernel_atax` that multiplies a matrix `A` with a vector `x`, stores an intermediate result in vector `tmp`, and then multiplies the transposed matrix `A` with `tmp` to produce the output vector `y`. Disable automatic pipelining for the outermost loop that initializes `y`. Use default tiling with a factor of 1 for the first inner loop initializing `tmp` and require that its iterations are not executed in parallel. For the second inner loop that computes the intermediate results `tmp`, enable parallel reduction and set the parallelization factor to 2. Finally, for the last inner loop that updates the output `y`, enable parallel reduction with a parallelization factor of 8.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C with the specified pragmas, use the following instruction:\n\n\"Implement a hardware kernel function named 'kernel_atax' that performs matrix-vector multiplication and accumulation. Ensure the outer vector 'y' is initialized to zero outside of the main computational loops. Apply automatic loop pipelining optimization for all loops where feasible. For the first main computation loop over the iterator 'i', automatically determine an optimal tiling factor but keep the loop iterations sequential. Within this loop, perform parallel reductions on the 'tmp' and 'y' arrays with automatically selected parallel factors of 32 and 5, respectively, to enable concurrent execution and summation across iterations for enhanced performance.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to optimize a function for hardware acceleration by defining it as a kernel, automatically determine the pipelining strategy for a loop, automatically decide the tiling factor for breaking down a loop into smaller sub-loops, sequentially execute the iterations of a specific loop, employ parallel execution with a custom factor for both the inner loops while performing different reduction operations on temporary arrays. Specifically, use a factor of 32 for the reduction on 'tmp' and a factor of 16 for the reduction on 'y'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given HLS #pragmas into one single natural language instruction for the provided C code:\n\n\"Implement a hardware-accelerated function named 'kernel_atax' with double precision arrays 'A', 'x', 'y', and 'tmp' as inputs and outputs, where the function initializes 'y' to zero, computes a matrix-vector multiplication storing intermediate results in 'tmp' without pipelining or tiling optimizations, and then accumulates the outer product of 'tmp' and 'A' into 'y' with the inner loop partially parallelized by a factor of 2 for computing 'tmp' and more aggressively parallelized by a factor of 32 for updating 'y'.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel function 'kernel_atax' that multiplies a matrix 'A' by a vector 'x', accumulates the intermediate results in a temporary array 'tmp', and then multiplies the transposed matrix 'A' by 'tmp' to update the output vector 'y'. Define the following constraints for the function's loops using pragmas: do not automatically pipeline the outermost loop, perform the loop iterations sequentially without partitioning them, make sure that each loop iteration is executed in order without parallelism, enable parallel processing with a reduction on 'tmp' with a factor of 4 for the second loop, and enforce serialized (non-parallel) processing with a reduction on 'y' for the third loop.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a dedicated hardware acceleration unit, turn off automatic loop pipelining for better control, set tiling to split the second outer loop into chunks of an inferred size, enforce sequential execution for the first 390 iterations of the second loop, implement parallel reduction with a grouping of 16 for the innermost loop calculating 'tmp', and perform a parallel reduction with a grouping of 10 for the final loop updating 'y'. Use the following pragma directives respectively:\n\n- '#pragma ACCEL kernel' to specify the function as a hardware kernel.\n- '#pragma ACCEL PIPELINE auto{off}' to disable automatic loop pipelining.\n- '#pragma ACCEL TILE FACTOR=auto{2}' to tile the loop with the inferred factor 2.\n- '#pragma ACCEL PARALLEL FACTOR=auto{1}' to keep the original execution order for the second loop.\n- '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}' to apply a reduction pattern to the 'tmp' array with factor 16.\n- '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}' to apply a reduction pattern to the 'y' array with factor 10.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for accelerating a matrix-vector multiplication kernel with the following instructions:\n\n1. Mark the function 'kernel_atax' as a hardware acceleration kernel.\n2. Initialize the output vector 'y' without using automatic pipelining.\n3. In the main processing loop over the 'tmp' vector:\n    - Do not tile the outer loop 'i' over 'tmp'.\n    - Execute loop iterations sequentially without parallelism.\n4. In the nested loop that computes intermediate results 'tmp':\n    - Apply parallelism with a factor of 16 and enable reduction on the 'tmp' array.\n5. In the following nested loop that updates the output vector 'y':\n    - Apply parallelism with a factor of 8 and enable reduction on the 'y' array.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{390}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Regard the function as a hardware acceleration kernel, but prevent the first loop from being pipelined. For the second loop, enforce that it runs fully without tiling beyond what's inherent to the hardware. This loop should run its iterations non-concurrently. For the inner nested loops, allow parallel execution with a reduction on temporary arrays, and automatically select a parallelism factor that maximizes performance without specifying an exact value.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Identify the function 'kernel_atax' as a candidate for hardware acceleration, disable automatic pipelining for the loop initializing array 'y', ensure the loop initializing array 'tmp' runs in its fully unrolled form without tiling and without parallelism, enable parallelism with a factor of 32 for the reduction on 'tmp', and employ a parallelism factor of 10 for the reduction on array 'y'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "In the given HLS C code, we are encrypting an input buffer using the AES-256 ECB method. The steps are as follow:\n\n1. Initialize 'rcon' as 1 and indicate the type of variable 'i'.\n2. Copy the input key to the 'enckey' and 'deckey' array of your context structure. This can be done with a parallelizable loop, where each element of the 'k' array (that will range from array indices 0 to 31) is copied to 'enckey' and 'deckey' arrays of your context structure.\n3. Call 'aes_expandEncKey_1' function 8 times to expand the decryption key stored in 'deckey' array of your context structure. This should be executed sequentially, therefore you should turn off the PIPELINE optimization for this loop.\n4. Call 'aes_addRoundKey_cpy_1' function once to xor the input buffer with the encryption key stored in 'enckey' array of your context structure and copy the result in 'key' array of your context structure.\n5. Start a new loop of 13 iterations that will:\n     - First, call 'aes_subBytes_1' function which substitutes all bytes in the input buffer with a substitution from predefined sbox array.\n     - Call 'aes_shiftRows_1' function which shifts rows in the input buffer.\n     - Call 'aes_mixColumns_1' function which alters each byte in each column.\n     - Checks if the loop iterator is odd, and if so, it will call 'aes_addRoundKey_1' to xor the input buffer with the second half of 'key' array, otherwise it will expand 'key' array by calling 'aes_expandEncKey_1' function and then xor the input buffer with the 'key' array using 'aes_addRoundKey_1' function.\n6. After the loop, call 'aes_subBytes_1' and 'aes_shiftRows_1' again and expand 'key' array one more time before the final call of 'aes_addRoundKey_1'. This marks the completion of the encryption process.\n\nThe inner loop should be flattened to merge all sub-loops into one main loop and allow parallelization. The TILE optimization can be enabled on the inner loop with defining the factor appropriately to dictate how many iterations of the loop to execute in parallel. Note: You should use the keyword 'auto' for the TILE optimization parameter to allow the HLS tool to choose the best factor.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{13}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "In order to correctly generate the given HLS codes in C, the developer needs to conduct the following processes:\n\n1. Define array `sbox` with 256 unsigned characters such as `0x63`, `0x7c`, and `0x77` among others. Define a struct called `aes256_context` with three unsigned character arrays each with size `32`.\n\n2. Declare 6 static functions which are: `aes_expandEncKey_1`, `aes_addRoundKey_cpy_1`, `aes_subBytes_1`, `aes_shiftRows_1`, `rj_xtime_1`, and `aes_mixColumns_1`. Implement their functionalities respectively.\n\n3. Mark the next function right after those 6 static functions `aes256_encrypt_ecb` that accepts two parameters `aes256_context *ctx` and `unsigned char k[32]` as a hardware kernel for acceleration with `#pragma ACCEL kernel`.\n\n4. In the `aes256_encrypt_ecb`, \n    - Initialize `rcon` and `unsigned char` `i` at the top.\n    - Then iterate over `unsigned char k[32]` and assign its value into `ctx -> enckey[_s_i_0]` and `ctx -> deckey[_s_i_0]`. \n    - Reset pipeline optimization with `#pragma ACCEL PIPELINE auto{off}` before performing the operation `aes_expandEncKey_1(ctx -> deckey,&rcon);` for 7 times. \n    - Further, implement encryption operation including operations `aes_subBytes_1(buf)`, `aes_shiftRows_1(buf)`, and `aes_mixColumns_1(buf)` loop for 13 times with pipeline and tile optimizations off by `#pragma ACCEL PIPELINE auto{off}` and `#pragma ACCEL TILE FACTOR=auto{1}` respectively before the loop.\n    - Lastly perform operations `aes_subBytes_1(buf)`, `aes_shiftRows_1(buf)`, `aes_expandEncKey_1(ctx -> key,&rcon)`, and `aes_addRoundKey_1(buf,ctx -> key)` in sequence.\n\nWith these instructions, these codes can be correctly and concisely generated.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS kernel function named `kernel_atax` which processes matrices and vectors, with specific optimization pragmas as follows: \n\n- Declare the function as a hardware acceleration kernel.\n- In the first loop initialization, ensure each element of vector `y` is set to zero without pipelining or parallel execution.\n- In the main loop over variable `i`:\n  - Initialize `tmp[i]` to zero without tiling the loop or executing iterations in parallel.\n  - Compute the matrix-vector product `tmp[i]` with matrix `A` and vector `x`, allowing a small factor of iterations to be executed in parallel (factor 2); enable the reduction for `tmp`.\n  - Update the vector `y` with the result of `tmp[i]` multiplied by the matrix `A`, this time with a larger number of iterations being processed in parallel (factor 10); enable the reduction for `y`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function called `kernel_2mm` to be a hardware acceleration kernel that performs a matrix-matrix multiplication followed by a scaled matrix addition without any loop pipelining, tiling, or parallel execution enhancements on the outermost loops. Specifically:\n\n- Designate the function `kernel_2mm` with argument types and names: two `int` variables `ni`, `nj`, `nk`, `nl`, and two `double` variables `alpha`, `beta`, and four 2D `double` arrays `tmp`, `A`, `B`, `C`, `D` as a hardware kernel. Do not apply any loop optimization pragmas to the first and last outermost loops: `{i = 0; i < 40; i++}`.\n\n- In the first nested loop `{j = 0; j < 50; j++}` that initializes the `tmp` array to zero, disable loop pipelining and tiling. Apply parallel execution with a factor of 4 to enhance loop performance.\n\n- Within the nested loop performing the operation `tmp[i][j] += alpha * A[i][k] * B[k][j];`, enable parallel execution with reduction on the `tmp` array using a factor of 2.\n\n- In the last nested loop `{j = 0; j < 80; j++}` that computes the final result `D[i][j]`, enable loop pipelining but do not flatten the loop, maintain the default loop tiling, and apply parallel execution with a full parallelism across all 80 iterations (factor of 80).\n\n- Apply parallel execution with reduction on the `D` array using the default factor within the final inner loop performing the operation `D[i][j] += tmp[i][k] * C[k][j];`. \n\nApply the pragmas according to these specifications:\n\n1. Treat `kernel_2mm` as an isolated hardware acceleration kernel with `#pragma ACCEL kernel`.\n2. Suppress automatic loop pipelining for all outermost loops with `#pragma ACCEL PIPELINE auto{off}`.\n3. Process the outermost loops in their original full form without tiling with `#pragma ACCEL TILE FACTOR=auto{1}`.\n4. Run the iterations of the outermost loops sequentially, with a parallel factor of 1, with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n5. For the first nested loop `{j = 0; j < 50; j++}`, enhance loop performance with a parallel factor of 4, `#pragma ACCEL PARALLEL FACTOR=auto{4}`.\n6. For the operation inside the first nested loop, `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}` enables parallel execution with reduction.\n7. For the last nested loop `{j = 0; j < 80; j++}`, fully parallelize the loop with `#pragma ACCEL PARALLEL FACTOR=auto{80}`.\n8. Allow loop pipelining but prevent loop flattening with `#pragma ACCEL PIPELINE auto{flatten}`.\n9. Enable parallel execution with default parallel factor and reduction on `D` array within the last inner loop using `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration candidate, disengage automatic pipelining for the initial loop assigning 'y' array values, inhibit the subdivision of the second loop by maintaining its full iteration range, ensure the second loop's iterations are not processed in parallel, apply a moderate factor of parallelism to the innermost loop performing the 'tmp' array reduction by doubling its resource allocation, and significantly increase parallelism when updating 'y' array by quintupling its resources while maintaining coherence through reduction.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel. For the first loop initializing array y, do not pipeline it automatically. For the outer loop iterating over array tmp, conduct operations in the original order without partitioning the loop into sub-loops, and execute iterations sequentially without parallelization. Inside this loop, use parallelization with a factor of 32 when accumulating into the 'tmp' array and with a factor of 10 when performing the reduction to the 'y' array.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is your comprehensive but very concise instruction to generate the provided HLS codes along with their corresponding #pragma values described in natural language:\n\n\"Create an accelerated hardware kernel function named 'kernel_atax' that initializes an output vector 'y' to zeros, then performs two nested loops on a matrix 'A' and vectors 'x', 'y', and 'tmp'. In the outermost loop, optimize the loop as a tiled loop with automatic factor selection. Ensure the first inner loop performs multiplications and accumulations on 'tmp' with a parallelization factor automatically set for reduction, and do the same for the second inner loop, which updates 'y', with a higher parallelization factor. For the first inner loop, select a parallelization factor of 16 for reduction on 'tmp', and for the second inner loop, choose a parallelization factor of 32 for reduction on 'y'. Automatically pipeline the second nested loop without specifying a factor.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function 'kernel_atax' as a hardware acceleration kernel and optimize its internal loops for parallel execution with the following directives:\n\n1. Disable automatic pipelining for the outer loop that initializes the array 'y' to zeros.\n\n2. Allow the compiler to automatically determine the best tiling factor for the outer loop iterating over 'tmp', but ensure that the loop executes in its full form as much as possible, without subdividing it into smaller independent loops.\n\n3. Execute the iterations of the loop that computes the temporary values in 'tmp' sequentially, but let the compiler choose the parallelization factor for the inner reduction operation where 'tmp' accumulates the product of matrix 'A' and vector 'x'.\n\n4. Likewise, for the loop updating 'y', let the compiler automatically determine the parallelization factor for the reduction operation which aggregates the product of 'tmp[i]' and 'A[i][j]'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "I need you to create an HLS function that accelerates matrix-vector multiplication following these specific optimization directives:\n\n1. Designate the entire function as a hardware acceleration kernel.\n2. For the main output vector initialization loop, disable automatic pipelining.\n3. Ensure that the main computation loops for 'tmp' and 'y' updates run in their original loop form without tile partitioning and that each iteration runs sequentially, but allow sub-loops that compute partial sums for 'tmp' and 'y' to have some parallelism with a parallel factor of 4 and 10 respectively.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a dedicated hardware acceleration unit. Within this function, maintain the outer loop 'y[i] = ((double )0)' sequentially without applying pipelining. For the first nested loop setting 'tmp[i] = 0.0', execute it in its entirety without partitioning or parallelism. In the subsequent nested loop that accumulates 'tmp[i]', enable limited parallel execution with a grouping of five iterations. Finally, for the loop that accumulates 'y[j]', enable a more aggressive parallelism with eight simultaneous operations, ensuring reduction to handle data dependencies correctly.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a target for hardware acceleration, ensure that the loop assigning initial values to 'y' is executed sequentially without pipelining. Within the two nested loops that perform the matrix-vector multiplication and accumulation, do not partition the loops 'i' and 'j' into smaller parallel chunks, however, explicitly parallelize the inner loop computations for 'tmp' and 'y' with variable reduction, optimizing for 8 and 10 parallel processing elements respectively.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with specific pragmas, use the following concise instructions:\n\n1. Treat the entire `kernel_atax` function as a hardware acceleration kernel.\n2. Do not automatically pipeline the first loop that initializes the `y` array.\n3. For the second loop that computes the `tmp` array, divide the iteration space of the loop into chunks of a certain factor, but do not specify that factor; allow the HLS tool to decide automatically, but hint that the factor should be more than 1.\n4. Ensure that the iterations of the second loop run sequentially, indicating that they should not be executed in parallel.\n5. For the nested loop inside the second loop that computes the temporary results in `tmp`, enable parallelism with a specific factor for the reduction operation, letting the HLS tool choose the factor automatically, but hint that a factor of 16 might be appropriate.\n6. For the final nested loop that accumulates the results in the `y` array with a reduction operation, also enable parallelism with a specific factor that the HLS tool should determine automatically, suggesting that a factor of 4 could be suitable.\n\nHere's how you can express the pragmas in natural language, mapping them to your instructions:\n\n- `#pragma ACCEL kernel`: Consider the `kernel_atax` function as a designated hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}`: Do not apply any pipeline optimization to the first loop by default.\n- `#pragma ACCEL TILE FACTOR=auto{2}`: Split the computation of the second loop into sub-sections to be determined by HLS, with a hint to avoid a factor of 1.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Execute the iterations of the second loop in sequence, suggesting no actual parallelism but allow HLS to automate the decision.\n- `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}`: Employ parallel processing when computing the `tmp` array with a reduction operation, and let HLS choose an appropriate factor around 16.\n- `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}`: Allow parallelism in the y array accumulation loop with reduction, suggesting a factor of 4 for automatic decision by HLS.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a hardware-accelerated function called 'kernel_atax', treating it as a distinct hardware kernel. Within this function, initialize an output array 'y' and avoid automatically pipelining the first loop where 'y' is set to zeros. Ensure the second loop, responsible for computing the intermediate 'tmp' array, is processed in its serial order without subdividing the iterations or running them in parallel; within this loop, for the nested loop calculating 'tmp[i]', allow a moderate parallelization with a factor of 5, optimizing the reduction operation on 'tmp'. For the final nested loop updating array 'y', expand parallel execution significantly with a factor of 16, again optimizing for reduction on 'y'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function named 'kernel_atax' with arguments as double-precision floating point two dimensional array 'A', and one dimensional arrays 'x', 'y', and 'tmp'. Initialize the 'y' array elements to zero without pipelining. For the first-level loop over 'tmp', prevent loop tiling and the parallel execution of iterations. Within this loop, use a parallel reduction on 'tmp' for the inner loop with a parallel factor of 5. Following that, apply another parallel reduction on 'y' with a parallel factor of 10 for the next inner loop.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_atax` as a hardware acceleration kernel, do not pipeline the first loop to keep its execution sequential, process the second loop in its entirety without tiling, and ensure the second loop's iterations are executed in the original order without parallelization. In the third loop, apply a reduction operation on the `tmp` array with a parallelization factor, and for the fourth loop, apply a reduction operation on the `y` array with a different parallelization factor.\n\nSpecifically, use these annotations:\n- Treat `kernel_atax` function as a hardware acceleration kernel.\n- For the first loop assigning zeros to the array `y`, instruct the compiler not to use automatic pipelining.\n- For the second loop initializing the `tmp` array, indicate no tiling should be performed and that iterations should run sequentially, not in parallel.\n- In the third nested loop that calculates the `tmp` array values, enable parallel loop execution with a high parallelization factor for the reduction operation on `tmp`.\n- In the fourth nested loop that updates the `y` array, apply a moderate parallelization factor for the reduction operation on `y`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration target, enable automatic loop pipelining, let the tool decide the tiling factor for the second loop, and utilize automatic factors for parallel processing in both the nested loops with specified reduction variables for concurrent summation.\n\nSpecifically, the following directives would be applied:\n1. '#pragma ACCEL kernel' tells the compiler to treat 'kernel_atax' as a hardware acceleration kernel.\n2. '#pragma ACCEL PIPELINE auto{}' allows the compiler to automatically determine if and how to pipeline the first loop to enhance throughput.\n3. '#pragma ACCEL TILE FACTOR=auto{2}' instructs the compiler to automatically determine the tiling factor for the second loop iteration space, which can improve data locality and reduce memory bandwidth requirements.\n4. '#pragma ACCEL PARALLEL FACTOR=auto{1}' allows the compiler to automatically identify an appropriate factor to parallelize the second loop while ensuring it maintains serial execution semantics for dependence.\n5. '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}' directs the compiler to automatically parallelize the third loop with a factor related to 16, using 'tmp' as the reduction variable to enable concurrent updates correctly.\n6. '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}' instructs the compiler to automatically parallelize the fourth loop with a factor related to 10, with 'y' as the reduction variable to handle concurrent additions accurately.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{10}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Identify the following function 'kernel_atax' as a hardware acceleration kernel, disable automatic pipelining for a particular loop, set tiling with a doubling factor for another loop, parallelize a loop with a factor of 15, and within that perform parallel reductions on the arrays 'tmp' and 'y' with factors of 16 and 410, respectively.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{410}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the function `kernel_atax` into a hardware kernel for acceleration purposes. Inside this function, for the first loop initializing `y`, disable automatic pipelining. For the outer loop that computes `tmp`, avoid the loop being tiled or parallelized beyond a single iteration at a time. Inside this loop, when calculating the temporary values, allow a parallel factor of 4 with reduction on `tmp`. Finally, employ a parallel factor of 5 with reduction on `y` for the innermost loop distributing the temporary values to `y`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a target for hardware acceleration. Implement the outer loop 'i' that initializes array 'y' without any special parallelization or pipelining transformation. In the first inner loop 'i' for computing 'tmp', enable pipelining with an automatic factor and tiling with an automatic factor for efficient data management, without parallelizing the loop iterations. However, for the second inner loop 'i', apply a fine-grained parallelism with an automatic factor of 16 during the reduction operation on 'tmp' to allow simultaneous updates. Then, in the third inner loop 'j', apply a coarser parallelism with an automatic factor of 4 during the reduction operation on 'y', allowing multiple updates to take place in parallel.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS code to accelerate a matrix-vector multiplication followed by a matrix-transpose-vector multiplication where the second operation reduces results in a vector y. Mark the entire function as a hardware kernel. Ensure the first-level loop iterates sequentially without any pipeline optimization, and operates entirely in its original form, executing iterations one after the other. Within this loop, for the computation of tmp, use parallelism with a factor of 8 in the reduction, and for updating y, perform parallel reduction with a factor of 4.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel, disable automatic pipelining for the first-level 'y' initialization loop, optimize the 'tmp' computation loop without splitting it into smaller parts, enforce sequential execution for the 'tmp' initialization loop, apply a moderate degree of parallelism for the internal 'tmp' accumulation loop, and a lower degree of parallelism for the 'y' accumulation loop. Specifically, set the pipelining of the 'y' initialization loop to 'off', leave the tiling of the 'tmp' computation loop at its full extent by implicitly specifying the maximum loop size, execute the 'tmp' initialization loop sequentially with a parallelization factor of 1, apply a parallelization factor of 16 for the 'tmp' update loop with a reduction clause, and apply a parallelization factor of 4 for the 'y' update loop with a reduction clause.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{390}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration module and automatically manage the loop pipelining, tiling, and parallelization pragmas for optimization, ensuring that data reduction is considered in the nested loops where 'tmp' and 'y' arrays are being computed, and allow the innermost loops to potentially run all iterations simultaneously.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{410}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a function 'kernel_atax' identified for hardware acceleration with arrays 'A', 'x', 'y', and 'tmp' as inputs. Within 'kernel_atax', initialize 'y' array elements to zero without using pipeline optimization. For the first loop iterating over 'tmp', disable tiling and parallel optimization. Process the second nested loop, which computes a dot product, with a parallel reduction on 'tmp' using a factor that allows 8 parallel operations. Finally, process the third nested loop, which updates 'y', with a parallel reduction on 'y' using a factor that allows 16 parallel operations.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a function called 'kernel_atax' that serves as a hardware acceleration kernel and operates on a two-dimensional array 'A', and one-dimensional arrays 'x', 'y', and 'tmp'. Disable automatic pipelining for all loops within the function. For the first outer loop that initializes array 'y', do not tile or parallelize the loop, ensuring it runs sequentially. In the second outer loop that calculates 'tmp', sequentially process the loop iterations, but for the inner loop that performs the reduction on 'tmp', allow a moderate level of parallelism with a factor of 5. Similarly, apply a parallelism factor of 5 to the inner loop of the third outer loop that performs the reduction on 'y', allowing for concurrent execution of loop iterations.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' to be synthesized into a hardware module, and within it, initialize array 'y' without optimizing the containing loop. Then, partition the first main loop to process eight iterations simultaneously, and follow that with a directive to execute sixteen consecutive iterations of the same loop in parallel. Inside this loop, for the inner first nested loop, allow thirty-two iterations to compute partial results of 'tmp' in parallel and enable their accumulation. For the second nested loop, enforce a strict sequential execution order when updating the 'y' array.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel, disable automatic pipelining for the first loop that initializes array y, tile the second main loop with a custom factor but let the tool determine the specific value with a preference for a factor of 2, execute 16 iterations of the second loop in parallel as if on 16 separate hardware execution units, in the nested loop that computes tmp, allow 8 loop iterations to be executed in parallel with intermediate results aggregated, and in the final nested loop that updates y, process the iterations sequentially without parallel execution.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a function `kernel_atax` that computes the ATAX matrix-vector product. Declare the function as a hardware acceleration kernel. Initialize the output vector `y` to zeros outside the main computation loops. Allow the compiler to automatically decide the pipeline settings for the first loop initializing `y`. For the second outer loop iterating over `tmp`, let the compiler choose the tiling factor but ensure it processes one iteration at a time sequentially within the original loop bounds. Inside the nested loops, apply parallelism to the computations of `tmp` and `y` updates, and allow the compiler to automatically select the parallel factor, aiming for 32 instances of parallel execution with reduction to accumulate the values properly.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel that consists of two matrix multiplication sequences. For the first sequence, you're advised to favor automatic decisions for pipelining to potentially enhance loop execution, and to allow the compiler to choose tiling and parallelization strategies for the outer two loops with particular emphasis on aggressive parallelization of the innermost loop. For the second sequence, you should switch off automatic pipelining for the outermost loop while maintaining a flat pipeline structure, keep the loop in full form without tiling, and enforce sequential execution by restricting parallelism in both outer and inner loops, except the middle loop which can have moderate parallel computing enabled. Use the following #pragma instructions respectively:\n\n1. `#pragma ACCEL kernel`: Prepare the function for hardware acceleration.\n2. `#pragma ACCEL PIPELINE auto{}`: Let the compiler automatically decide pipelining.\n3. `#pragma ACCEL TILE FACTOR=auto{1}`: Allow the compiler to decide on loop tiling, maintaining original loop form by setting a factor of 1 if chosen.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{8}`: Auto-tune the parallelization factor with a suggestion of 8 for the outer loop.\n5. `#pragma ACCEL PARALLEL FACTOR=auto{2}`: Suggest a parallel factor of 2 for the second level loop.\n6. `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}`: Aggressively parallelize the innermost loop with a suggestion of 32, applying a reduction operation on the `tmp` array.\n7. `#pragma ACCEL PIPELINE auto{off}`: Turn off automatic pipelining for the outermost loop.\n8. `#pragma ACCEL PIPELINE auto{flatten}`: Advise the compiler to flatten nested loops for pipelining purposes.\n9. `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`: Permit parallel processing with a factor suggestion of 1 while performing a reduction on the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The task is about the encryption of data with AES-256 (Advanced Encryption Standard with a 256-bit key) and its translation to hardware instructions (HLS codes). It consists of several functions, each of which represents a step of the AES-256 encryption process. These functions are in a specific order, forming a flow to accomplish the encryption.\n\n1. `aes256_encrypt_ecb`: This is the main function that accepts an `aes256_context` pointer and the 32-byte key used for encryption. Here, the supplied key is copied into the `enckey` and `deckey` fields of the context. Then, we enter a loop for eight iterations to key expansion on the `deckey`. After this preprocessing, the actual encryption process is started with adding the round key to `buf` which can be pipeline optimized with `off` pragma.\n   For 13 rounds, a sequence of subBytes, shiftRows, and mixColumns is performed. In the middle of these operations (depending on the iteration count), the result is XORed with either a partially expanded key or the original key. This process is tiled and parallelized where possible for efficiency by selecting the auto pragma tiling factor of 2. After all rounds, the final sequence is done with subBytes, shiftRows, AES key expansion, and adding round key addRoundKey.\n\n2. `aes_expandEncKey_1`: This function takes in a key and a `rcon` (round constant), and modifies the key using substitution and XOR operations.\n\n3. `aes_addRoundKey_cpy_1`: This function XORs the key with the data buffer. It also makes a backup of the key into `cpk` array.\n\n4. `aes_subBytes_1`: This function replaces each byte in the data buffer with its corresponding value in the `sbox`.\n\n5. `aes_shiftRows_1`: This function performs a circular shift on the second, third, and fourth row of the state.\n\n6. `rj_xtime_1`: This function multiplies a byte value with `0x02` in the GF(2^8) finite field.\n\n7. `aes_mixColumns_1`: This function transforms each column of the state using a linear mixing function. \n\n8. `aes_addRoundKey_1`: This function performs the step of adding the round key in the AES algorithm.\n\nIn the pragma directives, the kernel pragma is used to indicate that `aes256_encrypt_ecb` function is expected to be accelerated. The PIPELINE pragma is given with a factor `off` showing it's not to be optimized through pipelining. Finally, the TILE pragma is used with a factor `2` to optimize loop iterations to run in parallel if possible.", "output": "const unsigned char sbox[256] = {((unsigned char )0x63), ((unsigned char )0x7c), ((unsigned char )0x77), ((unsigned char )0x7b), ((unsigned char )0xf2), ((unsigned char )0x6b), ((unsigned char )0x6f), ((unsigned char )0xc5), ((unsigned char )0x30), ((unsigned char )0x01), ((unsigned char )0x67), ((unsigned char )0x2b), ((unsigned char )0xfe), ((unsigned char )0xd7), ((unsigned char )0xab), ((unsigned char )0x76), ((unsigned char )0xca), ((unsigned char )0x82), ((unsigned char )0xc9), ((unsigned char )0x7d), ((unsigned char )0xfa), ((unsigned char )0x59), ((unsigned char )0x47), ((unsigned char )0xf0), ((unsigned char )0xad), ((unsigned char )0xd4), ((unsigned char )0xa2), ((unsigned char )0xaf), ((unsigned char )0x9c), ((unsigned char )0xa4), ((unsigned char )0x72), ((unsigned char )0xc0), ((unsigned char )0xb7), ((unsigned char )0xfd), ((unsigned char )0x93), ((unsigned char )0x26), ((unsigned char )0x36), ((unsigned char )0x3f), ((unsigned char )0xf7), ((unsigned char )0xcc), ((unsigned char )0x34), ((unsigned char )0xa5), ((unsigned char )0xe5), ((unsigned char )0xf1), ((unsigned char )0x71), ((unsigned char )0xd8), ((unsigned char )0x31), ((unsigned char )0x15), ((unsigned char )0x04), ((unsigned char )0xc7), ((unsigned char )0x23), ((unsigned char )0xc3), ((unsigned char )0x18), ((unsigned char )0x96), ((unsigned char )0x05), ((unsigned char )0x9a), ((unsigned char )0x07), ((unsigned char )0x12), ((unsigned char )0x80), ((unsigned char )0xe2), ((unsigned char )0xeb), ((unsigned char )0x27), ((unsigned char )0xb2), ((unsigned char )0x75), ((unsigned char )0x09), ((unsigned char )0x83), ((unsigned char )0x2c), ((unsigned char )0x1a), ((unsigned char )0x1b), ((unsigned char )0x6e), ((unsigned char )0x5a), ((unsigned char )0xa0), ((unsigned char )0x52), ((unsigned char )0x3b), ((unsigned char )0xd6), ((unsigned char )0xb3), ((unsigned char )0x29), ((unsigned char )0xe3), ((unsigned char )0x2f), ((unsigned char )0x84), ((unsigned char )0x53), ((unsigned char )0xd1), ((unsigned char )0x00), ((unsigned char )0xed), ((unsigned char )0x20), ((unsigned char )0xfc), ((unsigned char )0xb1), ((unsigned char )0x5b), ((unsigned char )0x6a), ((unsigned char )0xcb), ((unsigned char )0xbe), ((unsigned char )0x39), ((unsigned char )0x4a), ((unsigned char )0x4c), ((unsigned char )0x58), ((unsigned char )0xcf), ((unsigned char )0xd0), ((unsigned char )0xef), ((unsigned char )0xaa), ((unsigned char )0xfb), ((unsigned char )0x43), ((unsigned char )0x4d), ((unsigned char )0x33), ((unsigned char )0x85), ((unsigned char )0x45), ((unsigned char )0xf9), ((unsigned char )0x02), ((unsigned char )0x7f), ((unsigned char )0x50), ((unsigned char )0x3c), ((unsigned char )0x9f), ((unsigned char )0xa8), ((unsigned char )0x51), ((unsigned char )0xa3), ((unsigned char )0x40), ((unsigned char )0x8f), ((unsigned char )0x92), ((unsigned char )0x9d), ((unsigned char )0x38), ((unsigned char )0xf5), ((unsigned char )0xbc), ((unsigned char )0xb6), ((unsigned char )0xda), ((unsigned char )0x21), ((unsigned char )0x10), ((unsigned char )0xff), ((unsigned char )0xf3), ((unsigned char )0xd2), ((unsigned char )0xcd), ((unsigned char )0x0c), ((unsigned char )0x13), ((unsigned char )0xec), ((unsigned char )0x5f), ((unsigned char )0x97), ((unsigned char )0x44), ((unsigned char )0x17), ((unsigned char )0xc4), ((unsigned char )0xa7), ((unsigned char )0x7e), ((unsigned char )0x3d), ((unsigned char )0x64), ((unsigned char )0x5d), ((unsigned char )0x19), ((unsigned char )0x73), ((unsigned char )0x60), ((unsigned char )0x81), ((unsigned char )0x4f), ((unsigned char )0xdc), ((unsigned char )0x22), ((unsigned char )0x2a), ((unsigned char )0x90), ((unsigned char )0x88), ((unsigned char )0x46), ((unsigned char )0xee), ((unsigned char )0xb8), ((unsigned char )0x14), ((unsigned char )0xde), ((unsigned char )0x5e), ((unsigned char )0x0b), ((unsigned char )0xdb), ((unsigned char )0xe0), ((unsigned char )0x32), ((unsigned char )0x3a), ((unsigned char )0x0a), ((unsigned char )0x49), ((unsigned char )0x06), ((unsigned char )0x24), ((unsigned char )0x5c), ((unsigned char )0xc2), ((unsigned char )0xd3), ((unsigned char )0xac), ((unsigned char )0x62), ((unsigned char )0x91), ((unsigned char )0x95), ((unsigned char )0xe4), ((unsigned char )0x79), ((unsigned char )0xe7), ((unsigned char )0xc8), ((unsigned char )0x37), ((unsigned char )0x6d), ((unsigned char )0x8d), ((unsigned char )0xd5), ((unsigned char )0x4e), ((unsigned char )0xa9), ((unsigned char )0x6c), ((unsigned char )0x56), ((unsigned char )0xf4), ((unsigned char )0xea), ((unsigned char )0x65), ((unsigned char )0x7a), ((unsigned char )0xae), ((unsigned char )0x08), ((unsigned char )0xba), ((unsigned char )0x78), ((unsigned char )0x25), ((unsigned char )0x2e), ((unsigned char )0x1c), ((unsigned char )0xa6), ((unsigned char )0xb4), ((unsigned char )0xc6), ((unsigned char )0xe8), ((unsigned char )0xdd), ((unsigned char )0x74), ((unsigned char )0x1f), ((unsigned char )0x4b), ((unsigned char )0xbd), ((unsigned char )0x8b), ((unsigned char )0x8a), ((unsigned char )0x70), ((unsigned char )0x3e), ((unsigned char )0xb5), ((unsigned char )0x66), ((unsigned char )0x48), ((unsigned char )0x03), ((unsigned char )0xf6), ((unsigned char )0x0e), ((unsigned char )0x61), ((unsigned char )0x35), ((unsigned char )0x57), ((unsigned char )0xb9), ((unsigned char )0x86), ((unsigned char )0xc1), ((unsigned char )0x1d), ((unsigned char )0x9e), ((unsigned char )0xe1), ((unsigned char )0xf8), ((unsigned char )0x98), ((unsigned char )0x11), ((unsigned char )0x69), ((unsigned char )0xd9), ((unsigned char )0x8e), ((unsigned char )0x94), ((unsigned char )0x9b), ((unsigned char )0x1e), ((unsigned char )0x87), ((unsigned char )0xe9), ((unsigned char )0xce), ((unsigned char )0x55), ((unsigned char )0x28), ((unsigned char )0xdf), ((unsigned char )0x8c), ((unsigned char )0xa1), ((unsigned char )0x89), ((unsigned char )0x0d), ((unsigned char )0xbf), ((unsigned char )0xe6), ((unsigned char )0x42), ((unsigned char )0x68), ((unsigned char )0x41), ((unsigned char )0x99), ((unsigned char )0x2d), ((unsigned char )0x0f), ((unsigned char )0xb0), ((unsigned char )0x54), ((unsigned char )0xbb), ((unsigned char )0x16)};\ntypedef struct {unsigned char key[32];unsigned char enckey[32];unsigned char deckey[32];}aes256_context;\n\nstatic void aes_expandEncKey_1(unsigned char *k,unsigned char *rc)\n{\n  register unsigned char i;\n  k[0] ^= ((int )sbox[k[29]]) ^ ((int )( *rc));\n  k[1] ^= ((int )sbox[k[30]]);\n  k[2] ^= ((int )sbox[k[31]]);\n  k[3] ^= ((int )sbox[k[28]]);\n   *rc = ((unsigned char )(((int )( *rc)) << 1 ^ (((int )( *rc)) >> 7 & 1) * 0x1b));\n  exp1:\n  for (i = ((unsigned char )4); ((int )i) < 16; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n  k[16] ^= ((int )sbox[k[12]]);\n  k[17] ^= ((int )sbox[k[13]]);\n  k[18] ^= ((int )sbox[k[14]]);\n  k[19] ^= ((int )sbox[k[15]]);\n  exp2:\n  for (i = ((unsigned char )20); ((int )i) < 32; i += 4) \n    (((k[i] ^= ((int )k[((int )i) - 4]) , k[((int )i) + 1] ^= ((int )k[((int )i) - 3])) , k[((int )i) + 2] ^= ((int )k[((int )i) - 2])) , k[((int )i) + 3] ^= ((int )k[((int )i) - 1]));\n/* aes_expandEncKey */\n}\n\nstatic void aes_addRoundKey_cpy_1(unsigned char *buf,unsigned char *key,unsigned char *cpk)\n{\n  register unsigned char i = (unsigned char )16;\n  cpkey:\n  while(i--)\n    (buf[i] ^= ((int )(cpk[i] = key[i])) , cpk[16 + ((int )i)] = key[16 + ((int )i)]);\n/* aes_addRoundKey_cpy */\n}\n\nstatic void aes_subBytes_1(unsigned char *buf)\n{\n  register unsigned char i = (unsigned char )16;\n  sub:\n  while(i--)\n    buf[i] = sbox[buf[i]];\n/* aes_subBytes */\n}\n\nstatic void aes_shiftRows_1(unsigned char *buf)\n{\n/* to make it potentially parallelable :) */\n  register unsigned char i;\n  register unsigned char j;\n  i = buf[1];\n  buf[1] = buf[5];\n  buf[5] = buf[9];\n  buf[9] = buf[13];\n  buf[13] = i;\n  i = buf[10];\n  buf[10] = buf[2];\n  buf[2] = i;\n  j = buf[3];\n  buf[3] = buf[15];\n  buf[15] = buf[11];\n  buf[11] = buf[7];\n  buf[7] = j;\n  j = buf[14];\n  buf[14] = buf[6];\n  buf[6] = j;\n/* aes_shiftRows */\n}\n\nstatic unsigned char rj_xtime_1(unsigned char x)\n{\n  return (unsigned char )(((int )x) & 0x80?((int )x) << 1 ^ 0x1b : ((int )x) << 1);\n/* rj_xtime */\n}\n\nstatic void aes_mixColumns_1(unsigned char *buf)\n{\n  register unsigned char i;\n  register unsigned char a;\n  register unsigned char b;\n  register unsigned char c;\n  register unsigned char d;\n  register unsigned char e;\n  int _s_i;\n  mix:\n/* Canonicalized from: for(_s_i =((unsigned char )0);((int )_s_i) < 16;_s_i += 4) {...} */\n/* Standardize from: for(_s_i =((unsigned char )0);_s_i <= 15;_s_i += 4) {...} */\n  for (_s_i = 0; _s_i <= 3; _s_i++) {\n    int _in_s_i = 0 + 4L * _s_i;\n    a = buf[_in_s_i];\n    b = buf[((int )_in_s_i) + 1];\n    c = buf[((int )_in_s_i) + 2];\n    d = buf[((int )_in_s_i) + 3];\n    e = ((unsigned char )(((int )a) ^ ((int )b) ^ ((int )c) ^ ((int )d)));\n    buf[_in_s_i] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )a) ^ ((int )b)))));\n    buf[((int )_in_s_i) + 1] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )b) ^ ((int )c)))));\n    buf[((int )_in_s_i) + 2] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )c) ^ ((int )d)))));\n    buf[((int )_in_s_i) + 3] ^= ((int )e) ^ ((int )(rj_xtime_1((unsigned char )(((int )d) ^ ((int )a)))));\n  }\n  _s_i = 12 + 4L;\n  i = _s_i;\n/* aes_mixColumns */\n}\n\nstatic void aes_addRoundKey_1(unsigned char *buf,unsigned char *key)\n{\n  register unsigned char i = (unsigned char )16;\n  addkey:\n  while(i--)\n    buf[i] ^= ((int )key[i]);\n/* aes_addRoundKey */\n}\n#pragma ACCEL kernel\n\nvoid aes256_encrypt_ecb(aes256_context *ctx,unsigned char k[32],unsigned char buf[16])\n{\n//INIT\n  unsigned char rcon = (unsigned char )1;\n  unsigned char i;\n  int _s_i_0;\n  ecb1:\n/* Canonicalized from: for(_s_i_0 =((unsigned char )0);((unsigned long )_s_i_0) <(1 * 32L);_s_i_0++) {...} */\n  for (_s_i_0 = ((unsigned char )0); _s_i_0 <= 31; ++_s_i_0) {\n    ctx -> enckey[_s_i_0] = ctx -> deckey[_s_i_0] = k[_s_i_0];\n  }\n  i = _s_i_0;\n  \n#pragma ACCEL PIPELINE auto{off}\n  ecb2:\n  for (i = ((unsigned char )8); --i; ) {\n    aes_expandEncKey_1(ctx -> deckey,&rcon);\n  }\n//DEC\n  aes_addRoundKey_cpy_1(buf,ctx -> enckey,ctx -> key);\n  int _s_i;\n  rcon = ((unsigned char )1);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  ecb3:\n/* Canonicalized from: for((_s_i =((unsigned char )1) , rcon =((unsigned char )1));((int )_s_i) < 14;++_s_i) {...} */\n  for (_s_i = ((unsigned char )1); _s_i <= 13; ++_s_i) {\n    aes_subBytes_1(buf);\n    aes_shiftRows_1(buf);\n    aes_mixColumns_1(buf);\n    if (((int )_s_i) & 1) {\n      aes_addRoundKey_1(buf,&ctx -> key[16]);\n    }\n     else {\n      (aes_expandEncKey_1(ctx -> key,&rcon) , aes_addRoundKey_1(buf,ctx -> key));\n    }\n  }\n  i = _s_i;\n  aes_subBytes_1(buf);\n  aes_shiftRows_1(buf);\n  aes_expandEncKey_1(ctx -> key,&rcon);\n  aes_addRoundKey_1(buf,ctx -> key);\n/* aes256_encrypt */\n}"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Compile the given function as a hardware kernel without any automatic optimization passes for pipelining, tiling, or parallel processing. Specifically, mandate all loops to process serially and disable pipelining optimization entirely. Maintain the native loop execution order without splitting. However, for one of the nested loops within the row sweep section, allow eight parallel operations, and for its subsequent loop, permit up to thirty-two parallel operations. Apply these settings using the appropriate #pragma directives.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a function designed to perform matrix-vector multiplication and subsequent vector update with the following characteristics: Treat the entire function as a dedicated hardware computation unit. Prevent automatic pipelining optimization for the main loop iterations. Ensure the main loop runs without being divided into smaller segments and without any parallel execution of its iterations. Within the nested loops, enable parallel processing with a high degree of parallelism for the first nested loop and without parallelism in the second nested loop, focusing on utilizing the available hardware to perform accumulation efficiently. Use the following pragma directives respectively: '#pragma ACCEL kernel', '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{1}', '#pragma ACCEL PARALLEL FACTOR=auto{1}', '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}', and '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the function 'kernel_2mm' as a dedicated hardware kernel with alpha, beta, tmp, A, B, C, and D matrices as inputs and outputs. Within the function, disable loop pipelining for the two main loops; these loops should not be split into parallel chunks and should run iteratively. Within the nested loops, enable automatic pipelining but prevent the loop from being tiled or parallelized beyond the sequential execution. For the innermost loops performing matrix operations, enable parallel execution with a reduction on the 'tmp' and 'D' matrices. Specifically, for the loop updating the 'tmp' matrix, allow a parallelism factor of 5, and for the loop updating the 'D' matrix, use a parallelism factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel, turn off automatic pipelining for the first loop, ensure the second loop runs without tiling and with its iterations in a strict sequence, perform the third loop with parallel computation using 8 compute units and enable a reduction operation on 'tmp', and finally, run the last loop with a single compute unit in parallel and enable a reduction operation on 'y'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the specified function as a hardware acceleration kernel that computes the result of `D := alpha*A*B*C + beta*D` for given matrix dimensions and coefficients. Disable automatic pipelining for the first outer loop and don't tile or parallelize it. In the nested loop inside, enable default pipelining, do not tile, but allow parallel execution with a factor of 2. For the reduction loop inside that, parallelize with a factor of 7 allowing reduction on the 'tmp' array. In the second set of nested loops for the 'D' matrix update, enable pipelining with flattening, do not tile, and set parallel execution with a factor of 8 for the outer loop while the reduction on 'D' inside that should not be parallelized.\n\nSpecific #pragma instructions:\n\n- For the function, use `#pragma ACCEL kernel` to indicate hardware kernel acceleration.\n- For the first outer loop, use `#pragma ACCEL PIPELINE auto{off}` to turn off automatic pipelining.\n- Also for the first outer loop, set `#pragma ACCEL TILE FACTOR=auto{1}` to disable tiling and `#pragma ACCEL PARALLEL FACTOR=auto{1}` to avoid parallelization.\n- In the nested loop for computing 'tmp', specify `#pragma ACCEL PIPELINE auto{}` to enable pipelining, `#pragma ACCEL TILE FACTOR=auto{1}` to avoid tiling, and `#pragma ACCEL PARALLEL FACTOR=auto{2}` for limited parallel execution.\n- For the reduction of 'tmp', use `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}` for parallel reduction with a specific factor.\n- For the second set of loops for updating 'D', write `#pragma ACCEL PIPELINE auto{}` to enable pipelining, `#pragma ACCEL TILE FACTOR=auto{1}` for no tiling, `#pragma ACCEL PARALLEL FACTOR=auto{1}` to disable parallelism in the outer loop.\n- Use `#pragma ACCEL PIPELINE auto{flatten}` in the nested loop to enable flattened pipelining,\n- Set `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{8}` in the nested loop for limited parallel execution.\n- Finally, for the reduction on 'D', use `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` to ensure no parallelism on reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{7}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code by treating a function as a dedicated hardware kernel, disabling automatic loop pipelining, processing loops without tiling them into smaller chunks, running loop iterations sequentially without parallelizing, and specifically parallelizing certain loops while setting custom factors for reduction operations on arrays:\n\n1. Mark the entire function as a hardware kernel for acceleration.\n2. For the first loop, disable automatic pipelining.\n3. For the second loop, ensure it is processed in its entirety without being tiled into smaller chunks and execute its iterations sequentially, not in parallel.\n4. In the nested loop inside the second loop, enable parallel processing with a reduction on the 'tmp' array and use a factor of 16 for the parallelization.\n5. In the subsequent loop, enable parallel processing with a reduction on the 'y' array while applying a factor of 2 for the parallelization.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` to be synthesized for hardware acceleration. When processing both the matrix multiplication `tmp = alpha * A * B` and matrix addition `D = beta * D + tmp * C`, ensure the outer loop iterations do not overlap, execute as they do in normal software, and without introducing extra levels of pipelining. Specifically, during the multiplication phase, accumulate into `tmp` using the maximum available parallelization across the reduction dimension without dividing into smaller parallel segments. In the addition phase, flatten the pipeline within the first outer loop and allow a moderate degree of parallel execution in the nested loop with a factor that divides the loop into optimized segments, while keeping other loops fully unparallelized and unoptimized for tiling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated version of the 2mm algorithm without automatic pipelining, without tiling, and running the outer two loops sequentially but parallelizing the inner multiplications with a full factor on the first multiplication and a factor of eight on the second, accounting for potential summation conflicts using reduction pragmas.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following HLS C code pragmas into natural language instructions:\n\n1. Treat the `kernel_atax` function as a hardware kernel suitable for acceleration.\n2. Automatically determine the best way to pipeline the outermost loop.\n3. Automatically select the most appropriate tile factor for the first for-loop inside the `kernel_atax` function.\n4. Parallelize the second for-loop inside the `kernel_atax` function using automatic factor selection, up to 30 iterations in parallel.\n5. Automatically choose the best factor to parallelize the reduction operation on `tmp` in the third for-loop within `kernel_atax`.\n6. Fully unroll the last for-loop with an automatic factor, enabling parallel reduction on `y` with all 410 iterations handled simultaneously if possible.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{410}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the `kernel_2mm` function as a hardware accelerator kernel. Within the first top-level loop, avoid automatic pipelining, prevent the breaking down of loops into smaller sections and execute iterations sequentially. Inside the nested loop, automatically optimize pipelining, but keep the loop unsegmented and still run in sequence. Specifically apply a parallel reduction to the 'tmp' array with as many compute resources as there are iterations in the innermost loop. In the second top-level loop, prevent the use of automatic pipelining, while flattening the next level of nested loops to improve performance, but maintain the original loop structure and schedules every eighth iteration of the innermost loop to be processed in parallel, with sequential execution when updating the 'D' array without parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a dedicated hardware function for acceleration, but do not automatically pipeline the first loop, keep it in its complete form without tiling it into pieces, and execute its iterations sequentially. Within the second loop, enable parallel processing with the ability to perform up to 8 operations simultaneously while accumulating values into a temporary array. Apply the same parallel processing strategy to the third loop to accumulate values into a different array, allowing for up to 8 simultaneous operations.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Accelerate the following function as a hardware kernel, but prevent pipelining, tiling, or parallelizing the outer loop; in the first inner loop, allow limited parallelism without automatic adjustment, and in the second inner loop, enforce sequential execution while performing reduction on the output array.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_2mm' function as a dedicated hardware kernel for FPGA acceleration. Within the function, disable automatic pipelining for the first set of nested loops, and ensure that the loops run sequentially without parallelization or tiling optimization. However, for the innermost loop, permit the loop's iterations to contribute to the accumulation of 'tmp' simultaneously but with no concurrency. For the second set of nested loops, turn on automatic pipelining, set the parallel factor of the outermost loop to the maximum (80), and allow simultaneous accumulation on 'D' within the innermost loop, but once again, with iterations running sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' to be synthesized as a hardware kernel, initialize array 'y' with zeros without automatic pipelining, fully process the first outer loop and its iterations sequentially without parallelization, compute 'tmp' within the inner loop with a moderate degree of parallelism, and compute 'y' in the second inner loop with a high degree of parallelism. Use the defaults for pipelining the first outer loop, set the tiling factor to 1 for no loop tiling, set the parallel factor to 1 for no loop parallelism in the first outer loop, set the parallel reduction for 'tmp' with a parallelism factor of 5, and set the parallel reduction for 'y' with a parallelism factor of 32.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Construct an accelerator kernel function named 'kernel_atax' that performs matrix-vector multiplication and addition, with input matrices 'A' and vector 'x', and output vectors 'y' and 'tmp'. For the first vector 'y', initialize each element to zero without pipelining. In the first loop, calculate the vector-matrix product without tiling or parallelizing. Within this loop, calculate the temporary vector 'tmp' and allow its inner loop to be parallelized with a factor of 4 with reduction on the 'tmp' vector. Finally, update the 'y' vector with the results, allowing its inner loop to be parallelized with a factor of 8 with reduction on the 'y' vector. Do not automatically pipeline the main loops, do not tile the first main loop, and run the iterations of both main loops sequentially.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code that defines a hardware kernel function named `kernel_atax` which takes a 2D array `A`, and two 1D arrays `x`, `y`, and `tmp` as inputs, and performs a matrix-vector multiplication followed by an accumulator operation. For the outer loop that initializes `y`, pipeline optimization should be disabled. The main computation loops for calculating `tmp` and updating `y` should not be tiled but processed sequentially. The loop calculating `tmp` shall have its iterations partially unrolled or executed in parallel with a factor of 16 and include a `reduction` clause for the `tmp` array, while the one updating `y` should be vectorized or executed in parallel with a factor of 5, also including a `reduction` clause for `y`. Here are the specific pragma directives to be used for each optimization constraint:\n\n1. Treat the `kernel_atax` function as hardware acceleration kernel.\n2. Suppress automatic loop pipelining for the initial loop that zero-initializes array `y`.\n3. Process the loop for zero-initializing array `tmp` in its original form without tiling.\n4. Execute the iterations of that same loop one after another sequentially, without parallelism.\n5. For the loop that computes `tmp[i]`, enable parallel processing with a factor of 16 and specify that the loop performs a reduction on `tmp`.\n6. For the final loop that updates `y[j]`, enable parallel processing with a factor of 5 and indicate that the loop performs a reduction on `y`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_adi` as a hardware acceleration kernel, and for the loops within this function, do not apply automatic pipelining strategies, maintain the original loop size without tiling, and execute loop iterations sequentially without parallel execution. Additionally, apply a parallelism factor of 2 to the innermost loops within the column and row sweeps, where appropriate.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C function called 'kernel_atax' that will be treated as a hardware kernel. Inside the function: \n\n1. Initialize an output array 'y' to zeroes.\n2. Enable automatic pipelining for a loop running 390 iterations that computes temporary values 'tmp' for each iteration using a matrix 'A' and a vector 'x'.\n3. Adjust the loop's tiling automatically for the loop iterating over 390 elements.\n4. Ensure that the computations run serially within the loop iterating over 'tmp' with 390 iterations.\n5. Apply parallelism with automatic factor and reduction on 'tmp' to a nested loop with 410 iterations that accumulates into 'tmp'.\n6. Apply parallelism with automatic factor and reduction on 'y' to another nested loop with 410 iterations that updates the 'y' array using the 'tmp' values and matrix 'A'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{390}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a dedicated hardware block for acceleration. Within this function, initialize each element of array 'y' sequentially. In the first loop over 'i', set each element of the 'tmp' array to zero without any optimizations to alter its iteration order, size, or execution style. When calculating 'tmp[i]', allow eight iterations of the inner loop to process concurrently, enabling parallel reduction on 'tmp'. For updating 'y[j]', permit up to thirty-two iterations of the inner loop to execute in parallel, also using parallel reduction on 'y'. Make sure neither of these loops is automatically pipelined.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To create the given HLS codes written in C, use the following comprehensive but concise natural language instruction:\n\n\"Designate a function as a hardware acceleration kernel, and within that function define two levels of nested loops. For the outer loops, disable automatic pipelining, specify no tiling, and run iterations sequentially without parallelism. In the inner loops, enable parallel reduction with a factor of 8 for the first loop and a factor of 5 for the second loop to optimize performance.\" \n\nCorresponding #pragma directives in natural language would be:\n\n1. \"#pragma ACCEL kernel\" -> Mark the function as an acceleration kernel.\n2. \"#pragma ACCEL PIPELINE auto{off}\" -> Prevent the first outer loop from being automatically pipelined by the compiler.\n3. \"#pragma ACCEL TILE FACTOR=auto{1}\" -> Indicate that the first outer loop should not be tiled.\n4. \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" -> Run the iterations of the first outer loop sequentially, without parallel processing.\n5. \"#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\" -> For the inner loop that updates 'tmp', enable parallel computation with eight parallel elements to accelerate the reduction process.\n6. \"#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\" -> For the inner loop that updates 'y', use parallel computation with a factor of five for the reduction operation to improve performance.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware kernel for acceleration, disable automatic pipelining, retain the original loop structure without tiling, and execute every iteration of the loops sequentially. Specifically, apply a parallel factor of 70 to the reduction loop on 'tmp' and a parallel factor of 50 to the reduction loop on 'D', while setting the parallel factor for the outer loops iterating over 'i' and 'j' to 8 when updating 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_adi` for hardware acceleration and ensure that all loops within the function run sequentially without pipelining, tiling, or parallelism optimization, except for the innermost `j` loop in the first column sweep where parallelize with a factor of 16. For all other instances, ensure that the loops run with a parallel factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_atax' as a hardware acceleration kernel, disable automatic pipelining for the first level loop, prevent the second level loop from being split into smaller chunks or run in parallel, and instruct the compiler to parallelize the two innermost loops with a specific degree of parallelism, while performing reductions on their respective accumulation arrays.\n\nTo generate the specified HLS C code, you should use the following natural language instruction:\n\n\"Create a hardware acceleration kernel named 'kernel_atax' that takes a two-dimensional array 'A', and one-dimensional arrays 'x', 'y', and 'tmp' as inputs. Disable automatic loop pipelining for loop iterating over 'y'. Ensure the loop that initializes 'tmp' runs in its entirety without any tiling or parallel execution optimizations. For the nested loop updating 'tmp', enable parallel execution with a degree of parallelism of 32 and specify that the temporary accumulation variable 'tmp' is targeted for reduction. Similarly, for the nested loop updating 'y', enable parallel execution with a degree of parallelism of 16 and target 'y' for reduction.\"\n\nCorresponding #pragma values for this instruction would be:\n\n- Treat the following function as a hardware acceleration kernel: `#pragma ACCEL kernel`\n- Disable automatic pipelining for the first level loop: `#pragma ACCEL PIPELINE auto{off}`\n- Prevent the second level loop from being split into smaller chunks or run in parallel: `#pragma ACCEL TILE FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Enable parallel execution with a degree of parallelism of 32 for the first innermost loop and target 'tmp' for reduction: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}`\n- Enable parallel execution with a degree of parallelism of 16 for the second innermost loop and target 'y' for reduction: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function named `kernel_atax` designed to perform the ATAX matrix-vector multiplication. Define the kernel function to process the 2D array `A` of size 390x410, and 1D arrays `x`, `y`, and `tmp` each of size 410 or 390 as applicable. Initialize the `y` array to zeroes outside of any acceleration pragmas.\n\nFor the outer loop iterating over the `y` array, do not apply any pipeline optimization. \n\nFor the first inner loop iterating over the `tmp` array, automatically select the optimal tiling factor, but do not break the loop into chunks smaller than 4 iterations. Use automatic parallelization with a constraint that iterations must not be parallelized in chunks smaller than 1.\n\nInside the loop that calculates the `tmp` values, parallelize the reduction operation on the `tmp` array with a hint to consider at least 32 operations to be done in parallel.\n\nFor the second inner loop that updates the `y` array based on the computed `tmp` values, enable a parallel reduction operation on the `y` array with a hint to consider at least 16 operations to be done in parallel.\n\nHere are the specific #pragma instructions in natural language, corresponding to their uses in the code:\n\n1. Treat the `kernel_atax` function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n\n2. Do not automatically pipeline the loop that initializes `y`: `#pragma ACCEL PIPELINE auto{}` should be placed somewhere it won't impact the designated loop. \n\n3. Allow the compiler to automatically select the tile factor for the first inner loop, ensuring at least 4 iterations are processed as a chunk: `#pragma ACCEL TILE FACTOR=auto{4}`\n\n4. Run iterations of the first inner loop strictly in sequence (not in parallel): `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n5. For the first inner, nested loop, automatically manage parallelism specifically for the reduction on `tmp` with at least 32 operations parallelized: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}`\n\n6. For the second inner, nested loop, automatically manage parallelism specifically for the reduction on `y` with at least 16 operations parallelized: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}`\n\nThese instructions are structured to optimize the matrix-vector multiplication in kernel_atax function for acceleration as a hardware kernel using various data tiling, pipelining, and parallelization strategies provided by the HLS compiler pragmas.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{16}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the given C code implementing a 2mm kernel using HLS (High-Level Synthesis) pragmas to guide synthesis optimization, you can provide the following condensed natural language instructions:\n\n1. Treat the entire `kernel_2mm` function as a hardware acceleration kernel.\n2. Prevent automatic pipelining, tiling, and parallelization optimizations for the outermost loops iterating over `i` and `j` by enforcing the original statement sequence and single-instance computation, but allow the tool to decide pipelining within these loops.\n3. For the innermost loop over `k` that calculates `tmp` based on matrices `A` and `B`, enable parallel computation for the summation of products with a reduction on the `tmp` variable, and let the tool choose the optimal factor.\n4. Enable automatic pipelining for the loop calculating matrix `D` from `tmp` and `C`, flatten the nested loops for efficient memory access but limit parallelism on the `j` loop to a factor of `8` while allowing the tool choice in pipelining.\n5. Explicitly disable the use of parallel computation for the innermost loop over `k` when updating matrix `D`, ensuring a single iteration at a time with a focus on reduction operations involving `D`.\n\nPlease note that in this context, `auto{}` implies letting the synthesis tool choose the appropriate strategy or factor, whereas `auto{off}` or `auto{1}` implies turning the optimization off or maintaining the original loop format respectively for the optimization directive preceded by `#pragma ACCEL`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_adi' function as a candidate for hardware acceleration, ensure that all loops within this function execute without any compiler optimizations such as pipelining, tiling, or parallel processing, except for the innermost j-loop in the column sweep where parallel factor is set to 2, and the innermost j-loop in the row sweep where parallel factor is set to 8.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named 'kernel_atax' designed for hardware acceleration, which operates on a two-dimensional array 'A', one-dimensional arrays 'x', 'y', and 'tmp'. In this function:\n\n- Do not apply automated pipelining to the outer loop iterating over 'y'.\n- Split the second outer loop iterating over 'tmp' into chunks, but let the compiler decide the chunk size.\n- Parallelize the second outer loop with multiple iterations executed simultaneously and let the compiler decide the number of parallel instances, with a suggestive hint of around 26.\n- Inside the loop calculating 'tmp', use parallel reduction with the parallel factor left for the compiler to determine, but suggest around 32 instances should be considered.\n- In the update loop for 'y', use parallel reduction where each iteration must be run serially, without parallelism.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{26}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware kernel function named `kernel_atax` that performs matrix transpose and vector multiplication. Disable automatic pipelining for a loop, keep the original loop structure for one loop, ensure another loop's iterations are executed sequentially, and for the inner loops, enable parallel reductions with specific factors for optimized performance. Use the following pragmas with their respective values:\n\n- Treat `kernel_atax` function as a hardware acceleration kernel.\n- Prevent the first loop that initializes array `y` from being automatically pipelined.\n- Execute the outer loop, responsible for updating `tmp`, with its original form without tiling.\n- Run the outer loop's iterations sequentially, without any parallelism.\n- In the first nested loop, perform a parallel reduction on `tmp` with a factor that allows 8 operations to be performed in parallel.\n- In the second nested loop, enable a parallel reduction on `y` with a factor that permits executing 2 operations concurrently.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{8}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a high-level synthesis (HLS) code for an accelerated hardware kernel function called 'kernel_atax' that operates on a matrix 'A' and vectors 'x', 'y', and 'tmp'. Declare 'y' as a zero-initialized vector outside the first loop. Disable automatic pipelining of the main iteration loop. Break the first inner loop into chunks of 8 but leave the number selection to the HLS tool, using a sensible default. Execute 15 iterations of the first inner loop in parallel, letting the HLS tool choose the number. In the second inner loop, perform a reduction on the 'tmp' array by running 32 iterations in parallel, figure chosen by the HLS tool. For the final inner loop, enforce the execution of iterations sequentially, with no parallelization, for a reduction on 'y'. \n\nThe HLS code should contain the following pragmas:\n- To designate the function as a hardware kernel: `#pragma ACCEL kernel`\n- To prevent automatic pipelining of the main loop: `#pragma ACCEL PIPELINE auto{off}`\n- To tile the first inner loop with a factor of 8: `#pragma ACCEL TILE FACTOR=auto{8}`\n- To run 15 iterations of the first inner loop in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{15}`\n- To run 32 iterations of the second inner loop in parallel for a reduction on 'tmp': `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}`\n- To enforce a sequential execution in the final inner loop for the 'y' reduction: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{8}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{15}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a specialized hardware kernel, instruct the compiler to not automatically pipeline any loops, to process all loops in their entirety without division, and to execute each loop iteratively without concurrent iteration execution. Additionally, specify that for the innermost parallel loop in the column sweep section, the iterations should be processed two at a time in parallel, and for the innermost parallel loops in the row sweep section, sixteen iterations should run concurrently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, avoid pipelining any loops within it, prevent loop tiling, and enforce sequential loop execution, but allow parallel processing with a factor of 8 for specific internal loops. Use '#pragma ACCEL kernel' to mark the function for acceleration, '#pragma ACCEL PIPELINE auto{off}' to disable automatic pipelining, '#pragma ACCEL TILE FACTOR=auto{1}' to avoid loop tiling, '#pragma ACCEL PARALLEL FACTOR=auto{1}' for sequential loop processing, and '#pragma ACCEL PARALLEL FACTOR=auto{8}' to enable eight-way parallelism within designated loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the `kernel_2mm` function as a hardware acceleration kernel. Within this function:\n1. Prevent the automatic pipelining of the outer loops iterating over the `i` index.\n2. Avoid tiling and parallelizing the outer loops beyond a single iteration.\n3. Inside the first set of nested loops where `tmp` is computed, disable automatic pipelining.\n4. In the loop calculating `tmp[i][j]`, perform parallel reduction on `tmp` with a factor that allows for 14 concurrent operations.\n5. In the second set of nested loops updating `D`, prevent pipelining, tile, and parallelize with a factor of 1 for the outer loop, but fully parallelize the innermost loop with a maximum factor of 80.\n6. Within the innermost loop updating `D[i][j]`, apply parallel reduction on `D` with a factor that enables 16 concurrent operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named 'kernel_adi' for accelerated hardware processing with arguments 'tsteps', 'n', and four two-dimensional arrays 'u', 'v', 'p', 'q'. Ensure each loop inside the function is executed sequentially with no automatic pipelining, without tiling, and with only a single iteration running at a time, except for the innermost 'j' loop in the first 'Column Sweep' section, which should handle two iterations in parallel. Additionally, in the last 'j' loop of the 'Row Sweep' section, use a parallel factor to attempt to unroll the loop with 29 iterations in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware acceleration kernel using the HLS C pragma. Within it, include pragmas that preserve the original sequencing of loop iterations without automatically optimizing pipelines, tiling, or parallel execution. Explicitly set the parallelization factor to the minimum for the first set of nested loops, and use the default minimum parallelization but with a specific factor of four for the backward iteration over 'j' in the column sweep section. Then, in the row sweep section, include a pragma that flattens all nested loops to a single level of hierarchy, and again ensure minimal parallelization and no loop restructuring by specifying respective pragmas accordingly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated kernel to perform a 2mm operation on matrices `A`, `B`, `C`, and `D`, scaling matrix `D` with `beta` and adding the product of matrices `A`, `B`, and `C` scaled by `alpha`. Do not apply automatic pipelining or tiling optimisations to the top-level loops. Keep the innermost loops' iterations sequential, but flatten the pipeline for the second level loops. Parallelise the reduction operations inside the inner loops with no parallel factor optimization, except for the last `C` matrix multiplication, which must be run with a default parallel factor. Use a single-dimensional array to store intermediate results of the operation on matrices `A` and `B`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following looped computations within a two-dimensional space into a hardware-accelerated kernel while maintaining the original loop structure without tiling or internal parallelization for all but one inner loop, where a moderate parallel factor is applied.\n\nFor the overall function and each loop layer except for the innermost computation of the 'Column Sweep' sequence:\n\n- Designate the function as a candidate for hardware acceleration.\n- Ensure loops are not automatically pipelined.\n- Disable loop tiling so that the loop's structure is effectively unaltered.\n- Run loop iterations sequentially without parallel execution.\n\nWithin the inner parallelizable loop of the 'Column Sweep', allow two simultaneous iterations. For the final parallelizable loop in the 'Row Sweep' sequence, enable significantly more parallel execution with sixteen simultaneous iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes with the specified pragmas, instruct the code generator with the following command: \"Define a hardware-accelerated kernel function 'kernel_atax' that takes a two-dimensional array 'A', and one-dimensional arrays 'x', 'y', and 'tmp'. Initialize array 'y' with zeros. For the first loop, process without pipeline optimization. Then, for each iteration of the main loop, initialize 'tmp[i]' to zero without tiling or parallel optimization. Inside the loop, for the first nested loop, use parallel reduction on 'tmp' with a parallelism factor of four. For the second nested loop, apply parallel reduction on 'y' with a parallelism factor of two.\" \n\nAdditionally, specify the pragma values for each directive as follows:\n- Forbidding loop pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- Prevent tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Avoiding parallel loop execution: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Enabling parallel reduction with respective factors: `#pragma ACCEL PARALLEL reduction=<variable> FACTOR=auto{<factor>}` where `<variable>` is the array being reduced and `<factor>` is the specified parallelism factor.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following matrix multiplication C function into HLS code with directives that: target hardware kernel acceleration, disable automatic loop pipelining, inhibit loop tiling and parallel processing optimization, with the exception of the last nested loop in the last matrix multiplication, where the parallel factor is explicitly set to two for potential parallel execution of iterations.\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni, int nj, int nk, int nl, int nm, double E[40][50], double A[40][60], double B[60][50], double F[50][70], double C[50][80], double D[80][70], double G[40][70]) {\n  // Matrix multiplication for E = A * B\n  for (int i = 0; i < ni; i++) {\n    for (int j = 0; j < nj; j++) {\n      E[i][j] = 0.0;\n      for (int k = 0; k < nk; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n\n  // Matrix multiplication for F = C * D\n for (int i = 0; i < nj; i++) {\n    for (int j = 0; j < nl; j++) {\n      F[i][j] = 0.0;\n      for (int k = 0; k < nm; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n\n  // Matrix multiplication for G = E * F\n  for (int i = 0; i < ni; i++) {\n    for (int j = 0; j < nl; j++) {\n      G[i][j] = 0.0;\n      for (int k = 0; k < nj; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n}\n```", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the aforementioned High-Level Synthesis (HLS) codes with the corresponding pragmas, use the following concise instruction:\n\n\"Designate the function as a hardware acceleration kernel, disable automatic pipeline optimization for the first loop, keep the second loop unchunked and with sequential iterations, apply a reduction strategy with moderate parallelization to the third loop, and maximize parallelism with reduction in the fourth loop.\"\n\nHere are the pragmas and what they should correspond to based on the instruction:\n\n- \"#pragma ACCEL kernel\" corresponds to \"Designate the function as a hardware acceleration kernel.\"\n- \"#pragma ACCEL PIPELINE auto{off}\" corresponds to \"Disable automatic pipeline optimization for the first loop.\"\n- \"#pragma ACCEL TILE FACTOR=auto{1}\" corresponds to \"Keep the second loop unchunked.\"\n- \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" corresponds to \"Run the second loop's iterations sequentially.\"\n- \"#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\" corresponds to \"Apply a reduction strategy with moderate parallelization to the third loop.\"\n- \"#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\" corresponds to \"Maximize parallelism with reduction in the fourth loop.\"", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a dedicated hardware acceleration kernel, ensuring none of the loops within 'kernel_2mm' are automatically pipelined, each loop iteration should not be split into smaller parts and should be processed sequentially. Within the loop that computes 'tmp', allow parallel reduction with a custom parallel factor for 'tmp' specifically. Finally, within the loop updating 'D', automatically optimize the loop for flattening and parallel processing for 'D', but run the innermost loop over 'k' sequentially without any parallelization. Use the appropriate pragmas to reflect these specifications in HLS code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a region in the code as a hardware-accelerated kernel, and within that region: disable automatic pipelining, preserve original loop dimensions without tiling and run all iterations serially (not in parallel), except for a specific loop where parallelism factor should be set to 8 and then 4 for two distinct inner loops.\n\nHere are the specific instructions for each pragma:\n\n1. `#pragma ACCEL kernel`: This designates the function `kernel_adi` as a target for hardware acceleration.\n   \n2. `#pragma ACCEL PIPELINE auto{off}`: Within the kernel, this pragma ensures that loops following the pragma will not be pipelined automatically and therefore have to abide by the original temporal order of execution.\n\n3. `#pragma ACCEL TILE FACTOR=auto{1}`: This instructs the compiler that the loop should not be tiled, preserving the loop as a single chunk as it is in the original code.\n\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}`: Applied to multiple loops within the kernel, this pragma indicates that the iterations of the loop should be executed sequentially, as opposed to a parallel execution model.\n\n5. `#pragma ACCEL PARALLEL FACTOR=auto{8}` and `#pragma ACCEL PARALLEL FACTOR=auto{4}`: These two pragmas specifically override the previous directive of serial execution, and instead allow 8-way and 4-way parallel execution for their respective loops, increasing the level of concurrency and potentially improving performance for those loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Add the following directives to the given C code to optimize it for HLS:\n\n1. Treat `kernel_atax` as a function to be synthesized into an FPGA hardware kernel to accelerate the computation.\n2. Allow the HLS tool to auto-optimize the pipeline of the outer loop where `y` is initialized.\n3. Direct the HLS compiler to consider the inner loops for possible automatic tiling optimization, but let it decide the tiling factor.\n4. Set the parallelism factor to 1 for the second loop over `i`, forcing the HLS tool to keep the original serial loop order.\n5. For the loop that computes `tmp`, allow the HLS tool to auto-select a parallelism factor for reduction, optimizing potential data dependencies and improving performance.\n6. For the loop that computes the final `y`, enable parallelism with automatic factor selection specifically aimed at optimization related to reduction.\n\nHere's a single concise instruction that encompasses all the directives:\n\n\"Synthesize the `kernel_atax` function as a hardware acceleration kernel with automatic pipeline optimization on the `y` initialization loop; enable auto-tiled factor optimization for the inner loops; maintain the original order for the second `i` loop with no parallelism; and apply automatic parallelism factor selection for the reduction operations within the `tmp` and `y` computation loops.\"\n\nSpecific `#pragma` directives to use:\n\n1. `#pragma ACCEL kernel` - Synthesize `kernel_atax` as a hardware kernel.\n2. `#pragma ACCEL PIPELINE auto{}` - Automatically optimize the pipeline.\n3. `#pragma ACCEL TILE FACTOR=auto{2}` - Auto-select the tiling factor for `i` loop, applied to the inner dimension.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{1}` - Keep the original loop order with a parallel factor of 1.\n5. `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}` - Automatically select the parallelism factor for the `tmp` loop with reduction.\n6. `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{410}` - Automatically select the parallelism factor for the `y` computation loop with reduction.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{410}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following function into a hardware-accelerated kernel by flagging it as such. For all loops within this function, explicitly disable automatic pipelining to ensure they run as originally written without any compiler modifications for continuous loop iteration. Retain the structure of the loops without any tiling optimizations, ensuring that each iteration is processed one after the other sequentially. Within the nested loops responsible for accumulating temporary products, perform the sum reduction in a non-parallel manner. However, for the loop updating matrix D, permit the innermost loop to process multiple iterations simultaneously up to the full width of matrix D, while still calculating the sum reduction without parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for the 'kernel_2mm' function which performs matrix-matrix multiplication followed by an element-wise addition and scaling operation. Implement the first outer loop without automatic pipelining, using one processing element per iteration, and without tiling. In the innermost loop of the first multiplication, perform reductions on 'tmp' concurrently but without parallelization beyond single-threaded execution.\n\nIn the second set of nested loops, disable automatic pipelining for the outer loop, but keep the loop in its unmodified form without tiling and process it serially. Within the innermost addition loop, perform reductions on 'D' serially. However, for the middle loop 'j', flatten the iteration space so all iterations can be processed in a single stage and parallelize this loop to use 80 processing elements simultaneously, essentially allowing full parallel execution across the 'j' iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS kernel function 'kernel_adi' that processes a 2D Alternating Direction Implicit (ADI) solver, which involves column and row sweeps over a 60x60 grid for 'tsteps' time steps. The computations involve arrays 'u', 'v', 'p', and 'q', constants 'DX', 'DY', 'DT', 'B1', 'B2', 'mul1', 'mul2', and intermediately computed variables 'a', 'b', 'c', 'd', 'e', 'f'. Configure the tool to avoid pipelining, tiling, and parallel processing of outer, column, and row sweeps loops automatically. Additionally, for one of the innermost loops during the column sweep phase, enable manual parallel processing with a factor of 2. During one of the row sweep phase's inner loops, enforce an aggressive manual parallel processing with a factor of 32.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function `kernel_adi` as a hardware acceleration kernel with no automatic pipelining, no loop tiling, and the outermost loop `t` and all nested loops running sequentially except for the innermost loops in the column and row sweeps, where the column sweep inner loop has a parallel factor of 2 and the row sweep have parallel factors of 16 and 58 for their respective inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code to define a hardware kernel named `kernel_2mm` that performs matrix multiplication followed by a matrix addition without auto-pipelining or auto-tiling the outer loops, and use manual parallelization factors for the inner loops, setting reductions for temporary result accumulation. Specifically, in the first nested loop, parallelize the innermost loop `k` with a parallel factor of 5 without pipelining or tiling, and for the second nested loop, parallelize the loop `j` with a parallel factor of 80, parallelize the innermost loop `k` with a parallel factor of 4, and pipeline the loops where unspecified.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function \"kernel_atax\" as a hardware acceleration kernel, disable automatic pipelining for the first loop ensuring it runs in its entirety before proceeding, set the second loop to be partially unrolled by a factor of 2 for potential performance improvement, permit the third loop to be parallelized with up to 30 concurrent threads, and fully unroll the fourth loop while enabling reduction for accumulating into \"tmp\", and finally, set the fifth loop to run sequentially, with a single iteration at a time while enabling reduction for accumulating into \"y\". Use the following pragmas respectively: `#pragma ACCEL kernel`, `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{2}`, `#pragma ACCEL PARALLEL FACTOR=auto{30}`, `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}`, and `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}`.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{30}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{1}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a hardware kernel function named 'kernel_atax' that multiplies a matrix 'A' with a vector 'x', and accumulates the result into vector 'y' using an intermediate vector 'tmp'. Disable automatic pipelining of the outer loop, and prevent its tiling or parallel execution. Within the first inner loop, sum products with parallel execution, allowing a reduction on 'tmp' and a parallelism factor of 5. In the second inner loop, conduct a parallel reduction on 'y' with a parallelism factor of 4. Use the following pragmas: for kernel identification, use '#pragma ACCEL kernel'; to disable automatic pipelining, use '#pragma ACCEL PIPELINE auto{off}'; to suppress loop tiling and parallel execution on the outer loop, use '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}' respectively; and for the inner loops' parallel reduction, use '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}' and '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}'.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive instruction to generate the provided HLS C code with specific #pragma values:\n\n\"Create a hardware-accelerated kernel function for a 2mm computation without enabling automatic pipelining, tiling, or parallelization, ensuring that all large loops process in serial order and in full without breaking them down into smaller parts, except for a 'PARALLEL' directive on the innermost loops with a reduction on 'tmp' and 'D' having factors of 14 and 1 respectively, and a high parallelization factor on one specific loop over the 'D' array.\"\n\nSpecifically, for each loop nest:\n- Declare the function as an acceleration kernel.\n- For the first two levels of nested loops, disable automatic pipelining, set tile factors to 1 (not allowing the compiler to break loops into smaller chunks), and set parallel factors to 1 (ensuring no parallel processing is done at these levels).\n- At the third level of the first nested loop, enable parallel reduction on 'tmp' with a factor of 14, allowing that specific loop to compute in parallel with limited unrolling as defined by the factor.\n- Similarly, enable parallel reduction on 'D' with a factor of 1 for the third level of the second nested loop.\n- For the second-level loop that iterates over the 'D' array, specify a high parallel factor of 80 to allow wide parallel processing of that loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the following function into a hardware-accelerated kernel, ensuring that all loops run in the original sequential order without any form of automatic unrolling, tiling, or pipelining optimization. Allow an exception for the innermost loop of the column sweep to unroll with a factor of 2 and for the innermost loop of the row sweep to unroll first with a factor of 16, then with a factor of 29.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with pragma annotations, instruct the HLS tool as follows: \"Implement the 'kernel_adi' function as a hardware acceleration kernel, but keep each loop in its entirety without pipelining, tiling, or parallelizing any of them. However, within the column and row sweep sections, attempt to parallelize the reverse traversal loop with a factor of 2.\" Here is how that maps to specific #pragma values:\n\n- \"Implement the 'kernel_adi' function as a hardware acceleration kernel\" corresponds to `#pragma ACCEL kernel`.\n- \"Keep each loop in its entirety without pipelining\" corresponds to `#pragma ACCEL PIPELINE auto{off}` for all loops where it is applied.\n- \"Keep each loop in its entirety without ... tiling\" corresponds to `#pragma ACCEL TILE FACTOR=auto{1}` for all loops where it is applied.\n- \"Keep each loop in its entirety without ... parallelizing any of them\" corresponds to `#pragma ACCEL PARALLEL FACTOR=auto{1}` for all loops where it is applied, except the one loop where parallel factor is set differently.\n- \"Attempt to parallelize the reverse traversal loop with a factor of 2\" corresponds to `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the specific loop in the column and row sweep sections where the index variable `_in_j` and `_in_j_0` are iteratively decremented.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a target for hardware acceleration, and for each loop within this function, ensure the following: the top-level loops do not employ automatic pipelining, tiling, or parallelization; calculate the 'tmp' array using a non-pipelined, non-tiled, and non-parallelized loop structure, but enable parallel reduction on the 'k' loop with a maximum number of parallel elements as allowed by the hardware resources; calculate the 'D' matrix by applying beta as a single operation before accumulating the result from 'tmp' and 'C', flattening any nested loops for the 'i' loop with a parallel factor of 8 for the 'j' loop; and allow only sequential processing with no parallel reduction on the innermost loop calculating the 'D' matrix update.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following C High-Level Synthesis (HLS) directives to natural language instructions for generating the corresponding HLS code:\n\n1. Treat the `kernel_2mm` function as a hardware kernel to be accelerated.\n2. Do not automatically pipeline the outermost for loops.\n3. Keep the outermost for loops in their full form without tiling.\n4. Execute the iterations of the outermost for loops sequentially, not in parallel.\n5. Within the first set of nested for loops, allow the innermost loop to be automatically pipelined.\n6. Perform a parallel reduction on the `tmp` array within the innermost loop of the first set of nested for loops, allowing up to five simultaneous operations.\n7. Within the second set of nested for loops, allow the innermost loop to be automatically pipelined.\n8. Execute the iterations of the innermost loop of the second set of nested for loops concurrently, up to a maximum of 80 simultaneous operations.\n9. Perform a parallel reduction on the `D` array within the innermost loop of the second set of nested for loops, permitting up to four simultaneous operations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{5}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function that calculates the equation `D := alpha*A*B*C + beta*D` for matrices of fixed sizes. Use pragma statements to control the hardware acceleration behavior as follows:\n\n1. Declare the function as a hardware acceleration kernel.\n2. In the first nested loop block, prevent any automatic pipeline optimization and keep the loop execution serial by both tiling and parallelizing with the minimum possible factor.\n3. Within the innermost loop of the first block, use a parallel reduction on `tmp` with an unspecified factor which the tool should automatically determine, but ensure the factor is suitable for a reduction operation across 70 iterations in parallel.\n4. In the second nested loop block, allow the tool to automatically decide on pipeline optimization. Keep the loop execution sequential with regard to tiling and execute eight iterations in parallel for the inner parallel loop.\n5. Within the innermost loop of the second block, use a parallel reduction on `D` with an unspecified factor automatically determined by the tool appropriate for 50 iterations.\n\nRemember to use pragma comments to guide the behavior of the HLS tool without specifying the exact factors where `auto` is indicated, thus allowing the tool to choose optimized values.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2D ADI solver kernel that treats the entire function as a hardware acceleration kernel. Use non-pipelined loops with no loop tiling or parallelism except for the final subtraction stages of both the column and row sweeps; there, use a parallel factor of 2 and 4, respectively. Do not partition the data arrays and use full rolling to preserve dependencies. Specify each directive as follows:\n\n- To designate the function as an acceleration kernel, write `#pragma ACCEL kernel` before the function definition.\n- Disable automatic pipelining for all loops using `#pragma ACCEL PIPELINE auto{off}` before each loop.\n- Prevent loop tiling by setting the tile factor to 1 using `#pragma ACCEL TILE FACTOR=auto{1}` before each loop.\n- For the main body of j-loops in both sweeps, allow sequential operation with a parallel factor of 1 using `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- For the final backward substitution loops (the transformed decreasing j-loops), use a parallel factor of 2 in the column sweep and 4 in the row sweep with `#pragma ACCEL PARALLEL FACTOR=auto{2}` and `#pragma ACCEL PARALLEL FACTOR=auto{4}`, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here is your comprehensive, concise instruction to generate the provided HLS code:\n\n\"Define a hardware acceleration kernel that performs the alternating direction implicit (ADI) method on a 60x60 grid without optimization. Disable automatic loop pipelining, enforce original loop iteration order, and prevent loop tiling as well as parallelization, except in one specific case where the reversed 'for' loop is allowed to process two iterations simultaneously.\"\n\nSpecific #pragma values:\n\n1. `\"#pragma ACCEL kernel\"` - Denote the function as a hardware acceleration kernel.\n2. `\"#pragma ACCEL PIPELINE auto{off}\"` - Disable automatic pipelining which normally parallelizes loop iterations to improve throughput.\n3. `\"#pragma ACCEL TILE FACTOR=auto{1}\"` - Prevent loop tiling, which otherwise groups iterations into smaller chunks for parallel execution.\n4. `\"#pragma ACCEL PARALLEL FACTOR=auto{1}\"` - Enforce that iterations of the loop are processed sequentially.\n5. `\"#pragma ACCEL PARALLEL FACTOR=auto{2}\"` (applied only to the reverse 'for' loop) - Allow two iterations of the loop to be processed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel, with all the specified loops running sequentially, disabling automatic pipelining, enforcing no loop tiling or parallel processing for every loop except one, where the iterations are unrolled by a factor of 8 and another one by a factor of 58 for parallel execution.\n\nIn this implementation:\n- Use `#pragma ACCEL kernel` to specify that 'kernel_adi' is a hardware acceleration kernel.\n- Apply `#pragma ACCEL PIPELINE off` to disable any automatic pipelining optimization for all loops.\n- Set `#pragma ACCEL TILE FACTOR=1` for all loops to prevent the loop tiling optimization that would break the loop iterations into smaller chunks.\n- Set `#pragma ACCEL PARALLEL FACTOR=1` to all but two loops to enforce that the iterations run sequentially.\n- For one loop, use `#pragma ACCEL PARALLEL FACTOR=8` to allow the iterations of that loop to run in parallel groups of eight.\n- For another loop, use `#pragma ACCEL PARALLEL FACTOR=58` to allow the loop iterations to be fully parallelized.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS-accelerated kernel named 'kernel_atax' that performs matrix-vector multiplication and accumulation, with a given matrix 'A[390][410]', a vector 'x[410]', and output vectors 'y[410]' and 'tmp[390]'. Initialize the vector 'y' with zeros. Ensure that the outer loop iterating over 'A' performs no automatic pipelining, uses maximum possible loop tiling (without actually modifying the loop), and iterates sequentially. For the inner first loop nested inside, add a reduction pragma for parallelizing the loop with 'tmp' as the target for reduction with a factor to enable intermediate parallelism, and use a similar pragma with a different factor for the next inner loop with 'y' as the target for reduction. Set the factor for parallelizing the loop with 'tmp' reduction to 5 and for 'y' reduction to 2, disabling any automatic parallel factor guessing.\n\nYour instruction should correspond to the following HLS pragmas for C code:\n\n- Treat the 'kernel_atax' function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- Do not automatically pipeline the first loop iterating over vector 'y': `#pragma ACCEL PIPELINE auto{off}`\n- Process the second loop over 'tmp' in its original form without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Ensuring the iterations of the second loop run sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Apply parallelism to the innermost loop that computes 'tmp', with explicit reduction and parallelism factor of 5: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}`\n- Apply parallelism to the subsequent inner loop that computes 'y', with reduction and a parallelism factor of 2: `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{5}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the function `kernel_2mm` into a hardware-accelerated kernel, but ensure that the main computational loops do not utilize hardware pipelining or tiling optimizations, executing iterations sequentially. Specifically, disable automatic pipelining for all loops and enforce non-parallel loop execution by setting the parallel factors to 1. For the initial multiplication loop, enable a parallel reduction operation on `tmp` with a parallel factor of 32. Lastly, for the accumulation into `D`, allow the innermost loop to execute in a flattened manner and set the outermost loop's parallel factor to 80, while keeping the reduction on `D` sequential.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes with the appropriate pragmas:\n\n1. Designate the function 'kernel_2mm' to be optimized for hardware acceleration by indicating that it is a function kernel.\n2. For the first two nested loops iterating over 'i' and 'j', disable automatic pipelining to ensure the loops are not unrolled, tiled, or reordered. However, for the innermost loop iterating over 'k' that computes the temporary matrix, enable parallel processing with a reduction on the 'tmp' array.\n3. In the second set of nested loops, allow the outermost loop to be considered for automatic pipelining optimization while setting tiling and parallel factor to their minimum to avoid any loop transformations for both the 'i' loop and the inner 'j' loop. Do not automatically pipeline the 'j' loop and force it to run serially, but do specify a parallel factor of 8 for it to hint at potential parallelization of at least 8 iterations. Lastly, enable parallel processing with a reduction on the 'D' array for the innermost 'k' loop.\n\nSpecific pragma instructions:\n- Use '#pragma ACCEL kernel' before the function declaration to enable hardware acceleration.\n- Insert '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{1}', and '#pragma ACCEL PARALLEL FACTOR=auto{1}' for both the 'i' and 'j' loops in the first set to avoid automated optimizations.\n- Use '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}' before the loop over 'k' to enable parallel accumulation into the 'tmp' array with as many parallel factors as loop iterations.\n- Apply '#pragma ACCEL PIPELINE auto{}' before the second 'i' loop to allow the compiler to decide on pipelining but keep '#pragma ACCEL TILE FACTOR=auto{1}' and '#pragma ACCEL PARALLEL FACTOR=auto{1}' unchanged.\n- For the second 'j' loop, use '#pragma ACCEL PIPELINE auto{off}', '#pragma ACCEL TILE FACTOR=auto{1}', but set the parallel factor with '#pragma ACCEL PARALLEL FACTOR=auto{8}'.\n- Finally, add '#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}' for the last 'k' loop to enable parallel reduction into the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel, leaving the first two nested loops in their complete, sequential form, while flattening the second set of nested loops for pipeline optimization. Additionally, avoid parallel execution and tiling of the outermost loops, but allow for parallelization with a factor of two for the innermost loop of the initial nested pair. For the second pair of outer and inner loops, maintain similar restrictions but enable tiling with a factor of 80 for the innermost loop, and allow both reductions to be executed sequentially without parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. For the first two-level loop nest within the function, suppress automatic loop pipelining, maintain the outer loop in its entirety, and run the inner loop iterations serially but flatten the loop for potential pipelining if beneficial. In the innermost loop, support parallel reduction for the `tmp` array with no specific factor restriction. For the second two-level loop nest within the function, allow the outer loop to be automatically pipelined, enforce serial execution for the middle loop with increased granularity by breaking it into chunks of 8, and run four iterations in parallel for the reduction on the `D` array in the innermost loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the embedded instructions in the provided HLS code into a comprehensive but concise natural language instruction as follows:\n\nDefine a function as a dedicated hardware acceleration unit, without automatically optimizing the primary loop structures for pipelining, tiling or parallel execution. In the nested loops, disable automatic loop pipelining and keep the loop iterations undivided. Within the innermost loop of the first double-nested loop structure, enable a specific parallel reduction operation on the 'tmp' array with a parallelism factor of 14. In the context of the second double-nested loop structure, iterate over the outer loops in a serial manner and set the parallelism factor for the innermost loop to 80 during the update of the 'D' array, while allowing a different parallel reduction operation with a parallelism factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a triple matrix multiplication kernel function that disables automatic loop pipelining, processes each loop in its original undivided form, and executes all loop iterations sequentially, treating the function as a hardware acceleration kernel. Specifically, apply the following directives: \n\n- Use '#pragma ACCEL kernel' to indicate the function should be accelerated.\n- Implement the first loop for E = A * B without loop pipelining, parallelism, or tiling by applying '#pragma ACCEL PIPELINE off', '#pragma ACCEL PARALLEL FACTOR=1', and '#pragma ACCEL TILE FACTOR=1', respectively.\n- Implement the second loop for F = C * D with the loop body pipelined but the outer loops still without pipelining, parallelism, or tiling, by applying '#pragma ACCEL PIPELINE off' to outer loops, and '#pragma ACCEL PIPELINE auto{flatten}' for the loop body, while maintaining the same directives for parallelism and tiling as used in the first loop.\n- Implement the third loop for G = E * F with the same restrictions as the first loop, applying the same corresponding directives for no pipelining, parallelism, or tiling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, use the following natural language instruction:\n\n\"Define a hardware-accelerated kernel function named 'kernel_atax' for matrix-vector multiplication and accumulation. The auto-generated code should not pipeline the outermost loop, and it should leave the two inner loops in their full original structure without tiling or parallelizing them. However, for the first inner loop, parallelize the reduction operation across 'tmp' with a high parallelization factor, and for the second inner loop, employ a moderate parallelization factor for the reduction on 'y'.\"\n\nSpecific Pragma Values Descriptions:\n\n- Treat 'kernel_atax' as a hardware acceleration kernel: `#pragma ACCEL kernel`\n- Do not automatically pipeline the initialization loop for 'y': `#pragma ACCEL PIPELINE auto{off}`\n- Do not tile the 'tmp' initialization loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Do not automatically parallelize the 'tmp' initialization loop: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Parallelize the loop for calculating 'tmp' with a high factor, optimizing for reduction on 'tmp': `#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}`\n- Parallelize the loop for updating 'y' with a moderate parallelization factor, optimizing for reduction on 'y': `#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}`", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{4}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function 'kernel_2mm' as a hardware acceleration kernel. For the initial loop over 'i', disable pipeline optimization, execute the loop in its entirety without tiling, and do not parallelize the loop iterations but instead run them sequentially. Within the nested 'j' loop inside the 'i' loop, enable loop flattening for pipelining, do not apply tiling, and parallelize every two iterations of the loop. For the innermost 'k' loop that initializes 'tmp', perform a parallel reduction on 'tmp' and do not parallelize iterations.\n\nFor the second set of nested loops over 'i' and 'j' that update matrix 'D', enable pipelining without specific optimization, do not apply tiling, and do not parallelize the outer 'i' loop iterations. Inside the 'j' loop, enable loop flattening for pipelining, do not use tiling, and run loop iterations sequentially. Lastly, parallelize the innermost 'k' loop with a reduction on 'D' without specific parallelization of iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for the given function, which processes a 2D numerical algorithm iteratively over a grid without automatic optimization of loop pipelining, tiling, or parallel execution within the top-level temporal loop. Utilize the default sequential behavior for the first loop nest responsible for the column sweep, handling elements in both forward and reverse order while manually setting the parallel factor to 2 for the inner 'j' loop. Maintain the default loop behavior for the second loop nest addressing the row sweep, but explicitly parallelize operations within the penultimate inner 'j' loop with a parallel factor of 29.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with appropriate #pragma directives, your instruction should be as follows:\n\n\"Define a hardware kernel function named 'kernel_2mm' for matrix multiplication and addition to accelerate the computation on FPGA. Disable automatic pipelining for all nested loops, but enable manual pipelining for the first loop over `j` without specifying the pipeline factor, and flatten the pipeline for the second loop over `i`. Do not tile any of the loops but use parallelism with a factor of 2 in the first nested loop over `j` only; all other loops should be executed serially, except for the innermost loops with reduction operations on `tmp` and `D`, which should use parallelism with an unspecified factor. Ensure that all these configurations are explicitly set to non-default values where applicable.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. In all nested loops within this function, deactivate automatic pipelining, enforce the loops to stay unchunked and run the iterations sequentially. Specifically, in the first loop nest, activate pipelining without any suggested transformations for the innermost loop; in its deepest loop, enable parallel processing over 14 iterations for accumulation into the 'tmp' array through reduction. In the second loop nest, use pipelining with loop flattening for the intermediate loop, and for the deepest loop, execute iterations in parallel with a factor of 80 when multiplying the 'beta' term into 'D', while subsequent accumulations into 'D' with the 'tmp' and 'C' matrix product should run sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code to define a function `kernel_adi` designated as a hardware acceleration kernel that takes integer parameters `tsteps` and `n` along with four double precision 60x60 matrices `u`, `v`, `p`, `q`. Within the function, disable automatic pipelining, tiling, and parallelism for the entire function's loops by setting their respective pragmas to 'off' or a factor of 1. In nested loops, selectively set parallelism: keep it at 1 for outer loops to maintain sequential processing but set a parallel factor of 2 for second-level nested loops and a factor of 16 for reversing third-level nested loops, ensuring only these specific loops are allowed to run iterations concurrently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, preventing the top-level loops from pipelining and tiling, but allowing the inner loops to exploit different degrees of parallelism and pipelining. Specifically, for the first loop iteration, entirely disable pipelining, use no tiling, and iterate sequentially. Within the nested loop, flatten the pipeline, apply no tiling, and use a minimal parallelism factor of 2 for the innermost loop, with a reduction applied to 'tmp'. For the second loop iteration, again disable top-level pipelining, apply no tiling, and iterate sequentially. Within the nested loop, allow automatic pipelining, tile with a factor of 8, and use a parallelism factor of 16 for the innermost loop with a reduction applied to 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function called 'kernel_adi' that performs an Alternating Direction Implicit (ADI) algorithm, with certain loop optimizations specified as follows:\n\n1. Treat the 'kernel_adi' function as a hardware acceleration kernel.\n2. For all outer loops, including the time step loop and the two spatial sweep loops:\n   - Disable automatic pipelining.\n   - Do not tile the loop.\n   - Do not parallelize the loop iterations.\n3. For the innermost loops:\n   - Apply a parallelization factor of 2 to the forward sweeps (first inner loops of column and row sweeps).\n   - Apply a parallelization factor of 32 only to the backward sweep of the row sweep (the last inner loop in the row sweep).\n\nEnsure that, for each #pragma directive, you toggle the 'auto' option off where indicated and assign the specified factor values to 'TILE' and 'PARALLEL' pragmas, also making sure to keep the pragma 'PIPELINE' turned off for all the loops where it is mentioned.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a target for hardware acceleration, ensure all loops within operate sequentially without compiler-introduced pipelining, tiling optimization, or parallelization, except for the innermost loop in the column sweep section which should be parallelized with a factor of 2, and the innermost loop in the row sweep section which should be parallelized with a factor of 29.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the entire function `kernel_adi` as a dedicated hardware acceleration kernel, but prevent automatic pipelining optimizations for each loop encountered within the function. Additionally, for each loop, maintain its original loop boundaries without tiling or partitioning, and execute loop iterations sequentially without introducing parallel execution, except in the innermost loop of the column sweep where the iterations should run two at a time. Specifically, you should enforce these directives:\n\n- Treat the `kernel_adi` function as a hardware kernel for acceleration using `#pragma ACCEL kernel`.\n- Disable automatic pipelining for all loops with `#pragma ACCEL PIPELINE auto{off}`.\n- Keep every loop from being divided into smaller chunks by setting the tile factor to its default value with `#pragma ACCEL TILE FACTOR=auto{1}`.\n- Ensure that by default, loops run sequentially, iteration after iteration, with `#pragma ACCEL PARALLEL FACTOR=auto{1}`, except for the inner loop after initializing `v[0][i]` where iterations should proceed in pairs (`#pragma ACCEL PARALLEL FACTOR=auto{2}`).", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware kernel, but ensure that no automatic optimizations are applied to the loops. Specifically: disable automatic pipelining, retain the original loop nesting by not tiling the loops, and maintain sequential execution for all loops, except for one inner loop in the column sweep where iterations can be processed two at a time, and another inner loop in the row sweep with a higher parallel processing factor of four for the first part and sixteen for the second descending part.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the encompassing function `kernel_adi` as a hardware acceleration kernel and orchestrate the execution of its loops with the following constraints: \n\n1. The outermost time iteration loop, as well as the two inner loops for 'Column Sweep' and 'Row Sweep', should execute sequentially without any compiler-imposed pipelining.\n2. Prevent the compiler from tiling these same loops, ensuring each is run as a single entity without being split into smaller tasks.\n3. Within the innermost loops of both 'Column Sweep' and 'Row Sweep', the initial loop which computes values for `p` and `q` should run its iterations sequentially with no parallel execution by the compiler. In contrast, the loop responsible for updating the `v` array during the 'Column Sweep' should have its its iterations distributed across two parallel processes, while the update loop for the `u` array in 'Row Sweep' should execute sequentially.\n\nTo achieve this, apply the following `#pragma` directives:\n\n- `#pragma ACCEL kernel` before the `kernel_adi` function definition to specify the function as a hardware acceleration kernel.\n- Utilize `#pragma ACCEL PIPELINE auto{off}` before the time iteration loop, and the 'Column Sweep' and 'Row Sweep' loops to prevent automatic pipelining of these loops.\n- Set `#pragma ACCEL TILE FACTOR=auto{1}` before the time iteration loop, and the 'Column Sweep' and 'Row Sweep' inner loops to inhibit loop tiling.\n- Employ `#pragma ACCEL PARALLEL FACTOR=auto{1}` before the innermost loops that compute `p` and `q` in both sweeps, indicating that these loop iterations should not be executed in parallel.\n- Insert `#pragma ACCEL PARALLEL FACTOR=auto{2}` before the `v` array update loop in 'Column Sweep' to instruct the compiler to run every two iterations of this loop in parallel.\n- Lastly, `#pragma ACCEL PIPELINE auto{flatten}` is to be placed before the 'Row Sweep' loop for accumulating the `u` array but it seems to be an erroneous instruction given the context and should be disregarded.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the entire function 'kernel_2mm' as a hardware acceleration kernel, and for each encountered loop within the function, follow these specific instructions: \n\n1. Prevent automatic pipelining.\n2. Do not automatically tile the loop, which means executing the loop in its original, full form without partitioning it into sub-loops.\n3. Ensure the loop runs serially, with iterations executing one after the other rather than in parallel.\n\nIn the nested loop where 'tmp' is updated, allow the innermost loop indexed by 'k' to be partly parallelized with a reduction on 'tmp', processing 14 iterations in parallel.\n\nIn the second nested loop where 'D' is updated, again prevent automatic pipelining and tiling for the 'i' and 'j' loops, but for the 'j' loop set up full parallelization with 80 instances running concurrently.\n\nFinally, within the innermost loop indexed by 'k' where 'D' is updated, specify that 50 iterations can be processed in parallel with a reduction operation on 'D'.\n\nTranslate this instruction to HLS pragmas accordingly in the provided code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the given function into a hardware-accelerated kernel, while disabling automatic loop pipelining, maintaining original loop sizes without tiling, and running loop iterations sequentially for the outer loops. For the specific innermost loops, adjust parallel processing factors to 2 and 29 respectively for different sweeps:\n\n1. Mark `kernel_adi` as an accelerator hardware kernel.\n2. Disable automatic loop pipelining for all loops in the function.\n3. Keep loops in their full, original form without tiling (using the tile size of 1).\n4. Execute all loops sequentially without parallelism (using a parallel factor of 1), except:\n   - Set a parallel factor of 2 for the column sweep's inner `j` loop.\n   - Set a parallel factor of 29 for the row sweep's inner `j` loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the HLS pragmas into natural language instructions, detailing the acceleration of each section of the code:\n\n1. Treat the entire `kernel_adi` function as a hardware kernel targeted for acceleration.\n2. For all the loops within the `kernel_adi` function, disable automatic pipelining, which is indicated by setting the pipeline pragma to \"off.\"\n3. For all the loops within the function, do not tile any of the loops, essentially processing each loop iteration in its full form without breaking it into smaller chunks, by setting the tile factor to 1.\n4. For all the loops within the function, ensure that iterations are run sequentially and not in parallel by setting the parallel factor to 1.\n5. In the innermost loop, where `j` iterates from 1 to 58 in the column sweep section, allow a moderate level of parallelism by doubling the number of iterations processed concurrently, setting the parallel factor to 2.\n6. In the reversed innermost loop, where `j` iterates from 58 down to 1 in the row sweep section, allow a higher level of parallelism by processing four iterations concurrently, setting the parallel factor to 4.\n\nTo summarize, in natural language, we instruct HLS as follows:\n- Accelerate the `kernel_adi` function while keeping the original loop structures intact and sequential, except for specific inner loops where limited parallel execution is permitted, with a parallelism factor of 2 for column updates and a factor of 4 for row updates.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_adi` as a hardware acceleration kernel without applying automatic loop pipelining, tiling, or parallelization pragmas, but explicitly set parallel factors for nested loops inside the column and row sweeps, with a factor of 2 for the inner loop of the column sweep and a factor of 4 for the inner loop of the row sweep.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS C code provided with appropriate pragmas, use the following instructions:\n\n1. Mark the function 'kernel_2mm' as a hardware acceleration kernel.\n2. Prevent the outermost loops from being automatically pipelined and ensure they maintain their full iteration space without being tiled or parallelized.\n3. Pipeline the first inner loop 'j' while flattening any nested loops.\n4. Execute the computations involving the 'tmp' array in fully parallel manner without any reduction or partitioning.\n5. Allow the second outer loop 'i' processing 'D' to be automatically pipelined but prevent its inner loop 'j' from being pipelined.\n6. Tile the second inner loop 'j' with a factor of 8.\n7. Parallelize the innermost loop 'k' that accumulates into 'D' with a parallelization factor of 10, explicitly reducing 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the 'kernel_adi' function as an acceleration kernel without enabling hardware optimization features such as loop pipelining, loop tiling, or automatic parallelization for the main iteration loops, except for two specific inner loops where parallelism factors of 2 and 16 should be applied respectively.\n\n1. Annotate the 'kernel_adi' function as a hardware acceleration kernel.\n2. Prevent automatic loop pipelining for all loops.\n3. Keep all loops in their original, full form without tiling.\n4. Execute all loop iterations sequentially, without parallel execution, except for the second innermost loop in the first for-loop block and the second innermost loop in the second for-loop block. For these two exceptions, set the parallelism factor to 2 and 16 respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Process the function 'kernel_2mm' as a dedicated hardware acceleration kernel. Within this function, ensure the outer loops for 'i' iterate in their complete sequences without pipelining or parallel execution. Internally, disable automatic pipelining and tiling for the first 'j' loop, but allow the loop iterations to be processed two at a time in parallel. For the innermost 'k' loop calculating the 'tmp' matrix, disable automatic pipelining while allowing its iterations to execute consecutively and enable a parallel reduction on 'tmp'. In the second 'i' loop calculating the 'D' matrix, manually set pipelining to default, tile the 'j' loop with a factor of 8, and allow its iterations to execute sequentially. Perform a parallel reduction on 'D' within the innermost 'k' loop with 50 iterations running in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes implemented in this C function, we need to provide the following instructions:\n\n- Indicate that the `kernel_adi` function should be compiled as a dedicated hardware kernel for acceleration.\n- Ensure that all loops within this function are executed in a sequential manner, with no automatic pipelining, tiling, or parallelization optimization applied by the HLS tool unless explicitly stated otherwise.\n- Apply a parallelism factor of 2 to specific inner loops that deal with the indices 'j' during both the 'Column Sweep' and 'Row Sweep' to allow these iterations to be partially unrolled, resulting in some degree of concurrent execution.\n\nSpecifically, you should include these instructions:\n\n- Treat the `kernel_adi` function as a hardware kernel.\n- Do not automatically pipeline any loops.\n- Run every loop in its original form without tiling.\n- Execute the loops' iterations sequentially unless otherwise stated.\n- Apply a parallelism factor of 2 to particular inner loops that have a 'j' index variable within both 'Column Sweep' and 'Row Sweep' sections of the code to improve concurrency where indicated.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2D Alternating Direction Implicit (ADI) solver kernel that will not have its loops automatically pipelined or tiled. The outer time loop and both inner loops for the column and row sweeps should have no automatic parallelization applied, except for one specific parallelization factor of 2 applied in the first inner column sweep loop and two different specific parallelization factors, 29 and 8, applied in the row sweep loops. Treat the entire function as a hardware kernel for acceleration. Use the corresponding pragmas to achieve this:\n\n1. Designate the function as an acceleration kernel.\n2. Prevent automatic pipelining of all loops.\n3. Avoid auto-tiling each loop, enforcing the loops to run without division.\n4. Inhibit any auto-parallelization aside from manually setting the given factors.\n5. Utilize a parallel factor of 2 for the first inner column sweep loop.\n6. Apply a parallel factor of 29 for the first inner row sweep loop, and a factor of 8 for the second inner row sweep loop to manually control parallel execution limits.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function as a hardware acceleration kernel, disable automatic pipelining, maintain original loop structure and execution ordering for the entirety of the function, allow a specific inner loop to run with doubled parallelism, and another to run with a parallelism of sixteen.\n\n- Use `#pragma ACCEL kernel` to indicate that `kernel_adi` is the function to be accelerated.\n- Apply `#pragma ACCEL PIPELINE auto{off}` to prevent the tool from automatically adding pipeline directives.\n- Insert `#pragma ACCEL TILE FACTOR=auto{1}` to keep the loops in their original form without tiling.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to ensure the loops execute sequentially, maintaining original ordering.\n- For the loop where `j` goes from 1 to 58 inside the 'Column Sweep' section, apply `#pragma ACCEL PARALLEL FACTOR=auto{2}` to allow for a modest increase in parallel execution with a parallelism factor of 2.\n- For the loop where `j` goes from 1 to 58 inside the 'Row Sweep' section, utilize `#pragma ACCEL PARALLEL FACTOR=auto{16}` to significantly increase parallel execution with a parallelism factor of 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the following loop-based computations into hardware acceleration definitions with specific constraints: Treat the main 'kernel_adi' function as an independent hardware-accelerated computation. Within this function, maintain the natural iteration order of all loops without any automatic optimization techniques such as pipelining, tiling, or parallel processing, except for the innermost loop within the column sweep section, which should process two iterations simultaneously.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function 'kernel_2mm' as a hardware kernel for acceleration without automatically pipelining any of its loops. Within the function, maintain the original, full iteration space of the outermost loops by explicitly setting the tile and parallel factors to '1'. For the first nested loop pairing pertaining to the 'tmp' array calculations, perform the innermost 'k' loop iterations with a parallel factor of '10', allowing ten iterations to execute in parallel but without pipelining, and apply a reduction strategy for the 'tmp' array. For the second nested loop pairing involving updates to the 'D' array, increase the parallel execution capacity of the innermost 'k' loop to a parallel factor of '25', also without pipelining, and again use a reduction strategy for the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{25}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_adi` as a dedicated hardware acceleration unit. Within this function, ensure that the pipeline optimization is deactivated for all loops so they execute sequentially as in the original code. Maintain the original loop structure without tiling, but parallelize the second nested loop with a factor of 2 iterations and the fourth nested loop with a factor of 4 iterations during the sweeps. Additionally, parallelize the backward iteration segments at the end of the sweeps with factors of 1 and 29, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Write an instruction to generate HLS C code for accelerating a matrix-matrix multiplication using the 2mm kernel algorithm, where the code specifies that the outer loops are to be processed sequentially without automatic pipelining or parallelization, while the inner loops use default pipelining, fine-grained parallelism with a maximum factor for the reduction loop, and flattened pipelining for the last loop nest. Use specific `#pragma` directives to indicate no optimization for the outer loop (`#pragma ACCEL PIPELINE \"off\"`, `#pragma ACCEL TILE FACTOR=1`, `#pragma ACCEL PARALLEL FACTOR=1`), default optimization for first inner loop (`#pragma ACCEL PIPELINE auto`, `#pragma ACCEL TILE FACTOR=1`, `#pragma ACCEL PARALLEL FACTOR=1`), aggressive parallel reduction with a maximum factor for the second inner loop (`#pragma ACCEL PARALLEL reduction = tmp FACTOR=32`), and a flattened pipeline with maximal parallelization for iterations in the final loop (`#pragma ACCEL PIPELINE auto{flatten}`, `#pragma ACCEL TILE FACTOR=1`, `#pragma ACCEL PARALLEL FACTOR=80`). Apply the parallel reduction with a factor of one for the deepest loop inside this last pipelined section (`#pragma ACCEL PARALLEL reduction = D FACTOR=1`). Also, add the `#pragma ACCEL kernel` to designate the function as a hardware acceleration kernel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as specialized hardware for acceleration. Within this function, disable automatic pipelining, prevent breaking loops into chunks, and enforce sequential loop iteration for the outermost loop. Retain the initial settings but allow two parallel instances for the first nested loop's second innermost loop, and retain sequential execution for its innermost loop. Keep sequential operation for the second nested loop's outermost and innermost loops while allowing 16 instances to run in parallel for its second innermost loop, and only four instances for the subsequent reversal loop. Use the following instructions for the respective code segments:\n\n1. \"#pragma ACCEL kernel\" is described as: Convert the entire function into a dedicated hardware module.\n2. \"#pragma ACCEL PIPELINE auto{off}\" is described as: Prevent the compiler from automatically optimizing loop operation through pipelining.\n3. \"#pragma ACCEL TILE FACTOR=auto{1}\" is described as: Forbid the division of loops into smaller tiled loops.\n4. \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" is described as: Execute loop iterations in a sequential, non-parallel manner.\n5. \"#pragma ACCEL PARALLEL FACTOR=auto{2}\" inside the first nested loop is described as: Permit two iterations of the loop to execute concurrently.\n6. \"#pragma ACCEL PARALLEL FACTOR=auto{16}\" inside the second nested loop is described as: Permit sixteen iterations of the loop to execute concurrently.\n7. \"#pragma ACCEL PARALLEL FACTOR=auto{4}\" after the standardization comment within the second nested loop is described as: Permit four iterations of the reversal loop to execute concurrently.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function called 'kernel_2mm' with its loops optimized following these specific directives: Firstly, for the entire function, treat it as a hardware acceleration kernel. Then, for the first two nested loops iterating over 40 and 50 elements, prevent the loop from being automatically pipelined, force it to execute without tiling, and ensure the outer loop runs sequentially instead of in parallel. Within the inner 'j' loop, allow the tool to decide on loop flattening, prevent tiling, but use a factor of 2 for running iterations in parallel, and use a reduction pragma for the 'tmp' array with the default parallelism factor. For the last two nested loops iterating over 40 and 80 elements, again prevent automatic pipelining and tiling for the outer 'i' loop, while also enforcing sequential execution, but for the 'j' loop, prevent pipelining, apply a tiling factor that corresponds to the full size of the 'C' array's second dimension (80), and ensure sequential execution. Inside this inner loop, apply parallel reduction to the 'D' array with maximum allowed parallelism based on the 'C' array's first dimension (50).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS codes in C using comprehensive but concise natural language instructions for each `#pragma`, you would instruct as follows:\n\n1. \"Define the following function as a hardware acceleration kernel.\" - This corresponds to `#pragma ACCEL kernel` at the beginning of the function definition.\n\n2. \"For the main matrix multiplication and accumulation loops, execute without applying any automatic optimizations such as pipelining, tiling, or parallel execution optimizations.\" - This corresponds to the following pragmas inside the first and second outer loops:\n    - `#pragma ACCEL PIPELINE auto{off}`\n    - `#pragma ACCEL TILE FACTOR=auto{1}`\n    - `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n3. \"In the innermost loop of the first matrix multiplication, use data reduction on the `tmp` array and parallelize the loop operations to a factor of 14, but do not apply tiling or pipeline optimizations.\" - This corresponds to the pragmas inside the first inner loop `k`:\n    - `#pragma ACCEL PIPELINE auto{off}`\n    - `#pragma ACCEL TILE FACTOR=auto{1}`\n    - `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}`\n\n4. \"Within the loop updating matrix D, scale the entire row of D matrix with beta and then perform the addition in parallel up to 80 times simultaneously if possible, followed by another innermost loop, within which apply a parallelism factor of 4 with data reduction on the D array, but again, no tiling or pipeline optimizations on both loops.\" - Matching this to the pragmas inside the second outer and inner `j` and `k` loops:\n    - `#pragma ACCEL PIPELINE auto{off}`\n    - `#pragma ACCEL TILE FACTOR=auto{1}`\n    - `#pragma ACCEL PARALLEL FACTOR=auto{80}` for the outer `j` loop\n    - `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}` for the innermost loop `k`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{4}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code written in C, use the following natural language instruction:\n\n\"Define a function 'kernel_2mm' as a hardware acceleration kernel that receives six numerical parameters: 'ni', 'nj', 'nk', 'nl', 'alpha', and 'beta', as well as four two-dimensional arrays 'tmp', 'A', 'B', and 'C', and computes the matrix multiplication with addition according to the equation 'D := alpha * A * B * C + beta * D'. Set all loops to not be auto-pipelined, maintaining their full iteration structure, and ensure the innermost loops run with specified parallelism: in the first nested loop pair, no parallelism, and in the second nested loop pair, a parallel factor of 2 for the 'j' loop and parallel factor of 50 for the 'k' loop within the 'j' loop. Additionally, enforce that no tiling is applied to most loops except the outermost loop of the second pair, which should be given a tile factor of 8.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to:\n- Accelerate the `kernel_adi` function by designating it as a hardware kernel.\n- Within the entire function and the two nested loops for column and row sweeps, disable automatic pipelining.\n- Prevent the breaking down of the loops into smaller chunks by using a tiling factor of 1.\n- Maintain the original sequence of iterations within the loops, executing them in a serial fashion with a parallelism factor of 1.\n- In the loop responsible for updating 'p' and 'q' in the column sweep, instruct the HLS tool to attempt a modest amount of parallel execution with a factor of 2.\n- In the loop for updating 'p' and 'q' during the row sweep, promote aggressive parallelism with a factor of 32 when calculating dependencies.\n- After the aggressive parallel computation in the row sweep, revert to a parallelism factor of 2 for the loop that reverses the iteration for updating 'u'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. In the body of 'kernel_2mm', disable automatic pipelining and maintain the original form of the outermost for-loop without tiling or parallelizing it.\n\nWithin the first nested loop where 'tmp' matrix is computed, enable automatic pipelining to allow the compiler to optimize the loop execution. Keep the loop in its full form without tiling, but permit parallel execution with a factor of 2 for both the second and the innermost loop, which is accumulating the 'tmp' matrix with a reduction operation.\n\nIn the second major loop block where 'D' matrix is updated, automatically enable pipelining and tile the loop with a factor of 2 for the inner loop. For the innermost loop, processing element 'D', allow maximal parallel execution with a specified factor of 32 and enable a reduction on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the provided algorithm into a hardware-accelerated kernel that does not automatically pipeline any of the loops, ensures each loop runs without tiling and without parallelization, except for two specific inner loops where one loop's iterations should be processed in pairs and another in sets of four, and a final loop where iterations should be processed in sets of eight.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the given HLS directives and code structure into a natural language instruction to generate a similar HLS C code:\n\nIdentify the following function as a candidate for hardware acceleration, don't perform automatic loop pipelining, and maintain the original loop structure without tiling or parallelizing the first loop. Within the nested loop, allow the compiler to flatten the loop hierarchy for pipelining, restrict tiling to the default behavior, and use a parallelization factor of two. For the reduction operation in the innermost loop, enforce an original sequential execution without parallel optimization. Then, in the second main loop, enable automatic pipelining, prevent tiling, and ensure sequential execution without parallelization in the top-level loop. In the nested loop, deactivate automatic pipelining, set a tiling factor of four for loop partitioning, and once again ensure sequential execution without parallel optimization. Lastly, for the reduction operation in the innermost loop, allow a parallelization factor of eight to speed up computation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{8}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as a hardware acceleration kernel, inhibiting the automatic pipelining, tiling, and parallel processing of the main time-stepping, column sweep and row sweep loops, while allowing for manual adjustment of parallel processing within nested loops\u2014doubling the parallel factor in the first column sweep and multiplying by eight and thirty-two respectively in the row sweep.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the input function 'kernel_adi' into a hardware-accelerated kernel, maintaining the original structure of the main loops without automatic pipelining or chunking. Process two iterations of the nested loop in column sweep and eight iterations of the nested loop in row sweep simultaneously, while handling the final loop in row sweep sequentially with a factor of 29.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function `kernel_2mm` as a dedicated hardware acceleration kernel. Inside this kernel, first compute the matrix multiplication of `A` and `B` and then scale the result `tmp` by `alpha`. Next, scale matrix `D` by `beta` and add the previously computed `tmp` matrix, now partially multiplied by `C`, to complete the `alpha*A*B*C + beta*D` operation. Prevent the first and third loops from being automatically pipelined, and ensure they are run entirely without tiling or parallelization. In the second loop nest, handle the outer loop similarly but allow the inner reduction loops over `k` to be parallelized with a factor of 2. In the fourth loop nest, automatically pipeline the outer loop, tile it with a factor of 2, and significantly parallelize the innermost loop with a factor of 32, using both loops to accumulate the products into `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS code for a matrix multiplication and addition kernel function called `kernel_2mm` that accelerates the operation `D := alpha*A*B*C + beta*D`. Disable loop pipelining for the outer loops but flatten the first j-loop. Let nested loops use a parallel factor of 1, but the second j-loop should be tiled with a factor of 4 and use a parallel reduction for the innermost k-loop with a factor of 50.\n\n- Treat `kernel_2mm` as a hardware kernel function.\n- For the first pair of nested loops (i and j) that initializes `tmp`:\n  - Do not pipeline the `i` loop.\n  - Keep the `j` loop in its original form without tiling.\n  - Process the `j` loop iterations one after the other.\n  - Flatten the `j` loop when pipelining.\n  - Inside the `k` loop, use a parallel reduction on `tmp` without changing the loop factor.\n- For the second pair of nested loops (i and j) that computes `D`:\n  - Again, do not pipeline the `i` loop.\n  - Tile the `j` loop with a factor of 4.\n  - Do not parallelize the `j` loop outside of tiling.\n  - Within the `k` loop, use a parallel reduction on `D` with a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "\"Generate an HLS C code for a 2-dimensional matrix multiplication and addition kernel, marking the top function as hardware accelerated. In the first major loop, disable automatic pipelining. In the nested loops, do not tile the iterations and execute two iterations in parallel for the second loop. Implement parallel reduction with a factor of four for the innermost loop that computes the product of matrix A and B. In the following major loop, again disable automatic pipelining, tile every second iteration of the nested loop with a factor of two, and use a parallel factor of one. Finally, for the innermost loop that accumulates the product of matrices tmp and C onto D, perform the parallel reduction with a high factor of 32.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_adi' function as a dedicated hardware module for acceleration purposes. Disable automatic pipelining for the structural loops within the function. While ensuring each loop executes without being tiled or having iterations run in parallel, set the parallel factor to automatically double within the nested 'j' loop in the column sweep, and in the row sweep, set the parallel factor to automatically quadruple and octuple for the respective 'j' loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes, use the following natural language instruction:\n\n\"Define a hardware kernel function that performs a two-matrix-multiply (2MM) operation with no automatic pipelining for the outer loops, but allowing reduction with maximal parallelism in the innermost loops. Specifically, for the computation of `tmp`, disable loop pipelining and tiling, and set parallel factor to 2 on the second loop, while enabling maximal parallel computation with a factor matching the loop bounds for the innermost loop. For computing `D`, flatten the first loop's pipeline, enable no tiling and parallelization on the second loop, and allow a single-threaded reduction in the innermost loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create HLS code for a kernel function `kernel_adi` that performs an Alternating Direction Implicit (ADI) algorithm, ensuring that no optimization techniques like loop pipelining, tiling, or parallel execution are applied generally but allowing specific settings for parallel loop execution with different factors where explicitly defined. The function arguments include the number of time steps `tsteps`, the grid dimension `n`, and four 2D arrays `u`, `v`, `p`, `q` representing computation matrices.\n\nSet the function as a hardware acceleration kernel, initialise constants `DX`, `DY`, `DT`, `B1`, `B2`, `mul1`, `mul2`, `a`, `b`, `c`, `d`, `e`, `f` accordingly inside a scope region. \n\nWithin the time-stepping loop:\n\n- Disable loop pipelining, tiling, and parallel factors by setting `auto{off}`, `auto{1}`, and `auto{1}` respectively for the outer loop.\n- Inside the first inner `i` loop for the column sweep, similarly disable the optimizations and leave the very inner `j` loop's parallel factor at the default value specified by the code (`auto{2}` and `auto{1}`).\n- Swap the decrementing loop for the backward substitution with an equivalent incrementing loop using a transformed index calculation to adhere to HLS requirements.\n- In the second inner `i` loop for the row sweep, once again turn off general optimizations at the `i` level.\n- Enable a higher level of parallelism for the `j` loop with a high parallel factor preset in the code (`auto{29}` for forward substitution and `auto{58}` for backward substitution), and standardize the decrementing loop similar to the column sweep.\n\nFinish with an end scope region after the time-stepping loop. The pragmas should be positioned directly before the loop constructs they affect.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS codes written in C with specific #pragma directives, use the following comprehensive but very concise natural language instruction:\n\n\"Define a function as a hardware kernel without any automatic optimizations for pipelining, tiling, or parallel execution, except allow the first outer loop to be flattened. All inner loops should run serially with a possible parallel reduction, and one of the inner loops should have a parallel factor of 2.\" \n\nHere are the corresponding #pragma directives for clarification, with what each directive means in this context:\n\n1. Treat the 'kernel_2mm' function as a hardware kernel for acceleration:\n   `#pragma ACCEL kernel`\n\n2. For the first outer loop:\n   - Do not apply automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n   - Do not tile this loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Do not parallelize this loop: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n3. For the second outer loop:\n   - Flatten, i.e., fully unroll or inline, if possible: `#pragma ACCEL PIPELINE auto{flatten}`\n   - Do not tile this loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Do not parallelize this loop: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n4. For the first inner loop inside the first outer loop:\n   - Do not apply automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n   - Do not tile this loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n   - Parallelize this loop with a factor of 2: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n5. For the innermost loops inside both outer loops:\n   - Apply a parallel reduction for the 'tmp' or 'D' array with a non-parallel factor: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` or `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{flatten}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_adi` as a hardware acceleration kernel, disable automatic loop pipelining, and preserve the original loop structure without parallel processing or tiling, while permitting parallelization within certain loop iterations with factors of 2 and 4, as specified. Use the following instructions:\n\n- Treat the `kernel_adi` function as a hardware kernel for acceleration.\n- For all the outer loops (`for (t = 1; t <= 40; t++)`, `for (i = 1; i < 59; i++)` in the column and row sweeps), set:\n  - Loop pipelining to off.\n  - Loop tiling with a factor of 1 (do not tile).\n  - Loop parallelization with a factor of 1 (run iterations sequentially).\n- Within the inner loops (`for (j = 1; j < 59; j++)` and the corresponding reverse loop), set the parallelization factor as follows:\n  - For the column sweep's calculation of `q[i][j]`, set a parallelization factor of 2.\n  - For the column sweep's calculation of `v[_in_j_0][i]`, keep the parallelization factor at 1.\n  - For the row sweep's calculation of `q[i][j]`, set a parallelization factor of 4.\n  - For the row sweep's calculation of `u[i][_in_j]`, set a parallelization factor of 4 again.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS C code, provide the following comprehensive yet very concise instructions:\n\n\"Encapsulate the given function as a hardware acceleration kernel, but ensure not to automatically pipeline any of the loops. Set all loops to process as a whole without tiling or parallel optimization\u2014except for the specific loop inside the column and row sweep steps, where you are to parallelize the innermost loops with respective factors of 2 and 29 for the column sweep, and 1 and 4 for the row sweep, while processing these iterations one after another.\"\n\nSpecific #pragma values, matched with their descriptions, would be as follows:\n\n- Treat the function as a hardware kernel: `#pragma ACCEL kernel`\n- Do not automatically pipeline any loop: `#pragma ACCEL PIPELINE auto{off}`\n- Keep each loop intact without tiling: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Process loop iterations sequentially, without parallelism: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\nFor the particular loops within the column and row sweep sections:\n- Parallelize the column sweep innermost loop with a factor of 2: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n- Parallelize the row sweep first innermost loop with a factor of 29: `#pragma ACCEL PARALLEL FACTOR=auto{29}`\n- And the second innermost loop with a factor of 4: `#pragma ACCEL PARALLEL FACTOR=auto{4}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS accelerator kernel for the function `kernel_2mm` that computes the 2mm algorithm without automatic pipelining, using the original loop bounds, and with parallelization only where explicitly specified. Specifically, set it up so that:\n\n1. No automatic pipelining is applied to the outer loops processing the `tmp` and `D` matrices.\n2. The loop iterations are executed using their original bounds without any loop tiling.\n3. The loop iterations are processed sequentially one after the other, except for the specified inner loops where a factor of 2 parallelism is applied for the `tmp` matrix calculation, and no parallelization is applied to inner loops that do not specify a factor.\n4. The innermost loops for computing `tmp` and `D` matrices involve a reduction operation on `tmp` and `D` which is to be processed without parallelism.\n\nTranslate into #pragma instructions as follows:\n\n- for disabling automatic pipelining: `#pragma ACCEL PIPELINE auto{off}`\n- for maintaining the original full form of loops: `#pragma ACCEL TILE FACTOR=auto{1}`\n- for running iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- for choosing where to apply parallelism:\n   - apply a parallel factor of 2 to the loop that initializes the `tmp` matrix: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n   - use no parallel factor explicit pragma for the loop that accumulates the product for the `D` matrix (default to 1 for sequential operation)\n- in loops where reduction is taking place, ensure there's a parallel pragma targeting the specific reduction on `tmp` or `D`: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` or `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`\n- for flattening nested loops into a single pipeline to increase performance: `#pragma ACCEL PIPELINE auto{flatten}`\n- don't forget to mark the function as a hardware kernel for acceleration: `#pragma ACCEL kernel`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C code for a function named 'kernel_adi' that accelerates a 2D Alternating Direction Implicit solver using a double-precision floating-point datatype. This function should include initialization of constants and multipliers and carry out two sweep processes in a timeloop of 40 iterations over a 60x60 grid. The first sweep (Column Sweep) should involve two nested loops with parallelization factor setting to consider two iterations at a time for the inner loop only. The second sweep (Row Sweep) should be similar, but the parallelization factor should be increased to optimize eight iterations simultaneously for the inner loop. In both sweeps, prevent the compiler from automatically pipelining any loop and retain the original loop structure without tiling. Hand-annotate the kernels for acceleration and ensure that the outer timeloop and both sweeps' outer loops are not parallelized or pipelined. Use '#pragma ACCEL ...' directives to customize the behavior according to these specifications, with appropriate options for kernel marking, pipeline disabling, tile factor disabling, and setting parallel factors to 1, 2, and 8 as needed.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, use the following instruction:\n\n\"Define a hardware kernel function for accelerating an alternating direction implicit (ADI) method, with variables for time steps and grid dimensions. Initialize constants inside the function and prevent any automatic optimizations or transformations for the main time-stepping loop and its nested loops, ensuring that the outermost loop runs without pipelining, individual loop iterations run in sequence (one after the other), and no loop tiling is applied. However, for specific inner loops, allow running two or four loop iterations in parallel as indicated, while the rest maintain sequential iteration.\"\n\nSpecifically, apply the following #pragma directives:\n\n- \"#pragma ACCEL kernel\" to define the hardware kernel for acceleration.\n- Set all \"#pragma ACCEL PIPELINE auto{off}\" to disable automatic pipelining.\n- Use \"#pragma ACCEL TILE FACTOR=auto{1}\" to restrict loop tiling.\n- Apply \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" to disable parallel loop iteration by default.\n- For certain loops, where parallel factor is explicitly set to 2 or 4 with \"#pragma ACCEL PARALLEL FACTOR=auto{2}\" or \"#pragma ACCEL PARALLEL FACTOR=auto{4}\", enable two or four loop iterations to run in parallel respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel for acceleration from a given C function for an Alternating Direction Implicit (ADI) computation, ensuring that the main time iteration loop, as well as the column sweep and row sweep nested loops, are not automatically pipelined, tiled, or parallelized, but allow manual overwriting of the parallelism factor for specific inner loops where one doubles the factor and another increases it significantly to the maximum number of iterations minus one.\n\n- For the hardware kernel, use '#pragma ACCEL kernel'.\n- For the main time iteration loop and the first level of nested loops within both column and row sweeps, use:\n  - '#pragma ACCEL PIPELINE auto{off}' to prevent automatic pipelining.\n  - '#pragma ACCEL TILE FACTOR=auto{1}' to avoid tiling the loops into smaller chunks.\n  - '#pragma ACCEL PARALLEL FACTOR=auto{1}' to ensure iterations are executed serially.\n- In the deeper nested loop within the column sweep, double the default parallelism factor with '#pragma ACCEL PARALLEL FACTOR=auto{2}'.\n- In the first nested loop within the row sweep, increase the parallelism factor fourfold with '#pragma ACCEL PARALLEL FACTOR=auto{4}'.\n- In the deeper nested loop within the row sweep, use '#pragma ACCEL PARALLEL FACTOR=auto{58}' to parallelize this loop almost completely, reflecting the loop's bounds of 57 iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following sequence of instructions into HLS #pragma directives for the function `kernel_adi`:\n\n1. Treat the `kernel_adi` function as a hardware-accelerated kernel.\n2. For all loops within the function, disable automatic pipelining, maintaining the original execution order without breaking them into smaller pipelined chunks.\n3. In the nested loop that calculates the `p` and `q` arrays for the column sweep, enable parallel processing for the inner loop with a parallel factor of 32, while the rest of the loops should not be parallelized.\n4. For all loops, do not split them into tiles, i.e., process them in their original, full form.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a kernel function performing matrix multiplication and addition that treats the entire function as a hardware acceleration kernel. For all loops, prevent automatic pipelining, run the loop iterations sequentially without breaking them into smaller chunks (tile factor of 1), and process the outermost loops without parallelism (parallel factor of 1). In the first nested loop, allow a parallel factor of 2 for the second level loop. In both nested loops, apply a reduction to the temporary arrays with the maximum possible parallel factor, which is equal to the size of the innermost loop index. Specifically for the 'tmp' array, use a factor of 70, and for the 'D' array, use a factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Implement a hardware kernel function named 'kernel_atax' to perform matrix-vector multiplication with two stages of computation, while optimizing for parallel execution and data tiling. In the first section of the loop where 'y' is initially zeroed out, enable automatic pipelining. For the next loop over variable 'i', use automatic tiling to break the loop into chunks for better parallel execution, and enable automatic parallel processing within the loop. Inside this loop, apply parallel reduction with an unspecified optimized factor to derive temporary values in 'tmp', and then perform another parallel reduction on 'y' with maximum parallelism, allowing the tool to determine the optimal factor for acceleration.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{410}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS code in C with the appropriate pragmas, use the following comprehensive directive:\n\n\"Define a function 'kernel_atax' to be optimized for hardware acceleration. In this function, disable automatic pipelining for the outermost initialization loop of array 'y'. For the first computational loop over 'tmp', tile it with a factor of 4 and parallelize it by a factor of 26 without specifying actual values, letting the HLS tool choose the best factors. Inside this loop, further parallelize the nested loop with reduction on 'tmp' using a parallel factor that the HLS tool will automatically determine, aiming for a factor of 32. Finally, parallelize the second nested loop with reduction on 'y' by a factor of 2 that the tool will adjust automatically.\"\n\nThe corresponding HLS directives in natural language for each #pragma are:\n\n1. '#pragma ACCEL kernel' - Treat this function as a specialized hardware computation unit.\n2. '#pragma ACCEL PIPELINE auto{off}' - Do not apply automatic pipelining optimization to the subsequent loop.\n3. '#pragma ACCEL TILE FACTOR=auto{4}' - Partition the forthcoming loop into smaller tiled blocks where the tool decides the optimal number of iterations per tile, aiming for a tile size of 4.\n4. '#pragma ACCEL PARALLEL FACTOR=auto{26}' - Execute iterations of the next loop in parallel groups where the HLS tool automatically selects the grouping, targeting a group size of 26.\n5. '#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}' - Perform the following loop's iterations in parallel while combining their results into the 'tmp' variable, with the tool choosing an appropriate factor close to 32.\n6. '#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}' - Enable parallel execution of the last loop's iterations with results accumulating in 'y', instructing the tool to pick a parallelization factor with a suggested value of 2.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_atax(double A[390][410],double x[410],double y[410],double tmp[390])\n{\n  int i;\n  int j;\n  for (i = 0; i < 410; i++) \n    y[i] = ((double )0);\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{4}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{26}\n  for (i = 0; i < 390; i++) {\n    tmp[i] = 0.0;\n    \n#pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{32}\n    for (j = 0; j < 410; j++) {\n      tmp[i] += A[i][j] * x[j];\n    }\n    \n#pragma ACCEL PARALLEL reduction=y FACTOR=auto{2}\n    for (j = 0; j < 410; j++) {\n      y[j] += A[i][j] * tmp[i];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a dedicated hardware acceleration module, but ensure that none of the loops within are automatically pipelined, tiled, or parallelized, except for two specific loops where one will process 32 iterations simultaneously, and the other processes 8 iterations simultaneously; for all the other loops, enforce serial execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function `kernel_adi` as a designated hardware acceleration kernel, and within its scope, disable automation of pipeline optimization, restrict the loop from being subdivided into smaller sections, and execute iterations sequentially \u2013 not in parallel, for all loops by default. Specifically, apply these settings to the top-level time step loop and the initial loops of column and row sweeps. For the nested j-loops in the column sweep, set the iterations to run in pairs, while keeping iterations sequential in the first j-loop of the row sweep. However, for the second j-loop in the row sweep, group the iterations to process four at a time.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2mm kernel function with specific optimizations by following these comprehensive but concise natural language instructions:\n\n1. Designate the function 'kernel_2mm' that takes parameters for matrix dimensions, two scalar values 'alpha' and 'beta', and matrices 'tmp', 'A', 'B', 'C', and 'D', as a hardware kernel for acceleration.\n\n2. For the outermost loop in the function (loop over 'i'), the directive is to suppress automatic pipelining.\n\n3. Inside this loop, do not automatically tile or unroll the subsequent loop (over 'j'), and process the 'j' loop iterations sequentially.\n\n4. In the innermost loop (over 'k'), flatten the control flow for better parallelism but enforce reduction on the 'tmp' matrix without unrolling.\n\n5. Then, for the second occurrence of the outer loop over 'i', allow default pipelining and unrolling optimizations.\n\n6. For the mid-level loop over 'j' within this second 'i' loop, disable pipelining, but define a tile size to break the loop body into chunks of size 8.\n\n7. Handle the nested loop over 'k', targeting the 'D' matrix, by specifying the parallel factor to partially unroll this loop five times and enforce reduction on 'D'.\n\nTranslate these natural language instructions into HLS code using the following corresponding #pragma directives:\n\n- Treat 'kernel_2mm' as a hardware kernel: `#pragma ACCEL kernel`\n- Suppress automatic pipelining for a loop: `#pragma ACCEL PIPELINE auto{off}`\n- Do not tile the loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n- Run iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n- Flatten the control flow: `#pragma ACCEL PIPELINE auto{flatten}`\n- Enable default pipelining and unrolling: Use `#pragma ACCEL PIPELINE auto{}` without specifying 'off'\n- Define a tile size of 8: `#pragma ACCEL TILE FACTOR=auto{8}`\n- Partially unroll the loop: `#pragma ACCEL PARALLEL FACTOR=auto{5}`\n- Apply reduction to 'tmp' and 'D' matrices: Use `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}`, respectively.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function with specific optimization pragmas:\n\n1. Declare the function as a hardware acceleration kernel: '#pragma ACCEL kernel'.\n2. Within the function, initially disable automatic pipelining for the outermost loop: '#pragma ACCEL PIPELINE auto{off}'.\n3. Prevent the first loop from being tiled: '#pragma ACCEL TILE FACTOR=auto{1}'.\n4. Ensure the first loop runs sequentially without parallelization: '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n5. Enable automatic pipelining with flattening for the second loop to merge nested loops where beneficial: '#pragma ACCEL PIPELINE auto{flatten}'.\n6. Similarly, don't tile the second loop: '#pragma ACCEL TILE FACTOR=auto{1}'.\n7. For the innermost loop of the first part, perform reductions on 'tmp' with the original loop structure preserved and minimal parallel execution of loop iterations: '#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}'.\n8. After the first main loop, enable default automatic pipelining for the next loop: '#pragma ACCEL PIPELINE auto{}'.\n9. As before, ensure the loop isn't tiled: '#pragma ACCEL TILE FACTOR=auto{1}'.\n10. Likewise, keep the loop running sequentially: '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n11. In the following loop, allow for default pipelining strategies: '#pragma ACCEL PIPELINE auto{}'.\n12. Tile the next inner loop with a factor of 8 to split it into smaller chunks if found beneficial by HLS: '#pragma ACCEL TILE FACTOR=auto{8}'.\n13. Again, run the loop iterations sequentially: '#pragma ACCEL PARALLEL FACTOR=auto{1}'.\n14. For the innermost loop of the second part, perform reductions on 'D' with significant parallel execution of loop iterations: '#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}'.\n\nEnsure that all these optimizations are applied to the respective loops as indicated in the provided code.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Transform the given 2D matrix multiply-and-add function into a hardware-accelerated kernel without any compiler-initiated optimization of loop pipelining, tiling, or parallel execution. Specifically, do the following:\n\n1. Declare the entire function as a kernel for hardware acceleration.\n2. In the first pair of nested loops, disable automatic loop pipelining and maintain the original loop structure without tiling or parallel processing.\n3. Manually enable loop pipelining within the innermost loop of the first nested loops, performing a parallel reduction on the `tmp` array with 70 iterations in parallel.\n4. In the second pair of nested loops, disable pipeline optimization but streamline the pipeline process without changing the loop structure or parallelization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the 'kernel_2mm' function as a dedicated hardware acceleration kernel, disable automatic pipelining and tiling across the entire function, but retain the default parallelization strategy. Inside the first loop, enable default pipelining and specify a single iteration stride for tiling while doubling parallelized loop iterations for the 'j' loop. For the innermost 'k' loop, enable parallel reduction for the 'tmp' array with full unrolling. In the second main loop, disable pipelining and override automatic loop flattening for the 'i' loop, manually tile the 'j' loop with an iteration stride matching its bound, and use default parallelization except for the innermost 'k' loop, where parallel reduction for the 'D' array is explicitly stated with no unrolling.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate a function as a hardware acceleration kernel, avoiding loop pipelining, maintaining original loop constructs without tiling or parallel processing, except for specific inner loop sections that should be parallel processed with a factor of 32 for the column sweep and 8 for the row sweep.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a hardware acceleration kernel. Within the main block of matrix multiplication and addition, inhibit pipeline optimization for the outermost loops. Preserve the full form of these loops, and ensure they run sequentially rather than in parallel. Inside the first nested loop block, apply default pipeline optimization, keep the loop in its entirety, and allow four iterations to run concurrently with a possibility of local accumulation. Specifically apply a reduction on the 'tmp' array allowing two iterations to overlap. In the final nested loop, flatten the pipeline optimization, maintain the loop's original form, and significantly parallelize the workload by allowing twenty iterations to happen simultaneously with no overlapping on the 'D' array iterations. Here are the specific pragma directives to include:\n\n- Treat the 'kernel_2mm' function as a hardware acceleration kernel with `#pragma ACCEL kernel`.\n- For the initial outermost loops for matrix operations, use these pragmas:\n  - Disable automatic pipelining using `#pragma ACCEL PIPELINE auto{off}`.\n  - Prevent the loop from being divided into smaller chunks using `#pragma ACCEL TILE FACTOR=auto{1}`.\n  - Run the iterations sequentially with `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Inside the first level of nested loops, apply these pragmas:\n  - Enable default pipeline optimization using `#pragma ACCEL PIPELINE auto{}`.\n  - Keep the loop in its full form using `#pragma ACCEL TILE FACTOR=auto{1}`.\n  - Allow four parallel iterations using `#pragma ACCEL PARALLEL FACTOR=auto{4}`.\n  - For the reduction on the 'tmp' array, overlap two iterations with `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}`.\n- In the last nested loop for the final accumulation in array 'D', include these pragmas:\n  - Flatten the pipeline with `#pragma ACCEL PIPELINE auto{flatten}`.\n  - Avoid breaking up the loop with `#pragma ACCEL TILE FACTOR=auto{1}`.\n  - Parallellize concretely with `#pragma ACCEL PARALLEL FACTOR=auto{20}`.\n  - Ensure that the iterations for the 'D' array do not overlap using `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Declare the function 'kernel_2mm' as a hardware acceleration kernel, and for each nested loop structure within this kernel, do not pipeline or parallelize the outer loops, but for the inner loops, selectively apply parallelism with specific factors or inhibit it, as indicated:\n\n1. Designate 'kernel_2mm', which performs a matrix multiplication and addition operation following the equation `D := alpha * A * B * C + beta * D`, as a kernel for hardware acceleration, and avoid automatic pipelining and chunking of loops at all levels of nesting.\n\n2. In the first nested loop block where 'tmp' matrix is calculated:\n\n   - Allow for parallel computation on the innermost loop for calculating the 'tmp' matrix with a parallel factor that enables 14 concurrent operations while ensuring reduction operations on 'tmp' are correctly handled.\n\n3. In the second nested loop block where the 'D' matrix is updated:\n\n   - Fully serialize the operations for the first two nested loops without applying any pipelining or parallelization.\n   \n   - On the second innermost loop, enable parallel execution with the maximal factor possible for the hardware architecture, which for this specific code implies a factor of 80.\n   \n   - On the innermost loop for updating the 'D' matrix, apply a parallelism factor that permits 5 concurrent operations, while addressing any associated reduction operations on 'D'.\n\nNote that `auto{value}` is a placeholder indicating that the actual value is to be automatically determined or that a particular setting is to be deduced directly from the code or design constraints. In this case, `auto{off}` and `auto{1}` disable pipelining and inhibit parallelization or tiling at that specific loop level, while `auto{14}`, `auto{80}`, and `auto{5}` define explicit parallel factors for the associated loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{80}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{5}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' handling matrix multiplication and combination as a hardware acceleration kernel, ensuring that the outermost loops iterate serially without optimization and the inner loops make use of different levels of parallelism and pipelining as per the specified pragma configurations.\n\nIn particular, for the 'kernel_2mm' function using double precision floating-point arithmetic:\n1. Do not employ pipelining optimization or loop tiling for the first loop level and ensure that iterations are carried out serially.\n2. For the second loop level, automatically determine the potential for pipelining, do not tile the loop, and double the default parallel processing factor.\n3. Within the innermost loop of the first double-nested loop section, apply a parallel reduction to the 'tmp' array with the iterations running one at a time.\n4. Similarly, for the last loop level, automatically identify pipelining possibilities, apply a tiling factor of eight, and set a parallel processing factor of one.\n5. For the innermost loop of the last double-nested loop section, apply a parallel reduction to the 'D' array but maximize parallelism with a factor of fifty, effectively allowing concurrent updates to the 'D' array.\n\nMake sure to denote the calculation being performed: 'D' is updated with the matrix multiplication of 'A', 'B', and 'C' scaled by 'alpha', added to 'D' itself scaled by 'beta'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for accelerating a 2mm kernel computation with the following specifications:\n\n1. Define the 2mm kernel function as a hardware acceleration target.\n2. In the first loop nest that computes the product of matrices A and B:\n   - Do not automatically pipeline the outer loop.\n   - Process the outer loop in its original form without any tiling optimizations.\n   - Execute the iterations of the outer loop sequentially, without any parallelization.\n   - For the inner loop, apply pipelining with default settings.\n   - The inner loop should not be tiled but should execute iterations in two parallel threads, with the partial products for each entry of the 'tmp' matrix accumulated in a reduction manner across all threads, specifically tailored for 70 iterations.\n\n3. In the second loop nest, where matrix D is updated:\n   - Apply pipelining with default settings for the outer loop, but flatten the loop structure if applicable.\n   - Tile the outer loop with eight iterations per chunk.\n   - Run ten iterations of the outer loop in parallel.\n   - In the inner loop, keep D's updates sequential and perform a reduction operation for each entry of the 'D' matrix using the computed 'tmp' matrix and matrix C.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a C function named `kernel_2mm` that performs a 2mm operation on matrices `A`, `B`, `C`, and `D` with loop indexes `i`, `j`, `k`, and scaling factors `alpha` and `beta`. Designate the function as a hardware kernel for acceleration but prevent automated pipelining, tiling, and parallelization for the outermost loops, requiring them to be executed exactly as written. Within the nested loops, disable pipelining for the `i` loop but set the parallel factor of the `j` loop within the first double-nested loop to 2, and enable a high degree of parallel reduction on the `k` loop with a factor of 32. For the second double-nested loop, enable loop flattening to optimize pipeline execution, but force the `j` loop to be run sequentially, and maintain a parallel reduction on the `k` loop with a factor of 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_2mm' function as a hardware acceleration kernel and direct the High-Level Synthesis (HLS) tool to avoid automatically pipelining the outermost loops. Internally within the loop structures, activate default pipelining for certain loops, retain others in their original form, and specify parallelism levels.\n\nSpecifically, for the outermost loops iterating over indices 'i' and 'j', do the following:\n\n1. Do not automatically pipeline the loop iterating over 'i' for the first matrix operation, but pipeline the nested loops normally, unless indicated otherwise.\n2. Maintain the original full-loop form for each outer 'i' loop by setting the tiling factor to 1.\n3. Execute the iterations of each outer 'i' loop sequentially with a parallelism factor of 1.\n\nWithin the inner matrix multiplication loop associated with 'tmp':\n\n1. Pipeline the loop iterating over 'j' and apply parallelization with a factor of 2 for the iterations.\n2. Use an unrolled loop with a complete unroll factor to accumulate the matrix product into 'tmp,' where the summation reduction is applied over 'tmp'.\n\nFor the final matrix accumulation into 'D':\n\n1. Activate pipelining for the loop iterating over 'i', while opting for a loop flattening strategy.\n2. Set a tiling factor of 80 for the loop iterating over 'j', optimizing memory access patterns.\n3. Maintain a sequential execution for the innermost loop with a parallelism factor of 1, while designating 'D' as the reduction variable, allowing the HLS tool to optimize data access and storage accordingly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for an accelerated alternating direction implicit (ADI) solver with specific optimization directives. The solver should perform calculations of the ADI method for a grid of size 60x60 for a given number of timesteps. Initialize the variables `DX`, `DY`, `DT`, `B1`, `B2`, `mul1`, and `mul2` with appropriate constants derived from given values. In the main loop, perform column and row sweeps to update the grid values `v` and `u` using the Thomas algorithm. The code should contain the following pragma directives:\n\n1. Use `#pragma ACCEL kernel` to indicate that the main function should be compiled as a hardware kernel.\n2. Selectively disable loop pipelining with `#pragma ACCEL PIPELINE auto{off}` to prevent the automatic pipelining of loops.\n3. Preserve the natural loop dimensions without tiling by using `#pragma ACCEL TILE FACTOR=auto{1}` to avoid loop nest tiling optimizations.\n4. Configure the loops to execute iterations sequentially rather than in parallel using `#pragma ACCEL PARALLEL FACTOR=auto{1}`. \n5. Apply moderate parallelism to specific loops by setting `#pragma ACCEL PARALLEL FACTOR=auto{2}` or higher factor values where appropriate, such as `#pragma ACCEL PARALLEL FACTOR=auto{8}` for the final loop in the row sweep, to enable parallel processing of iterations while preserving the dependencies within the Thomas algorithm.\n\nInclude necessary initializations, boundary conditions, and Thomas algorithm calculations respecting the mathematical dependencies in the ADI method. The use of advanced pragmas suggests optimization for hardware targets without disturbing algorithmic integrity. Ensure the grid boundaries are maintained correctly, and the loop transformations standardize reverse iteration indexes into positive strides for hardware compatibility.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS codes written in C with the specified pragmas, use the following comprehensive but concise natural language instruction:\n\n\"Define a hardware kernel function named 'kernel_3mm' that performs three matrix multiplications (E=A*B, F=C*D, G=E*F) on matrices with fixed dimensions, explicitly specifying there should be no automatic optimizations. For the main loops that compute the matrix multiplications, disable automatic pipelining, avoid tiling and parallelization optimizations by maintaining the original full iteration space, and run each loop iteration sequentially. However, for the innermost loop in the computation of matrix G, allow for a parallelism factor of 2.\" \n\nSpecifically, you will:\n\n- Treat the 'kernel_3mm' function as a hardware acceleration kernel.\n- For all main loops and the first-level nested loops:\n  - Use '#pragma ACCEL PIPELINE auto{off}' to ensure the loops are not automatically pipelined.\n  - Use '#pragma ACCEL TILE FACTOR=auto{1}' to avoid tiling and thus maintain the original iteration space of the loops.\n  - Use '#pragma ACCEL PARALLEL FACTOR=auto{1}' to run the loop iterations in series, without introducing parallelism in the loops.\n- Additionally, for the innermost loop in the computation of matrix G:\n  - Use '#pragma ACCEL PARALLEL FACTOR=auto{2}' to introduce a parallelism factor of 2, allowing two iterations of the loop to be executed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the provided C code with HLS pragmas into natural language instructions as follows:\n\n1. Designate the function `kernel_2mm` as a hardware acceleration kernel.\n2. For the first top-level loop:\n   - Do not pipeline the loop; execute as is.\n   - Do not split the loop into smaller chunks; maintain original loop size.\n   - Execute iterations serially, not in parallel.\n3. Inside the first top-level loop, for the nested loop:\n   - Completely unroll the loop if able to optimize for full parallelism.\n   - Do not split the nested loop into smaller chunks.\n   - Execute every pair of iterations in parallel.\n4. For the reduction loop inside the nested loop:\n   - Perform the reduction on `tmp` array without parallelization; handle one element at a time.\n5. For the second top-level loop:\n   - Allow the tool to decide on pipelining.\n   - Do not split the second top-level loop into smaller chunks.\n   - Execute iterations serially, not in parallel.\n6. Inside the second top-level loop, for the nested loop:\n   - Completely unroll the loop if able to optimize for full parallelism.\n   - Split the nested loop into chunks, each chunk dealing with 80 iterations (the full dimension).\n   - Execute iterations serially, not in parallel.\n7. For the reduction loop inside the second nested loop:\n   - Perform the reduction on the `D` array without parallelization; handle one element at a time.\n\nSpecific `#pragma` values to use are:\n\n- `#pragma ACCEL kernel` to treat the function as a hardware kernel.\n- `#pragma ACCEL PIPELINE auto{off}` to disable automatic loop pipelining.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to maintain the original loop form.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to execute loop iterations serially.\n- `#pragma ACCEL PIPELINE auto{flatten}` to optimize loops for full parallelism by loop flattening or unrolling.\n- `#pragma ACCEL PARALLEL reduction = variable FACTOR=auto{1}` to specify the variable (`tmp` or `D`) on which to perform serial reduction.\n- `#pragma ACCEL TILE FACTOR=auto{80}` to specify tiling with a factor of 80 for the second nested loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for alternating-direction implicit (ADI) algorithm applied to a 2D heat conduction problem. Use data types for variables consistent with double-precision floating-point calculations. Designate the function as a hardware acceleration kernel. Do not allow automatic pipelining, tiling, or parallelization for the overall time-step loop. Apply the same restrictions to the column and row sweep sub-loops, ensuring sequential processing to preserve dependencies. However, within the inner loops that update the 'p' and 'q' auxiliary arrays, allow parallel processing: for the column updates, permit simultaneous execution of 32 iterations, and for the row updates before final results calculation, allow 4 simultaneous iterations, increasing to 8 for the final calculation. Use index transformation for reverse iterations to maintain loop standardization for compatibility with HLS tools.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for an ADI kernel function that performs computations on 2D arrays representing a grid, utilizing specific pragmas to control hardware acceleration characteristics. Define the function with parameters for time steps and grid size, initialize domain-specific constants and temporary variables. Ensure loops are not automatically pipelined, maintain original loop iteration order without tiling or parallelization, except for two inner loops where one should have a parallel factor of 2 and another a parallel factor of 4. For the outermost loop, flatten the pipeline pragma. Use variables with underscore-prefixed index transformation for reversed loop indexing to uphold the original loop's logic. Apply kernel acceleration pragma to the entire function.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS code, you can use this comprehensive yet concise natural language instruction:\n\n\"Create an HLS C kernel function called 'kernel_adi' to run an Alternating Direction Implicit (ADI) solver for 2D heat distribution, handling a 60x60 grid, with double precision floating-point arrays for 'u', 'v', 'p', and 'q'. Consider time steps 'tsteps' and the space size 'n'. Initialize diffusion equations constants inside the kernel. Disable automatic pipelining, tiling, and parallelization for the outer loops and use a parallel factor of 2 for inner loops without reductions. However, for the innermost loop with the reverse iteration for both column and row sweeps, apply a parallel factor of 58 to the reverse iteration, keeping the loop in its complete original form without pipelining or tiling. Additionally, ensure all loops process iterations sequentially.\"\n\nSpecific pragma values are:\n\n- Treat the 'kernel_adi' function as a hardware kernel with `#pragma ACCEL kernel`.\n- For all loops where it states `auto{off}`, use: `#pragma ACCEL PIPELINE off`.\n- For loops with `auto{1}`, enforce: `#pragma ACCEL TILE FACTOR=1` and `#pragma ACCEL PARALLEL FACTOR=1`, effectively disabling automatic loop optimization features.\n- For loops with `auto{2}`, set: `#pragma ACCEL PARALLEL FACTOR=2`, enabling parallel processing of iterations with a factor of two.\n- For the particular reverse iteration loop with `auto{58}`, use: `#pragma ACCEL PARALLEL FACTOR=58` to specify that 58 iterations can be computed in parallel.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the following function as a hardware acceleration kernel. For the first and second major loops, ensure that there is no loop pipelining, execute the loop as a single chunk without tiling, and execute iterations sequentially without parallelization. In the first major loop's nested loop, still prevent loop pipelining, manually set the tile factor to 1, and explicitly specify a parallelism factor of 2. For reduction in the innermost nested loop, allow a parallel factor of 4 with respect to the 'tmp' array. For the second major loop, enable automatic loop pipelining, set the tile factor to the default, and ensure iterations are processed sequentially without explicit parallelization. In the nested loop, prevent loop pipelining, tile the loop with a factor of 2, and process iterations sequentially. Finally, in the innermost loop of the second major loop, allow a high degree of parallelism with a parallel factor of 32 pertaining to the 'D' array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Mark the function 'kernel_adi' as a target for hardware acceleration, ensure every loop runs sequentially without using automatic pipelining, unrolling, or tiling optimizations, except for the specific loops within 'Column Sweep' and 'Row Sweep' for which you should implement unrolling with a factor of 2 for the innermost loop in 'Column Sweep', and unrolling with factors of 29 and 16 for the two innermost loops in 'Row Sweep'.\n\nHere are the relevant directives in natural language for the associated code:\n\n1. \"#pragma ACCEL kernel\": Treat the function 'kernel_adi' as a hardware acceleration kernel.\n2. For all loops by default:\n   - \"#pragma ACCEL PIPELINE off\": Don't automatically pipeline any loop.\n   - \"#pragma ACCEL TILE FACTOR=1\": Process loops in their original full form without tiling.\n   - \"#pragma ACCEL PARALLEL FACTOR=1\": Run all loop iterations sequentially, without parallel processing.\n3. For the specific loops that require different settings:\n   - Inside 'Column Sweep', for the innermost j-loop:\n     - \"#pragma ACCEL PARALLEL FACTOR=2\": Unroll the loop with a factor of 2 to partially parallelize the iterations.\n   - Inside 'Row Sweep', for the first j-loop:\n     - \"#pragma ACCEL PARALLEL FACTOR=29\": Unroll this loop with a factor of 29 to partially parallelize the iterations.\n   - Inside 'Row Sweep', for the second j-loop after reversing the loop order:\n     - \"#pragma ACCEL PARALLEL FACTOR=16\": Unroll this loop with a factor of 16 to partially parallelize the iterations.\n\nGiven these instructions, set the #pragma values as follows:\n\n1. Above the function declaration:\n   - `#pragma ACCEL kernel`\n   \n2. At the start of any loop structure:\n   - `#pragma ACCEL PIPELINE auto{off}`\n   - `#pragma ACCEL TILE FACTOR=auto{1}`\n   - `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n3. For the specific loops with unrolling:\n   - Inside 'Column Sweep':\n     - Before the innermost j-loop: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n   - Inside 'Row Sweep':\n     - Before the first j-loop: `#pragma ACCEL PARALLEL FACTOR=auto{29}`\n     - Before the second j-loop after reversing the loop order: `#pragma ACCEL PARALLEL FACTOR=auto{16}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, instruct the HLS tool with the following descriptions:\n\n1. Specify that the `kernel_adi` function should be treated as a hardware kernel for acceleration by saying \"Declare the function as a hardware accelerator kernel.\"\n\n2. For all loops, indicate that they should be run in their full form without pipelining, without tiling (i.e., not subdivided) and in a sequential manner (with a parallel factor of 1) by saying \"For each loop, disable automatic pipelining, do not subdivide the iterations and ensure they run sequentially.\"\n\n3. Specifically for the innermost loop where index `j` is used, parallelize the computation with a parallel factor of 32 during the column sweep and use a parallel factor of 2 and 4 during the row sweep by saying \"For the innermost `j` loop in the column sweep, enable parallelism with a factor of 32, and in the row sweep, apply a parallel factor of 2 in the first inner loop and 4 in the second inner loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the given function as a hardware acceleration kernel, and enforce different parallelization and pipelining constraints on the nested loops to ensure there is no automatic optimization applied to the outer loops (both the time step loop and the column and row sweep loops), while allowing for a parallel factor of 32 on the inner parallel loop for column updates, and a parallel factor of 4 on the reverse sequential inner loop for row updates.\n\nSpecifically:\n- Use 'Treat the following function as a hardware kernel for acceleration' to specify `#pragma ACCEL kernel` at the beginning of the function.\n- Use 'Don't automatically pipeline this loop' to indicate each usage of `#pragma ACCEL PIPELINE auto{off}` for the outer loops including time step, column sweep, and row sweep.\n- Use 'Process this loop in its original, full form without breaking it into smaller chunks' to show `#pragma ACCEL TILE FACTOR=auto{1}` for the same outer loops.\n- Use 'Run the iterations of this loop one after the other, not in parallel' to denote `#pragma ACCEL PARALLEL FACTOR=auto{1}` for those outer loops as well.\n- For the inner loop of the column sweep where the parallel factor is 32, depict it with 'Allow this loop to compute 32 iterations in parallel' corresponding to `#pragma ACCEL PARALLEL FACTOR=auto{32}`.\n- For the reverse sequential inner loop of the row sweep with a parallel factor of 4, symbolize this with 'Execute four iterations of this loop in parallel' meaning `#pragma ACCEL PARALLEL FACTOR=auto{4}`.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' to be synthesized into a hardware kernel, suppress automatic pipelining for the outer loop calculations, carry out the inner loop iterations for computing 'tmp' sequentially without concurrent execution, and execute the 'tmp' reduction with a slight concurrency of two iterations at a time. Then, allowing automatic pipelining for the calculation of the 'D' matrix, perform the subsequent outermost loop with unaltered iterations, but for the inner data-dependent loop calculations of 'D', enforce an increased concurrency of thirty-two iterations at a time. \n\nHLS directives in natural language for the provided C code:\n\n- Treat 'kernel_2mm' function as a hardware acceleration kernel: `#pragma ACCEL kernel`\n- Prevent automatic loop pipelining for the first outer loop: `#pragma ACCEL PIPELINE \"off\"`\n- Execute the first outermost loop without tiling: `#pragma ACCEL TILE FACTOR=1`\n- Process the first outermost loop iterations sequentially without parallelism: `#pragma ACCEL PARALLEL FACTOR=1`\n- Suppress pipelining for the secondary outer loop: `#pragma ACCEL PIPELINE \"off\"`\n- Do not tile the secondary outer loop: `#pragma ACCEL TILE FACTOR=1`\n- Implement pair-wise parallel execution in the secondary outer loop: `#pragma ACCEL PARALLEL FACTOR=2`\n- Enable concurrent execution of two iterations during the reduction of the 'tmp' array: `#pragma ACCEL PARALLEL reduction=tmp FACTOR=2`\n- Allow automatic decisions on pipelining for the following loops involved in computing the 'D' matrix: `#pragma ACCEL PIPELINE auto{}`\n- Perform the third outer loop without tiling: `#pragma ACCEL TILE FACTOR=1`\n- Keep the sequential execution of the third outermost loop: `#pragma ACCEL PARALLEL FACTOR=1`\n- Allow automatic pipelining within the final outer loop: `#pragma ACCEL PIPELINE auto{}`\n- Increase concurrency to two iterations at a time for the next level of looping: `#pragma ACCEL TILE FACTOR=2`\n- Preserve the sequential execution of the next level's inner loop: `#pragma ACCEL PARALLEL FACTOR=1`\n- During the reduction of the 'D' array, enable a large parallelization factor for efficiency: `#pragma ACCEL PARALLEL reduction=D FACTOR=32`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code with appropriately described pragmas, issue the following comprehensive but concise instruction:\n\n\"Generate a function named 'kernel_adi' to perform the Alternating Direction Implicit (ADI) method for numerical solving partial differential equations, designated as a hardware kernel for acceleration. Within this kernel, disable loop pipelining and tiling optimizations and prevent parallel execution in the outermost loop running time steps; this loop should execute with only one iteration at a time in its original form. For the first inner loop, where a tridiagonal system is solved column-wise, similarly disable pipelining and tiling while setting parallel execution factor to 32 for the j loop, which fills the 'p' and 'q' matrices. In the second j loop, iteratively compute 'v' in reverse, sequentially. For the second inner loop, performing a row-wise sweep, again turn off pipelining and tiling but set the parallel factor to 4 for the j loop solving another tridiagonal system. As before, calculate 'u' in reverse order serially in the succeeding j loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Treat the function 'kernel_2mm' as a hardware kernel for acceleration. For the outermost loops iterating over 'i', disable automatic pipelining, maintain the loops in their full form without tiling, and execute iterations sequentially. Inside the first 'i' loop, enable automatic pipelining for the 'j' loop, keep it non-tiled, but allow two iterations of the 'j' loop to be executed in parallel. Apply parallel reduction with a dynamic factor for the 'k' loop when accumulating 'tmp[i][j]', with an emphasis on the 'tmp' array. For the second 'i' loop, disable pipelining again, leave the loop untiled, and run iterations sequentially. Within this loop, flatten the pipeline for the 'j' loop, maintain it non-tiled, but enable ten iterations to execute in parallel. For innermost 'k' loop updating 'D[i][j]', allow reduction on 'D' and execute iterations sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Convert the provided alternating direction implicit (ADI) time stepping kernel into a high-level synthesis (HLS) C code with the following directives:\n\n1. Treat the entire `kernel_adi` function as an acceleration kernel to harness dedicated hardware resources for its computation.\n2. Prevent automatic pipelining of the main time stepping loop (`for (t = 1; t <= 40; t++)`) and its nested loops (`for (i = 1; i < 59; i++)` and `for (j = 1; j < 59; j++)`) for the column and row sweeps to maintain the loop's original sequential processing order.\n3. Keep the loops in their full iteration space, preventing the HLS tool from applying loop tiling optimizations that could divide the loops into smaller chunks.\n4. Execute iterations of the loops sequentially, avoid data-level parallelization to ensure that each loop iteration completes before the next begins.\n\nFor the loop that traverses the array backwards within both the column and row sweep sections, the directive `#pragma ACCEL PARALLEL FACTOR=auto{1}` is used to enforce sequential execution. Conversely, utilize `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the forward-pass of the column sweep loop to suggest a parallelization factor of 2, indicating modest parallel execution. Similarly, suggest higher parallelization for the forward-pass of the row sweep loop with `#pragma ACCEL PARALLEL FACTOR=auto{16}` and for the backward pass with `#pragma ACCEL PARALLEL FACTOR=auto{32}`.\n\nSpecifically, the following `#pragma` directives should be applied in the HLS code:\n- `#pragma ACCEL kernel` for denoting the `kernel_adi` function as an acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` to signal the HLS tool not to perform automatic pipelining on the immediate subsequent loop.\n- `#pragma ACCEL TILE FACTOR=auto{1}` to keep loops un-tiled.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` to enforce serial loop iteration.\n- `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the column forward pass loop, hinting at a parallelization factor of 2.\n- `#pragma ACCEL PARALLEL FACTOR=auto{16}` for the row forward pass loop, suggesting a parallelization factor of 16.\n- `#pragma ACCEL PARALLEL FACTOR=auto{32}` for the backward passes of both sweeps, indicating aggressive parallel execution with a factor of 32.\n\nBy specifying these directives, the HLS tool will be informed on how to treat and optimize each segment of the code for high-level synthesis into a hardware-accelerated system.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code that defines a hardware-accelerated kernel function `kernel_adi` for solving a numerical problem with 60x60 matrices `u`, `v`, `p`, and `q` over `tsteps`. Disable automatic loop pipelining and tiling while ensuring all outer loops run without parallelism by setting the following pragmas:\n\n- Treat the entire function `kernel_adi` as a hardware acceleration kernel.\n- Do not automatically pipeline any of the loops; all pipeline pragmas should be turned off.\n- Maintain the original loop form without any automatic loop tiling; set all tile factors to `1`.\n- Run all outer loops sequentially with no parallelism; set all parallel factors to `1`.\n\nFor inner loops that allow some parallelism:\n\n- Set a parallel factor of `32` for the first inner parallel loop where `j` goes from `1` to `58`.\n- Set a parallel factor of `4` for the second parallel inner loop where `j` again goes from `1` to `58`.\n- Set a parallel factor of `2` for the final inner parallel loop with a countdown index running from `58` down to `1`.\n\nAdditionally, standardize any reversed loops by introducing a transformed index that counts down, ensuring that any unspecified parallel factors within inner loops default to `1`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a 2mm kernel function by designating it for hardware acceleration, without pipelining the top-level loops, maintaining their original structure, and executing their iterations serially. For the first nested loop block, allow the innermost loop to compute reductions in parallel with an unspecified factor, while flattening the pipeline without fully unrolling the middle loop. For the second nested loop block, divide the middle loop into chunks of 8, allowing the innermost loop to compute reductions and parallelize with a factor of 10, leaving the pipelining of the top-level loop to be inferred by the tool.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{10}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Please, generate an HLS C code for a kernel function used in an ADI scheme that processes a two-dimensional grid of size 60x60 over a fixed number of time steps. Define the numerical constants inside the function, implement two sweeps (column and row) within a time step loop, and enforce the following constraints: Treat the outer time step loop and the sweep loops as non-pipelined operations, ensuring that the loops are executed in sequential order without any optimization that changes the iteration execution order. Set the tiling factor of the loops to 1 to maintain the original loop structure, and for the innermost loop of the column sweep (calculating 'p' and 'q'), set the parallel factor to allow 4 iterations to be processed in parallel. For all other loops, set the parallel factor to 1 to run iterations sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware acceleration kernel. For the outer loops, disable automatic loop pipelining, enforce no loop tiling, and ensure loop iterations execute serially. However, for the inner loop iterating over `j`, keep parallelization limited to pairs of iterations. For the nested most loops handling multiplication and accumulation, enable reduction on the `tmp` and `D` arrays with full serialization in the first case and parallelize with a factor of 16 in the latter.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use the following directives for the code:\n\n- Treat the `kernel_adi` function as an accelerator hardware kernel.\n- Keep the body of all top-level and nested loops in their unoptimized sequential form and ensure they don't execute in parallel.\n- Specifically, in the innermost loops where `j` is parallelized, parallelize one loop with a factor of 32 iterations and another with a factor of 2 iterations, then revert the last loop's parallelization factor back to 1 iteration.\n- Ensure that the loop reversal that normalizes the loop index for `j` is not parallelized; it should run sequentially.\n\nTo map these requirements to the specific #pragma directives mentioned:\n\n- Use `#pragma ACCEL kernel` to mark the `kernel_adi` function for hardware acceleration.\n- For the top-level loop and outer loops, do not apply automatic pipelining and parallelization optimizations by using `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}`.\n- Within the column sweep section's inner loop, parallelize the loop with a factor of 32 using `#pragma ACCEL PARALLEL FACTOR=auto{32}`.\n- In the row sweep section, use `#pragma ACCEL PARALLEL FACTOR=auto{2}` to parallelize the loop with a factor of 2.\n- Use `#pragma ACCEL PARALLEL FACTOR=auto{1}` to keep the last j-index normalization loop sequential, confirming that this loop operates without parallelism.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS codes provided, use the following single natural language instruction:\n\nGenerate a hardware kernel function named 'kernel_2mm' for matrix calculations with specified dimensions and coefficients, which performs matrix multiplication followed by matrix addition, ensuring that the outer loops for these calculations are not pipelined nor tiled, and execute serially. Nevertheless, for the inner loop responsible for the temporary matrix product, use reduction to handle dependencies with maximum parallelism but prevent its pipelining. In the subsequent nested loops for updating matrix 'D', enable partial loop flattening for pipelining and a moderate level of loop tiling and parallelization, excluding the innermost loop which should be parallelized at a minimal level without reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a function `kernel_adi` to be treated as a dedicated hardware acceleration kernel, targeting a digital signal processing operation, which iteratively processes two-dimensional arrays without pipelining, tiling, or parallelizing the outermost loop. Within this loop, process two sweeps, column and row, where both sweeps are composed of nested loops: the innermost loop concerning column sweep should apply a degree of parallel processing with a factor of 32, and the innermost loop concerning row sweep should apply a degree of parallel processing with a factor of 4, while the other loops should maintain the default sequential execution order without any loop transformations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, you will need to use the following simple natural language instructions to describe the required pragmas and their values:\n\n1. \"Declare the function `kernel_2mm` as a hardware acceleration kernel.\"\n2. \"In the first loop nest inside `kernel_2mm`, prevent any automatic pipelining optimization and execute the loop in its original form while keeping iterations sequential without parallelization.\"\n3. \"In the second loop (`j` loop) within the same nest, avoid automatic pipelining and maintain the loop in its full form but allow parallel execution with a parallelization factor of 16 for the `j` loop.\"\n4. \"Inside the innermost `k` loop of the first loop nest, enable parallel reduction for `tmp` array with a high parallelization factor, preferably 32, to optimize loop execution.\"\n5. \"In the second loop nest inside `kernel_2mm`, eschew automatic pipelining for the `i` loop, but you can use loop flattening to combine nested loops if possible, also ensure both `i` and `j` loops are processed in original form with no parallelization or tiling optimizations.\"\n6. \"For the `k` loop within the second loop nest, allow parallel reduction for the `D` array without specifying any parallelization factor, implying using the default settings.\"\n\nThese instructions correspond to the #pragma directives as follows:\n\n- '#pragma ACCEL kernel' to qualify the function for hardware acceleration.\n- '#pragma ACCEL PIPELINE \"off\"' to avoid automatic pipelining.\n- '#pragma ACCEL TILE FACTOR=1' to execute loops without tiling transformations.\n- '#pragma ACCEL PARALLEL FACTOR=1' to run loop iterations one after another with no parallelism, except where it is specifically set to 16 for the `j` loop inside the first block and 32 for the `k` loop for parallel reduction.\n- '#pragma ACCEL PARALLEL reduction = tmp FACTOR=32' to enable parallel reduction on `tmp` with the suggested factor.\n- '#pragma ACCEL PARALLEL reduction = D' to enable parallel reduction on `D` using default settings.\n- '#pragma ACCEL PIPELINE auto{flatten}' to suggest that loop flattening optimization should be considered.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware-accelerated 2-matrix multiplication kernel function, `kernel_2mm`, that includes the following characteristics:\n\n1. Designate the function `kernel_2mm` with parameters `int ni, int nj, int nk, int nl, double alpha, double beta, double tmp[40][50], double A[40][70], double B[70][50], double C[50][80], double D[40][80]` for hardware acceleration.\n\n2. For the first main loop, prevent automatic pipelining, avoid tiling the loop into smaller chunks, and run its iterations in their original sequence without parallelization.\n\n3. Inside the second nested loop, which iterates over `tmp`, enable automatic pipelining, keep the loop un-tiled, and set the innermost loop parallelization factor to 2, with a parallel reduction operation on the `tmp` array and an explicit parallelization factor of 32.\n   \n4. For the third main loop, prevent automatic pipelining again, avoid tiling the loop, and run the loop iterations in sequence without parallelization.\n\n5. Within the fourth nested loop, which updates the `D` matrix, use the 'flatten' pipeline directive to allow nested loops to be combined into a single pipeline stage, avoid further tiling, maintain the original loop sequence without parallelization, and provide a parallel reduction operation on the `D` array with a parallelization factor of 1.\n\nHere are the respective HLS pragmas and their descriptions:\n\n- Treat `kernel_2mm` as a hardware kernel: `#pragma ACCEL kernel`\n\n- Prevent automatic pipelining of the loop: `#pragma ACCEL PIPELINE auto{off}`\n\n- Avoid tiling the loop: `#pragma ACCEL TILE FACTOR=auto{1}`\n\n- Run iterations in original sequence without parallelization: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n- Enable automatic pipelining within a loop: `#pragma ACCEL PIPELINE auto{}`\n\n- Set innermost loop parallelization factor to 2: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n- Perform a parallel reduction operation on an array with a specified factor: `#pragma ACCEL PARALLEL reduction = array_name FACTOR=auto{factor_value}`\n\n- Use 'flatten' pipeline directive within a loop: `#pragma ACCEL PIPELINE auto{flatten}`\n\nPlease ensure that the pragma statements correspond to the correct loop in the code based on their descriptions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` to be synthesised as a dedicated hardware acceleration kernel. For the outermost loop, prohibit automatic pipelining, preserve its complete iteration space without tiling, and execute its iterations sequentially without parallelism. Within this loop, for the intermediary loop over `j`, enable implicit pipelining decision-making, use a parallelism factor of two, and maintain the iteration space without tiling. For the innermost loop over `k`, apply reduction on the `tmp` array with a high degree of parallelism, using a parallelism factor of thirty-two.\n\nFor the second set of nested loops, implicitly decide pipelining strategies, adhere to one iteration space per tile, and execute each iteration of the outer loop sequentially. Within this loop, allow the parallelism of the inner 'j' loop to be automatically flattened for pipelining, maintain its untiled iteration space, and use a parallelism factor of ten. In the final innermost loop over `k`, apply a reduction to the `D` array with consecutive execution (parallelism factor of one).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{32}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware acceleration kernel for a 2-matrix multiplication (2mm) with specific loop optimizations. In the outer loops, prevent pipeline optimization and maintain the original loop structure without tiling or parallelizing the iterations. Specifically, in the first loop nest, where `tmp` matrix is calculated, use a flattened pipeline for the second loop, keep the loop structure intact, and parallelize the innermost loop with a parallel factor of 2 for reduction on `tmp`. In the second loop nest, where `D` matrix is updated, use a flattened pipeline for the second loop, tile it with a factor of 8, and parallelize the innermost loop with a parallel factor of 2 for reduction on `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a hardware-accelerated kernel function designated for solving the ADI equations on a 2D grid using alternating row and column sweeps, ensuring the following specifics:\n\n1. Mark the function (`kernel_adi`) as an acceleration kernel.\n2. Disable automatic pipelining for all loops within the function.\n3. Preserve the entirety of each loop without tiling or dividing it into smaller chunks.\n4. Run all loops in their sequential order, without parallel execution, except for specific inner loop instances that have their parallelization factor pragmas individually set.\n5. Set the parallelization factor to 2 for the inner loop calculating the `q[i][j]` values during the column sweep.\n6. Set the parallelization factor to 8 for the inner loop calculating the `q[i][j]` values during the row sweep.\n7. Revert the parallelization factor back to 1 for the reverse-direction loops converting the temporary `p` and `q` values into the final `v` and `u` arrays during the column and row sweeps, respectively.\n8. The `DX`, `DY`, `DT`, `B1`, `B2`, `mul1`, `mul2`, `a`, `b`, `c`, `d`, `e`, and `f` variables initialization should not be pipelined or parallelized.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware acceleration kernel from this function, preventing automatic pipelining on the main loops but allowing flattening on the column sweep. Use a tile factor of 1 consistently, and adjust parallel factors from 1 to 2 for the inner loops of the row sweep while maintaining a parallel factor of 1 for other inner loops.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, refrain from automatic pipelining of both the outermost 'i' loop and its immediate 'j' loop, while processing the first 'i' loop and its contents without partitioning it. In the innermost 'k' loop of the first 'j' loop, enable parallel execution with a full replication of resources corresponding to the range of 'k'. For the second 'i' loop, enable default pipelining behavior, but for its inner 'j' loop, explicitly request that loop flattening is performed. Set tile size to eight for the 'j' loop and allow a parallelization factor of ten. Within the 'k' loop, parallel processing should occur without any factorization, effectively running sequentially.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the 'kernel_adi' function as a hardware acceleration kernel, automatically manage the pipelining of loops to optimize throughput, and auto-tune the tiling and parallelism factors for all loops, except for the last loop in the row sweep where you should specifically set the parallelism factor to 16.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as a hardware acceleration kernel, allow the compiler to automatically manage the pipelining, tiling, and parallel processing directives for the outer loop and the first inner loop, but explicitly set the parallel factor to 2 for the second-inner loop parallelizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the \u2018kernel_adi\u2019 function as a hardware acceleration kernel. Within this function, use the natural execution order for the outermost loop, without pipelining or partitioning it. For the first nested loop, perform pipelining automatically, while keeping the loop intact and not performing parallel execution of iterations. For the innermost loop nested within the first nested loop, allow parallel execution with a factor of 8. For the second loop iteration count being reversed, standardize the loop to run from the start while enabling parallelization with a factor of 4. Proceed to the second nested loop 'Row Sweep,' enforce loop flattening for pipelining to remove loop nestings, maintain loop integrity without tiling, and disable parallel iteration execution with a factor of 1. Within this section's inner loop, retain the original sequential execution without parallelism.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as the target for hardware acceleration. In the outer loops for calculating `tmp` and `D`, disable automatic pipelining. Inside the first outer loop, for computing `tmp`, use the default settings for pipelining, keep the loop un-tiled, and set the parallelism factor to two, with the inner loop having a reduction operation on `tmp` and also a parallelism factor of two. In the second outer loop for updating `D`, apply pipelining with default settings, tile the inner loop with a factor of eight, and set its parallelism to two with a reduction on `D` with a parallelism factor of sixteen.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{2}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{8}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{16}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_2mm` as a hardware kernel for acceleration, within it, for the two major loops, both processing 40 iterations:\n- The first loop over `tmp` computation should not be automatically pipelined, should be executed as a single block without tiling, and the innermost loop should run two iterations in parallel without reducing the array `tmp`.\n- The second loop updating `D` should be automatically pipelined and flattened, with no tiling applied, and the innermost loop should sequentially perform an addition reduction operation on the array `D`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a kernel function called `kernel_adi` that processes 2D arrays for ADI (Alternating Direction Implicit) method. The kernel function should not pipeline loops, process loops without tiling (i.e., in their original full form), and handle loop iterations sequentially (i.e., not parallelized) by default, except for a specific inner loop where a higher parallelization factor is explicitly specified.\n\nHere are the instructions:\n\n1. Declare the function `kernel_adi` to be a hardware acceleration kernel.\n2. Initialize constants and compute coefficients used in the ADI method.\n3. Disable loop pipelining for the primary time-stepping loop which performs \"Column Sweep\" and \"Row Sweep\" operations.\n4. Do not tile the primary loop and execute its iterations sequentially.\n5. Within the \"Column Sweep\" section, for the innermost loop updating the `q` array, set the parallel factor to 2 allowing some parallel computation.\n6. During the \"Row Sweep\" section, within the loop updating the `q` array, set a parallelism factor of 32 to enable more aggressive parallel computation.\n7. For all remaining loops and array initialization, ensure that no additional pipelining, tiling, or parallel optimizations are applied unless specified.\n8. Use the appropriate `#pragma ACCEL` directives to achieve the described behavior.\n\nFor the following loops, use the respective `#pragma` directives:\n- For general loop control preventing pipelining, tiling, and parallel execution, place `#pragma ACCEL PIPELINE auto{off}`, `#pragma ACCEL TILE FACTOR=auto{1}`, and `#pragma ACCEL PARALLEL FACTOR=auto{1}` before the loop.\n- Specifically, for loop iterations that should run partially in parallel with a factor of 2, use `#pragma ACCEL PARALLEL FACTOR=auto{2}`.\n- For the loop that should have a parallel factor of 32, use `#pragma ACCEL PARALLEL FACTOR=auto{32}`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use these instructions to generate the corresponding HLS pragmas for the function `kernel_2mm`:\n\n1. Define `kernel_2mm` as a hardware acceleration kernel.\n2. Keep the first-level loop over ni in its most natural form without automatic optimization, pipelining, or partitioning, and run its iterations serially.\n3. Keep the second-level loop over nj in its natural form without pipelining but allow parallel execution of the loop body with two iterations being able to execute concurrently.\n4. Enable aggressive loop-level parallelism inside the innermost nk loop with a parallel factor of 10, focusing on reducing the loop's accumulation into the `tmp` array.\n5. Apply automatic pipelining optimization to the outer loop over nl, allowing the tool to decide the optimal setting.\n6. Keep the nested nj loop structure within the nl loop natural, without pipelining but allow parallel execution with two iterations concurrently.\n7. Within the innermost loop over nk inside the nl loop, parallelize with a high factor of 50 to reduce the latency associated with the loop's accumulation into the `D` array.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{10}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to optimize the given function as a hardware acceleration kernel with automated loop optimization suggestions, applying the tool's default pipelining, tiling, and parallelization strategies as appropriate, but enforce a parallelism factor of 1 for all loops except the innermost row sweep loop, where the parallelism factor is incrementally doubled at each nested level.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to designate the function `kernel_2mm` as a hardware acceleration kernel, do not apply automatic pipelining or parallelization to the first outer and inner loops, and use the default target for loop transformations. Specifically:\n\n- For the first set of nested loops iterating over `i` and `j`, disable automatic pipelining, maintain the loops in their original form without tiling or parallelizing the outer loop, and within the second `j` loop, run two iterations in parallel.\n- Inside the innermost loop under `j`, enable a reduction on `tmp` with four-way parallel execution.\n- For the second set of nested loops iterating over `i` and `j`, enable automatic pipelining with loop flattening on the first outer loop, tile the first loop with a factor of four, and run sixteen parallel iterations of the inner `j` loop.\n- Finally, within the innermost loop under the second `j` loop, disable the parallel execution and perform reduction on `D` with the default factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{4}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code above, use the following comprehensive but concise instructions:\n\n1. For the `kernel_2mm` function, mark it for hardware acceleration.\n2. In the first loop nest starting with `for (i = 0; i < 40; i++)`, prevent any automatic pipelining optimizations.\n3. Within the same loop nest, ensure that the loop is executed in its entirety, without tiling or parallel execution optimizations.\n4. Inside the inner `j` loop, again prevent automatic pipelining and tiling but allow a parallel factor of two for the nested `k` loop which computes the matrix multiplication with the accumulative reduction on `tmp`.\n5. For the second loop nest, disable automatic pipelining for the outer `i` loop and enforce no tiling or parallel optimizations, but within the inner `j` loop, flatten any loop optimizations enable automatic tiling and parallelization settings for innermost loop.\n6. In both outer `i` loops, the corresponding inner `k` loops have parallel reductions specified for `tmp` and `D` arrays with the default factors set by the compiler.\n\nNow, in terms of #pragma values for the given code:\n\n- `#pragma ACCEL kernel` marks the function as a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` specifies that the loop should not be automatically pipelined.\n- `#pragma ACCEL TILE FACTOR=auto{1}` ensures the loop is not tiled and is executed in full.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` and `#pragma ACCEL PARALLEL FACTOR=auto{2}` set the loop to run with a specific parallel factor, with `1` meaning no parallelization and `2` allowing a limited parallel execution where specified.\n- `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}` and `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}` apply a parallel reduction to the specified arrays with the factor determined by the compiler.\n- `#pragma ACCEL PIPELINE auto{flatten}` suggests the compiler to consider flattening any nested loops within this context for potential pipelining optimizations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the provided HLS C code for an alternating direction implicit (ADI) solver kernel intended for hardware acceleration, ensuring the following criteria are met:\n\n1. Designate the entire `kernel_adi` function for hardware acceleration.\n2. Completely disable loop pipelining for all loops within the function.\n3. Ensure that all loops are unaltered, meaning they should not be tiled or divided into smaller subtiles; loops should run in their entirety from beginning to end.\n4. Maintain original sequential execution for all loop iterations, with no parallel processing for individual loop iterations, except for specific loops:\n   a. In the loop with iterator `j` nested within the loop with iterator `i` in the column sweep phase, enable parallel execution for a pair of iterations at a time.\n   b. In the first `j` loop of the row sweep phase, increase parallelization to allow eight simultaneous iterations.\n   c. In the second `j` loop of the row sweep phase, further increase parallelization to allow sixteen simultaneous iterations.\n\nFor the specific `#pragma` values, use the following:\n\n- Use `#pragma ACCEL kernel` to mark the `kernel_adi` function as a hardware acceleration kernel.\n- Apply `#pragma ACCEL PIPELINE auto{off}` before each loop to prevent automatic pipelining.\n- Insert `#pragma ACCEL TILE FACTOR=auto{1}` before each loop to avoid loop tiling.\n- Position `#pragma ACCEL PARALLEL FACTOR=auto{1}` before each loop to disable parallelization of loop iterations, except for the three instances where specific parallel factors are provided; for those, use:\n  - `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the column sweep's inner `j` loop.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{8}` for the row sweep's first `j` loop.\n  - `#pragma ACCEL PARALLEL FACTOR=auto{16}` for the row sweep's second `j` loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel, and for the outer two loops of the kernel, disable automatic pipelining, prevent tiling optimization by processing the loop in full without subdividing, and execute iterations sequentially. Specifically, for the first-level 'j' loop within the first 'i' loop, allow the second run of iterations to occur in pairs. In the nested 'k' loop that calculates the 'tmp' matrix, enable parallel execution with a grouping of 14 iterations contributing to the 'tmp' reduction. For the second-level 'j' loop within the second 'i' loop, again allow the iterations to occur in pairs, and within its nested 'k' loop that updates the 'D' matrix with reduction, enable parallel execution with a grouping of 32 iterations contributing to the 'D' reduction.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as a hardware acceleration kernel, disable automatic pipelining of the main loops, enforce loops to process without being divided into smaller parts, and run iterations sequentially. Specifically, within the 'Column Sweep' section, allow the innermost loop to run in a flat, pipelined manner, but without parallelism. For the 'Row Sweep,' maintain a sequential loop execution, but allow for dual parallel processing in the innermost loop. Apply the necessary pragmas directly before each loop or loop nest to conform to the stated constraints.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function `kernel_adi` as a hardware acceleration kernel, disable automatic pipelining for the outermost loop, ensure the loops process without splitting into smaller chunks or running iterations in parallel, but set the innermost loop of the column sweep with a parallelism factor of 4 and flatten the pipeline for the row sweep.\n\nSpecifically, use '#pragma ACCEL kernel' before the function definition to indicate it as an acceleration kernel. For the top level loop, apply '#pragma ACCEL PIPELINE off', '#pragma ACCEL TILE FACTOR=1', and '#pragma ACCEL PARALLEL FACTOR=1' to prevent pipelining, tiling, and parallel execution, respectively. Within the column sweep, enable pipelining with '#pragma ACCEL PIPELINE' while maintaining the non-splitting and sequential processing of the outer loops as before, but increase the parallelism factor to 4 for the innermost loop with '#pragma ACCEL PARALLEL FACTOR=4'. During the row sweep, use '#pragma ACCEL PIPELINE flatten' to flatten the pipeline. Keep the individual loop configurations the same as the top level by using '#pragma ACCEL TILE FACTOR=1' and '#pragma ACCEL PARALLEL FACTOR=1' for the outer loops, with the innermost parallelism remaining at 1.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware kernel function named `kernel_adi` for accelerating a two-dimensional Alternating Direction Implicit (ADI) solver, with loop bodies explicitly unoptimized, each one preserving its full original execution without pipelining or dividing. Specifically, prevent pipelining and tiling in all outer loops and the first inner reduction loop, but allow different degrees of parallelism for the subsequent inner loops: use a parallel factor of 32 for the first column reduction, 1 for the column result computation, 8 for the row reduction, and 2 for the row result computation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the corresponding HLS code in C with the specified #pragma directives, provide the following instruction:\n\nDefine a hardware acceleration kernel for a two-matrix multiplication algorithm, where for the first matrix multiplication, you should not automatically pipeline the outermost loop and keep it in its full original form without tiling or parallelizing it. However, within the innermost loop of the first matrix multiplication, automatically flatten the pipeline, don't tile the loop, and run just a pair of iterations in parallel. Ensure to use parallel reduction for the summation on the temporary result array, 'tmp', without any parallelization factor.\n\nFor the second matrix multiplication, use automatic pipelining at the outermost level, keep the loop in its full original form without tiling and without any parallelization factor. Within the innermost loop of the second matrix multiplication, do not automatically pipeline, set a tile factor equivalent to the length of the 'D' array's second dimension, and run fifty iterations in parallel, using parallel reduction on the 'D' array.\n\nTranslate the above natural language instruction into the corresponding HLS code with these #pragma values:\n\n```c\n#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni, int nj, int nk, int nl, double alpha, double beta, double tmp[40][50], double A[40][70], double B[70][50], double C[50][80], double D[40][80]) {\n  int i, j, k;\n\n  /* D := alpha*A*B*C + beta*D */\n  \n  // For the first matrix multiplication\n  #pragma ACCEL PIPELINE auto{off}\n  #pragma ACCEL TILE FACTOR=auto{1}\n  #pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    #pragma ACCEL PIPELINE auto{flatten}\n    #pragma ACCEL TILE FACTOR=auto{1}\n    #pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      #pragma ACCEL PARALLEL reduction=tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n  // For the second matrix multiplication\n  #pragma ACCEL PIPELINE auto{}\n  #pragma ACCEL TILE FACTOR=auto{1}\n  #pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    #pragma ACCEL PIPELINE auto{off}\n    #pragma ACCEL TILE FACTOR=auto{80}\n    #pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      #pragma ACCEL PARALLEL reduction=D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n}\n```\n\nRemember, \"auto{value}\" indicates that the directive will deduce its behavior automatically but with a constraint provided by the value within the braces.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{80}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the HLS code provided, the following comprehensive but concise natural language instruction could be used:\n\n\"Implement a hardware kernel function named 'kernel_adi' that simulates an Alternating Direction Implicit solver. The function receives the number of time steps and the size of the grid, as well as arrays u, v, p, q representing a 2D grid. Initialize the constants and coefficients involved in the computation with reciprocal and multiplication operations. Do not automatically pipeline or tile any of the loops, to preserve the original loop structures; however, do allow the innermost loops of each sweep to run with a small parallelization factor where applicable. Process all outer loops under these constraints one iteration at a time without pipelining to maintain the data dependencies as in the sequential execution. Additionally, during the resolution phase of each sweep, when updating the v and u arrays, explicitly convert the standard reverse loop into a forward loop and execute it without parallelization to respect the original sequential order of dependencies.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a kernel function named `kernel_2mm` that performs a 2D matrix multiplication with additional scalar operations, applying the following directives to optimize for hardware acceleration:\n\n- Mark the function as a hardware acceleration kernel.\n- Disable automatic pipelining for the top-level loop bodies.\n- Keep the top-level loops in their complete, unaltered form without tiling.\n- Ensure the outermost loops run in a strictly sequential manner.\n- For the inner loops within the first two-level nested loop block:\n  - Enable pipelining and allow the compiler to decide how to flatten the pipeline.\n  - Force no tiling transformation.\n  - Set up the second level loop to attempt parallel execution of two iterations at a time.\n  - For the innermost loop, allow parallelization with a reduction on the `tmp` array, without specifying a factor.\n- For the inner loops within the second two-level nested loop block:\n  - Enable pipelining again but let the compiler determine the flattening.\n  - Avoid tiling transformation.\n  - Configure the second level loop to allow two iterations to run in parallel.\n  - In the innermost loop, enable parallelism with a reduction on the `D` array, with an unspecified parallelism factor.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for a hardware-accelerated kernel that performs the alternating direction implicit (ADI) method on 2D arrays using the following directives:\n\n- Define the entire function as an acceleration kernel.\n- For the primary time-stepping loop, disable automatic pipelining.\n- Prevent sub-loop unrolling and tiling, maintaining the original sequential execution of both column and row sweeps.\n- Pipeline the innermost loops where possible, but without modifying the loop structure.\n- For the final reversal loop in the row sweep, use full parallelism corresponding to the loop size.\n\nSpecifically, apply these #pragma values:\n\n- Use `#pragma ACCEL kernel` to indicate that the function is a hardware kernel.\n- Use `#pragma ACCEL PIPELINE off` to disable automatic pipelining of the outer time-stepping loop.\n- Use `#pragma ACCEL TILE FACTOR=1` and `#pragma ACCEL PARALLEL FACTOR=1` to maintain the original form of loops without tiling or parallel execution, except for the last reversal loop in the row sweep.\n- Use `#pragma ACCEL PIPELINE flatten` for the inner column sweep loop to enable pipelining while preserving nested loop structure.\n- For the last reversal loop in the row sweep, apply `#pragma ACCEL PARALLEL FACTOR=58` to enable full parallelism for this loop since it operates on 58 elements.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the following HLS C code for accelerating an Alternating Direction Implicit (ADI) kernel using the specified pragmas:\n\n- Designate the function as a hardware kernel by applying the directive 'Treat the following function as a hardware kernel for acceleration.'\n- For each loop in the code, disable automated loop pipelining by using 'Don't automatically pipeline this loop.'\n- Ensure every loop is executed in full without being split into smaller pieces by using 'Process this loop in its original, full form without breaking it into smaller chunks.'\n- Execute all iterations of every loop sequentially, without parallelization, by applying 'Run the iterations of this loop one after the other, not in parallel.'\n- For the specific loop where computation of 'p[i][j]' and 'q[i][j]' takes place in the first 'Column Sweep' block, enable parallel processing with a factor of 32 using the directive 'Allow this loop to process thirty-two iterations in parallel to improve throughput.'\n- In the 'Row Sweep' block, where 'p[i][j]' and 'q[i][j]' are also computed, specify parallel processing with a factor of 2 by using 'Allow this loop to process two iterations in parallel to enhance performance.'\n\nEnsure the resulting HLS C code includes all necessary pragmas for these directives at the appropriate locations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define the function 'kernel_adi' as a hardware acceleration kernel. Disable automatic pipelining of the outermost loop and run its iterations sequentially. Inside this loop, automatically flatten the next nested loop to optimize it, but do not split it into smaller chunks or parallelize its iterations. For the final inner loop, set no automatic optimization, maintaining sequential execution. Reapply these optimization settings to the subsequent similar loop structures within the function, except for the last inner loop in the 'Row Sweep' section, which should have iterations executed in pairs, if possible.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Translate the following loop control strategies into HLS pragmas for the kernel_adi function: \n1. For the entire function, indicate that it should be treated as a hardware acceleration kernel.\n2. Ensure that none of the loops are automatically pipelined.\n3. Keep all loops in their original full form without tile optimization.\n4. Avoid parallelizing the innermost loop iterations by default.\n5. Specifically for the innermost loop updating 'v', allow pairs of iterations to occur in parallel.\n6. For the innermost loop updating 'q', enhance parallelism by a factor of four.\n7. In the reversal loop for 'v', reset any parallelism factor applied earlier to the default setting.\n8. Dramatically increase the parallelism for the reversal loop in 'u' updates to a factor of thirty-two.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the provided HLS C code, use the instruction: \"Create an algorithm for Alternating Direction Implicit solver in hardware, where the outer loop and both inner loops are executed sequentially without parallelization or pipelining optimizations. However, for the second loop in the column sweep and the row sweep, parallelize the computation with a factor of 2 and 8 respectively for the forward sweep, and with a factor of 58 for the backward sweep, while keeping other loops in their original form.\"\n\nSpecific pragma instructions would be:\n- \"#pragma ACCEL kernel\" to indicate that the entire `kernel_adi` function is intended to be a hardware kernel.\n- Each occurrence of \"#pragma ACCEL PIPELINE auto{off}\" indicates that the loops following it should not be pipelined, thus they execute in a sequential manner. \n- Each occurrence of \"#pragma ACCEL TILE FACTOR=auto{1}\" indicates that no tiling optimization should be applied to the loop, meaning the loop should run in its full original form.\n- \"#pragma ACCEL PARALLEL FACTOR=auto{1}\" is used to keep the first and last loops of the inner loops in a sequential execution order.\n- \"#pragma ACCEL PARALLEL FACTOR=auto{2}\" and \"#pragma ACCEL PARALLEL FACTOR=auto{8}\" are used in the second loops of the column and row sweep, respectively, allowing some degree of parallelism with the specified factors during the forward sweep.\n- Finally, \"#pragma ACCEL PARALLEL FACTOR=auto{58}\" is used in the reverse loop during the row sweep to maximize parallel execution with a specific unrolling factor of all but the last iteration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS code for a 2-matrix multiplication kernel, treating the function as hardware acceleration, where the first loop complies with the original flow without pipelining, tiling, or parallel processing; the second loop applies no pipelining but permits parallel execution of the innermost loop with factor 70 for sum reduction; then, for the final block, pipeline the outer loop with layout optimization and permit parallel execution of the innermost loop with a factor of 1 while maintaining original loop iterations for the outer loop. Use the following specific pragma directives: '#pragma ACCEL kernel' at the beginning, '#pragma ACCEL PIPELINE auto{off}' for non-pipelined loops, '#pragma ACCEL TILE FACTOR=auto{1}' to avoid tiling, '#pragma ACCEL PARALLEL FACTOR=auto{1}' for sequential outer loops, '#pragma ACCEL PARALLEL reduction = variable_name FACTOR=auto{70}' for parallel inner loops with sum reduction, and '#pragma ACCEL PIPELINE auto{flatten}' for pipelining with layout optimization.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Here's a comprehensive but concise natural language instruction for the respective HLS C code pragmas:\n\nOptimize the function 'kernel_2mm' for hardware acceleration, disabling automatic loop pipelining and maintaining loops in their original, sequential form without tiling or any form of parallel processing for the outer loops. Run the first inner 'j' loop's iterations sequentially but allow a degree of parallel processing in the innermost 'k' loop with a parallel factor of 2 for the 'tmp' array reduction. Then, run the second outer 'i' loop's iterations sequentially but automatically decide pipelining, tile the second inner 'j' loop with a factor of 2, and enable a high degree of parallel processing with a factor of 16 for the next loop level. For the innermost 'k' loop in this section, allow parallel reduction on the 'D' array with a parallel factor of 2.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{2}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the provided function 'kernel_2mm' as a hardware acceleration kernel. For the outer loops processing 40 iterations, disable automatic pipelining, maintain the loops' original structure without tiling, and execute iterations sequentially without parallelization. For the nested loop of 50 iterations in the first double-nested loop structure, again disable pipelining, maintain the loop's original structure with a tile factor of 1, but allow two iterations to run in parallel. As for the innermost loop of 70 iterations, use parallel reduction on the 'tmp' array with the maximum possible parallel factor. In the second double-nested loop structure, return to the default settings for pipelining, tiling, and parallelization in the outer 40-iteration loop but flatten the loop structure when pipelining. Keep sequential execution for the nested 80-iteration loop, and use parallelization with a factor of 1 for the reduction on the 'D' array in the innermost loop. \n\nHere's the comprehensive but very concise instruction:\n\n\"Mark 'kernel_2mm' as an acceleration kernel. For both outermost loops running 40 iterations, cancel pipelining and prevent tiling or parallel processing. In the first middle-level loop of 50 iterations, disable pipelining and tiling but allow a parallelism factor of 2. Utilize full parallelism for the reduction on 'tmp' in the innermost loop of 70 iterations. Enable default pipelining with loop flattening, maintain single-block tiling, and enforce sequential execution for the second set of nested loops, except use a parallelism factor of 1 for the reduction on 'D' in the innermost loop.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the following function as a dedicated hardware acceleration kernel. Within the function, perform the first main loop without any automatic pipelining optimizations, also keep the loop body intact and ensure that iterations are executed sequentially. For the nested loop inside the first main loop, enable loop flattening for potential performance improvement, ensure the loop body is unaltered and perform every two iterations in parallel. Apply a parallel reduction on the innermost loop, with one iteration at a time to accumulate the results correctly. Repeat similar instructions for the second main loop, which also initializes elements sequentially, flattens the nested loop, and performs two parallel operations, with a parallel reduction in the innermost loop to handle accumulator updates correctly.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. Disable automatic loop pipelining and prevent loop tiling and parallelization for the top-level loops. Within the first nested loop, allow the loop to be flattened for pipelining, but without altering loop tiling and parallelism which should be set to execute two iterations in parallel. Enforce a reduction pattern with no parallelism for the innermost loop updating 'tmp'. In the second set of nested loops, disable loop pipelining, set the tiling factor to four, and parallelize the second outer loop with two iterations executed concurrently. Finally, heavily parallelize the innermost loop updating 'D' with a parallel factor of 50.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{1}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the specified function as a hardware acceleration kernel, ensuring that the outermost loop is executed sequentially without automatic pipelining. Within this loop, apply implicit flattening pipelining to the secondary loop while keeping its iterations sequential. Maintain the original loop structure by avoiding loop tiling and parallelization, except for the third-level nested loop within the 'Row Sweep' section, where you should unroll the loop with a factor of 58 to promote parallel execution of its iterations.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create an HLS C function named `kernel_adi` to perform an Alternating Direction Implicit (ADI) method for solving partial differential equations iteratively over a two-dimensional grid with specific dimensions. The function has integer parameters `tsteps` and `n`, plus four 2D double-precision floating-point arrays `u`, `v`, `p`, and `q`. Initialize constants for discretization and coefficients inside the function. Ensure the following behaviors in the function's loops using HLS pragmas:\n\n1. Prevent loop pipelining in the entire function and within each individual loop nest by disabling automatic pipeline optimization.\n2. Prevent any loop tiling optimizations by using the minimal tiling factor, effectively leaving loops un-tiled.\n3. Disable parallelization of the outer loops by keeping iterations sequential with a parallel factor of one.\n4. Enable a higher level of parallelization, with a factor of 32, for a specific inner loop calculating the `p` and `q` intermediate arrays in the column sweep but revert back to sequential processing for the backward iteration loop.\n5. Maintain sequential processing for the row sweep calculations except increase the parallel factor to 2 in the backward iteration loop of the row sweep.\n\nTo achieve these behaviors, you will need to use the following HLS pragmas accordingly:\n\n- Use `#pragma ACCEL kernel` to designate the function as a hardware kernel for acceleration.\n- For all loops where automatic pipelining should be disabled, use `#pragma ACCEL PIPELINE auto{off}`.\n- For all loops where tiling should be disabled (un-tiled), use `#pragma ACCEL TILE FACTOR=auto{1}`.\n- For keeping iterations sequential in loops with `#pragma ACCEL PARALLEL FACTOR=auto{1}`, except increase the value to `#pragma ACCEL PARALLEL FACTOR=auto{32}` for the specific inner loop in the column sweep, and to `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the specific backward iteration loop in the row sweep.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate an HLS C code for a kernel function called `kernel_adi` that performs an Alternating Direction Implicit (ADI) method on a 2D grid with dimensions 60x60 over a number of time steps. This function will take as inputs the current time step count `tsteps`, the grid size `n`, and four double-precision floating-point arrays `u`, `v`, `p`, and `q`. Pre-calculate constants with divisions outside of the time-step loop to avoid repeated division operations. Pipeline all loops automatically where possible to optimize for performance, and maintain original loop order with an exception to parallelize the row sweep innermost loop with a factor of 2 and the backward substitution in the row sweep with a factor of 8. Utilize tiling and parallelism pragmas as necessary with automatic factor determination, but ensure the tile and parallel factors in the single for-loops are explicitly set to 1, emphasizing no tiling or parallelization for those loops. Mark the `kernel_adi` function for hardware acceleration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the specified HLS C code, instruct the code generator as follows:\n\n\"Create a function marked for hardware acceleration that performs an Alternating Direction Implicit (ADI) method for a 2D heat distribution over a 60x60 grid, iterating over a specified number of time steps. Calculate constants using the grid size and given parameters. In the main time-stepping loop, and both the 'Column Sweep' and 'Row Sweep' sections, disable loop pipelining, default the tiling and parallelism factors to a single iteration to ensure no automatic optimization is applied. For the innermost loop in the 'Column Sweep' section, enable a parallelism factor of 32, and in one of the innermost loops of the 'Row Sweep' section, set a parallelism factor of 2. Maintain the reverse iteration process in both sweeps by standardizing the decrementing loop into an equivalent incrementing form. The function signature, variable declarations, and the specific #pragma directives with rule values such as 'off' and '1' or the parallel factors '32' and '2', as shown in the provided code, should be preserved exactly.\"", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS codes by defining a hardware acceleration kernel with specific loop optimization directives. Initialize variables using fixed arithmetic values and dimensions. For the first set of nested loops (Column Sweep), apply the following: \n\n1. Prevent automated pipelining.\n2. Do not tile the loops.\n3. Execute the iterations of these loops serially.\n\nWithin these loops, standardize backward iterations to forward loops using an index transformation for parallelization. For the second set of nested loops (Row Sweep):\n\n1. Disable automated loop pipelining.\n2. Avoid loop tiling, maintaining the original loop size.\n3. Parallelize the innermost loop (j) with a factor of 58 for full parallel execution.\n4. Subsequent backward iteration loop should be parallelized with a factor of 4.\n\nEnsure that the values `1` and `-1` at the end of the respective loop bodies are translatable to equivalent functionality without using loop constructs.\n", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate a hardware acceleration kernel to optimize the 2mm matrix-matrix multiplication algorithm. Disable automatic pipelining for the main matrix computations, and keep the loop structure intact by setting tile and parallel factors to their minimum granularity. Specifically, for the first-level loops, use minimal pipelining and avoid any loop unrolling or tiling. For the second-level loop that computes intermediate results, enable pipelining but still prevent loop tiling or unrolling. The innermost third-level loops that perform the reduction should have pipelining enabled, with a specified parallel factor to unroll the loops partially for parallel execution, and specify the array to be reduced. Use the maximum unrolling factor that fits the hardware limits for the corresponding reductions.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "For the provided HLS C code, you would give the following concise natural language instruction:\n\nDefine a hardware acceleration kernel function 'kernel_2mm' that takes matrix dimensions along with scalar factors alpha and beta and matrices A, B, C, D, and tmp as arguments. Perform two matrix multiplication operations and a matrix addition operation to compute D as the product of the scalar alpha and the matrix product of A and B, followed by matrix multiplication with C; then, add the product of scalar beta and matrix D.\n\nEnsure that all loops within the kernel function:\n\n1. Do not automatically pipeline any of the loops.\n2. Do not tile any of the loops, meaning do not break down the loops into smaller chunks.\n3. Run the outer loops sequentially, not in parallel.\n\nIn the first nested loop where tmp is calculated:\n1. For the innermost loop, perform parallel reduction on the 'tmp' matrix and set the parallelization factor to 14, indicating 14 iterations can be processed in parallel.\n\nIn the second nested loop where D is updated:\n1. Automatically choose whether to pipeline the outer loop.\n2. Flatten the loop for pipelining when deemed beneficial.\n3. Parallelize the j-loop with a factor of 10, indicating that 10 iterations can be executed in parallel.\n4. For the innermost loop where D is updated, perform parallel reduction on the 'D' matrix with the default parallelization settings.\n\nSpecifically, the #pragma directives and their respective values would be:\n1. Treat the following function as a hardware kernel: `#pragma ACCEL kernel`\n2. For the first set of nested loops corresponding to the computation of `tmp`:\n    - Do not automatically pipeline the loop: `#pragma ACCEL PIPELINE auto{off}`\n    - Process the loops in their original full form: `#pragma ACCEL TILE FACTOR=auto{1}`\n    - Run the iterations sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the i-loop and `#pragma ACCEL PARALLEL FACTOR=auto{2}` for the j-loop.\n    - Perform parallel reduction on `tmp` with a parallelization factor of 14: `#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}`\n3. For the second set of nested loops corresponding to the update of `D`:\n    - Decide whether to automatically pipeline the loop: `#pragma ACCEL PIPELINE auto{}`\n    - Flatten the i-loop: `#pragma ACCEL PIPELINE auto{flatten}`\n    - Parallelize the j-loop with a factor of 10: `#pragma ACCEL PARALLEL FACTOR=auto{10}`\n    - Perform parallel reduction on `D` with a parallelization factor of 1: `#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{14}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Define a hardware kernel function called 'kernel_2mm' that computes the matrix multiplication and addition expression `D := alpha * A * B * C + beta * D`. The function has parameters for matrix dimensions and scalars `alpha` and `beta`. The input matrices `A`, `B`, `C`, and `D` and an intermediate matrix `tmp` are all 2D arrays with varying sizes up to 80x80 elements. \n\n- Deactivate automatic pipelining for the outer two loops iterating over the `i` index, and run these loops without any modification and without parallelism.\n- Within the outer loop focused on computing the intermediate matrix `tmp`, disable pipelining, set tiling to the minimum (single iteration per tile), and allow for the innermost `k` loop to run 70 iterations in parallel while performing a reduction on `tmp`.\n- For the second outer loop where the final matrix `D` is updated, enable a loop flattening optimization for the `i` loop, minimize tiling, and set optimized parallel execution so that every tenth iteration of the `j` loop can be executed in parallel. In the innermost `k` loop, allow no parallelism but enable the reduction operation on `D`. \n\nThis should translate to the following #pragma directives in HLS code:\n\n- `#pragma ACCEL kernel` before the function signature to define it as a hardware acceleration kernel.\n- `#pragma ACCEL PIPELINE auto{off}` for disabling automatic pipelining of the outer `i` loops.\n- `#pragma ACCEL TILE FACTOR=auto{1}` for setting the tiling factor to 1 for the outer `i` loops and the inner `j` loops.\n- `#pragma ACCEL PARALLEL FACTOR=auto{1}` for the outer `i` loops to ensure that iterations are not run in parallel.\n- `#pragma ACCEL PARALLEL FACTOR=auto{2}` in the inner `j` loop of the first matrix multiplication to allow two iterations (which is just an example value in this case, as the original `FACTOR=auto{70}` would indicate the fully parallelized innermost `k` loop during the computation of `tmp`).\n- `#pragma ACCEL PIPELINE auto{flatten}` for the `i` loop of the `D` update sequence to enable loop flattening optimization.\n- `#pragma ACCEL PARALLEL FACTOR=auto{10}` in the inner `j` loop of the `D` update sequence to allow for parallel execution of every tenth iteration.\n- Use `#pragma ACCEL PARALLEL reduction = variable_name FACTOR=auto{value}` with appropriate `variable_name` and `value` to denote the variables being reduced across iterations of the parallelized `k` loops in the matrix operations (`tmp` and `D` respectively).", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{10}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "To generate the given HLS (High-Level Synthesis) codes in C with the appropriate `#pragma` values, please use the following comprehensive but concise instruction:\n\n\"Define a function as a hardware acceleration kernel. For the outermost time stepping loop, do not automatically pipeline, and do not partition or parallelize the loop. Inside this loop, for column sweep calculations, automatically flatten the pipeline, but prevent loop tiling and parallelization. In the subsequent reversed loop, maintain the default parallel factor. For row sweep calculations, automatically set the pipeline, tile, and parallel factors, but explicitly specify a parallel factor of 2 for the forward loop iteration and restore to the default factor for the reversed loop. Ensure that loop variables 'i' and 'j' for the reversed loop iterations are re-indexed to count backwards from 58.\"\n\nAnd here is the mapping of this instruction to the specific `#pragma` values:\n\n- Treat the following function as a hardware kernel for acceleration: `#pragma ACCEL kernel`\n- For the outermost loop (time stepping loop):\n  - Don't automatically pipeline this loop: `#pragma ACCEL PIPELINE auto{off}`\n  - Process this loop in its original, full form without breaking it into smaller chunks: `#pragma ACCEL TILE FACTOR=auto{1}`\n  - Run the iterations of this loop one after the other, not in parallel: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n- Inside the column sweep loop:\n  - Automatically decide whether to flatten the pipeline: `#pragma ACCEL PIPELINE auto{flatten}`\n  - Keep the loop un-tiled: `#pragma ACCEL TILE FACTOR=auto{1}`\n  - Keep the loop iterations running sequentially: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n- For the reversed column loop:\n  - Set the parallel factor to its default value: `#pragma ACCEL PARALLEL FACTOR=auto{1}`\n\n- In the row sweep loop section:\n  - Let HLS tool automatically decide the pipeline strategy: `#pragma ACCEL PIPELINE auto{}`\n  - Keep the loop un-tiled: `#pragma ACCEL TILE FACTOR=auto{1}`\n  - Explicitly set the parallel factor to 2 for the first half of the loop iterations: `#pragma ACCEL PARALLEL FACTOR=auto{2}`\n\n- For the reversed row loop, set the parallel factor back to its default value: `#pragma ACCEL PARALLEL FACTOR=auto{1}`", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as a hardware acceleration kernel, do not automatically pipeline the outer time loop nor the Row Sweep loop, but flatten the pipeline for the inner Column Sweep loop for potentially better parallelism. Disallow tiling and parallel execution optimization within the loops by setting their factors to 1, except for the parallelization inside the Row Sweep loop, where a parallel factor of 29 is permitted for the outer loop and a factor of 2 for the reversed inner loop to introduce some degree of parallel computation.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Create a hardware-accelerated function named `kernel_adi` that iterates over a 2D space using two sweeps (column and row), where each sweep updates a pair of matrices (`u` and `v`) using values computed via a tridiagonal solver algorithm with fixed parameters and boundary conditions. Specifically, ensure the top-level loop is not automatically pipelined, remains unchunked, and its iterations run sequentially. In the first sweep, flatten the pipeline for loop `i`, leave it with its default chunk size, and run iterations sequentially for all its nested loops. For the second sweep, do not flatten the pipeline for loop `i`, ensure it runs with a parallelization factor of 29, and use a parallelization factor of 1 and 16 for its nested inner loops, respectively.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{29}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{16}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Instruct the HLS tool to treat the `kernel_2mm` function as a hardware acceleration kernel without applying automatic pipelining to the outermost loops. Inside the first loop block, automatic pipelining is enabled for the loop over `j`, but no loop tiling is used. The innermost loop over `k`, which computes the product of `alpha * A[i][k] * B[k][j]` and stores it in `tmp[i][j]`, is set to be processed in parallel with a specified number of 70 iterations to be performed in parallel with a reduction applied to `tmp`. In the second loop block, automatic pipelining is turned off for the loop over `j`, and parallel execution of the loop over `k`, which calculates `D[i][j] += tmp[i][k] * C[k][j]`, is set to a factor of 50 with a reduction applied to `D`. No tiling is applied to any of the loops inside this block, so each loop is processed in its entirety in its original form. Here are the corresponding natural language instructions for the HLS pragmas:\n\n1. Treat the `kernel_2mm` function as a hardware kernel for acceleration.\n2. Do not automatically pipeline the first and second main loops.\n3. Automatically pipeline the inner loops of the two main loops, except for the second loop in the second main loop (set pipeline to \"off\").\n4. Do not break any of the loops into smaller chunks (use a tile factor of 1).\n5. For the first reduction loop, perform 70 iterations in parallel with a reduction applied to `tmp`.\n6. For the second reduction loop, perform 50 iterations in parallel with a reduction applied to `D`.\n7. Run the other iterations one after the other, not in parallel, using a parallel factor of 1 or 2 according to the context in which it is used.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate the HLS code for a double matrix multiply and add kernel function `kernel_2mm` with dimensions `ni=40`, `nj=50`, `nk=70`, and `nl=80`, using variables `alpha` and `beta` as scalar multipliers. Declare `tmp`, `A`, `B`, `C`, and `D` as two-dimensional arrays with corresponding sizes. The kernel will calculate `D = alpha * A * B * C + beta * D`.\n\nConfigure the HLS pragmas in the following ways:\n\n1. Define the entire function as a hardware kernel for acceleration.\n2. For the outermost loops that iterate over the first dimension of `tmp` and `D`, disable automatic pipelining, maintain the loops' original structure without tiling, and run iterations sequentially (not in parallel).\n3. For the first inner loop that initializes `tmp`, disable automatic pipelining, maintain the loop's original structure without tiling, but allow for two iterations to run in parallel.\n4. Within the nested loop calculating `tmp` values, enable parallel computation and reduction on `tmp` variable with as many parallel accumulations as the loop range of `k`.\n5. For the second inner loop updating `D`, again disable automatic pipelining, maintain the loop's original structure without tiling, but permit running two iterations in parallel.\n6. Lastly, for the nested loop updating `D` values, enable parallel computation and reduction on `D` with as many parallel accumulations as the loop range of `k`.\n\nPlease note that the pragmas setting factors to 'auto{1}' or 'auto{off}' suggest the usage of the automatic behavior for a setting of '1' (which implies no change from the sequential behavior) or 'off' (which disables the feature), respectively. This provides a pattern to represent the intents without specifying exact values for the pragmas. Also, where 'auto' is used without braces, it implies letting the tool decide the optimization automatically.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{50}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the primary function as the targeted hardware acceleration kernel. For the main iterative time-stepping loop, prevent loop pipelining, maintain the loop structure without tiling, and do not parallelize the iterations. Within this loop, for the column sweep section, enforce flat pipelining, disable automatic tiling and parallel loop execution. In the standard loop that reverses the iteration order, maintain loop iterations in series without parallelization. For the row sweep, again, prevent pipelining and tiling, but this time allow two parallel threads for the first inner loop and parallelize fully the second inner loop with all available iterations, then revert to two parallel threads for the reversed loop iteration.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{58}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. For the first level of nested loops, disable automatic pipelining, maintain the original loop structure without tiling, and do not parallelize the outer loop. However, within the second loop, parallelize the innermost loop 'k' using a factor that allows eight simultaneous iterations and apply reduction to the 'tmp' array.\n\nIn the subsequent level of nested loops that update matrix 'D', allow the compiler to decide on pipelining but retain the loop structure without tiling and run the outer loop without parallelization. For the second loop, disable automatic pipelining, use the default loop structure, and parallelize the innermost loop 'k' with a factor that supports thirty-two concurrent iterations with reduction applied to matrix 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{8}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{32}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_adi' as the target for hardware acceleration. Within this function, ensure that the primary time-stepping loop, as well as the inner column and row sweep loops, execute sequentially without pipelining, tiling, or parallelization by disabling these optimizations. Within the column sweep's innermost loop, parallelize with eight processing elements, and in the backward iteration of the column sweep, use four processing elements. For the row sweep, flatten its first pipeline pragma and keep the innermost loop sequential. Set specific #pragma directives to realize these optimizations as follows:\n\n- Declare 'kernel_adi' as an accelerable hardware kernel.\n- Disable automatic pipelining, tiling, and parallelization for the time-stepping loop and the initial setup of the column and row sweep loops.\n- For the inner loop of the column sweep that computes 'p' and 'q', use eight parallel processing elements.\n- For backwards traversing of the column sweep loop j, employ four parallel processing elements.\n- Flatten the pipelining for the row sweep's outermost loop.\n- Keep the parallelization factor as one (no parallelization) for the row sweep's innermost loop.\n\nRefer to the provided code and incorporate the instructions using #pragma ACCEL directives with the appropriate factor values or keywords such as 'off', 'auto', and 'flatten'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Use the following natural language instructions to generate the provided HLS C code with specific #pragma directives:\n\n1. Treat the entire 'kernel_2mm' function as a hardware acceleration kernel.\n2. For the outermost loops 'i' and 'i' within the function, prohibit automatic pipelining, preserve their original structure without tiling, and execute their iterations in sequence rather than in parallel.\n3. Inside the first loop 'i', apply automatic pipelining and loop tiling with a factor of 1 to the loop 'j', but enforce two iterations of loop 'j' to execute in parallel.\n4. Enable partial parallel execution with a reduction on 'tmp' for the innermost loop 'k' of the first loop nest, and automatically determine the parallelism level, here represented by the factor of 16.\n5. In the second loop 'i', automatically pipeline the nested structure, enforce full loop tiling, and execute twenty iterations of loop 'j' in parallel.\n6. Keep loop 'k' of the second loop nest without parallel execution by setting the parallel factor to 1, while allowing reductions on 'D'.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{16}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{20}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Generate HLS C code for an ADI kernel function with the following directives:\n\n1. Mark the function as a hardware acceleration kernel.\n2. Disable automatic pipelining for all the loops inside the function.\n3. Maintain the original loop structure without tiling or breaking into smaller chunks.\n4. For the innermost loop that updates `q[i][j]` during the column sweep, allow up to 32 iterations to be processed in parallel.\n5. For the innermost loop reversing `v[_in_j_0][i]` in the column sweep, process iterations sequentially.\n6. For the innermost loop updating `q[i][j]` during the row sweep, allow up to 8 iterations to be processed in parallel.\n7. For the innermost loop reversing `u[_in_j][_in_j]` in the row sweep, allow up to 4 iterations to be processed in parallel.\n\nEnsure that you use the appropriate #pragma values: for the directive to disable automatic pipelining and maintain the original loop structure without tiling, use \"off\" and \"1\" respectively. For the parallel loop factors, provide specific values as mentioned, i.e., 32 for the `q[i][j]` update in the column sweep, 1 for the reversing `v[_in_j_0][i]`, 8 for the `q[i][j]` update in the row sweep, and 4 for the reversing `u[_in_j][_in_j]`.", "output": "#pragma ACCEL kernel\n\nvoid kernel_adi(int tsteps,int n,double u[60][60],double v[60][60],double p[60][60],double q[60][60])\n{\n  int t;\n  int i;\n  int j;\n  double DX;\n  double DY;\n  double DT;\n  double B1;\n  double B2;\n  double mul1;\n  double mul2;\n  double a;\n  double b;\n  double c;\n  double d;\n  double e;\n  double f;\n//#pragma scop\n  DX = 1.0 / ((double )60);\n  DY = 1.0 / ((double )60);\n  DT = 1.0 / ((double )40);\n  B1 = 2.0;\n  B2 = 1.0;\n  mul1 = B1 * DT / (DX * DX);\n  mul2 = B2 * DT / (DY * DY);\n  a = -mul1 / 2.0;\n  b = 1.0 + mul1;\n  c = a;\n  d = -mul2 / 2.0;\n  e = 1.0 + mul2;\n  f = d;\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (t = 1; t <= 40; t++) {\n//Column Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      v[0][i] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = v[0][i];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{32}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -c / (a * p[i][j - 1] + b);\n        q[i][j] = (-d * u[j][i - 1] + (1.0 + 2.0 * d) * u[j][i] - f * u[j][i + 1] - a * q[i][j - 1]) / (a * p[i][j - 1] + b);\n      }\n      v[60 - 1][i] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n      for (j = 0; j <= 57; j++) {\n        int _in_j_0 = 58 + -1 * j;\n        v[_in_j_0][i] = p[i][_in_j_0] * v[_in_j_0 + 1][i] + q[i][_in_j_0];\n      }\n      j = 1 + -1;\n    }\n//Row Sweep\n    \n#pragma ACCEL PIPELINE auto{off}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n    for (i = 1; i < 59; i++) {\n      u[i][0] = 1.0;\n      p[i][0] = 0.0;\n      q[i][0] = u[i][0];\n      \n#pragma ACCEL PARALLEL FACTOR=auto{8}\n      for (j = 1; j < 59; j++) {\n        p[i][j] = -f / (d * p[i][j - 1] + e);\n        q[i][j] = (-a * v[i - 1][j] + (1.0 + 2.0 * a) * v[i][j] - c * v[i + 1][j] - d * q[i][j - 1]) / (d * p[i][j - 1] + e);\n      }\n      u[i][60 - 1] = 1.0;\n/* Standardize from: for(j = 60 - 2;j >= 1;j--) {...} */\n      \n#pragma ACCEL PARALLEL FACTOR=auto{4}\n      for (j = 0; j <= 57; j++) {\n        int _in_j = 58 + -1 * j;\n        u[i][_in_j] = p[i][_in_j] * u[i][_in_j + 1] + q[i][_in_j];\n      }\n      j = 1 + -1;\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "Designate the function 'kernel_2mm' as a hardware acceleration kernel. In the first two-dimensional loop computing 'tmp', disable automatic pipelining for the outer loop and retain it as a single, complete iteration without subdividing into smaller parallel executions or datapaths. Inside the nested loop, enable pipelining with the default settings, keep loop iterations whole without tiling, but force the execution of two iterations of 'j' in parallel. When accumulating 'tmp', allow a parallel reduction across all iterations of 'k' with a factor that permits the execution of all iterations in parallel, up to the total number of iterations available, capped at 70.\n\nFor the second two-dimensional loop updating 'D', similarly, turn off automatic pipelining for the outer loop and maintain each iteration in its full form. Inside the nested loop, flatten the operations inside to yield a single, optimized datapath, keep iterations whole without tiling, and execute two iterations in parallel for 'j'. The reduction updating 'D' should allow for sequential execution across iterations of 'k' with a parallel factor that ensures one operation at a time.", "output": "#pragma ACCEL kernel\n\nvoid kernel_2mm(int ni,int nj,int nk,int nl,double alpha,double beta,double tmp[40][50],double A[40][70],double B[70][50],double C[50][80],double D[40][80])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* D := alpha*A*B*C + beta*D */\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 50; j++) {\n      tmp[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL reduction = tmp FACTOR=auto{70}\n      for (k = 0; k < 70; ++k) {\n        tmp[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{off}\n  \n#pragma ACCEL TILE FACTOR=auto{1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{1}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{flatten}\n    \n#pragma ACCEL TILE FACTOR=auto{1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{2}\n    for (j = 0; j < 80; j++) {\n      D[i][j] *= beta;\n      \n#pragma ACCEL PARALLEL reduction = D FACTOR=auto{1}\n      for (k = 0; k < 50; ++k) {\n        D[i][j] += tmp[i][k] * C[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
