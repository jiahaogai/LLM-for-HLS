{"instruction": "Generate HLS code with the following instructions:", "input": "The provided HLS code defines a function `needwun` that carries out sequence alignment. The sequences for alignment are passed as parameters SEQA and SEQB. The sequence alignments are stored in alignedA and alignedB. The function is defined as a hardware kernel for acceleration using the `#pragma ACCEL kernel` directive.\n\nThis function consists of several sections. The first two sections initialize the first row and column of array M with negative values, essentially representing a cost for initiating the alignment. These sections use `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}` directive respectively, indicating that loop iterations are processed in parallel in a manner optimized by the system.\n\nThe next main section uses a nested loop to fill the rest of the costs in array M. It uses `#pragma ACCEL PIPELINE auto{__PIPE__L2}`, `#pragma ACCEL TILE FACTOR=auto{__TILE__L2}`, and `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}` directives which specify that the loop iterations are pipelined, broken into smaller chunks, and processed in parallel at a rate optimized by the system. \n\nThe inner loop of this section also uses the `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}` directive, indicating that loop iterations are again processed in parallel in a manner optimized by the system.\n\nPlease note, the last commented-out section in this code indicates the traceback operation, a crucial step in sequence alignment, which will use the matrix M and array ptr for generating the aligned sequences.\n", "output": "#pragma ACCEL kernel\n\nvoid needwun(char SEQA[128],char SEQB[128],char alignedA[256],char alignedB[256],int M[16641],char ptr[16641])\n{\n  int score;\n  int up_left;\n  int up;\n  int left;\n  int max;\n  int row;\n  int row_up;\n  int r;\n  int a_idx;\n  int b_idx;\n  int a_str_idx;\n  int b_str_idx;\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  init_row:\n  for (a_idx = 0; a_idx < 128 + 1; a_idx++) {\n    M[a_idx] = a_idx * - 1;\n  }\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  init_col:\n  for (b_idx = 0; b_idx < 128 + 1; b_idx++) {\n    M[b_idx * (128 + 1)] = b_idx * - 1;\n  }\n// Matrix filling loop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  fill_out:\n  for (b_idx = 1; b_idx < 128 + 1; b_idx++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    fill_in:\n    for (a_idx = 1; a_idx < 128 + 1; a_idx++) {\n      if (((int )SEQA[a_idx - 1]) == ((int )SEQB[b_idx - 1])) {\n        score = 1;\n      }\n       else {\n        score = - 1;\n      }\n      row_up = (b_idx - 1) * (128 + 1);\n      row = b_idx * (128 + 1);\n      up_left = M[row_up + (a_idx - 1)] + score;\n      up = M[row_up + a_idx] + - 1;\n      left = M[row + (a_idx - 1)] + - 1;\n      max = (up_left > ((up > left?up : left))?up_left : ((up > left?up : left)));\n      M[row + a_idx] = max;\n      if (max == left) {\n        ptr[row + a_idx] = ((char )60);\n      }\n       else {\n        if (max == up) {\n          ptr[row + a_idx] = ((char )94);\n        }\n         else {\n          ptr[row + a_idx] = ((char )92);\n        }\n      }\n    }\n  }\n// TraceBack (n.b. aligned sequences are backwards to avoid string appending)\n  a_idx = 128;\n  b_idx = 128;\n  a_str_idx = 0;\n  b_str_idx = 0;\n/*\n    trace: while(a_idx>0 || b_idx>0) {\n        r = b_idx*(ALEN+1);\n        if (ptr[r + a_idx] == ALIGN){\n            alignedA[a_str_idx++] = SEQA[a_idx-1];\n            alignedB[b_str_idx++] = SEQB[b_idx-1];\n            a_idx--;\n            b_idx--;\n        }\n        else if (ptr[r + a_idx] == SKIPB){\n            alignedA[a_str_idx++] = SEQA[a_idx-1];\n            alignedB[b_str_idx++] = '-';\n            a_idx--;\n        }\n        else{ // SKIPA\n            alignedA[a_str_idx++] = '-';\n            alignedB[b_str_idx++] = SEQB[b_idx-1];\n            b_idx--;\n        }\n    }\n    // Pad the result\n    pad_a: for( ; a_str_idx<ALEN+BLEN; a_str_idx++ ) {\n      alignedA[a_str_idx] = '_';\n    }\n    pad_b: for( ; b_str_idx<ALEN+BLEN; b_str_idx++ ) {\n      alignedB[b_str_idx] = '_';\n    }\n*/\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This code begins by declaring a kernel function named \"kernel_gemver\" with input parameters. Afterward, it specifies that the subsequent module would be treated as a hardware kernel for acceleration. \n\nIn the first loop, it uses automatic pipelining, automatic parallelizing and automatic unroll-and-jamming by specifying 'auto' as the argument to the respective pragmas. This enables the compiler to determine the optimal factors based on the particular targeted hardware and design. Parallel reduction of array 'A' is also performed inside the inner loop where 'A[i][j]' is updated.\n\nThe second loop repeats the same automatic operations as the first for a different set of operations on array 'x' and 'A'. In addition to these, an extra loop is also specified where each element of 'x' is incremented by corresponding elements of 'z'. This loop also utilizes automatic parallelization.\n\nIn the last loop, automatic pipelining, automatic unroll-and-jamming, and parallel reduction are again utilized, this time to update array 'w'. The scope of the pragmas end after this loop.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemver(int n,double alpha,double beta,double A[120][120],double u1[120],double v1[120],double u2[120],double v2[120],double w[120],double x[120],double y[120],double z[120])\n{\n  int i;\n  int j;\n  \n#pragma scop\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 120; i++) {\n    \n#pragma ACCEL PARALLEL reduction=A FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 120; j++) {\n      A[i][j] += u1[i] * v1[j] + u2[i] * v2[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 120; i++) {\n    \n#pragma ACCEL PARALLEL reduction=x FACTOR=auto{__PARA__L5}\n    for (j = 0; j < 120; j++) {\n      x[i] += beta * A[j][i] * y[j];\n    }\n  }\n  \n#pragma ACCEL PARALLEL reduction=x FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 120; i++) {\n    x[i] +=  z[i];\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n  for (i = 0; i < 120; i++) {\n    \n#pragma ACCEL PARALLEL reduction=w FACTOR=auto{__PARA__L6}\n    for (j = 0; j < 120; j++) {\n      w[i] += alpha * A[i][j] * x[j];\n    }\n  }\n  \n#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given code represents a kernel that performs a triangular matrix multiplication operation (abbreviated as TRMM in linear algebra routines). The function `kernel_trmm` is treated as a hardware kernel for acceleration as indicated by '#pragma ACCEL kernel'.\n\nThis kernel receives three parameters: alpha which is a scalar, and matrices A and B. The matrix multiplication operation is performed according to the rules specified in the code comments. The matrix A is a square matrix, and matrix B has more columns than rows.\n\nThe outermost for-loop iterates over the rows of the matrices, the second for-loop iterates over the columns of matrix B. An innermost loop performs computations on the elements of the matrices, adding up the products of corresponding elements in A and B if the condition \"(k > i)\" is met. \n\nThe loop optimizations are handled by using HLS pragmas. For each loop, the compiler is set to automatically decide the pipeline stage balancing '#pragma ACCEL PIPELINE auto', the loop tiling factor '#pragma ACCEL TILE FACTOR=auto', and the factor for loop unrolling or how many iterations should be executed in parallel '#pragma ACCEL PARALLEL FACTOR=auto'.\n\nOn the innermost loop, there is also a reduction clause along with ACCEL PARALLEL pragma, implying that the 'sum' variable is meant to accumulate (or reduce) the values processed within the loop in a parallelizable fashion. The reduction computes the sum of these products. The final assignment multiplies the sum by the scalar alpha and assigns it to the element B[i][j].", "output": "#pragma ACCEL kernel\n\nvoid kernel_trmm(double alpha,double A[60][60],double B[60][80])\n{\n//BLAS parameters\n//SIDE   = 'L'\n//UPLO   = 'L'\n//TRANSA = 'T'\n//DIAG   = 'U'\n// => Form  B := alpha*A**T*B.\n// A is MxM\n// B is MxN\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (int i = 0; i < 60; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (int j = 0; j < 80; j++) {\n      double sum = B[i][j];\n#pragma ACCEL PARALLEL reduction=sum FACTOR=auto{__PARA__L2}\n      for (int k = 0; k < 60; k++) {\n        if (k > i) {\n          sum += A[k][i] * B[k][j];\n        }\n      }\n      B[i][j] = alpha * sum;\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "The given HLS C code defines a function named 'spmv' that takes in 5 arguments - an array of double type 'val[1666]', two integer arrays 'cols[1666]', 'rowDelimiters[495]', and two double arrays 'vec[494]' and 'out[494]'. The function is primarily used to perform Sparse Matrix-Vector Multiplication. \n\nThe '#pragma ACCEL kernel' directive means the function will be treated as a hardware kernel for acceleration.\n\nInside the function, a double variable 'sum' and 'Si' are declared along with two integer variables 'i' and 'j'. Two nested for-loops are present where 'i' varies from 0 to 493 (inclusive) and 'j' varies between the values at index 'i' and 'i+1' of 'rowDelimiters' array.\n\nIn the inner loop, the product of values at index 'j' of 'val' and at index 'cols[j]' of 'vec' is calculated and stored in 'Si'. This value 'Si' is added to the 'sum'. Finally, 'sum' is assigned to the 'out[i]'.\n\nThe '#pragma ACCEL PIPELINE auto{__PIPE__L0}', '#pragma ACCEL TILE FACTOR=auto{__TILE__L0}', and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' directives tell the compiler to automatically determine the pipeline depth, tile factor, and parallel factor respectively for optimized acceleration of the 'spmv' function.", "output": "#pragma ACCEL kernel\n\nvoid spmv(double val[1666],int cols[1666],int rowDelimiters[495],double vec[494],double out[494])\n{\n  int i;\n  int j;\n  double sum;\n  double Si;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  spmv_1:\n  for (i = 0; i < 494; i++) {\n    sum = ((double )0);\n    Si = ((double )0);\n    int tmp_begin = rowDelimiters[i];\n    int tmp_end = rowDelimiters[i + 1];\n    spmv_2:\n    for (j = tmp_begin; j < tmp_end; j++) {\n      Si = val[j] * vec[cols[j]];\n      sum = sum + Si;\n    }\n    out[i] = sum;\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C code contains a function, named \"stencil\", that performs a stencil operation on the provided \"orig\" array using the provided \"filter\". The output is stored in the \"sol\" array. The operation involves two outer loops, indexed by \"r\" and \"c\", which iterate over the rows and columns of the \"orig\" array, with each iteration calculating a value based on a smaller 3-by-3 patch of the \"orig\" array, multiplied by corresponding values in the filter, and summing those results. This sum is then stored in the corresponding position in the \"sol\" array. \n\nThe pragmas that precede each loop are optimization hints for a High-Level Synthesis (HLS) compiler:\n- `#pragma ACCEL kernel` tells the compiler to treat the stencil function as an acceleratable hardware module, which could improve the overall performance of the function. \n- `#pragma ACCEL PIPELINE auto{__PIPE__LX}` instructs the compiler to automatically determine the optimal pipelining strategy for each loop in the form of achieving a balance between throughput and resource use. The \"auto\" keyword means the compiler determines an appropriate value. \n- `#pragma ACCEL TILE FACTOR=auto{__TILE__LX}` suggests the HLS compiler to determine an appropriate size of data chunks (tiles) each loop should work with at a time. This can help to ensure that the data used in each loop iteration fits into on-chip storage as efficiently as possible.\n- `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__LX}` directs the HLS compiler to determine the optimal number of these loop iterations that can be executed in parallel. It can often help to speed up the execution of loop iterations, although it can increase resource use. The \"auto\" keyword same as before, lets the HLS compiler decide the best value.", "output": "#pragma ACCEL kernel\n\nvoid stencil(int orig[8192],int sol[8192],int filter[9])\n{\n  int r;\n  int c;\n  int k1;\n  int k2;\n  int temp;\n  int mul;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  stencil_label1:\n  for (r = 0; r < 128 - 2; r++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    stencil_label2:\n    for (c = 0; c < 64 - 2; c++) {\n      temp = ((int )0);\n      \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n      stencil_label3:\n      for (k1 = 0; k1 < 3; k1++) {\n        stencil_label4:\n        for (k2 = 0; k2 < 3; k2++) {\n          mul = filter[k1 * 3 + k2] * orig[(r + k1) * 64 + c + k2];\n          temp += mul;\n        }\n      }\n      sol[r * 64 + c] = temp;\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This is a C code to create a 3D kernel for stencil operation with the help of HLS pragmas from the ACCEL suit. The kernel's configuration attributes are automatically determined.\n\nHere the function 'stencil3d' is declared as a hardware kernel with '#pragma ACCEL kernel'. Then, we declare four 'long' type variables: sum0, sum1, mul0, and mul1.\n\nInside the function, we have 3 nested loops traversing i, j, and ko respectively. Each one of the outer two loops, i and j, includes '#pragma ACCEL PIPELINE auto' indicating that an optimization technique should be automatically employed to overlap the execution of multiple loop iterations to maximize performance. In the similar fashion, '#pragma ACCEL TILE FACTOR=auto' allows the system to choose the best tile size for these loops automatically.\n\nThe innermost loop (ko-loop) utilizes '#pragma ACCEL PARALLEL FACTOR=auto', automatically deciding and running a portion of iterations concurrently, based on the code structure and available resources, for increased data parallelism. Within the ko-loop, various arithmetic operations are performed related to three dimensional arrays orig and sol. \n\nThe resulting solution, 'sol', is produced by the summation of products from 'orig' array elements and constants C0 and C1. After all these computations, result is stored back into 'sol' array.", "output": "#pragma ACCEL kernel\n\nvoid stencil3d(long C0,long C1,long orig[39304],long sol[32768])\n{\n  long sum0;\n  long sum1;\n  long mul0;\n  long mul1;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  for (long i = 1; i < 33; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n    for (long j = 1; j < 33; j++) {\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n      for (long ko = 0; ko <= 31; ko++) {\n        long _in_ko = 1L + 1L * ko;\n        sum0 = orig[_in_ko + (0) + (34) * (j + (34) * i)];\n        sum1 = orig[_in_ko + (0) + (34) * (j + (34) * (i + (1)))] + orig[_in_ko + (0) + (34) * (j + (34) * (i - (1)))] + orig[_in_ko + (0) + (34) * (j + (1) + (34) * i)] + orig[_in_ko + (0) + (34) * (j - (1) + (34) * i)] + orig[_in_ko + (0) + (1) + (34) * (j + (34) * i)] + orig[_in_ko + (0) - (1) + (34) * (j + (34) * i)];\n        mul0 = sum0 * C0;\n        mul1 = sum1 * C1;\n        sol[_in_ko + (0) + (34) * (j + (34) * i)] = mul0 + mul1;\n      }\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C function named \"kernel_3mm\" with input parameters \"ni\", \"nj\", \"nk\", \"nl\" and \"nm\" and arrays \"E\", \"A\", \"B\", \"F\", \"C\", \"D\", and \"G\" is marked as a hardware acceleration kernel using '#pragma ACCEL kernel'. \n\nThe function performs three matrix multiplications: \n1.  The product of matrices \"A\" and \"B\" is accumulated in the \"E\" array.\n2.  The product of matrices \"C\" and \"D\" is accumulated in the \"F\" array.\n3.  Finally, the product of the matrices \"E\" and \"F\" is held in the \"G\" array.\n\nFor each series of nested loops, the HLS compiler is given instructions about how to handle loop transformations:\n-   '#pragma ACCEL PIPELINE' is where the auto pipeline is directed, leading the HLS compiler to determine the pipeline initiation interval.\n-   Similarly, with '#pragma ACCEL TILE FACTOR', the HLS compiler is directed to determine the best size for tiling to improve data locality.\n-   The '#pragma ACCEL PARALLEL FACTOR' is also directed to auto, advising the HLS compiler to determine the appropriate number of loop iterations to be executed in parallel.\n\nAll these pragma directives help increase the parallelism and performance of the code when implemented on hardware such as a Field Programmable Gate Array (FPGA).", "output": "#pragma ACCEL kernel\n\nvoid kernel_3mm(int ni,int nj,int nk,int nl,int nm,double E[40][50],double A[40][60],double B[60][50],double F[50][70],double C[50][80],double D[80][70],double G[40][70])\n{\n  int i;\n  int j;\n  int k;\n//#pragma scop\n/* E := A*B */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L3}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L3}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 50; j++) {\n      E[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L6}\n      for (k = 0; k < 60; ++k) {\n        E[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n/* F := C*D */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 50; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L4}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L4}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L4}\n    for (j = 0; j < 70; j++) {\n      F[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L7}\n      for (k = 0; k < 80; ++k) {\n        F[i][j] += C[i][k] * D[k][j];\n      }\n    }\n  }\n/* G := E*F */\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n  for (i = 0; i < 40; i++) {\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L5}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L5}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L5}\n    for (j = 0; j < 70; j++) {\n      G[i][j] = 0.0;\n      \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L8}\n      for (k = 0; k < 50; ++k) {\n        G[i][j] += E[i][k] * F[k][j];\n      }\n    }\n  }\n//#pragma endscop\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This HLS code snippet defines a hardware-accelerated function named `kernel_mvt`, which operates on the input arrays `x1`, `x2`, `y_1`, `y_2 `, and `A`. The function consists of two main loops: both loops iterate over the elements of `x1` and `x2` and multiply and add values from `A` and `y_1`, `y_2` to them.\n\nFor optimization, there are several pragma directives:\n\n1. `#pragma ACCEL kernel`: The HLS tool treats this function as a hardware kernel for acceleration purposes.\n2. `#pragma ACCEL PIPELINE auto{__PIPE__L0}`: It optimizes the first main loop with automatic pipelining to overlap loop iterations for enhanced parallelism. \n3. `#pragma ACCEL TILE FACTOR=auto{__TILE__L0}`: This pragma further optimizes the first loop by tiling, dividing the data to handle a smaller piece at a time.\n4. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}`: This sets up the first loop for automatic parallelism. Each iteration runs independently.\n5. `#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}`: The nested loop is optimized for parallel execution, where the operation on `x1[i]` is a reduction operation, minimizing data dependencies.\n6. `#pragma ACCEL PIPELINE auto{__PIPE__L1}`: The second main loop is optimized with automatic pipelining.\n7. `#pragma ACCEL TILE FACTOR=auto{__TILE__L1}`: The second main loop is also optimized by automatic tiling.\n8. `#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}`: The second loop execution is set for parallel execution.\n9. `#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}`: The second nested loop is optimized with reduction operation on `x2[i]`.\n\nIn all pragma directives, `auto` indicates that the compiler automatically decides the optimum value for each factor.", "output": "\n#pragma ACCEL kernel\n\nvoid kernel_mvt(double x1[400],double x2[400],double y_1[400],double y_2[400],double A[400][400])\n{\n  int i;\n  int j;\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x1 FACTOR=auto{__PARA__L2}\n    for (j = 0; j < 400; j++) {\n      x1[i] += A[i][j] * y_1[j];\n    }\n  }\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L1}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n  for (i = 0; i < 400; i++) {\n    \n#pragma ACCEL PARALLEL reduction = x2 FACTOR=auto{__PARA__L3}\n    for (j = 0; j < 400; j++) {\n      x2[i] += A[j][i] * y_2[j];\n    }\n  }\n}\n"}
{"instruction": "Generate HLS code with the following instructions:", "input": "This C code defines a function named 'kernel_gemm' that's designated as a hardware acceleration kernel. The function performs a generalized matrix multiplication (GEMM). The operation performed is C = alpha*A*B + beta*C where A, B and C are matrices, alpha and beta are scalars and A is ni by nk in size, B is nk by nj in size, and C is  ni by nj in size. \n\nThe '#pragma ACCEL PIPELINE auto{__PIPE__L0}' directive instructs the compiler to automatically determine the pipelining factor for the outer loop. \n\nThe '#pragma ACCEL TILE FACTOR=auto{__TILE__L0}' directive requests the compiler to automatically decide the tile size for the outer loop. \n\nThe '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}' pragma tells the compiler to figure out automatically the degree of parallelism for the outer loop.\n\nThe 'for(j)' loop inside the 'for(i)' loop multiplying the matrix C by scalar beta has pragma '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}' requesting the compiler to automatically determine parallelism level for this loop. \n\nSimilarly the outer k-loop and inner j-loop has '#pragma ACCEL PIPELINE auto{__PIPE__L2}', '#pragma ACCEL TILE FACTOR=auto{__TILE__L2}', and '#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}' pragmas requesting compiler to determine pipelining factors, tile sizes, and parallelism automatically, respectively. \n\nThen, there is the '#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}', which asks the compiler to perform reduction on C and determine the parallelism automatically. This inner loop computes the generalized matrix multiplication. \n\nThe alpha scalar multiplies the elements from matrix A and B, the result is then added to the corresponding element in matrix C. The pragma directives included where necessary are directives to optimise the loop and execution order, using techniques such as pipelining, tiling, and parallel execution.", "output": "#pragma ACCEL kernel\n\nvoid kernel_gemm(int ni,int nj,int nk,double alpha,double beta,double C[200][220],double A[200][240],double B[240][220])\n{\n  int i;\n  int j;\n  int k;\n//BLAS PARAMS\n//TRANSA = 'N'\n//TRANSB = 'N'\n// => Form C := alpha*A*B + beta*C,\n//A is NIxNK\n//B is NKxNJ\n//C is NIxNJ\n  \n#pragma ACCEL PIPELINE auto{__PIPE__L0}\n  \n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\n  \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\n  for (i = 0; i < 200; i++) {\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\n    for (j = 0; j < 220; j++) {\n      C[i][j] *= beta;\n    }\n    \n#pragma ACCEL PIPELINE auto{__PIPE__L2}\n    \n#pragma ACCEL TILE FACTOR=auto{__TILE__L2}\n    \n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\n    for (k = 0; k < 240; k++) {\n      \n#pragma ACCEL PARALLEL reduction=C FACTOR=auto{__PARA__L3}\n      for (j = 0; j < 220; j++) {\n        C[i][j] += alpha * A[i][k] * B[k][j];\n      }\n    }\n  }\n}\n"}
